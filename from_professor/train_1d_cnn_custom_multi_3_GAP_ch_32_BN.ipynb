{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 96)           384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1552        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 96)           384         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1552        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 128)          512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2064        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 32)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 160)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 160)          640         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2576        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 192)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 192)          768         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           3088        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 192)          0           global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 192)          768         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           3088        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1777 - acc: 0.3405\n",
      "Epoch 00001: val_loss improved from inf to 2.19247, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/001-2.1925.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 2.1777 - acc: 0.3405 - val_loss: 2.1925 - val_acc: 0.3229\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7293 - acc: 0.4791\n",
      "Epoch 00002: val_loss improved from 2.19247 to 1.72190, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/002-1.7219.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 1.7294 - acc: 0.4791 - val_loss: 1.7219 - val_acc: 0.4854\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5429 - acc: 0.5438\n",
      "Epoch 00003: val_loss did not improve from 1.72190\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.5429 - acc: 0.5437 - val_loss: 1.9126 - val_acc: 0.3783\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4273 - acc: 0.5827\n",
      "Epoch 00004: val_loss improved from 1.72190 to 1.53412, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/004-1.5341.hdf5\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.4273 - acc: 0.5826 - val_loss: 1.5341 - val_acc: 0.5309\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.6090\n",
      "Epoch 00005: val_loss improved from 1.53412 to 1.36645, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/005-1.3665.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.3432 - acc: 0.6090 - val_loss: 1.3665 - val_acc: 0.6003\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2783 - acc: 0.6283\n",
      "Epoch 00006: val_loss did not improve from 1.36645\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.2783 - acc: 0.6285 - val_loss: 1.9749 - val_acc: 0.3899\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2288 - acc: 0.6418\n",
      "Epoch 00007: val_loss did not improve from 1.36645\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.2288 - acc: 0.6418 - val_loss: 1.6378 - val_acc: 0.4666\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1866 - acc: 0.6522\n",
      "Epoch 00008: val_loss did not improve from 1.36645\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 1.1867 - acc: 0.6522 - val_loss: 1.4208 - val_acc: 0.5409\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1503 - acc: 0.6633\n",
      "Epoch 00009: val_loss did not improve from 1.36645\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.1503 - acc: 0.6633 - val_loss: 1.3887 - val_acc: 0.5618\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1236 - acc: 0.6699\n",
      "Epoch 00010: val_loss improved from 1.36645 to 1.32120, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/010-1.3212.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.1238 - acc: 0.6700 - val_loss: 1.3212 - val_acc: 0.5758\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0994 - acc: 0.6760\n",
      "Epoch 00011: val_loss did not improve from 1.32120\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.0992 - acc: 0.6761 - val_loss: 1.7334 - val_acc: 0.4661\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0793 - acc: 0.6818\n",
      "Epoch 00012: val_loss improved from 1.32120 to 1.31656, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/012-1.3166.hdf5\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0792 - acc: 0.6817 - val_loss: 1.3166 - val_acc: 0.5989\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0560 - acc: 0.6900\n",
      "Epoch 00013: val_loss did not improve from 1.31656\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0561 - acc: 0.6900 - val_loss: 1.7424 - val_acc: 0.4563\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0421 - acc: 0.6916\n",
      "Epoch 00014: val_loss improved from 1.31656 to 1.16387, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/014-1.1639.hdf5\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 1.0424 - acc: 0.6915 - val_loss: 1.1639 - val_acc: 0.6410\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0267 - acc: 0.6977\n",
      "Epoch 00015: val_loss did not improve from 1.16387\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0273 - acc: 0.6976 - val_loss: 1.7081 - val_acc: 0.4628\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0079 - acc: 0.7040\n",
      "Epoch 00016: val_loss did not improve from 1.16387\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 1.0080 - acc: 0.7039 - val_loss: 1.3729 - val_acc: 0.5782\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9994 - acc: 0.7031\n",
      "Epoch 00017: val_loss did not improve from 1.16387\n",
      "36805/36805 [==============================] - 28s 756us/sample - loss: 0.9997 - acc: 0.7031 - val_loss: 1.6376 - val_acc: 0.5036\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.7095\n",
      "Epoch 00018: val_loss improved from 1.16387 to 1.11357, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/018-1.1136.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9817 - acc: 0.7095 - val_loss: 1.1136 - val_acc: 0.6692\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9740 - acc: 0.7114\n",
      "Epoch 00019: val_loss did not improve from 1.11357\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9740 - acc: 0.7115 - val_loss: 1.2685 - val_acc: 0.5949\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9657 - acc: 0.7130\n",
      "Epoch 00020: val_loss improved from 1.11357 to 1.06739, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/020-1.0674.hdf5\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.9663 - acc: 0.7129 - val_loss: 1.0674 - val_acc: 0.6744\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9560 - acc: 0.7180\n",
      "Epoch 00021: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9560 - acc: 0.7180 - val_loss: 1.1249 - val_acc: 0.6615\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9482 - acc: 0.7198\n",
      "Epoch 00022: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.9482 - acc: 0.7199 - val_loss: 1.6176 - val_acc: 0.4703\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.7223\n",
      "Epoch 00023: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.9354 - acc: 0.7224 - val_loss: 1.6016 - val_acc: 0.5113\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9314 - acc: 0.7254\n",
      "Epoch 00024: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.9312 - acc: 0.7254 - val_loss: 1.5160 - val_acc: 0.5141\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9217 - acc: 0.7265\n",
      "Epoch 00025: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.9216 - acc: 0.7265 - val_loss: 1.2042 - val_acc: 0.6289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.7325\n",
      "Epoch 00026: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9161 - acc: 0.7325 - val_loss: 1.3371 - val_acc: 0.5679\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9061 - acc: 0.7292\n",
      "Epoch 00027: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.9062 - acc: 0.7292 - val_loss: 1.4922 - val_acc: 0.5313\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7342\n",
      "Epoch 00028: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9013 - acc: 0.7341 - val_loss: 1.4059 - val_acc: 0.5702\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8919 - acc: 0.7344\n",
      "Epoch 00029: val_loss did not improve from 1.06739\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8918 - acc: 0.7344 - val_loss: 1.7517 - val_acc: 0.4633\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7378\n",
      "Epoch 00030: val_loss improved from 1.06739 to 0.90316, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/030-0.9032.hdf5\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8865 - acc: 0.7378 - val_loss: 0.9032 - val_acc: 0.7345\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7387\n",
      "Epoch 00031: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8816 - acc: 0.7386 - val_loss: 1.1602 - val_acc: 0.6315\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7391\n",
      "Epoch 00032: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8741 - acc: 0.7391 - val_loss: 1.1502 - val_acc: 0.6389\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8682 - acc: 0.7420\n",
      "Epoch 00033: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8682 - acc: 0.7420 - val_loss: 1.2415 - val_acc: 0.5996\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8690 - acc: 0.7427\n",
      "Epoch 00034: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8690 - acc: 0.7426 - val_loss: 0.9834 - val_acc: 0.7030\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8567 - acc: 0.7453\n",
      "Epoch 00035: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8567 - acc: 0.7453 - val_loss: 1.1141 - val_acc: 0.6313\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8550 - acc: 0.7463\n",
      "Epoch 00036: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8548 - acc: 0.7464 - val_loss: 1.4473 - val_acc: 0.5695\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.7488\n",
      "Epoch 00037: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8479 - acc: 0.7488 - val_loss: 0.9452 - val_acc: 0.7133\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8481 - acc: 0.7495\n",
      "Epoch 00038: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8486 - acc: 0.7495 - val_loss: 1.8327 - val_acc: 0.4810\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.7538\n",
      "Epoch 00039: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8378 - acc: 0.7538 - val_loss: 1.4605 - val_acc: 0.5553\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7521\n",
      "Epoch 00040: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8347 - acc: 0.7522 - val_loss: 1.2755 - val_acc: 0.5952\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7543\n",
      "Epoch 00041: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8315 - acc: 0.7542 - val_loss: 2.0142 - val_acc: 0.4899\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7566\n",
      "Epoch 00042: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8277 - acc: 0.7566 - val_loss: 1.2621 - val_acc: 0.6112\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7572\n",
      "Epoch 00043: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8194 - acc: 0.7572 - val_loss: 2.1446 - val_acc: 0.4382\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7544\n",
      "Epoch 00044: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8234 - acc: 0.7544 - val_loss: 1.2151 - val_acc: 0.6164\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.7567\n",
      "Epoch 00045: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8164 - acc: 0.7567 - val_loss: 1.1046 - val_acc: 0.6499\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8116 - acc: 0.7597\n",
      "Epoch 00046: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8119 - acc: 0.7596 - val_loss: 2.0587 - val_acc: 0.4484\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.7626\n",
      "Epoch 00047: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.8033 - acc: 0.7626 - val_loss: 2.0018 - val_acc: 0.4768\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8012 - acc: 0.7632\n",
      "Epoch 00048: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8012 - acc: 0.7632 - val_loss: 1.2699 - val_acc: 0.6075\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8017 - acc: 0.7624\n",
      "Epoch 00049: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.8012 - acc: 0.7624 - val_loss: 1.2640 - val_acc: 0.6073\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8007 - acc: 0.7625\n",
      "Epoch 00050: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.8007 - acc: 0.7625 - val_loss: 1.3854 - val_acc: 0.5809\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7649\n",
      "Epoch 00051: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7931 - acc: 0.7649 - val_loss: 1.4521 - val_acc: 0.5630\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7658\n",
      "Epoch 00052: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7938 - acc: 0.7658 - val_loss: 1.0406 - val_acc: 0.6855\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7882 - acc: 0.7664\n",
      "Epoch 00053: val_loss did not improve from 0.90316\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7881 - acc: 0.7664 - val_loss: 0.9881 - val_acc: 0.6713\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7869 - acc: 0.7658\n",
      "Epoch 00054: val_loss improved from 0.90316 to 0.88387, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/054-0.8839.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7872 - acc: 0.7657 - val_loss: 0.8839 - val_acc: 0.7370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7797 - acc: 0.7678\n",
      "Epoch 00055: val_loss did not improve from 0.88387\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7803 - acc: 0.7677 - val_loss: 1.2426 - val_acc: 0.6308\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7677\n",
      "Epoch 00056: val_loss did not improve from 0.88387\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7790 - acc: 0.7678 - val_loss: 0.9703 - val_acc: 0.7086\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.7705\n",
      "Epoch 00057: val_loss improved from 0.88387 to 0.78613, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv_checkpoint/057-0.7861.hdf5\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7768 - acc: 0.7704 - val_loss: 0.7861 - val_acc: 0.7775\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7697\n",
      "Epoch 00058: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7733 - acc: 0.7697 - val_loss: 1.9612 - val_acc: 0.4540\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7726 - acc: 0.7688\n",
      "Epoch 00059: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7726 - acc: 0.7688 - val_loss: 1.9109 - val_acc: 0.5171\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.7714\n",
      "Epoch 00060: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7678 - acc: 0.7715 - val_loss: 1.9375 - val_acc: 0.4805\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.7735\n",
      "Epoch 00061: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7650 - acc: 0.7735 - val_loss: 1.0569 - val_acc: 0.6713\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7755\n",
      "Epoch 00062: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7584 - acc: 0.7756 - val_loss: 1.5381 - val_acc: 0.5670\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.7761\n",
      "Epoch 00063: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7564 - acc: 0.7761 - val_loss: 1.1736 - val_acc: 0.6338\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7771\n",
      "Epoch 00064: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.7543 - acc: 0.7772 - val_loss: 1.0882 - val_acc: 0.6767\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7542 - acc: 0.7780\n",
      "Epoch 00065: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7542 - acc: 0.7780 - val_loss: 1.4299 - val_acc: 0.5828\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7518 - acc: 0.7783\n",
      "Epoch 00066: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7516 - acc: 0.7784 - val_loss: 1.3476 - val_acc: 0.5938\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7776\n",
      "Epoch 00067: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7484 - acc: 0.7776 - val_loss: 1.0662 - val_acc: 0.6846\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7784\n",
      "Epoch 00068: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7486 - acc: 0.7783 - val_loss: 1.4459 - val_acc: 0.5483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7443 - acc: 0.7794\n",
      "Epoch 00069: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7444 - acc: 0.7794 - val_loss: 1.7590 - val_acc: 0.4859\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7797\n",
      "Epoch 00070: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7428 - acc: 0.7796 - val_loss: 0.9746 - val_acc: 0.7021\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7824\n",
      "Epoch 00071: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.7413 - acc: 0.7824 - val_loss: 1.0197 - val_acc: 0.6858\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7815\n",
      "Epoch 00072: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7396 - acc: 0.7815 - val_loss: 1.5154 - val_acc: 0.5288\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.7829\n",
      "Epoch 00073: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7347 - acc: 0.7829 - val_loss: 2.6850 - val_acc: 0.3869\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.7829\n",
      "Epoch 00074: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7352 - acc: 0.7829 - val_loss: 1.9604 - val_acc: 0.4780\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7839\n",
      "Epoch 00075: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7316 - acc: 0.7839 - val_loss: 3.3765 - val_acc: 0.2986\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7283 - acc: 0.7866\n",
      "Epoch 00076: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 0.7285 - acc: 0.7866 - val_loss: 1.9654 - val_acc: 0.4815\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7281 - acc: 0.7855\n",
      "Epoch 00077: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7281 - acc: 0.7855 - val_loss: 1.8234 - val_acc: 0.4941\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7276 - acc: 0.7842\n",
      "Epoch 00078: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7276 - acc: 0.7842 - val_loss: 1.4006 - val_acc: 0.5667\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.7867\n",
      "Epoch 00079: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7252 - acc: 0.7868 - val_loss: 2.5238 - val_acc: 0.4104\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7206 - acc: 0.7869\n",
      "Epoch 00080: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7208 - acc: 0.7869 - val_loss: 0.9684 - val_acc: 0.6958\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7193 - acc: 0.7868\n",
      "Epoch 00081: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7194 - acc: 0.7867 - val_loss: 1.8032 - val_acc: 0.5064\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7209 - acc: 0.7870\n",
      "Epoch 00082: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7208 - acc: 0.7871 - val_loss: 1.0180 - val_acc: 0.6879\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7172 - acc: 0.7877\n",
      "Epoch 00083: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7174 - acc: 0.7877 - val_loss: 2.1194 - val_acc: 0.4260\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7195 - acc: 0.7875\n",
      "Epoch 00084: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7195 - acc: 0.7875 - val_loss: 3.8905 - val_acc: 0.3072\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7144 - acc: 0.7887\n",
      "Epoch 00085: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7144 - acc: 0.7887 - val_loss: 1.2387 - val_acc: 0.6469\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.7888\n",
      "Epoch 00086: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.7109 - acc: 0.7889 - val_loss: 1.7494 - val_acc: 0.5097\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7130 - acc: 0.7884\n",
      "Epoch 00087: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7132 - acc: 0.7883 - val_loss: 1.2864 - val_acc: 0.5977\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7094 - acc: 0.7932\n",
      "Epoch 00088: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.7096 - acc: 0.7932 - val_loss: 1.5037 - val_acc: 0.5751\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7059 - acc: 0.7913\n",
      "Epoch 00089: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7059 - acc: 0.7913 - val_loss: 2.2895 - val_acc: 0.4757\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7912\n",
      "Epoch 00090: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7055 - acc: 0.7912 - val_loss: 1.4824 - val_acc: 0.5635\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7023 - acc: 0.7924\n",
      "Epoch 00091: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7025 - acc: 0.7924 - val_loss: 1.5776 - val_acc: 0.5679\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6988 - acc: 0.7939\n",
      "Epoch 00092: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.6990 - acc: 0.7940 - val_loss: 0.9853 - val_acc: 0.7032\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7000 - acc: 0.7912\n",
      "Epoch 00093: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.7002 - acc: 0.7910 - val_loss: 1.0128 - val_acc: 0.6725\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.7932\n",
      "Epoch 00094: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.6978 - acc: 0.7932 - val_loss: 1.0157 - val_acc: 0.7032\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7939\n",
      "Epoch 00095: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.6948 - acc: 0.7940 - val_loss: 0.9845 - val_acc: 0.7060\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.7957\n",
      "Epoch 00096: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.6943 - acc: 0.7957 - val_loss: 1.6570 - val_acc: 0.5430\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7948\n",
      "Epoch 00097: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.6918 - acc: 0.7948 - val_loss: 1.5378 - val_acc: 0.5392\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.7976\n",
      "Epoch 00098: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.6896 - acc: 0.7976 - val_loss: 0.9741 - val_acc: 0.7070\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6902 - acc: 0.7956\n",
      "Epoch 00099: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.6903 - acc: 0.7955 - val_loss: 2.0946 - val_acc: 0.4347\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6877 - acc: 0.7975\n",
      "Epoch 00100: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.6876 - acc: 0.7974 - val_loss: 1.3309 - val_acc: 0.6091\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6902 - acc: 0.7980\n",
      "Epoch 00101: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.6904 - acc: 0.7979 - val_loss: 1.6167 - val_acc: 0.5330\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.7993\n",
      "Epoch 00102: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.6844 - acc: 0.7993 - val_loss: 0.8440 - val_acc: 0.7498\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6845 - acc: 0.7997\n",
      "Epoch 00103: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.6844 - acc: 0.7997 - val_loss: 1.5318 - val_acc: 0.5875\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6846 - acc: 0.7980\n",
      "Epoch 00104: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.6844 - acc: 0.7980 - val_loss: 2.8119 - val_acc: 0.3683\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6782 - acc: 0.7973\n",
      "Epoch 00105: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.6785 - acc: 0.7973 - val_loss: 1.0783 - val_acc: 0.6553\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6777 - acc: 0.7991\n",
      "Epoch 00106: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.6777 - acc: 0.7991 - val_loss: 0.9923 - val_acc: 0.6865\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.7998\n",
      "Epoch 00107: val_loss did not improve from 0.78613\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.6779 - acc: 0.7998 - val_loss: 0.9035 - val_acc: 0.7345\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 380us/sample - loss: 0.8552 - acc: 0.7443\n",
      "Loss: 0.8551614138691349 Accuracy: 0.7443406\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0436 - acc: 0.3812\n",
      "Epoch 00001: val_loss improved from inf to 2.13253, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/001-2.1325.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 2.0431 - acc: 0.3814 - val_loss: 2.1325 - val_acc: 0.2509\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5543 - acc: 0.5349\n",
      "Epoch 00002: val_loss improved from 2.13253 to 1.50920, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/002-1.5092.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.5543 - acc: 0.5349 - val_loss: 1.5092 - val_acc: 0.5362\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3735 - acc: 0.5930\n",
      "Epoch 00003: val_loss improved from 1.50920 to 1.41541, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/003-1.4154.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 1.3734 - acc: 0.5931 - val_loss: 1.4154 - val_acc: 0.5469\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2545 - acc: 0.6305\n",
      "Epoch 00004: val_loss improved from 1.41541 to 1.39206, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/004-1.3921.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.2544 - acc: 0.6303 - val_loss: 1.3921 - val_acc: 0.5607\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1735 - acc: 0.6514\n",
      "Epoch 00005: val_loss did not improve from 1.39206\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1735 - acc: 0.6513 - val_loss: 1.4198 - val_acc: 0.5344\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1055 - acc: 0.6729\n",
      "Epoch 00006: val_loss did not improve from 1.39206\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.1061 - acc: 0.6728 - val_loss: 1.4708 - val_acc: 0.5381\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0595 - acc: 0.6884\n",
      "Epoch 00007: val_loss improved from 1.39206 to 1.22213, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/007-1.2221.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0594 - acc: 0.6882 - val_loss: 1.2221 - val_acc: 0.6045\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0182 - acc: 0.6999\n",
      "Epoch 00008: val_loss improved from 1.22213 to 1.08869, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/008-1.0887.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.0182 - acc: 0.6999 - val_loss: 1.0887 - val_acc: 0.6688\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9824 - acc: 0.7102\n",
      "Epoch 00009: val_loss did not improve from 1.08869\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.9819 - acc: 0.7104 - val_loss: 1.2485 - val_acc: 0.6059\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.7171\n",
      "Epoch 00010: val_loss improved from 1.08869 to 0.96454, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/010-0.9645.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9559 - acc: 0.7171 - val_loss: 0.9645 - val_acc: 0.7140\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9340 - acc: 0.7244\n",
      "Epoch 00011: val_loss did not improve from 0.96454\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9340 - acc: 0.7243 - val_loss: 1.2550 - val_acc: 0.5905\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9065 - acc: 0.7322\n",
      "Epoch 00012: val_loss did not improve from 0.96454\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9062 - acc: 0.7323 - val_loss: 1.0102 - val_acc: 0.6939\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8931 - acc: 0.7362\n",
      "Epoch 00013: val_loss did not improve from 0.96454\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8932 - acc: 0.7361 - val_loss: 1.4198 - val_acc: 0.5409\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7451\n",
      "Epoch 00014: val_loss did not improve from 0.96454\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8741 - acc: 0.7450 - val_loss: 1.3150 - val_acc: 0.5903\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8549 - acc: 0.7484\n",
      "Epoch 00015: val_loss did not improve from 0.96454\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8549 - acc: 0.7483 - val_loss: 1.7309 - val_acc: 0.4610\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7522\n",
      "Epoch 00016: val_loss improved from 0.96454 to 0.89463, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/016-0.8946.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8441 - acc: 0.7521 - val_loss: 0.8946 - val_acc: 0.7342\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8328 - acc: 0.7557\n",
      "Epoch 00017: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8328 - acc: 0.7558 - val_loss: 1.2361 - val_acc: 0.6217\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8185 - acc: 0.7587\n",
      "Epoch 00018: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8185 - acc: 0.7585 - val_loss: 0.9310 - val_acc: 0.7160\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.7637\n",
      "Epoch 00019: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8095 - acc: 0.7636 - val_loss: 1.0752 - val_acc: 0.6618\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7682\n",
      "Epoch 00020: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8004 - acc: 0.7681 - val_loss: 1.4596 - val_acc: 0.5674\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7903 - acc: 0.7691\n",
      "Epoch 00021: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7903 - acc: 0.7689 - val_loss: 1.3176 - val_acc: 0.5684\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7813 - acc: 0.7708\n",
      "Epoch 00022: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7814 - acc: 0.7708 - val_loss: 1.5232 - val_acc: 0.5402\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7744\n",
      "Epoch 00023: val_loss did not improve from 0.89463\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7736 - acc: 0.7742 - val_loss: 1.3144 - val_acc: 0.6117\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7638 - acc: 0.7744\n",
      "Epoch 00024: val_loss improved from 0.89463 to 0.88195, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/024-0.8819.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7636 - acc: 0.7745 - val_loss: 0.8819 - val_acc: 0.7340\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7779\n",
      "Epoch 00025: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7556 - acc: 0.7779 - val_loss: 1.0546 - val_acc: 0.6753\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7467 - acc: 0.7822\n",
      "Epoch 00026: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7471 - acc: 0.7821 - val_loss: 1.2827 - val_acc: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7384 - acc: 0.7843\n",
      "Epoch 00027: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7389 - acc: 0.7842 - val_loss: 1.2211 - val_acc: 0.6389\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7369 - acc: 0.7843\n",
      "Epoch 00028: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7368 - acc: 0.7844 - val_loss: 1.0439 - val_acc: 0.6811\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.7907\n",
      "Epoch 00029: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7281 - acc: 0.7907 - val_loss: 1.5431 - val_acc: 0.5304\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.7900\n",
      "Epoch 00030: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7199 - acc: 0.7901 - val_loss: 1.3108 - val_acc: 0.5912\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7920\n",
      "Epoch 00031: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7120 - acc: 0.7920 - val_loss: 1.1474 - val_acc: 0.6613\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7095 - acc: 0.7923\n",
      "Epoch 00032: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7095 - acc: 0.7922 - val_loss: 0.9574 - val_acc: 0.6781\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7011 - acc: 0.7953\n",
      "Epoch 00033: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7010 - acc: 0.7953 - val_loss: 2.3107 - val_acc: 0.4102\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7974\n",
      "Epoch 00034: val_loss did not improve from 0.88195\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6961 - acc: 0.7974 - val_loss: 0.9460 - val_acc: 0.7147\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6865 - acc: 0.7987\n",
      "Epoch 00035: val_loss improved from 0.88195 to 0.78562, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/035-0.7856.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.6867 - acc: 0.7986 - val_loss: 0.7856 - val_acc: 0.7682\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6867 - acc: 0.7990\n",
      "Epoch 00036: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6868 - acc: 0.7990 - val_loss: 0.8284 - val_acc: 0.7554\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.8019\n",
      "Epoch 00037: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.6796 - acc: 0.8018 - val_loss: 1.5827 - val_acc: 0.5649\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.8040\n",
      "Epoch 00038: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.6758 - acc: 0.8040 - val_loss: 0.8814 - val_acc: 0.7321\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.8074\n",
      "Epoch 00039: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6686 - acc: 0.8074 - val_loss: 0.8203 - val_acc: 0.7454\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6641 - acc: 0.8077\n",
      "Epoch 00040: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.6642 - acc: 0.8077 - val_loss: 0.9487 - val_acc: 0.6960\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6568 - acc: 0.8105\n",
      "Epoch 00041: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6565 - acc: 0.8105 - val_loss: 1.2600 - val_acc: 0.6436\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6594 - acc: 0.8068\n",
      "Epoch 00042: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.6596 - acc: 0.8067 - val_loss: 1.5441 - val_acc: 0.5528\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.8103\n",
      "Epoch 00043: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6487 - acc: 0.8103 - val_loss: 1.4852 - val_acc: 0.5914\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.8102\n",
      "Epoch 00044: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.6490 - acc: 0.8101 - val_loss: 1.6808 - val_acc: 0.5043\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6414 - acc: 0.8136\n",
      "Epoch 00045: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6416 - acc: 0.8135 - val_loss: 0.9083 - val_acc: 0.7352\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6335 - acc: 0.8152\n",
      "Epoch 00046: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.6333 - acc: 0.8153 - val_loss: 1.2556 - val_acc: 0.5952\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.8142\n",
      "Epoch 00047: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.6335 - acc: 0.8143 - val_loss: 0.8994 - val_acc: 0.7393\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8164\n",
      "Epoch 00048: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6311 - acc: 0.8162 - val_loss: 1.2611 - val_acc: 0.5877\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6287 - acc: 0.8180\n",
      "Epoch 00049: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6289 - acc: 0.8180 - val_loss: 0.9815 - val_acc: 0.7163\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.8211\n",
      "Epoch 00050: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.6221 - acc: 0.8212 - val_loss: 2.0973 - val_acc: 0.4833\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6186 - acc: 0.8198\n",
      "Epoch 00051: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6185 - acc: 0.8199 - val_loss: 1.3480 - val_acc: 0.6084\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.8229\n",
      "Epoch 00052: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.6130 - acc: 0.8228 - val_loss: 1.5280 - val_acc: 0.5577\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.8219\n",
      "Epoch 00053: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.6112 - acc: 0.8220 - val_loss: 1.4183 - val_acc: 0.5891\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6054 - acc: 0.8245\n",
      "Epoch 00054: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6056 - acc: 0.8245 - val_loss: 1.3349 - val_acc: 0.5896\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6069 - acc: 0.8237\n",
      "Epoch 00055: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.6070 - acc: 0.8236 - val_loss: 0.8418 - val_acc: 0.7324\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.8261\n",
      "Epoch 00056: val_loss did not improve from 0.78562\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6031 - acc: 0.8261 - val_loss: 0.9387 - val_acc: 0.7338\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.8271\n",
      "Epoch 00057: val_loss improved from 0.78562 to 0.76980, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/057-0.7698.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.5995 - acc: 0.8271 - val_loss: 0.7698 - val_acc: 0.7722\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.8285\n",
      "Epoch 00058: val_loss improved from 0.76980 to 0.73899, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/058-0.7390.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.5914 - acc: 0.8284 - val_loss: 0.7390 - val_acc: 0.7852\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8290\n",
      "Epoch 00059: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.5943 - acc: 0.8290 - val_loss: 1.1374 - val_acc: 0.6487\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5861 - acc: 0.8312\n",
      "Epoch 00060: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.5861 - acc: 0.8312 - val_loss: 0.8163 - val_acc: 0.7598\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.8308\n",
      "Epoch 00061: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.5849 - acc: 0.8308 - val_loss: 0.7869 - val_acc: 0.7771\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.8308\n",
      "Epoch 00062: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5822 - acc: 0.8305 - val_loss: 0.8422 - val_acc: 0.7573\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.8324\n",
      "Epoch 00063: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.5796 - acc: 0.8324 - val_loss: 1.1767 - val_acc: 0.6366\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8341\n",
      "Epoch 00064: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5755 - acc: 0.8340 - val_loss: 0.8208 - val_acc: 0.7671\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8343\n",
      "Epoch 00065: val_loss did not improve from 0.73899\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5747 - acc: 0.8342 - val_loss: 0.8193 - val_acc: 0.7508\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8349\n",
      "Epoch 00066: val_loss improved from 0.73899 to 0.72712, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/066-0.7271.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.5686 - acc: 0.8349 - val_loss: 0.7271 - val_acc: 0.7880\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8372\n",
      "Epoch 00067: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5679 - acc: 0.8371 - val_loss: 1.1817 - val_acc: 0.6529\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8370\n",
      "Epoch 00068: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5625 - acc: 0.8370 - val_loss: 1.1211 - val_acc: 0.6893\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8368\n",
      "Epoch 00069: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5632 - acc: 0.8367 - val_loss: 1.1541 - val_acc: 0.6692\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.8393\n",
      "Epoch 00070: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5559 - acc: 0.8394 - val_loss: 1.0825 - val_acc: 0.6622\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5567 - acc: 0.8409\n",
      "Epoch 00071: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.5566 - acc: 0.8410 - val_loss: 0.9104 - val_acc: 0.7114\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8396\n",
      "Epoch 00072: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.5502 - acc: 0.8396 - val_loss: 1.0204 - val_acc: 0.6744\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.8409\n",
      "Epoch 00073: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.5494 - acc: 0.8409 - val_loss: 0.7939 - val_acc: 0.7626\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8406\n",
      "Epoch 00074: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5472 - acc: 0.8407 - val_loss: 0.9523 - val_acc: 0.7149\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.8409\n",
      "Epoch 00075: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5452 - acc: 0.8409 - val_loss: 1.1043 - val_acc: 0.6408\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8435\n",
      "Epoch 00076: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5424 - acc: 0.8434 - val_loss: 1.0638 - val_acc: 0.6909\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8439\n",
      "Epoch 00077: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.5419 - acc: 0.8439 - val_loss: 0.9455 - val_acc: 0.7167\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.8444\n",
      "Epoch 00078: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5387 - acc: 0.8444 - val_loss: 1.2261 - val_acc: 0.6343\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5364 - acc: 0.8434\n",
      "Epoch 00079: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.5365 - acc: 0.8434 - val_loss: 0.7698 - val_acc: 0.7694\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8453\n",
      "Epoch 00080: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5299 - acc: 0.8453 - val_loss: 0.7451 - val_acc: 0.7864\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8455\n",
      "Epoch 00081: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5330 - acc: 0.8453 - val_loss: 1.1108 - val_acc: 0.6725\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8476\n",
      "Epoch 00082: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5261 - acc: 0.8476 - val_loss: 0.9507 - val_acc: 0.7244\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.8490\n",
      "Epoch 00083: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5233 - acc: 0.8491 - val_loss: 0.7762 - val_acc: 0.7640\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8486\n",
      "Epoch 00084: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.5215 - acc: 0.8485 - val_loss: 0.9301 - val_acc: 0.7137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.8484\n",
      "Epoch 00085: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.5206 - acc: 0.8483 - val_loss: 1.0341 - val_acc: 0.7137\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8509\n",
      "Epoch 00086: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5165 - acc: 0.8509 - val_loss: 1.2608 - val_acc: 0.6445\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.8515\n",
      "Epoch 00087: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.5134 - acc: 0.8515 - val_loss: 1.0917 - val_acc: 0.6837\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.8533\n",
      "Epoch 00088: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.5117 - acc: 0.8534 - val_loss: 0.9430 - val_acc: 0.7079\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8518\n",
      "Epoch 00089: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.5116 - acc: 0.8519 - val_loss: 0.8680 - val_acc: 0.7591\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8512\n",
      "Epoch 00090: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5120 - acc: 0.8511 - val_loss: 1.5055 - val_acc: 0.5693\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8534\n",
      "Epoch 00091: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5052 - acc: 0.8534 - val_loss: 1.0517 - val_acc: 0.6769\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8528\n",
      "Epoch 00092: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5057 - acc: 0.8528 - val_loss: 0.8462 - val_acc: 0.7389\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.8536\n",
      "Epoch 00093: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.5025 - acc: 0.8536 - val_loss: 2.1874 - val_acc: 0.4633\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8540\n",
      "Epoch 00094: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.5003 - acc: 0.8540 - val_loss: 1.1077 - val_acc: 0.6881\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8566\n",
      "Epoch 00095: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4978 - acc: 0.8564 - val_loss: 1.2673 - val_acc: 0.6415\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.8575\n",
      "Epoch 00096: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4958 - acc: 0.8575 - val_loss: 0.8959 - val_acc: 0.7345\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4956 - acc: 0.8564\n",
      "Epoch 00097: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4958 - acc: 0.8564 - val_loss: 0.8395 - val_acc: 0.7610\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8577\n",
      "Epoch 00098: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4910 - acc: 0.8576 - val_loss: 0.9062 - val_acc: 0.7298\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8584\n",
      "Epoch 00099: val_loss did not improve from 0.72712\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4912 - acc: 0.8583 - val_loss: 0.8062 - val_acc: 0.7573\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8576\n",
      "Epoch 00100: val_loss improved from 0.72712 to 0.65892, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/100-0.6589.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4910 - acc: 0.8576 - val_loss: 0.6589 - val_acc: 0.8127\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4889 - acc: 0.8578\n",
      "Epoch 00101: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.4888 - acc: 0.8578 - val_loss: 0.7540 - val_acc: 0.7773\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.8591\n",
      "Epoch 00102: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4860 - acc: 0.8591 - val_loss: 0.7198 - val_acc: 0.7850\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8597\n",
      "Epoch 00103: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4799 - acc: 0.8597 - val_loss: 0.9853 - val_acc: 0.6976\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4779 - acc: 0.8618\n",
      "Epoch 00104: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4779 - acc: 0.8618 - val_loss: 1.1656 - val_acc: 0.6632\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4809 - acc: 0.8609\n",
      "Epoch 00105: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4809 - acc: 0.8609 - val_loss: 0.8245 - val_acc: 0.7365\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8630\n",
      "Epoch 00106: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4746 - acc: 0.8631 - val_loss: 0.8770 - val_acc: 0.7629\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8626\n",
      "Epoch 00107: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4772 - acc: 0.8627 - val_loss: 0.9747 - val_acc: 0.7018\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8634\n",
      "Epoch 00108: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4729 - acc: 0.8634 - val_loss: 0.8708 - val_acc: 0.7447\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8629\n",
      "Epoch 00109: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4706 - acc: 0.8628 - val_loss: 0.8685 - val_acc: 0.7633\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8625\n",
      "Epoch 00110: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4709 - acc: 0.8624 - val_loss: 1.1506 - val_acc: 0.6592\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8648\n",
      "Epoch 00111: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4686 - acc: 0.8648 - val_loss: 1.0898 - val_acc: 0.6799\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4656 - acc: 0.8664\n",
      "Epoch 00112: val_loss did not improve from 0.65892\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4658 - acc: 0.8663 - val_loss: 0.9944 - val_acc: 0.7321\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8645\n",
      "Epoch 00113: val_loss improved from 0.65892 to 0.62241, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv_checkpoint/113-0.6224.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4641 - acc: 0.8644 - val_loss: 0.6224 - val_acc: 0.8265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8649\n",
      "Epoch 00114: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4644 - acc: 0.8650 - val_loss: 0.7023 - val_acc: 0.7997\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8667\n",
      "Epoch 00115: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.4575 - acc: 0.8665 - val_loss: 1.6297 - val_acc: 0.6392\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8657\n",
      "Epoch 00116: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4616 - acc: 0.8658 - val_loss: 1.0570 - val_acc: 0.6962\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.8670\n",
      "Epoch 00117: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4553 - acc: 0.8670 - val_loss: 1.0076 - val_acc: 0.6872\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4576 - acc: 0.8664\n",
      "Epoch 00118: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4580 - acc: 0.8663 - val_loss: 0.8737 - val_acc: 0.7431\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8680\n",
      "Epoch 00119: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4548 - acc: 0.8680 - val_loss: 0.9581 - val_acc: 0.7198\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8691\n",
      "Epoch 00120: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4510 - acc: 0.8692 - val_loss: 0.7481 - val_acc: 0.7852\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.8686\n",
      "Epoch 00121: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4517 - acc: 0.8685 - val_loss: 0.9278 - val_acc: 0.7352\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8714\n",
      "Epoch 00122: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4464 - acc: 0.8714 - val_loss: 1.4252 - val_acc: 0.6082\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8696\n",
      "Epoch 00123: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4496 - acc: 0.8696 - val_loss: 0.7952 - val_acc: 0.7813\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8696\n",
      "Epoch 00124: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4460 - acc: 0.8695 - val_loss: 0.8690 - val_acc: 0.7505\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8708\n",
      "Epoch 00125: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4462 - acc: 0.8708 - val_loss: 0.7634 - val_acc: 0.7871\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8723\n",
      "Epoch 00126: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4421 - acc: 0.8722 - val_loss: 0.7118 - val_acc: 0.8011\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8719\n",
      "Epoch 00127: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.4432 - acc: 0.8719 - val_loss: 1.4524 - val_acc: 0.6096\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4392 - acc: 0.8714\n",
      "Epoch 00128: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4393 - acc: 0.8713 - val_loss: 0.7797 - val_acc: 0.7778\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8702\n",
      "Epoch 00129: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4365 - acc: 0.8702 - val_loss: 1.6911 - val_acc: 0.5502\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8725\n",
      "Epoch 00130: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4355 - acc: 0.8724 - val_loss: 1.4016 - val_acc: 0.6229\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8750\n",
      "Epoch 00131: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.4335 - acc: 0.8749 - val_loss: 0.8959 - val_acc: 0.7352\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8738\n",
      "Epoch 00132: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4324 - acc: 0.8738 - val_loss: 0.8032 - val_acc: 0.7673\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8733\n",
      "Epoch 00133: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.4346 - acc: 0.8733 - val_loss: 1.5591 - val_acc: 0.5917\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.8745\n",
      "Epoch 00134: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4333 - acc: 0.8745 - val_loss: 1.3786 - val_acc: 0.6266\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8757\n",
      "Epoch 00135: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4287 - acc: 0.8756 - val_loss: 0.8747 - val_acc: 0.7419\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8736\n",
      "Epoch 00136: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.4304 - acc: 0.8735 - val_loss: 0.9672 - val_acc: 0.7261\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8757\n",
      "Epoch 00137: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4247 - acc: 0.8758 - val_loss: 0.9774 - val_acc: 0.7028\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8779\n",
      "Epoch 00138: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.4247 - acc: 0.8779 - val_loss: 1.5859 - val_acc: 0.6231\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8768\n",
      "Epoch 00139: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4253 - acc: 0.8769 - val_loss: 0.9769 - val_acc: 0.7412\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8760\n",
      "Epoch 00140: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.4236 - acc: 0.8761 - val_loss: 0.9175 - val_acc: 0.7247\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8783\n",
      "Epoch 00141: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4200 - acc: 0.8783 - val_loss: 0.6542 - val_acc: 0.8167\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8763\n",
      "Epoch 00142: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4213 - acc: 0.8763 - val_loss: 0.6914 - val_acc: 0.8011\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8789\n",
      "Epoch 00143: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4187 - acc: 0.8787 - val_loss: 0.9388 - val_acc: 0.7393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8802\n",
      "Epoch 00144: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4131 - acc: 0.8802 - val_loss: 0.9414 - val_acc: 0.7419\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8776\n",
      "Epoch 00145: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4178 - acc: 0.8775 - val_loss: 0.6625 - val_acc: 0.8178\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8782\n",
      "Epoch 00146: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4174 - acc: 0.8783 - val_loss: 0.8884 - val_acc: 0.7647\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8772\n",
      "Epoch 00147: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4171 - acc: 0.8773 - val_loss: 1.1588 - val_acc: 0.6734\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8806\n",
      "Epoch 00148: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4122 - acc: 0.8807 - val_loss: 0.6900 - val_acc: 0.8055\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8797\n",
      "Epoch 00149: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4141 - acc: 0.8797 - val_loss: 0.7298 - val_acc: 0.8036\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8798\n",
      "Epoch 00150: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4085 - acc: 0.8797 - val_loss: 0.8473 - val_acc: 0.7480\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8802\n",
      "Epoch 00151: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4063 - acc: 0.8803 - val_loss: 0.6432 - val_acc: 0.8127\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8801\n",
      "Epoch 00152: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4057 - acc: 0.8800 - val_loss: 0.6829 - val_acc: 0.8074\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8804\n",
      "Epoch 00153: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4052 - acc: 0.8804 - val_loss: 0.8463 - val_acc: 0.7631\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8832\n",
      "Epoch 00154: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4026 - acc: 0.8832 - val_loss: 1.3185 - val_acc: 0.6506\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8831\n",
      "Epoch 00155: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.4015 - acc: 0.8831 - val_loss: 2.2908 - val_acc: 0.4962\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8822\n",
      "Epoch 00156: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4027 - acc: 0.8822 - val_loss: 0.7581 - val_acc: 0.7789\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8814\n",
      "Epoch 00157: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4023 - acc: 0.8815 - val_loss: 1.3134 - val_acc: 0.6515\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8818\n",
      "Epoch 00158: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4009 - acc: 0.8818 - val_loss: 0.9118 - val_acc: 0.7477\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8847\n",
      "Epoch 00159: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3955 - acc: 0.8847 - val_loss: 0.8225 - val_acc: 0.7636\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3977 - acc: 0.8852\n",
      "Epoch 00160: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3977 - acc: 0.8852 - val_loss: 0.9106 - val_acc: 0.7498\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8854\n",
      "Epoch 00161: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3943 - acc: 0.8853 - val_loss: 1.2260 - val_acc: 0.6755\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8852\n",
      "Epoch 00162: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.3947 - acc: 0.8853 - val_loss: 0.6523 - val_acc: 0.8153\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8841\n",
      "Epoch 00163: val_loss did not improve from 0.62241\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.3951 - acc: 0.8841 - val_loss: 0.7096 - val_acc: 0.7948\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 385us/sample - loss: 0.7048 - acc: 0.7981\n",
      "Loss: 0.7048168689910002 Accuracy: 0.79813087\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8906 - acc: 0.4316\n",
      "Epoch 00001: val_loss improved from inf to 1.82372, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/001-1.8237.hdf5\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 1.8906 - acc: 0.4317 - val_loss: 1.8237 - val_acc: 0.4456\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3242 - acc: 0.6129\n",
      "Epoch 00002: val_loss improved from 1.82372 to 1.23849, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/002-1.2385.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.3241 - acc: 0.6129 - val_loss: 1.2385 - val_acc: 0.6508\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6759\n",
      "Epoch 00003: val_loss improved from 1.23849 to 1.07177, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/003-1.0718.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 1.1302 - acc: 0.6759 - val_loss: 1.0718 - val_acc: 0.6990\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.7144\n",
      "Epoch 00004: val_loss improved from 1.07177 to 1.03199, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/004-1.0320.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 1.0056 - acc: 0.7143 - val_loss: 1.0320 - val_acc: 0.7011\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9214 - acc: 0.7414\n",
      "Epoch 00005: val_loss improved from 1.03199 to 0.86754, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/005-0.8675.hdf5\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.9216 - acc: 0.7413 - val_loss: 0.8675 - val_acc: 0.7750\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8468 - acc: 0.7640\n",
      "Epoch 00006: val_loss did not improve from 0.86754\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.8466 - acc: 0.7640 - val_loss: 0.8798 - val_acc: 0.7459\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7909 - acc: 0.7793\n",
      "Epoch 00007: val_loss improved from 0.86754 to 0.77811, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/007-0.7781.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.7908 - acc: 0.7792 - val_loss: 0.7781 - val_acc: 0.7820\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.7941\n",
      "Epoch 00008: val_loss did not improve from 0.77811\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.7431 - acc: 0.7942 - val_loss: 0.8341 - val_acc: 0.7577\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7071 - acc: 0.8054\n",
      "Epoch 00009: val_loss improved from 0.77811 to 0.74712, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/009-0.7471.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.7069 - acc: 0.8055 - val_loss: 0.7471 - val_acc: 0.8004\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.8165\n",
      "Epoch 00010: val_loss improved from 0.74712 to 0.67922, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/010-0.6792.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.6671 - acc: 0.8167 - val_loss: 0.6792 - val_acc: 0.8274\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6396 - acc: 0.8232\n",
      "Epoch 00011: val_loss improved from 0.67922 to 0.66478, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/011-0.6648.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.6395 - acc: 0.8232 - val_loss: 0.6648 - val_acc: 0.8220\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6116 - acc: 0.8316\n",
      "Epoch 00012: val_loss did not improve from 0.66478\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.6120 - acc: 0.8315 - val_loss: 0.7642 - val_acc: 0.7787\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5917 - acc: 0.8344\n",
      "Epoch 00013: val_loss did not improve from 0.66478\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.5920 - acc: 0.8344 - val_loss: 0.9562 - val_acc: 0.6848\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.8432\n",
      "Epoch 00014: val_loss improved from 0.66478 to 0.59067, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/014-0.5907.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5686 - acc: 0.8431 - val_loss: 0.5907 - val_acc: 0.8395\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.8479\n",
      "Epoch 00015: val_loss did not improve from 0.59067\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.5498 - acc: 0.8477 - val_loss: 0.6052 - val_acc: 0.8283\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.8541\n",
      "Epoch 00016: val_loss did not improve from 0.59067\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.5305 - acc: 0.8542 - val_loss: 0.6631 - val_acc: 0.8181\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8580\n",
      "Epoch 00017: val_loss improved from 0.59067 to 0.57733, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/017-0.5773.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.5131 - acc: 0.8580 - val_loss: 0.5773 - val_acc: 0.8358\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8619\n",
      "Epoch 00018: val_loss improved from 0.57733 to 0.55555, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/018-0.5556.hdf5\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.5044 - acc: 0.8618 - val_loss: 0.5556 - val_acc: 0.8521\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8651\n",
      "Epoch 00019: val_loss did not improve from 0.55555\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.4854 - acc: 0.8650 - val_loss: 0.6211 - val_acc: 0.8113\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8693\n",
      "Epoch 00020: val_loss improved from 0.55555 to 0.55486, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/020-0.5549.hdf5\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.4743 - acc: 0.8693 - val_loss: 0.5549 - val_acc: 0.8512\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8703\n",
      "Epoch 00021: val_loss did not improve from 0.55486\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.4631 - acc: 0.8703 - val_loss: 0.5949 - val_acc: 0.8304\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8747\n",
      "Epoch 00022: val_loss improved from 0.55486 to 0.51016, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/022-0.5102.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4506 - acc: 0.8746 - val_loss: 0.5102 - val_acc: 0.8579\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8781-  -\n",
      "Epoch 00023: val_loss did not improve from 0.51016\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.4406 - acc: 0.8780 - val_loss: 0.5508 - val_acc: 0.8509\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8800\n",
      "Epoch 00024: val_loss did not improve from 0.51016\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.4312 - acc: 0.8799 - val_loss: 0.5730 - val_acc: 0.8416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8832\n",
      "Epoch 00025: val_loss did not improve from 0.51016\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.4215 - acc: 0.8833 - val_loss: 0.5704 - val_acc: 0.8314\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8830\n",
      "Epoch 00026: val_loss improved from 0.51016 to 0.44782, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/026-0.4478.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.4137 - acc: 0.8830 - val_loss: 0.4478 - val_acc: 0.8777\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8868\n",
      "Epoch 00027: val_loss did not improve from 0.44782\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.4040 - acc: 0.8870 - val_loss: 0.4873 - val_acc: 0.8637\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8896\n",
      "Epoch 00028: val_loss did not improve from 0.44782\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3931 - acc: 0.8896 - val_loss: 0.5424 - val_acc: 0.8428\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8905\n",
      "Epoch 00029: val_loss did not improve from 0.44782\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3896 - acc: 0.8904 - val_loss: 0.4619 - val_acc: 0.8672\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8925\n",
      "Epoch 00030: val_loss did not improve from 0.44782\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3822 - acc: 0.8925 - val_loss: 0.4737 - val_acc: 0.8644\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8954\n",
      "Epoch 00031: val_loss improved from 0.44782 to 0.42936, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/031-0.4294.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3747 - acc: 0.8953 - val_loss: 0.4294 - val_acc: 0.8807\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8960\n",
      "Epoch 00032: val_loss improved from 0.42936 to 0.42713, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/032-0.4271.hdf5\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3680 - acc: 0.8960 - val_loss: 0.4271 - val_acc: 0.8772\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8996\n",
      "Epoch 00033: val_loss improved from 0.42713 to 0.41456, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/033-0.4146.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3582 - acc: 0.8996 - val_loss: 0.4146 - val_acc: 0.8880\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8996\n",
      "Epoch 00034: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.3543 - acc: 0.8997 - val_loss: 0.5072 - val_acc: 0.8549\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.9020\n",
      "Epoch 00035: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3469 - acc: 0.9020 - val_loss: 0.4202 - val_acc: 0.8828\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.9043\n",
      "Epoch 00036: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3425 - acc: 0.9043 - val_loss: 0.5505 - val_acc: 0.8344\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.9066\n",
      "Epoch 00037: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.3359 - acc: 0.9066 - val_loss: 0.4705 - val_acc: 0.8635\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.9070\n",
      "Epoch 00038: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3329 - acc: 0.9070 - val_loss: 0.4340 - val_acc: 0.8779\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.9101\n",
      "Epoch 00039: val_loss did not improve from 0.41456\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.3224 - acc: 0.9100 - val_loss: 0.6712 - val_acc: 0.7866\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.9084\n",
      "Epoch 00040: val_loss improved from 0.41456 to 0.40762, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/040-0.4076.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.3242 - acc: 0.9084 - val_loss: 0.4076 - val_acc: 0.8863\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9098\n",
      "Epoch 00041: val_loss did not improve from 0.40762\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.3183 - acc: 0.9098 - val_loss: 0.4293 - val_acc: 0.8826\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9120\n",
      "Epoch 00042: val_loss improved from 0.40762 to 0.38516, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/042-0.3852.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3122 - acc: 0.9120 - val_loss: 0.3852 - val_acc: 0.8935\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9142\n",
      "Epoch 00043: val_loss did not improve from 0.38516\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3066 - acc: 0.9142 - val_loss: 0.4840 - val_acc: 0.8532\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9144\n",
      "Epoch 00044: val_loss did not improve from 0.38516\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.3036 - acc: 0.9145 - val_loss: 0.3939 - val_acc: 0.8933\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9161\n",
      "Epoch 00045: val_loss did not improve from 0.38516\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2972 - acc: 0.9160 - val_loss: 0.4332 - val_acc: 0.8747\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9176\n",
      "Epoch 00046: val_loss improved from 0.38516 to 0.35585, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/046-0.3559.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2943 - acc: 0.9175 - val_loss: 0.3559 - val_acc: 0.9024\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9166\n",
      "Epoch 00047: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.2914 - acc: 0.9166 - val_loss: 0.4506 - val_acc: 0.8742\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9187\n",
      "Epoch 00048: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.2864 - acc: 0.9187 - val_loss: 0.3768 - val_acc: 0.8973\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9198\n",
      "Epoch 00049: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2851 - acc: 0.9197 - val_loss: 0.4606 - val_acc: 0.8672\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9208\n",
      "Epoch 00050: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2801 - acc: 0.9208 - val_loss: 0.4490 - val_acc: 0.8765\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9221\n",
      "Epoch 00051: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2745 - acc: 0.9220 - val_loss: 0.4837 - val_acc: 0.8670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9236\n",
      "Epoch 00052: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2703 - acc: 0.9236 - val_loss: 0.3663 - val_acc: 0.8980\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9245\n",
      "Epoch 00053: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2654 - acc: 0.9245 - val_loss: 0.4176 - val_acc: 0.8742\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9243\n",
      "Epoch 00054: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2650 - acc: 0.9243 - val_loss: 0.4295 - val_acc: 0.8758\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9264\n",
      "Epoch 00055: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2619 - acc: 0.9263 - val_loss: 0.4788 - val_acc: 0.8658\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9258\n",
      "Epoch 00056: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2586 - acc: 0.9258 - val_loss: 0.3671 - val_acc: 0.9003\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9258\n",
      "Epoch 00057: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2585 - acc: 0.9258 - val_loss: 0.4689 - val_acc: 0.8616\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9266\n",
      "Epoch 00058: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2540 - acc: 0.9266 - val_loss: 0.3710 - val_acc: 0.8949\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9279- ETA: 1s - loss: 0.2\n",
      "Epoch 00059: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2516 - acc: 0.9279 - val_loss: 0.4005 - val_acc: 0.8849\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9302\n",
      "Epoch 00060: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.2423 - acc: 0.9302 - val_loss: 0.3827 - val_acc: 0.8933\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9299\n",
      "Epoch 00061: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2410 - acc: 0.9300 - val_loss: 0.3776 - val_acc: 0.8935\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9323\n",
      "Epoch 00062: val_loss did not improve from 0.35585\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.2370 - acc: 0.9322 - val_loss: 0.4052 - val_acc: 0.8870\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9317\n",
      "Epoch 00063: val_loss improved from 0.35585 to 0.35217, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/063-0.3522.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2379 - acc: 0.9316 - val_loss: 0.3522 - val_acc: 0.9019\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9330\n",
      "Epoch 00064: val_loss did not improve from 0.35217\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2330 - acc: 0.9330 - val_loss: 0.4318 - val_acc: 0.8742\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9344\n",
      "Epoch 00065: val_loss did not improve from 0.35217\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2319 - acc: 0.9343 - val_loss: 0.3716 - val_acc: 0.8996\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9339\n",
      "Epoch 00066: val_loss did not improve from 0.35217\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2328 - acc: 0.9339 - val_loss: 0.4629 - val_acc: 0.8682\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9346\n",
      "Epoch 00067: val_loss did not improve from 0.35217\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2263 - acc: 0.9346 - val_loss: 0.5668 - val_acc: 0.8353\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9367\n",
      "Epoch 00068: val_loss did not improve from 0.35217\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2216 - acc: 0.9367 - val_loss: 0.3683 - val_acc: 0.8998\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9376\n",
      "Epoch 00069: val_loss improved from 0.35217 to 0.33390, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/069-0.3339.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.2190 - acc: 0.9376 - val_loss: 0.3339 - val_acc: 0.9073\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9385\n",
      "Epoch 00070: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.2179 - acc: 0.9384 - val_loss: 0.4357 - val_acc: 0.8728\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9365\n",
      "Epoch 00071: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.2193 - acc: 0.9365 - val_loss: 0.5168 - val_acc: 0.8465\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9382\n",
      "Epoch 00072: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2120 - acc: 0.9381 - val_loss: 0.3383 - val_acc: 0.9082\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9381\n",
      "Epoch 00073: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.2160 - acc: 0.9381 - val_loss: 0.3516 - val_acc: 0.9071\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9385\n",
      "Epoch 00074: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2081 - acc: 0.9385 - val_loss: 0.3784 - val_acc: 0.8991\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9414\n",
      "Epoch 00075: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.2051 - acc: 0.9414 - val_loss: 0.3806 - val_acc: 0.8866\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9431\n",
      "Epoch 00076: val_loss did not improve from 0.33390\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.2038 - acc: 0.9431 - val_loss: 0.3550 - val_acc: 0.9061\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9440\n",
      "Epoch 00077: val_loss improved from 0.33390 to 0.33291, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/077-0.3329.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1971 - acc: 0.9440 - val_loss: 0.3329 - val_acc: 0.9073\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9425\n",
      "Epoch 00078: val_loss did not improve from 0.33291\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1988 - acc: 0.9425 - val_loss: 0.3535 - val_acc: 0.9022\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9438\n",
      "Epoch 00079: val_loss did not improve from 0.33291\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1962 - acc: 0.9438 - val_loss: 0.4433 - val_acc: 0.8672\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9436\n",
      "Epoch 00080: val_loss did not improve from 0.33291\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1948 - acc: 0.9435 - val_loss: 0.3686 - val_acc: 0.8912\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9454\n",
      "Epoch 00081: val_loss did not improve from 0.33291\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1917 - acc: 0.9454 - val_loss: 0.3430 - val_acc: 0.9050\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9455\n",
      "Epoch 00082: val_loss improved from 0.33291 to 0.33111, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/082-0.3311.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1911 - acc: 0.9454 - val_loss: 0.3311 - val_acc: 0.9075\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9458\n",
      "Epoch 00083: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1896 - acc: 0.9458 - val_loss: 0.4022 - val_acc: 0.8917\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9478\n",
      "Epoch 00084: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1850 - acc: 0.9477 - val_loss: 0.5864 - val_acc: 0.8439\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9448\n",
      "Epoch 00085: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1898 - acc: 0.9448 - val_loss: 0.3440 - val_acc: 0.9019\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9463\n",
      "Epoch 00086: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1852 - acc: 0.9462 - val_loss: 0.3700 - val_acc: 0.8956\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9486\n",
      "Epoch 00087: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1783 - acc: 0.9485 - val_loss: 0.6996 - val_acc: 0.8199\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9482\n",
      "Epoch 00088: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1808 - acc: 0.9481 - val_loss: 0.3687 - val_acc: 0.8947\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9471\n",
      "Epoch 00089: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1805 - acc: 0.9471 - val_loss: 0.5399 - val_acc: 0.8570\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9492\n",
      "Epoch 00090: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1751 - acc: 0.9492 - val_loss: 0.3810 - val_acc: 0.9017\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9497\n",
      "Epoch 00091: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1733 - acc: 0.9497 - val_loss: 0.4120 - val_acc: 0.8903\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9494\n",
      "Epoch 00092: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1718 - acc: 0.9494 - val_loss: 0.3672 - val_acc: 0.9005\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9509\n",
      "Epoch 00093: val_loss did not improve from 0.33111\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1705 - acc: 0.9509 - val_loss: 0.3801 - val_acc: 0.9024\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9513\n",
      "Epoch 00094: val_loss improved from 0.33111 to 0.32201, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/094-0.3220.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1677 - acc: 0.9513 - val_loss: 0.3220 - val_acc: 0.9117\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9521\n",
      "Epoch 00095: val_loss did not improve from 0.32201\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1670 - acc: 0.9521 - val_loss: 0.3465 - val_acc: 0.9045\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9521\n",
      "Epoch 00096: val_loss did not improve from 0.32201\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1646 - acc: 0.9521 - val_loss: 0.3742 - val_acc: 0.8975\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9521\n",
      "Epoch 00097: val_loss improved from 0.32201 to 0.32036, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/097-0.3204.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1652 - acc: 0.9521 - val_loss: 0.3204 - val_acc: 0.9117\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9544\n",
      "Epoch 00098: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1605 - acc: 0.9544 - val_loss: 0.3232 - val_acc: 0.9140\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9554\n",
      "Epoch 00099: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1577 - acc: 0.9554 - val_loss: 0.4454 - val_acc: 0.8805\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9524\n",
      "Epoch 00100: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1612 - acc: 0.9524 - val_loss: 0.4487 - val_acc: 0.8838\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9561\n",
      "Epoch 00101: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1551 - acc: 0.9560 - val_loss: 0.4733 - val_acc: 0.8758\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9544\n",
      "Epoch 00102: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1589 - acc: 0.9544 - val_loss: 0.3267 - val_acc: 0.9117\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9549\n",
      "Epoch 00103: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1553 - acc: 0.9549 - val_loss: 0.3792 - val_acc: 0.8961\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9562\n",
      "Epoch 00104: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1508 - acc: 0.9562 - val_loss: 0.3445 - val_acc: 0.9078\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9550\n",
      "Epoch 00105: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1510 - acc: 0.9551 - val_loss: 0.3425 - val_acc: 0.9140\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9570\n",
      "Epoch 00106: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1487 - acc: 0.9570 - val_loss: 0.4058 - val_acc: 0.8915\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9579\n",
      "Epoch 00107: val_loss did not improve from 0.32036\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1448 - acc: 0.9578 - val_loss: 0.3415 - val_acc: 0.9073\n",
      "Epoch 108/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9567\n",
      "Epoch 00108: val_loss improved from 0.32036 to 0.31706, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv_checkpoint/108-0.3171.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1481 - acc: 0.9567 - val_loss: 0.3171 - val_acc: 0.9171\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9578\n",
      "Epoch 00109: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1439 - acc: 0.9577 - val_loss: 0.3341 - val_acc: 0.9106\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9565\n",
      "Epoch 00110: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1471 - acc: 0.9564 - val_loss: 0.3375 - val_acc: 0.9108\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9596\n",
      "Epoch 00111: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1401 - acc: 0.9596 - val_loss: 0.3226 - val_acc: 0.9159\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9588\n",
      "Epoch 00112: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.1403 - acc: 0.9588 - val_loss: 0.3399 - val_acc: 0.9073\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9594\n",
      "Epoch 00113: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1407 - acc: 0.9593 - val_loss: 0.5432 - val_acc: 0.8598\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9569\n",
      "Epoch 00114: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1463 - acc: 0.9569 - val_loss: 0.3661 - val_acc: 0.9033\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9610\n",
      "Epoch 00115: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1325 - acc: 0.9609 - val_loss: 0.3198 - val_acc: 0.9133\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9601\n",
      "Epoch 00116: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1374 - acc: 0.9601 - val_loss: 0.4026 - val_acc: 0.8959\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9630\n",
      "Epoch 00117: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1276 - acc: 0.9629 - val_loss: 0.3573 - val_acc: 0.9019\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9622\n",
      "Epoch 00118: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.1295 - acc: 0.9621 - val_loss: 0.3349 - val_acc: 0.9115\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9624\n",
      "Epoch 00119: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1310 - acc: 0.9625 - val_loss: 0.3943 - val_acc: 0.8977\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9638\n",
      "Epoch 00120: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1269 - acc: 0.9637 - val_loss: 0.3752 - val_acc: 0.9019\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9624\n",
      "Epoch 00121: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1326 - acc: 0.9623 - val_loss: 0.3856 - val_acc: 0.8952\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9618\n",
      "Epoch 00122: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1289 - acc: 0.9619 - val_loss: 0.3853 - val_acc: 0.8966\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9622\n",
      "Epoch 00123: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1277 - acc: 0.9622 - val_loss: 0.4405 - val_acc: 0.8854\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9644- ETA: 1s - loss: 0\n",
      "Epoch 00124: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1236 - acc: 0.9644 - val_loss: 0.4514 - val_acc: 0.8840\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9638\n",
      "Epoch 00125: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1236 - acc: 0.9638 - val_loss: 0.3602 - val_acc: 0.9038\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9648\n",
      "Epoch 00126: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1237 - acc: 0.9648 - val_loss: 0.3960 - val_acc: 0.8926\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9633\n",
      "Epoch 00127: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1262 - acc: 0.9633 - val_loss: 0.3821 - val_acc: 0.8998\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9657\n",
      "Epoch 00128: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.1212 - acc: 0.9657 - val_loss: 0.5921 - val_acc: 0.8484\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9658\n",
      "Epoch 00129: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1189 - acc: 0.9658 - val_loss: 0.4313 - val_acc: 0.8861\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9663\n",
      "Epoch 00130: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1178 - acc: 0.9662 - val_loss: 0.3660 - val_acc: 0.9043\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9675- ETA: 0s - loss: 0.1142 - acc: 0.967\n",
      "Epoch 00131: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1143 - acc: 0.9674 - val_loss: 0.5425 - val_acc: 0.8602\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9669\n",
      "Epoch 00132: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1166 - acc: 0.9669 - val_loss: 0.3483 - val_acc: 0.9061\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9662\n",
      "Epoch 00133: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1158 - acc: 0.9662 - val_loss: 0.3813 - val_acc: 0.9024\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9680\n",
      "Epoch 00134: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.1107 - acc: 0.9680 - val_loss: 0.3778 - val_acc: 0.9019\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9675- ETA: 0s - loss: 0.1128 - ac\n",
      "Epoch 00135: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1123 - acc: 0.9675 - val_loss: 0.4510 - val_acc: 0.8821\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9677\n",
      "Epoch 00136: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.1119 - acc: 0.9677 - val_loss: 0.4175 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9683\n",
      "Epoch 00137: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1125 - acc: 0.9682 - val_loss: 0.4541 - val_acc: 0.8880\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9689\n",
      "Epoch 00138: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.1101 - acc: 0.9689 - val_loss: 0.4343 - val_acc: 0.8928\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9697\n",
      "Epoch 00139: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1063 - acc: 0.9697 - val_loss: 0.4199 - val_acc: 0.9012\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9701\n",
      "Epoch 00140: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.1061 - acc: 0.9700 - val_loss: 0.5766 - val_acc: 0.8630\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9684\n",
      "Epoch 00141: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1096 - acc: 0.9684 - val_loss: 0.3636 - val_acc: 0.9073\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9713\n",
      "Epoch 00142: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1029 - acc: 0.9712 - val_loss: 0.3770 - val_acc: 0.9043\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9702- ETA: 0s - loss: 0.1049 - acc: 0.970\n",
      "Epoch 00143: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.1050 - acc: 0.9702 - val_loss: 0.4932 - val_acc: 0.8786\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9696\n",
      "Epoch 00144: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.1043 - acc: 0.9696 - val_loss: 0.3600 - val_acc: 0.9113\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9716\n",
      "Epoch 00145: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.0977 - acc: 0.9715 - val_loss: 0.3954 - val_acc: 0.8980\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9713\n",
      "Epoch 00146: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.1014 - acc: 0.9713 - val_loss: 0.3942 - val_acc: 0.8963\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9717\n",
      "Epoch 00147: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.0985 - acc: 0.9718 - val_loss: 0.4661 - val_acc: 0.8835\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9716\n",
      "Epoch 00148: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.0999 - acc: 0.9716 - val_loss: 0.4278 - val_acc: 0.8935\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9722\n",
      "Epoch 00149: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.0973 - acc: 0.9722 - val_loss: 0.4072 - val_acc: 0.9047\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9725\n",
      "Epoch 00150: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.0932 - acc: 0.9725 - val_loss: 0.4750 - val_acc: 0.8824\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9732\n",
      "Epoch 00151: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.0933 - acc: 0.9732 - val_loss: 0.4081 - val_acc: 0.9017\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9709\n",
      "Epoch 00152: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.0966 - acc: 0.9710 - val_loss: 0.3530 - val_acc: 0.9117\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9728\n",
      "Epoch 00153: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.0935 - acc: 0.9728 - val_loss: 0.5634 - val_acc: 0.8649\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9747\n",
      "Epoch 00154: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.0917 - acc: 0.9747 - val_loss: 0.3585 - val_acc: 0.9154\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9741\n",
      "Epoch 00155: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.0936 - acc: 0.9740 - val_loss: 0.3679 - val_acc: 0.9133\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9735\n",
      "Epoch 00156: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0907 - acc: 0.9736 - val_loss: 0.3391 - val_acc: 0.9115\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9725\n",
      "Epoch 00157: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.0931 - acc: 0.9724 - val_loss: 0.3588 - val_acc: 0.9150\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9727\n",
      "Epoch 00158: val_loss did not improve from 0.31706\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.0929 - acc: 0.9726 - val_loss: 0.5428 - val_acc: 0.8679\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 422us/sample - loss: 0.3709 - acc: 0.8860\n",
      "Loss: 0.37090363464127696 Accuracy: 0.8859813\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6769 - acc: 0.4949\n",
      "Epoch 00001: val_loss improved from inf to 1.65198, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/001-1.6520.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.6769 - acc: 0.4949 - val_loss: 1.6520 - val_acc: 0.4843\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0530 - acc: 0.7061\n",
      "Epoch 00002: val_loss improved from 1.65198 to 0.94194, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/002-0.9419.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.0531 - acc: 0.7061 - val_loss: 0.9419 - val_acc: 0.7193\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8181 - acc: 0.7783\n",
      "Epoch 00003: val_loss improved from 0.94194 to 0.75407, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/003-0.7541.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8181 - acc: 0.7783 - val_loss: 0.7541 - val_acc: 0.7918\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.8167\n",
      "Epoch 00004: val_loss improved from 0.75407 to 0.63155, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/004-0.6316.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.6773 - acc: 0.8166 - val_loss: 0.6316 - val_acc: 0.8302\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.8433\n",
      "Epoch 00005: val_loss improved from 0.63155 to 0.52715, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/005-0.5272.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.5857 - acc: 0.8433 - val_loss: 0.5272 - val_acc: 0.8642\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8601\n",
      "Epoch 00006: val_loss improved from 0.52715 to 0.51122, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/006-0.5112.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.5221 - acc: 0.8601 - val_loss: 0.5112 - val_acc: 0.8633\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4690 - acc: 0.8727\n",
      "Epoch 00007: val_loss did not improve from 0.51122\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4691 - acc: 0.8726 - val_loss: 0.5427 - val_acc: 0.8449\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8819\n",
      "Epoch 00008: val_loss improved from 0.51122 to 0.39975, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/008-0.3997.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4348 - acc: 0.8819 - val_loss: 0.3997 - val_acc: 0.8912\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8910\n",
      "Epoch 00009: val_loss did not improve from 0.39975\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4002 - acc: 0.8909 - val_loss: 0.6420 - val_acc: 0.7964\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8982\n",
      "Epoch 00010: val_loss did not improve from 0.39975\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3728 - acc: 0.8981 - val_loss: 0.4517 - val_acc: 0.8698\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.9048\n",
      "Epoch 00011: val_loss improved from 0.39975 to 0.34732, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/011-0.3473.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3483 - acc: 0.9048 - val_loss: 0.3473 - val_acc: 0.8994\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.9115- ETA: 0s - loss: 0.3269 - acc: 0\n",
      "Epoch 00012: val_loss did not improve from 0.34732\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3273 - acc: 0.9115 - val_loss: 0.3664 - val_acc: 0.8952\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9143\n",
      "Epoch 00013: val_loss improved from 0.34732 to 0.33426, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/013-0.3343.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3098 - acc: 0.9143 - val_loss: 0.3343 - val_acc: 0.9017\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9180\n",
      "Epoch 00014: val_loss improved from 0.33426 to 0.32851, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/014-0.3285.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2965 - acc: 0.9179 - val_loss: 0.3285 - val_acc: 0.9022\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9220\n",
      "Epoch 00015: val_loss improved from 0.32851 to 0.31587, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/015-0.3159.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2828 - acc: 0.9220 - val_loss: 0.3159 - val_acc: 0.9115\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9255\n",
      "Epoch 00016: val_loss improved from 0.31587 to 0.30630, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/016-0.3063.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2688 - acc: 0.9255 - val_loss: 0.3063 - val_acc: 0.9115\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9277\n",
      "Epoch 00017: val_loss improved from 0.30630 to 0.29730, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/017-0.2973.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2571 - acc: 0.9277 - val_loss: 0.2973 - val_acc: 0.9145\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9321\n",
      "Epoch 00018: val_loss improved from 0.29730 to 0.27959, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/018-0.2796.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2458 - acc: 0.9321 - val_loss: 0.2796 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9326\n",
      "Epoch 00019: val_loss did not improve from 0.27959\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2363 - acc: 0.9326 - val_loss: 0.2847 - val_acc: 0.9152\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9370\n",
      "Epoch 00020: val_loss did not improve from 0.27959\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2243 - acc: 0.9370 - val_loss: 0.3291 - val_acc: 0.9047\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9373\n",
      "Epoch 00021: val_loss did not improve from 0.27959\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2192 - acc: 0.9373 - val_loss: 0.2915 - val_acc: 0.9113\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9410\n",
      "Epoch 00022: val_loss improved from 0.27959 to 0.25139, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/022-0.2514.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2088 - acc: 0.9410 - val_loss: 0.2514 - val_acc: 0.9287\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9414\n",
      "Epoch 00023: val_loss did not improve from 0.25139\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2063 - acc: 0.9414 - val_loss: 0.2642 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9422\n",
      "Epoch 00024: val_loss improved from 0.25139 to 0.24324, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/024-0.2432.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1982 - acc: 0.9421 - val_loss: 0.2432 - val_acc: 0.9276\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9461\n",
      "Epoch 00025: val_loss did not improve from 0.24324\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1923 - acc: 0.9460 - val_loss: 0.2770 - val_acc: 0.9161\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9478\n",
      "Epoch 00026: val_loss did not improve from 0.24324\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1858 - acc: 0.9478 - val_loss: 0.2444 - val_acc: 0.9259\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9495\n",
      "Epoch 00027: val_loss improved from 0.24324 to 0.24185, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/027-0.2419.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1760 - acc: 0.9495 - val_loss: 0.2419 - val_acc: 0.9271\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9507\n",
      "Epoch 00028: val_loss did not improve from 0.24185\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1727 - acc: 0.9506 - val_loss: 0.2478 - val_acc: 0.9222\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9508\n",
      "Epoch 00029: val_loss did not improve from 0.24185\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1685 - acc: 0.9508 - val_loss: 0.2723 - val_acc: 0.9175\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9546\n",
      "Epoch 00030: val_loss improved from 0.24185 to 0.24131, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/030-0.2413.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1606 - acc: 0.9546 - val_loss: 0.2413 - val_acc: 0.9241\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9563\n",
      "Epoch 00031: val_loss did not improve from 0.24131\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1556 - acc: 0.9563 - val_loss: 0.2597 - val_acc: 0.9241\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9557\n",
      "Epoch 00032: val_loss did not improve from 0.24131\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1525 - acc: 0.9557 - val_loss: 0.3338 - val_acc: 0.9010\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9576\n",
      "Epoch 00033: val_loss improved from 0.24131 to 0.23918, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/033-0.2392.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1483 - acc: 0.9576 - val_loss: 0.2392 - val_acc: 0.9271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9588\n",
      "Epoch 00034: val_loss did not improve from 0.23918\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1428 - acc: 0.9588 - val_loss: 0.2557 - val_acc: 0.9217\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9617\n",
      "Epoch 00035: val_loss did not improve from 0.23918\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1391 - acc: 0.9616 - val_loss: 0.2499 - val_acc: 0.9238\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9606\n",
      "Epoch 00036: val_loss improved from 0.23918 to 0.23206, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/036-0.2321.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1401 - acc: 0.9606 - val_loss: 0.2321 - val_acc: 0.9311\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9620\n",
      "Epoch 00037: val_loss did not improve from 0.23206\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1326 - acc: 0.9620 - val_loss: 0.2579 - val_acc: 0.9236\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9636\n",
      "Epoch 00038: val_loss improved from 0.23206 to 0.22853, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/038-0.2285.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1280 - acc: 0.9636 - val_loss: 0.2285 - val_acc: 0.9287\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9647\n",
      "Epoch 00039: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1239 - acc: 0.9647 - val_loss: 0.2548 - val_acc: 0.9224\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9631\n",
      "Epoch 00040: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1286 - acc: 0.9631 - val_loss: 0.2523 - val_acc: 0.9234\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9683\n",
      "Epoch 00041: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1147 - acc: 0.9683 - val_loss: 0.2410 - val_acc: 0.9266\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9682\n",
      "Epoch 00042: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1136 - acc: 0.9682 - val_loss: 0.2364 - val_acc: 0.9299\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9679\n",
      "Epoch 00043: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1107 - acc: 0.9679 - val_loss: 0.2291 - val_acc: 0.9329\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9701\n",
      "Epoch 00044: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1079 - acc: 0.9701 - val_loss: 0.2640 - val_acc: 0.9245\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9696\n",
      "Epoch 00045: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1097 - acc: 0.9696 - val_loss: 0.2478 - val_acc: 0.9264\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9710\n",
      "Epoch 00046: val_loss did not improve from 0.22853\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1011 - acc: 0.9710 - val_loss: 0.2659 - val_acc: 0.9241\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9708\n",
      "Epoch 00047: val_loss improved from 0.22853 to 0.22709, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/047-0.2271.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1025 - acc: 0.9708 - val_loss: 0.2271 - val_acc: 0.9373\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9723\n",
      "Epoch 00048: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0981 - acc: 0.9723 - val_loss: 0.2336 - val_acc: 0.9324\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9737\n",
      "Epoch 00049: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0944 - acc: 0.9737 - val_loss: 0.2382 - val_acc: 0.9299\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9748\n",
      "Epoch 00050: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0923 - acc: 0.9748 - val_loss: 0.2275 - val_acc: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9756\n",
      "Epoch 00051: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0904 - acc: 0.9756 - val_loss: 0.2380 - val_acc: 0.9341\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9740\n",
      "Epoch 00052: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.0906 - acc: 0.9739 - val_loss: 0.2351 - val_acc: 0.9320\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9766\n",
      "Epoch 00053: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0849 - acc: 0.9766 - val_loss: 0.2275 - val_acc: 0.9329\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9784\n",
      "Epoch 00054: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0806 - acc: 0.9784 - val_loss: 0.2549 - val_acc: 0.9266\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9786\n",
      "Epoch 00055: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0805 - acc: 0.9786 - val_loss: 0.2510 - val_acc: 0.9245\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9778\n",
      "Epoch 00056: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0818 - acc: 0.9778 - val_loss: 0.2582 - val_acc: 0.9278\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9775\n",
      "Epoch 00057: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0790 - acc: 0.9775 - val_loss: 0.3035 - val_acc: 0.9145\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9802\n",
      "Epoch 00058: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0731 - acc: 0.9802 - val_loss: 0.2627 - val_acc: 0.9280\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9803\n",
      "Epoch 00059: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0737 - acc: 0.9802 - val_loss: 0.2718 - val_acc: 0.9227\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9793\n",
      "Epoch 00060: val_loss did not improve from 0.22709\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0756 - acc: 0.9793 - val_loss: 0.3037 - val_acc: 0.9178\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9815\n",
      "Epoch 00061: val_loss improved from 0.22709 to 0.22277, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv_checkpoint/061-0.2228.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0691 - acc: 0.9816 - val_loss: 0.2228 - val_acc: 0.9392\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9818\n",
      "Epoch 00062: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0679 - acc: 0.9818 - val_loss: 0.2384 - val_acc: 0.9304\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9824\n",
      "Epoch 00063: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0669 - acc: 0.9823 - val_loss: 0.2928 - val_acc: 0.9201\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9831\n",
      "Epoch 00064: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0629 - acc: 0.9831 - val_loss: 0.2613 - val_acc: 0.9285\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9811\n",
      "Epoch 00065: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0689 - acc: 0.9811 - val_loss: 0.2336 - val_acc: 0.9334\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9851\n",
      "Epoch 00066: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0600 - acc: 0.9851 - val_loss: 0.2614 - val_acc: 0.9273\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9849\n",
      "Epoch 00067: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0587 - acc: 0.9849 - val_loss: 0.2884 - val_acc: 0.9199\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9860\n",
      "Epoch 00068: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0555 - acc: 0.9860 - val_loss: 0.2530 - val_acc: 0.9317\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9858\n",
      "Epoch 00069: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0554 - acc: 0.9858 - val_loss: 0.3265 - val_acc: 0.9166\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9864\n",
      "Epoch 00070: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0543 - acc: 0.9864 - val_loss: 0.2826 - val_acc: 0.9248\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9855\n",
      "Epoch 00071: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0570 - acc: 0.9855 - val_loss: 0.2856 - val_acc: 0.9257\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9879\n",
      "Epoch 00072: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0490 - acc: 0.9879 - val_loss: 0.2746 - val_acc: 0.9278\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9876\n",
      "Epoch 00073: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0510 - acc: 0.9875 - val_loss: 0.2947 - val_acc: 0.9147\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9862\n",
      "Epoch 00074: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0536 - acc: 0.9862 - val_loss: 0.3050 - val_acc: 0.9234\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9865\n",
      "Epoch 00075: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0520 - acc: 0.9865 - val_loss: 0.2736 - val_acc: 0.9276\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9857\n",
      "Epoch 00076: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0518 - acc: 0.9857 - val_loss: 0.2553 - val_acc: 0.9299\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9903\n",
      "Epoch 00077: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0425 - acc: 0.9903 - val_loss: 0.2868 - val_acc: 0.9236\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9882\n",
      "Epoch 00078: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0479 - acc: 0.9882 - val_loss: 0.2447 - val_acc: 0.9317\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9880\n",
      "Epoch 00079: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0460 - acc: 0.9880 - val_loss: 0.2852 - val_acc: 0.9234\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9881\n",
      "Epoch 00080: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0460 - acc: 0.9881 - val_loss: 0.2701 - val_acc: 0.9308\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9904\n",
      "Epoch 00081: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0401 - acc: 0.9903 - val_loss: 0.2666 - val_acc: 0.9329\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9879\n",
      "Epoch 00082: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0458 - acc: 0.9879 - val_loss: 0.2664 - val_acc: 0.9266\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9903\n",
      "Epoch 00083: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0398 - acc: 0.9903 - val_loss: 0.2781 - val_acc: 0.9283\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9902\n",
      "Epoch 00084: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0417 - acc: 0.9902 - val_loss: 0.2705 - val_acc: 0.9311\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9910\n",
      "Epoch 00085: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0389 - acc: 0.9910 - val_loss: 0.2619 - val_acc: 0.9327\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9907\n",
      "Epoch 00086: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0382 - acc: 0.9907 - val_loss: 0.3035 - val_acc: 0.9255\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9907\n",
      "Epoch 00087: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0384 - acc: 0.9907 - val_loss: 0.2640 - val_acc: 0.9271\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9904\n",
      "Epoch 00088: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0381 - acc: 0.9904 - val_loss: 0.2809 - val_acc: 0.9276\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9906\n",
      "Epoch 00089: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0384 - acc: 0.9905 - val_loss: 0.2487 - val_acc: 0.9357\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9897\n",
      "Epoch 00090: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0413 - acc: 0.9897 - val_loss: 0.2586 - val_acc: 0.9299\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9904\n",
      "Epoch 00091: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0379 - acc: 0.9904 - val_loss: 0.3185 - val_acc: 0.9180\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9924\n",
      "Epoch 00092: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0320 - acc: 0.9924 - val_loss: 0.2872 - val_acc: 0.9297\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9904\n",
      "Epoch 00093: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0371 - acc: 0.9903 - val_loss: 0.3197 - val_acc: 0.9208\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9909\n",
      "Epoch 00094: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0374 - acc: 0.9909 - val_loss: 0.2642 - val_acc: 0.9359\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9932\n",
      "Epoch 00095: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0301 - acc: 0.9931 - val_loss: 0.2663 - val_acc: 0.9380\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9919\n",
      "Epoch 00096: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0336 - acc: 0.9918 - val_loss: 0.4373 - val_acc: 0.8926\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9921\n",
      "Epoch 00097: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0336 - acc: 0.9921 - val_loss: 0.2871 - val_acc: 0.9313\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9924\n",
      "Epoch 00098: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0312 - acc: 0.9924 - val_loss: 0.3102 - val_acc: 0.9241\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9920\n",
      "Epoch 00099: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0327 - acc: 0.9920 - val_loss: 0.2931 - val_acc: 0.9304\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9930\n",
      "Epoch 00100: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0297 - acc: 0.9930 - val_loss: 0.3143 - val_acc: 0.9264\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9924\n",
      "Epoch 00101: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0329 - acc: 0.9924 - val_loss: 0.3001 - val_acc: 0.9243\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9924\n",
      "Epoch 00102: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0318 - acc: 0.9923 - val_loss: 0.2660 - val_acc: 0.9317\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9927\n",
      "Epoch 00103: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.0301 - acc: 0.9927 - val_loss: 0.2620 - val_acc: 0.9355\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9939\n",
      "Epoch 00104: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0273 - acc: 0.9939 - val_loss: 0.2900 - val_acc: 0.9311\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9950\n",
      "Epoch 00105: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.0253 - acc: 0.9950 - val_loss: 0.3178 - val_acc: 0.9208\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9934\n",
      "Epoch 00106: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0283 - acc: 0.9934 - val_loss: 0.2883 - val_acc: 0.9276\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9929\n",
      "Epoch 00107: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0289 - acc: 0.9929 - val_loss: 0.2971 - val_acc: 0.9238\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9910\n",
      "Epoch 00108: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0329 - acc: 0.9910 - val_loss: 0.2899 - val_acc: 0.9266\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9961\n",
      "Epoch 00109: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0218 - acc: 0.9961 - val_loss: 0.2876 - val_acc: 0.9299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9934\n",
      "Epoch 00110: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0268 - acc: 0.9933 - val_loss: 0.3228 - val_acc: 0.9278\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9939\n",
      "Epoch 00111: val_loss did not improve from 0.22277\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0267 - acc: 0.9939 - val_loss: 0.2627 - val_acc: 0.9364\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 483us/sample - loss: 0.2754 - acc: 0.9252\n",
      "Loss: 0.2753885194272391 Accuracy: 0.92523366\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4730 - acc: 0.5617\n",
      "Epoch 00001: val_loss improved from inf to 1.27073, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/001-1.2707.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.4730 - acc: 0.5617 - val_loss: 1.2707 - val_acc: 0.6345\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8105 - acc: 0.7757\n",
      "Epoch 00002: val_loss improved from 1.27073 to 0.67689, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/002-0.6769.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.8106 - acc: 0.7757 - val_loss: 0.6769 - val_acc: 0.8104\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8370\n",
      "Epoch 00003: val_loss improved from 0.67689 to 0.50453, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/003-0.5045.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.5943 - acc: 0.8370 - val_loss: 0.5045 - val_acc: 0.8719\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4765 - acc: 0.8702\n",
      "Epoch 00004: val_loss improved from 0.50453 to 0.48359, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/004-0.4836.hdf5\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.4765 - acc: 0.8702 - val_loss: 0.4836 - val_acc: 0.8612\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8905\n",
      "Epoch 00005: val_loss improved from 0.48359 to 0.37149, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/005-0.3715.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.4027 - acc: 0.8904 - val_loss: 0.3715 - val_acc: 0.9045\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.9048\n",
      "Epoch 00006: val_loss did not improve from 0.37149\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.3531 - acc: 0.9047 - val_loss: 0.4418 - val_acc: 0.8726\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.9156\n",
      "Epoch 00007: val_loss improved from 0.37149 to 0.30330, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/007-0.3033.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.3105 - acc: 0.9156 - val_loss: 0.3033 - val_acc: 0.9203\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9223\n",
      "Epoch 00008: val_loss did not improve from 0.30330\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.2829 - acc: 0.9223 - val_loss: 0.3227 - val_acc: 0.9022\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9285\n",
      "Epoch 00009: val_loss did not improve from 0.30330\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2574 - acc: 0.9285 - val_loss: 0.3149 - val_acc: 0.9071\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9364\n",
      "Epoch 00010: val_loss improved from 0.30330 to 0.27135, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/010-0.2714.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2350 - acc: 0.9364 - val_loss: 0.2714 - val_acc: 0.9229\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9392\n",
      "Epoch 00011: val_loss improved from 0.27135 to 0.25779, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/011-0.2578.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2181 - acc: 0.9392 - val_loss: 0.2578 - val_acc: 0.9269\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9435\n",
      "Epoch 00012: val_loss improved from 0.25779 to 0.24107, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/012-0.2411.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.2031 - acc: 0.9435 - val_loss: 0.2411 - val_acc: 0.9329\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9469\n",
      "Epoch 00013: val_loss did not improve from 0.24107\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1912 - acc: 0.9469 - val_loss: 0.2468 - val_acc: 0.9317\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9511\n",
      "Epoch 00014: val_loss improved from 0.24107 to 0.21745, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/014-0.2175.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1778 - acc: 0.9511 - val_loss: 0.2175 - val_acc: 0.9373\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9534\n",
      "Epoch 00015: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1679 - acc: 0.9533 - val_loss: 0.2360 - val_acc: 0.9359\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9549\n",
      "Epoch 00016: val_loss did not improve from 0.21745\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1579 - acc: 0.9549 - val_loss: 0.2240 - val_acc: 0.9322\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9573\n",
      "Epoch 00017: val_loss improved from 0.21745 to 0.21060, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/017-0.2106.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1502 - acc: 0.9573 - val_loss: 0.2106 - val_acc: 0.9404\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9611\n",
      "Epoch 00018: val_loss did not improve from 0.21060\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1399 - acc: 0.9611 - val_loss: 0.2556 - val_acc: 0.9236\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9626\n",
      "Epoch 00019: val_loss did not improve from 0.21060\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.1332 - acc: 0.9626 - val_loss: 0.2191 - val_acc: 0.9336\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9647\n",
      "Epoch 00020: val_loss improved from 0.21060 to 0.19738, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/020-0.1974.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.1278 - acc: 0.9647 - val_loss: 0.1974 - val_acc: 0.9429\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9670\n",
      "Epoch 00021: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1192 - acc: 0.9670 - val_loss: 0.2092 - val_acc: 0.9364\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9687\n",
      "Epoch 00022: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1130 - acc: 0.9687 - val_loss: 0.2155 - val_acc: 0.9345\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9703\n",
      "Epoch 00023: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1078 - acc: 0.9703 - val_loss: 0.2220 - val_acc: 0.9406\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9717\n",
      "Epoch 00024: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1018 - acc: 0.9716 - val_loss: 0.2346 - val_acc: 0.9301\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9734\n",
      "Epoch 00025: val_loss improved from 0.19738 to 0.19447, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/025-0.1945.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0975 - acc: 0.9734 - val_loss: 0.1945 - val_acc: 0.9371\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9760\n",
      "Epoch 00026: val_loss did not improve from 0.19447\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0901 - acc: 0.9760 - val_loss: 0.2059 - val_acc: 0.9390\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9761\n",
      "Epoch 00027: val_loss improved from 0.19447 to 0.18977, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/027-0.1898.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0878 - acc: 0.9761 - val_loss: 0.1898 - val_acc: 0.9443\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9776\n",
      "Epoch 00028: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0833 - acc: 0.9775 - val_loss: 0.2038 - val_acc: 0.9415\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9782\n",
      "Epoch 00029: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0815 - acc: 0.9782 - val_loss: 0.2027 - val_acc: 0.9422\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9801\n",
      "Epoch 00030: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0757 - acc: 0.9801 - val_loss: 0.2111 - val_acc: 0.9401\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9835\n",
      "Epoch 00031: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0662 - acc: 0.9835 - val_loss: 0.1984 - val_acc: 0.9418\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9799\n",
      "Epoch 00032: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0712 - acc: 0.9799 - val_loss: 0.2026 - val_acc: 0.9411\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9832\n",
      "Epoch 00033: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0645 - acc: 0.9832 - val_loss: 0.2089 - val_acc: 0.9411\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9854\n",
      "Epoch 00034: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0585 - acc: 0.9854 - val_loss: 0.2585 - val_acc: 0.9264\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9843\n",
      "Epoch 00035: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0590 - acc: 0.9843 - val_loss: 0.2388 - val_acc: 0.9322\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9852\n",
      "Epoch 00036: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0583 - acc: 0.9852 - val_loss: 0.1988 - val_acc: 0.9460\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9853\n",
      "Epoch 00037: val_loss did not improve from 0.18977\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0559 - acc: 0.9852 - val_loss: 0.2129 - val_acc: 0.9411\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9850\n",
      "Epoch 00038: val_loss improved from 0.18977 to 0.18969, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv_checkpoint/038-0.1897.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0565 - acc: 0.9850 - val_loss: 0.1897 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9889\n",
      "Epoch 00039: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0449 - acc: 0.9889 - val_loss: 0.2160 - val_acc: 0.9422\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9891\n",
      "Epoch 00040: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0459 - acc: 0.9891 - val_loss: 0.2184 - val_acc: 0.9399\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9900\n",
      "Epoch 00041: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0419 - acc: 0.9900 - val_loss: 0.2207 - val_acc: 0.9359\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9900\n",
      "Epoch 00042: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0411 - acc: 0.9900 - val_loss: 0.2376 - val_acc: 0.9338\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9905\n",
      "Epoch 00043: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0399 - acc: 0.9905 - val_loss: 0.2305 - val_acc: 0.9355\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9912\n",
      "Epoch 00044: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0393 - acc: 0.9912 - val_loss: 0.2063 - val_acc: 0.9453\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9910\n",
      "Epoch 00045: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0370 - acc: 0.9910 - val_loss: 0.2364 - val_acc: 0.9352\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9926\n",
      "Epoch 00046: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0347 - acc: 0.9926 - val_loss: 0.2147 - val_acc: 0.9425\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0413 - acc: 0.9898 - val_loss: 0.2183 - val_acc: 0.9404\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9940\n",
      "Epoch 00048: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0292 - acc: 0.9939 - val_loss: 0.2113 - val_acc: 0.9415\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9929\n",
      "Epoch 00049: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0330 - acc: 0.9929 - val_loss: 0.2279 - val_acc: 0.9429\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9933\n",
      "Epoch 00050: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0298 - acc: 0.9933 - val_loss: 0.2398 - val_acc: 0.9436\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9925\n",
      "Epoch 00051: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0302 - acc: 0.9925 - val_loss: 0.2515 - val_acc: 0.9334\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9909\n",
      "Epoch 00052: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0365 - acc: 0.9909 - val_loss: 0.2111 - val_acc: 0.9455\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9940\n",
      "Epoch 00053: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0261 - acc: 0.9940 - val_loss: 0.2221 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9937\n",
      "Epoch 00054: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0280 - acc: 0.9938 - val_loss: 0.2332 - val_acc: 0.9418\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9960\n",
      "Epoch 00055: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0215 - acc: 0.9959 - val_loss: 0.2605 - val_acc: 0.9355\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9909\n",
      "Epoch 00056: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0373 - acc: 0.9909 - val_loss: 0.2319 - val_acc: 0.9383\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9937\n",
      "Epoch 00057: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0288 - acc: 0.9936 - val_loss: 0.2303 - val_acc: 0.9387\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9933\n",
      "Epoch 00058: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0290 - acc: 0.9933 - val_loss: 0.2116 - val_acc: 0.9457\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9953\n",
      "Epoch 00059: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0230 - acc: 0.9953 - val_loss: 0.2545 - val_acc: 0.9338\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9945\n",
      "Epoch 00060: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0245 - acc: 0.9945 - val_loss: 0.2298 - val_acc: 0.9422\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9970\n",
      "Epoch 00061: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0174 - acc: 0.9970 - val_loss: 0.2187 - val_acc: 0.9448\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0188 - acc: 0.9963 - val_loss: 0.2535 - val_acc: 0.9392\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9956\n",
      "Epoch 00063: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0200 - acc: 0.9956 - val_loss: 0.2888 - val_acc: 0.9257\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9940\n",
      "Epoch 00064: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0246 - acc: 0.9940 - val_loss: 0.2584 - val_acc: 0.9383\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9941\n",
      "Epoch 00065: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0259 - acc: 0.9941 - val_loss: 0.2387 - val_acc: 0.9399\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9964\n",
      "Epoch 00066: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0180 - acc: 0.9964 - val_loss: 0.2456 - val_acc: 0.9383\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9972\n",
      "Epoch 00067: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0155 - acc: 0.9971 - val_loss: 0.2351 - val_acc: 0.9436\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9915\n",
      "Epoch 00068: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0318 - acc: 0.9915 - val_loss: 0.2274 - val_acc: 0.9436\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9947\n",
      "Epoch 00069: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0226 - acc: 0.9947 - val_loss: 0.2110 - val_acc: 0.9460\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0143 - acc: 0.9970 - val_loss: 0.2441 - val_acc: 0.9415\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9975\n",
      "Epoch 00071: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0135 - acc: 0.9975 - val_loss: 0.2245 - val_acc: 0.9432\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9972\n",
      "Epoch 00072: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0142 - acc: 0.9972 - val_loss: 0.2271 - val_acc: 0.9422\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9976\n",
      "Epoch 00073: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0135 - acc: 0.9976 - val_loss: 0.2312 - val_acc: 0.9448\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9964\n",
      "Epoch 00074: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0163 - acc: 0.9963 - val_loss: 0.2356 - val_acc: 0.9460\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9938\n",
      "Epoch 00075: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0240 - acc: 0.9938 - val_loss: 0.2746 - val_acc: 0.9345\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9950\n",
      "Epoch 00076: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0207 - acc: 0.9949 - val_loss: 0.2437 - val_acc: 0.9457\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0172 - acc: 0.9963 - val_loss: 0.2352 - val_acc: 0.9476\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9979\n",
      "Epoch 00078: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0120 - acc: 0.9979 - val_loss: 0.2488 - val_acc: 0.9387\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9987\n",
      "Epoch 00079: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0105 - acc: 0.9987 - val_loss: 0.2481 - val_acc: 0.9448\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9945\n",
      "Epoch 00080: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0230 - acc: 0.9944 - val_loss: 0.2377 - val_acc: 0.9460\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9954\n",
      "Epoch 00081: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0196 - acc: 0.9954 - val_loss: 0.2301 - val_acc: 0.9490\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9987\n",
      "Epoch 00082: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0093 - acc: 0.9987 - val_loss: 0.2477 - val_acc: 0.9457\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9988\n",
      "Epoch 00083: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0084 - acc: 0.9988 - val_loss: 0.2438 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9949\n",
      "Epoch 00084: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0205 - acc: 0.9949 - val_loss: 0.2446 - val_acc: 0.9429\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9982\n",
      "Epoch 00085: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0106 - acc: 0.9982 - val_loss: 0.2351 - val_acc: 0.9457\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9986\n",
      "Epoch 00086: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0093 - acc: 0.9986 - val_loss: 0.2911 - val_acc: 0.9383\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9949\n",
      "Epoch 00087: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0192 - acc: 0.9949 - val_loss: 0.3311 - val_acc: 0.9324\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9952\n",
      "Epoch 00088: val_loss did not improve from 0.18969\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0190 - acc: 0.9952 - val_loss: 0.2489 - val_acc: 0.9394\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 485us/sample - loss: 0.2501 - acc: 0.9300\n",
      "Loss: 0.25009127608959797 Accuracy: 0.9300104\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5330 - acc: 0.5314\n",
      "Epoch 00001: val_loss improved from inf to 1.51663, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/001-1.5166.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.5329 - acc: 0.5314 - val_loss: 1.5166 - val_acc: 0.5101\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7422 - acc: 0.7886\n",
      "Epoch 00002: val_loss improved from 1.51663 to 0.56937, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/002-0.5694.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.7422 - acc: 0.7886 - val_loss: 0.5694 - val_acc: 0.8444\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5119 - acc: 0.8531\n",
      "Epoch 00003: val_loss improved from 0.56937 to 0.40744, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/003-0.4074.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.5119 - acc: 0.8531 - val_loss: 0.4074 - val_acc: 0.8877\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8833\n",
      "Epoch 00004: val_loss improved from 0.40744 to 0.33855, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/004-0.3385.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.4040 - acc: 0.8832 - val_loss: 0.3385 - val_acc: 0.9066\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.9042\n",
      "Epoch 00005: val_loss improved from 0.33855 to 0.32290, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/005-0.3229.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.3375 - acc: 0.9041 - val_loss: 0.3229 - val_acc: 0.9036\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9161\n",
      "Epoch 00006: val_loss improved from 0.32290 to 0.28096, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/006-0.2810.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.2905 - acc: 0.9160 - val_loss: 0.2810 - val_acc: 0.9192\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9268\n",
      "Epoch 00007: val_loss improved from 0.28096 to 0.26404, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/007-0.2640.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.2535 - acc: 0.9268 - val_loss: 0.2640 - val_acc: 0.9224\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9342\n",
      "Epoch 00008: val_loss improved from 0.26404 to 0.23447, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/008-0.2345.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2283 - acc: 0.9342 - val_loss: 0.2345 - val_acc: 0.9311\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9420\n",
      "Epoch 00009: val_loss improved from 0.23447 to 0.23136, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/009-0.2314.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2024 - acc: 0.9420 - val_loss: 0.2314 - val_acc: 0.9306\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9455\n",
      "Epoch 00010: val_loss improved from 0.23136 to 0.20648, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/010-0.2065.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1864 - acc: 0.9454 - val_loss: 0.2065 - val_acc: 0.9411\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9511\n",
      "Epoch 00011: val_loss did not improve from 0.20648\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1694 - acc: 0.9510 - val_loss: 0.2414 - val_acc: 0.9250\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9534\n",
      "Epoch 00012: val_loss did not improve from 0.20648\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.1599 - acc: 0.9534 - val_loss: 0.2124 - val_acc: 0.9329\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9596\n",
      "Epoch 00013: val_loss improved from 0.20648 to 0.18480, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/013-0.1848.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1415 - acc: 0.9595 - val_loss: 0.1848 - val_acc: 0.9441\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9616\n",
      "Epoch 00014: val_loss did not improve from 0.18480\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1332 - acc: 0.9616 - val_loss: 0.2025 - val_acc: 0.9415\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9652\n",
      "Epoch 00015: val_loss did not improve from 0.18480\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1210 - acc: 0.9652 - val_loss: 0.1883 - val_acc: 0.9448\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9690\n",
      "Epoch 00016: val_loss improved from 0.18480 to 0.18126, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/016-0.1813.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1115 - acc: 0.9690 - val_loss: 0.1813 - val_acc: 0.9467\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9705\n",
      "Epoch 00017: val_loss did not improve from 0.18126\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1039 - acc: 0.9705 - val_loss: 0.1981 - val_acc: 0.9380\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9717\n",
      "Epoch 00018: val_loss did not improve from 0.18126\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0985 - acc: 0.9716 - val_loss: 0.2044 - val_acc: 0.9380\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9757\n",
      "Epoch 00019: val_loss improved from 0.18126 to 0.17905, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/019-0.1790.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0895 - acc: 0.9757 - val_loss: 0.1790 - val_acc: 0.9453\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9779\n",
      "Epoch 00020: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0818 - acc: 0.9779 - val_loss: 0.2022 - val_acc: 0.9408\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9799\n",
      "Epoch 00021: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0755 - acc: 0.9799 - val_loss: 0.2011 - val_acc: 0.9397\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9819\n",
      "Epoch 00022: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0696 - acc: 0.9819 - val_loss: 0.1928 - val_acc: 0.9427\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9822\n",
      "Epoch 00023: val_loss did not improve from 0.17905\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0658 - acc: 0.9822 - val_loss: 0.2128 - val_acc: 0.9383\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9853\n",
      "Epoch 00024: val_loss improved from 0.17905 to 0.17567, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv_checkpoint/024-0.1757.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0587 - acc: 0.9853 - val_loss: 0.1757 - val_acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9863\n",
      "Epoch 00025: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0544 - acc: 0.9863 - val_loss: 0.1872 - val_acc: 0.9429\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9858\n",
      "Epoch 00026: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0532 - acc: 0.9858 - val_loss: 0.2025 - val_acc: 0.9362\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9872\n",
      "Epoch 00027: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0514 - acc: 0.9872 - val_loss: 0.1945 - val_acc: 0.9387\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9878\n",
      "Epoch 00028: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0471 - acc: 0.9878 - val_loss: 0.2081 - val_acc: 0.9383\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9908\n",
      "Epoch 00029: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0393 - acc: 0.9907 - val_loss: 0.1924 - val_acc: 0.9455\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9900\n",
      "Epoch 00030: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0418 - acc: 0.9899 - val_loss: 0.2181 - val_acc: 0.9378\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9906\n",
      "Epoch 00031: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0405 - acc: 0.9906 - val_loss: 0.2033 - val_acc: 0.9413\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9913\n",
      "Epoch 00032: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0356 - acc: 0.9913 - val_loss: 0.2208 - val_acc: 0.9387\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9931\n",
      "Epoch 00033: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0314 - acc: 0.9930 - val_loss: 0.2055 - val_acc: 0.9432\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9892\n",
      "Epoch 00034: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0407 - acc: 0.9891 - val_loss: 0.1994 - val_acc: 0.9413\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9911\n",
      "Epoch 00035: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0359 - acc: 0.9911 - val_loss: 0.2095 - val_acc: 0.9439\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9923\n",
      "Epoch 00036: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0317 - acc: 0.9923 - val_loss: 0.1866 - val_acc: 0.9467\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9935\n",
      "Epoch 00037: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0299 - acc: 0.9935 - val_loss: 0.2489 - val_acc: 0.9341\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9926\n",
      "Epoch 00038: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0306 - acc: 0.9926 - val_loss: 0.1868 - val_acc: 0.9520\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9949\n",
      "Epoch 00039: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0229 - acc: 0.9949 - val_loss: 0.1891 - val_acc: 0.9469\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9964\n",
      "Epoch 00040: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0197 - acc: 0.9964 - val_loss: 0.2152 - val_acc: 0.9429\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9953\n",
      "Epoch 00041: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0221 - acc: 0.9953 - val_loss: 0.2138 - val_acc: 0.9464\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9959\n",
      "Epoch 00042: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0209 - acc: 0.9959 - val_loss: 0.2079 - val_acc: 0.9429\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9957\n",
      "Epoch 00043: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0198 - acc: 0.9957 - val_loss: 0.2025 - val_acc: 0.9476\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9955\n",
      "Epoch 00044: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0205 - acc: 0.9955 - val_loss: 0.2231 - val_acc: 0.9427\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9943\n",
      "Epoch 00045: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0232 - acc: 0.9943 - val_loss: 0.2331 - val_acc: 0.9432\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9942\n",
      "Epoch 00046: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0234 - acc: 0.9941 - val_loss: 0.2085 - val_acc: 0.9464\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9949\n",
      "Epoch 00047: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0233 - acc: 0.9949 - val_loss: 0.2085 - val_acc: 0.9481\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9955\n",
      "Epoch 00048: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0196 - acc: 0.9955 - val_loss: 0.2070 - val_acc: 0.9469\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9974\n",
      "Epoch 00049: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0138 - acc: 0.9974 - val_loss: 0.2030 - val_acc: 0.9520\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9973\n",
      "Epoch 00050: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0155 - acc: 0.9972 - val_loss: 0.2071 - val_acc: 0.9450\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9941\n",
      "Epoch 00051: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0235 - acc: 0.9941 - val_loss: 0.1993 - val_acc: 0.9490\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9979\n",
      "Epoch 00052: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0113 - acc: 0.9979 - val_loss: 0.2052 - val_acc: 0.9495\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9976\n",
      "Epoch 00053: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0122 - acc: 0.9976 - val_loss: 0.2533 - val_acc: 0.9422\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9933\n",
      "Epoch 00054: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0250 - acc: 0.9933 - val_loss: 0.2065 - val_acc: 0.9457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9978\n",
      "Epoch 00055: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0115 - acc: 0.9977 - val_loss: 0.2305 - val_acc: 0.9443\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9958\n",
      "Epoch 00056: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0169 - acc: 0.9958 - val_loss: 0.1990 - val_acc: 0.9506\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9948\n",
      "Epoch 00057: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0215 - acc: 0.9947 - val_loss: 0.1961 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9957\n",
      "Epoch 00058: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 0.1981 - val_acc: 0.9522\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 00059: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0111 - acc: 0.9978 - val_loss: 0.2043 - val_acc: 0.9492\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0169 - acc: 0.9955 - val_loss: 0.2251 - val_acc: 0.9483\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9985\n",
      "Epoch 00061: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0087 - acc: 0.9985 - val_loss: 0.2072 - val_acc: 0.9485\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9983\n",
      "Epoch 00062: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0091 - acc: 0.9983 - val_loss: 0.2432 - val_acc: 0.9420\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9965\n",
      "Epoch 00063: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0141 - acc: 0.9965 - val_loss: 0.2458 - val_acc: 0.9441\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9959\n",
      "Epoch 00064: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0160 - acc: 0.9959 - val_loss: 0.2146 - val_acc: 0.9504\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9982\n",
      "Epoch 00065: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0094 - acc: 0.9982 - val_loss: 0.2183 - val_acc: 0.9499\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9949\n",
      "Epoch 00066: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0188 - acc: 0.9948 - val_loss: 0.2399 - val_acc: 0.9455\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9954\n",
      "Epoch 00067: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0163 - acc: 0.9954 - val_loss: 0.2060 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9979\n",
      "Epoch 00068: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0102 - acc: 0.9979 - val_loss: 0.2240 - val_acc: 0.9497\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9984\n",
      "Epoch 00069: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0083 - acc: 0.9984 - val_loss: 0.2258 - val_acc: 0.9467\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 00070: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0097 - acc: 0.9977 - val_loss: 0.2494 - val_acc: 0.9394\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9939\n",
      "Epoch 00071: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0204 - acc: 0.9939 - val_loss: 0.2217 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
      "Epoch 00072: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0081 - acc: 0.9985 - val_loss: 0.2242 - val_acc: 0.9483\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9941\n",
      "Epoch 00073: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0202 - acc: 0.9941 - val_loss: 0.2116 - val_acc: 0.9490\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9990\n",
      "Epoch 00074: val_loss did not improve from 0.17567\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0064 - acc: 0.9990 - val_loss: 0.2147 - val_acc: 0.9504\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 529us/sample - loss: 0.2623 - acc: 0.9261\n",
      "Loss: 0.26232732015111615 Accuracy: 0.9260644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GAP_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 96)           384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 511us/sample - loss: 0.8552 - acc: 0.7443\n",
      "Loss: 0.8551614138691349 Accuracy: 0.7443406\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 96)           384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 525us/sample - loss: 0.7048 - acc: 0.7981\n",
      "Loss: 0.7048168689910002 Accuracy: 0.79813087\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 128)          512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 561us/sample - loss: 0.3709 - acc: 0.8860\n",
      "Loss: 0.37090363464127696 Accuracy: 0.8859813\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 160)          640         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 592us/sample - loss: 0.2754 - acc: 0.9252\n",
      "Loss: 0.2753885194272391 Accuracy: 0.92523366\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 192)          768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 607us/sample - loss: 0.2501 - acc: 0.9300\n",
      "Loss: 0.25009127608959797 Accuracy: 0.9300104\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 192)          768         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 633us/sample - loss: 0.2623 - acc: 0.9261\n",
      "Loss: 0.26232732015111615 Accuracy: 0.9260644\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GAP_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 32)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 96)           0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 96)           384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1552        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,816\n",
      "Trainable params: 12,432\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 577us/sample - loss: 0.9686 - acc: 0.7065\n",
      "Loss: 0.9686453261974568 Accuracy: 0.7065421\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 32)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96)           0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 96)           384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1552        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 18,096\n",
      "Trainable params: 17,648\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 620us/sample - loss: 0.8263 - acc: 0.7607\n",
      "Loss: 0.826322969704526 Accuracy: 0.7607477\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 32)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 128)          512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2064        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 29,296\n",
      "Trainable params: 28,656\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 636us/sample - loss: 0.6091 - acc: 0.8378\n",
      "Loss: 0.6090822651131255 Accuracy: 0.83779854\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 32)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 160)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 160)          640         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2576        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,736\n",
      "Trainable params: 49,904\n",
      "Non-trainable params: 832\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 677us/sample - loss: 0.3188 - acc: 0.9234\n",
      "Loss: 0.318825320322318 Accuracy: 0.92336446\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 64)           0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 192)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 192)          768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           3088        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 72,176\n",
      "Trainable params: 71,152\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 740us/sample - loss: 0.3130 - acc: 0.9254\n",
      "Loss: 0.3130183479558146 Accuracy: 0.9254413\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 64)           0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 192)          768         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           3088        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 92,976\n",
      "Trainable params: 91,824\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 765us/sample - loss: 0.3095 - acc: 0.9315\n",
      "Loss: 0.3094815848079952 Accuracy: 0.9314642\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
