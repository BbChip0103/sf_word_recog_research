{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('/users/jhlee/data/img_snd/train_data.npz')\n",
    "test_data = np.load('/users/jhlee/data/img_snd/test_data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['X_train']\n",
    "y_train = train_data['y_train']\n",
    "X_test = test_data['X_test']\n",
    "y_test = test_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = (99, 257)\n",
    "img_dim_crop = (99, 257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=64, \n",
    "                    shuffle=False, random_seed=None):\n",
    "    \n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wrapper functions \n",
    "\n",
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels, \n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ##   input tensor shape: \n",
    "        ##   [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1] \n",
    "\n",
    "        weights_shape = (list(kernel_size) + \n",
    "                         [n_input_channels, n_output_channels])\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_channels]))\n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input=input_tensor, \n",
    "                            filter=weights,\n",
    "                            strides=strides, \n",
    "                            padding=padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases, \n",
    "                              name='net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name='activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(input_tensor, name, \n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor, \n",
    "                                      shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                              name='net_pre-activation')\n",
    "        print(layer)\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        \n",
    "        layer = activation_fn(layer, name='activation')\n",
    "        print(layer)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_1conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv1_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_1conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv1_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "   ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_2conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv2_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_2conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv2_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "    ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_3conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv3_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_3conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv3_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_4conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                   n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv4_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_4conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv4_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_5conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 5th layer: Conv_5\n",
    "    print('\\nBuilding 5th layer: ')\n",
    "    conv5 = conv_layer(conv4_pool, name='conv_5',\n",
    "                    kernel_size=(2,2),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=128)\n",
    "    ## MaxPooling \n",
    "    conv5_pool = tf.nn.max_pool(conv5,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv5_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_5conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(5, 5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(5,5),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 5th layer: Conv_5\n",
    "    print('\\nBuilding 5th layer: ')\n",
    "    conv5 = conv_layer(conv4_pool, name='conv_5',\n",
    "                    kernel_size=(2,2),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=128)\n",
    "    ## MaxPooling \n",
    "    conv5_pool = tf.nn.max_pool(conv5,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv5_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_6conv_1fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(2, 3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 5th layer: Conv_5\n",
    "    print('\\nBuilding 5th layer: ')\n",
    "    conv5 = conv_layer(conv4_pool, name='conv_5',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=128)\n",
    "    ## MaxPooling \n",
    "    conv5_pool = tf.nn.max_pool(conv5,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 6th layer: Conv_6\n",
    "    print('\\nBuilding 6th layer: ')\n",
    "    conv6 = conv_layer(conv5_pool, name='conv_6',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=256)\n",
    "    ## MaxPooling \n",
    "    conv6_pool = tf.nn.max_pool(conv6,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv6_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc1_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "    ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_6conv_2fc():\n",
    "    ## Placeholders for X and y:\n",
    "    tf_x = tf.placeholder(tf.float32, shape=[None, np.prod(img_dim_crop)],\n",
    "                          name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape=[None],\n",
    "                          name='tf_y')\n",
    "\n",
    "    # reshape x to a 4D tensor: \n",
    "    # [batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape=[-1, img_dim_crop[0], img_dim_crop[1], 1],\n",
    "                            name='tf_x_reshaped')\n",
    "    ## One-hot encoding:\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth=16,\n",
    "                             dtype=tf.float32,\n",
    "                             name='tf_y_onehot')\n",
    "\n",
    "    ## 1st layer: Conv_1\n",
    "    print('\\nBuilding 1st layer: ')\n",
    "    conv1 = conv_layer(tf_x_image, name='conv_1',\n",
    "                    kernel_size=(2, 3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=8)\n",
    "    ## MaxPooling\n",
    "    conv1_pool = tf.nn.max_pool(conv1,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "    ## 2n layer: Conv_2\n",
    "    print('\\nBuilding 2nd layer: ')\n",
    "    conv2 = conv_layer(conv1_pool, name='conv_2',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=16)\n",
    "    ## MaxPooling \n",
    "    conv2_pool = tf.nn.max_pool(conv2,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 3rd layer: Conv_3\n",
    "    print('\\nBuilding 3rd layer: ')\n",
    "    conv3 = conv_layer(conv2_pool, name='conv_3',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=32)\n",
    "    ## MaxPooling \n",
    "    conv3_pool = tf.nn.max_pool(conv3,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 4th layer: Conv_4\n",
    "    print('\\nBuilding 4th layer: ')\n",
    "    conv4 = conv_layer(conv3_pool, name='conv_4',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=64)\n",
    "    ## MaxPooling \n",
    "    conv4_pool = tf.nn.max_pool(conv4,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 5th layer: Conv_5\n",
    "    print('\\nBuilding 5th layer: ')\n",
    "    conv5 = conv_layer(conv4_pool, name='conv_5',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=128)\n",
    "    ## MaxPooling \n",
    "    conv5_pool = tf.nn.max_pool(conv5,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 6th layer: Conv_6\n",
    "    print('\\nBuilding 6th layer: ')\n",
    "    conv6 = conv_layer(conv5_pool, name='conv_6',\n",
    "                    kernel_size=(2,3),\n",
    "                    padding_mode='VALID',\n",
    "                    n_output_channels=256)\n",
    "    ## MaxPooling \n",
    "    conv6_pool = tf.nn.max_pool(conv6,\n",
    "                             ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc1 = fc_layer(conv6_pool, name='fc_1',\n",
    "                  n_output_units=1024,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob=keep_prob,\n",
    "                            name='fc1_dropout_layer')\n",
    "\n",
    "    ## 1st FC layer: Fully Connected\n",
    "    print('\\nBuilding 1st FC layer:')\n",
    "    fc2 = fc_layer(fc1_drop, name='fc_2',\n",
    "                  n_output_units=512,\n",
    "                  activation_fn=tf.nn.relu)\n",
    "\n",
    "    ## Dropout\n",
    "#    keep_prob = tf.placeholder(tf.float32, name='fc_keep_prob')\n",
    "    fc2_drop = tf.nn.dropout(fc2, keep_prob=keep_prob,\n",
    "                            name='fc2_dropout_layer')\n",
    "\n",
    "    ## 4th layer: Fully Connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    output_layer = fc_layer(fc2_drop, name='output_layer',\n",
    "                  n_output_units=16,\n",
    "                  activation_fn=None)\n",
    "\n",
    "        ## Prediction\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(output_layer, name='probabilities'),\n",
    "        'labels' : tf.cast(tf.argmax(output_layer, axis=1), tf.int32,\n",
    "                           name='labels')\n",
    "    }\n",
    "\n",
    "    ## Visualize the graph with TensorBoard:\n",
    "\n",
    "    ## Loss Function and Optimization\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output_layer, labels=tf_y_onehot),\n",
    "        name='cross_entropy_loss')\n",
    "\n",
    "    ## Optimizer:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                   name='train_op')\n",
    "\n",
    "    ## Computing the prediction accuracy\n",
    "    correct_predictions = tf.equal(\n",
    "        predictions['labels'],\n",
    "        tf_y, name='correct_preds')\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_predictions, tf.float32),\n",
    "        name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in {} at epoch# {}'.format(path, epoch))\n",
    "    saver.save(sess, os.path.join(path,'cnn-model.ckpt'),\n",
    "               global_step=epoch)\n",
    "\n",
    "\n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "            path, 'cnn-model.ckpt-%d' % epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, training_set, validation_set=None, test_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=0.5, random_seed=None, path='./model'):\n",
    "\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "    train_acc, valid_acc, test_acc = [], [], []\n",
    "\n",
    "    ## initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed) # for shuflling in batch_generator\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(\n",
    "                        X_data, y_data, batch_size=64,\n",
    "                        shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0': batch_x,\n",
    "                    'tf_y:0': batch_y,\n",
    "                    'fc_keep_prob:0': dropout}\n",
    "            loss, _ = sess.run(\n",
    "                    ['cross_entropy_loss:0', 'train_op'],\n",
    "                    feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "            avg_acc += sess.run('accuracy:0', feed_dict=feed)\n",
    "\n",
    "        train_acc.append(avg_acc / (i+1))\n",
    "        training_loss.append(avg_loss / (i+1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f, Train Acc: %7.3f,' % (\n",
    "            epoch, avg_loss, train_acc[-1]), end=' ')\n",
    "\n",
    "\n",
    "        if validation_set is not None:\n",
    "            batch_gen_valid = batch_generator(validation_set[0], validation_set[1], \\\n",
    "                        batch_size=64, shuffle=shuffle)\n",
    "            avg_acc =0.0\n",
    "            for j, (batch_valid_x, batch_valid_y) in enumerate(batch_gen_valid):\n",
    "\n",
    "                feed = {'tf_x:0': batch_valid_x, 'tf_y:0': batch_valid_y, \\\n",
    "                        'fc_keep_prob:0':1.0}\n",
    "                avg_acc += sess.run('accuracy:0', feed_dict=feed)\n",
    "\n",
    "            valid_acc.append(avg_acc / (j+1))\n",
    "            print(' Validation Acc: %7.3f,' % valid_acc[-1])\n",
    "\n",
    "        if test_set is not None:\n",
    "            batch_gen_test = batch_generator(test_set[0], test_set[1], \\\n",
    "                        batch_size=64, shuffle=shuffle)\n",
    "            avg_acc = 0.0\n",
    "            for j, (batch_test_x, batch_test_y) in enumerate(batch_gen_test):\n",
    "\n",
    "                feed = {'tf_x:0': batch_test_x, 'tf_y:0': batch_test_y, \\\n",
    "                        'fc_keep_prob:0':1.0}\n",
    "                avg_acc += sess.run('accuracy:0', feed_dict=feed)\n",
    "\n",
    "            test_acc.append(avg_acc / (j+1))\n",
    "            print(' Test Acc: %7.3f' % test_acc[-1])\n",
    "\n",
    "#            feed = {'tf_x:0': validation_set[0],\n",
    "#                    'tf_y:0': validation_set[1],\n",
    "#                    'fc_keep_prob:0':1.0}\n",
    "#            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "#            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            save(saver, sess, epoch=epoch, path=path)\n",
    "            sio.savemat(path+\"/accuracy.mat\", mdict={'training_loss': training_loss, \\\n",
    "            'train_acc': train_acc, 'valid_acc': valid_acc, 'test_acc': test_acc})\n",
    "\n",
    "    save(saver, sess, epoch=epoch, path=path)\n",
    "    sio.savemat(path+\"/accuracy.mat\", mdict={'training_loss': training_loss, \\\n",
    "    'train_acc': train_acc, 'valid_acc': valid_acc, 'test_acc': test_acc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_x:0': X_test,\n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building 1st layer: \n",
      "Tensor(\"conv_1/_weights/read:0\", shape=(5, 5, 1, 8), dtype=float32)\n",
      "Tensor(\"conv_1/_biases/read:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 95, 253, 8), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 95, 253, 8), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 95, 253, 8), dtype=float32)\n",
      "\n",
      "Building 1st FC layer:\n",
      "Tensor(\"fc_1/_weights/read:0\", shape=(48768, 1024), dtype=float32)\n",
      "Tensor(\"fc_1/_biases/read:0\", shape=(1024,), dtype=float32)\n",
      "Tensor(\"fc_1/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_1/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_1/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "Tensor(\"output_layer/_weights/read:0\", shape=(1024, 16), dtype=float32)\n",
      "Tensor(\"output_layer/_biases/read:0\", shape=(16,), dtype=float32)\n",
      "Tensor(\"output_layer/MatMul:0\", shape=(?, 16), dtype=float32)\n",
      "Tensor(\"output_layer/net_pre-activation:0\", shape=(?, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "random_seed = 123\n",
    "\n",
    "model_path = './model/cnn_1conv_1fc/'\n",
    "#model_path = './model/cnn_1conv_2fc/'\n",
    "# model_path = './model/cnn_2conv_1fc/'\n",
    "#model_path = './model/cnn_2conv_2fc/'\n",
    "#model_path = './model/cnn_3conv_1fc/'\n",
    "#model_path = './model/cnn_3conv_2fc/'\n",
    "#model_path = './model/cnn_4conv_1fc/'\n",
    "#model_path = './model/cnn_4conv_2fc/'\n",
    "#model_path = './model/cnn_5conv_1fc/'\n",
    "#model_path = './model/cnn_5conv_2fc/'\n",
    "#model_path = './model/cnn_6conv_1fc/'\n",
    "#model_path = './model/cnn_6conv_2fc/'\n",
    "\n",
    "## create a default Graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn_1conv_1fc()\n",
    "#     build_cnn_1conv_2fc()\n",
    "#     build_cnn_2conv_1fc()\n",
    "    #build_cnn_2conv_2fc()\n",
    "    #build_cnn_3conv_1fc()\n",
    "    #build_cnn_3conv_2fc()\n",
    "    #build_cnn_4conv_1fc()\n",
    "    #build_cnn_4conv_2fc()\n",
    "    #build_cnn_5conv_1fc()\n",
    "    #build_cnn_5conv_2fc()\n",
    "    #build_cnn_6conv_1fc()\n",
    "    #build_cnn_6conv_2fc()\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_path = '/users/jhlee/data/img_snd/'\n",
    "root_path = '/data/01_experiment_data/image_sound/backup_sound/img_snd'\n",
    "test_list = os.path.join(root_path, 'test_16words_png.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model/cnn_1conv_1fc/\n"
     ]
    }
   ],
   "source": [
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=100, path=model_path)\n",
    "    \n",
    "    with tf.variable_scope('conv_1', reuse=True):\n",
    "        weight = tf.get_variable( name='_weights',\n",
    "                                  shape=(5, 5, 1, 8))\n",
    "        weight_value = sess.run(weight) \n",
    "        sio.savemat('conv1.mat', {'conv1_weights':weight_value})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sound_path = '/users/jhlee/data/img_snd/'\n",
    "root_path = '/data/01_experiment_data/image_sound/backup_sound/img_snd'\n",
    "test_list = os.path.join(root_path, 'test_16words_png.txt')\n",
    "\n",
    "\n",
    "file_hit = open(os.path.join(model_path, 'test_16words_png_hit.txt'),\"w\")\n",
    "file_miss = open(os.path.join(model_path, 'test_16words_png_miss.txt'),\"w\")\n",
    "\n",
    "    \n",
    "## create a new session \n",
    "## and restore the model\n",
    "with tf.Session(graph=g) as sess:\n",
    "    load(saver, sess, \n",
    "         epoch=100, path=model_path)\n",
    "    \n",
    "    ### the hit/miss for the test samples\n",
    "    batch_gen_test = batch_generator(X_test_centered, y_test, \\\n",
    "                        batch_size=64, shuffle=False)\n",
    "    test_acc = []\n",
    "    avg_acc = 0.0\n",
    "    list_correct_preds = []\n",
    "    for j, (batch_test_x, batch_test_y) in enumerate(batch_gen_test):\n",
    "        feed = {'tf_x:0': batch_test_x, 'tf_y:0': batch_test_y, \\\n",
    "                       'fc_keep_prob:0':1.0}\n",
    "        avg_acc += sess.run('accuracy:0', feed_dict=feed)\n",
    "        list_correct_preds.append(sess.run('correct_preds:0', feed_dict=feed))\n",
    "        \n",
    "    test_acc.append(avg_acc / (j+1))\n",
    "    print(' Test Acc: %7.3f' % test_acc[-1])\n",
    "    print(list_correct_preds)\n",
    "    \n",
    "    list_correct_preds = list(itertools.chain(*list_correct_preds))\n",
    "    \n",
    "    ### load the list of sample and save the hit/miss sample separately\n",
    "    with open(test_list, 'rt') as filelist:\n",
    "        idx = 0\n",
    "        for fname in filelist.read().splitlines():\n",
    "            img = np.array(Image.open(os.path.join(sound_path,fname)))\n",
    "            if img.shape == img_dim:\n",
    "                if list_correct_preds[idx]:\n",
    "                    file_hit.write(fname + '\\n')\n",
    "                else:\n",
    "                    file_miss.write(fname + '\\n')\n",
    "\n",
    "                idx += 1\n",
    "\n",
    "            if idx % 1000 == 0:\n",
    "                print(idx)\n",
    "                \n",
    "        print(idx) \n",
    "    \n",
    "    file_hit.close()\n",
    "    file_miss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flatten(list_correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(itertools.chain(*list_correct_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
