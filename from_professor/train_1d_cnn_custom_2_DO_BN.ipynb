{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3,4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 65,538,576\n",
      "Trainable params: 65,538,064\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 22,175,504\n",
      "Trainable params: 22,174,480\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 7,939,088\n",
      "Trainable params: 7,937,552\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 3,414,288\n",
      "Trainable params: 3,412,240\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,557,392\n",
      "Trainable params: 1,555,088\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,369,616\n",
      "Trainable params: 1,367,056\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,362,064\n",
      "Trainable params: 1,359,248\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,415,952\n",
      "Trainable params: 1,412,880\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,444,944\n",
      "Trainable params: 1,441,744\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0185 - acc: 0.3787\n",
      "Epoch 00001: val_loss improved from inf to 1.47871, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/001-1.4787.hdf5\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 2.0184 - acc: 0.3787 - val_loss: 1.4787 - val_acc: 0.5113\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2026 - acc: 0.6201\n",
      "Epoch 00002: val_loss improved from 1.47871 to 0.87692, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/002-0.8769.hdf5\n",
      "36805/36805 [==============================] - 423s 12ms/sample - loss: 1.2026 - acc: 0.6201 - val_loss: 0.8769 - val_acc: 0.7414\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9513 - acc: 0.7063\n",
      "Epoch 00003: val_loss did not improve from 0.87692\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.9514 - acc: 0.7063 - val_loss: 0.8901 - val_acc: 0.7377\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8166 - acc: 0.7509\n",
      "Epoch 00004: val_loss improved from 0.87692 to 0.69552, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/004-0.6955.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.8167 - acc: 0.7509 - val_loss: 0.6955 - val_acc: 0.8104\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7151 - acc: 0.7831\n",
      "Epoch 00005: val_loss improved from 0.69552 to 0.67228, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/005-0.6723.hdf5\n",
      "36805/36805 [==============================] - 423s 12ms/sample - loss: 0.7150 - acc: 0.7831 - val_loss: 0.6723 - val_acc: 0.8043\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.8068\n",
      "Epoch 00006: val_loss improved from 0.67228 to 0.63525, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/006-0.6352.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.6391 - acc: 0.8068 - val_loss: 0.6352 - val_acc: 0.8216\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.8252\n",
      "Epoch 00007: val_loss improved from 0.63525 to 0.59004, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/007-0.5900.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.5894 - acc: 0.8252 - val_loss: 0.5900 - val_acc: 0.8367\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8399\n",
      "Epoch 00008: val_loss improved from 0.59004 to 0.53284, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/008-0.5328.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.5355 - acc: 0.8398 - val_loss: 0.5328 - val_acc: 0.8588\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8518\n",
      "Epoch 00009: val_loss improved from 0.53284 to 0.46322, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/009-0.4632.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.5004 - acc: 0.8517 - val_loss: 0.4632 - val_acc: 0.8693\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8617\n",
      "Epoch 00010: val_loss did not improve from 0.46322\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.4609 - acc: 0.8617 - val_loss: 0.4869 - val_acc: 0.8651\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8700\n",
      "Epoch 00011: val_loss improved from 0.46322 to 0.44541, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/011-0.4454.hdf5\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.4341 - acc: 0.8699 - val_loss: 0.4454 - val_acc: 0.8730\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8801\n",
      "Epoch 00012: val_loss did not improve from 0.44541\n",
      "36805/36805 [==============================] - 424s 12ms/sample - loss: 0.4005 - acc: 0.8801 - val_loss: 0.5278 - val_acc: 0.8532\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8878\n",
      "Epoch 00013: val_loss did not improve from 0.44541\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.3772 - acc: 0.8878 - val_loss: 0.4941 - val_acc: 0.8647\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8904\n",
      "Epoch 00014: val_loss did not improve from 0.44541\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.3565 - acc: 0.8903 - val_loss: 0.5999 - val_acc: 0.8286\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8983\n",
      "Epoch 00015: val_loss did not improve from 0.44541\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.3334 - acc: 0.8983 - val_loss: 0.4608 - val_acc: 0.8798\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9045\n",
      "Epoch 00016: val_loss improved from 0.44541 to 0.43545, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/016-0.4355.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.3174 - acc: 0.9045 - val_loss: 0.4355 - val_acc: 0.8768\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9076\n",
      "Epoch 00017: val_loss did not improve from 0.43545\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.3044 - acc: 0.9076 - val_loss: 0.4407 - val_acc: 0.8826\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9144\n",
      "Epoch 00018: val_loss improved from 0.43545 to 0.40555, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/018-0.4056.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2795 - acc: 0.9144 - val_loss: 0.4056 - val_acc: 0.8935\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9201\n",
      "Epoch 00019: val_loss improved from 0.40555 to 0.38774, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/019-0.3877.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2649 - acc: 0.9200 - val_loss: 0.3877 - val_acc: 0.8926\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9212\n",
      "Epoch 00020: val_loss improved from 0.38774 to 0.38546, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/020-0.3855.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2587 - acc: 0.9212 - val_loss: 0.3855 - val_acc: 0.8898\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9245\n",
      "Epoch 00021: val_loss did not improve from 0.38546\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2406 - acc: 0.9245 - val_loss: 0.3960 - val_acc: 0.8901\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9286\n",
      "Epoch 00022: val_loss did not improve from 0.38546\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2306 - acc: 0.9286 - val_loss: 0.3987 - val_acc: 0.8938\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9321\n",
      "Epoch 00023: val_loss did not improve from 0.38546\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2186 - acc: 0.9321 - val_loss: 0.3955 - val_acc: 0.8984\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9343\n",
      "Epoch 00024: val_loss did not improve from 0.38546\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2131 - acc: 0.9342 - val_loss: 0.4456 - val_acc: 0.8798\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9349\n",
      "Epoch 00025: val_loss did not improve from 0.38546\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.2095 - acc: 0.9348 - val_loss: 0.4056 - val_acc: 0.8991\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9391\n",
      "Epoch 00026: val_loss improved from 0.38546 to 0.37648, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/026-0.3765.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1941 - acc: 0.9391 - val_loss: 0.3765 - val_acc: 0.9038\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9419\n",
      "Epoch 00027: val_loss did not improve from 0.37648\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1822 - acc: 0.9419 - val_loss: 0.4525 - val_acc: 0.8824\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9464\n",
      "Epoch 00028: val_loss improved from 0.37648 to 0.36143, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/028-0.3614.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1710 - acc: 0.9464 - val_loss: 0.3614 - val_acc: 0.9073\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9482\n",
      "Epoch 00029: val_loss did not improve from 0.36143\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1660 - acc: 0.9482 - val_loss: 0.3861 - val_acc: 0.9043\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9499\n",
      "Epoch 00030: val_loss improved from 0.36143 to 0.35123, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/030-0.3512.hdf5\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1595 - acc: 0.9500 - val_loss: 0.3512 - val_acc: 0.9080\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9507\n",
      "Epoch 00031: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1543 - acc: 0.9506 - val_loss: 0.4475 - val_acc: 0.8931\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9516\n",
      "Epoch 00032: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1538 - acc: 0.9516 - val_loss: 0.4567 - val_acc: 0.8963\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9555\n",
      "Epoch 00033: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1420 - acc: 0.9555 - val_loss: 0.3652 - val_acc: 0.9003\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9561\n",
      "Epoch 00034: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1363 - acc: 0.9561 - val_loss: 0.3548 - val_acc: 0.9052\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9603\n",
      "Epoch 00035: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1247 - acc: 0.9603 - val_loss: 0.5159 - val_acc: 0.8798\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9571\n",
      "Epoch 00036: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1379 - acc: 0.9571 - val_loss: 0.4050 - val_acc: 0.8952\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9622\n",
      "Epoch 00037: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1203 - acc: 0.9622 - val_loss: 0.5383 - val_acc: 0.8737\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9643\n",
      "Epoch 00038: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1172 - acc: 0.9643 - val_loss: 0.3727 - val_acc: 0.9082\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9640\n",
      "Epoch 00039: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1140 - acc: 0.9640 - val_loss: 0.4459 - val_acc: 0.8959\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9634\n",
      "Epoch 00040: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1145 - acc: 0.9634 - val_loss: 0.4199 - val_acc: 0.9024\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9650\n",
      "Epoch 00041: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.1125 - acc: 0.9650 - val_loss: 0.3583 - val_acc: 0.9136\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9695\n",
      "Epoch 00042: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0988 - acc: 0.9695 - val_loss: 0.3929 - val_acc: 0.9043\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9694\n",
      "Epoch 00043: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0976 - acc: 0.9694 - val_loss: 0.4635 - val_acc: 0.8928\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9693\n",
      "Epoch 00044: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 425s 12ms/sample - loss: 0.0972 - acc: 0.9693 - val_loss: 0.3941 - val_acc: 0.9096\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9724\n",
      "Epoch 00045: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0895 - acc: 0.9724 - val_loss: 0.4374 - val_acc: 0.8940\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9714\n",
      "Epoch 00046: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0908 - acc: 0.9713 - val_loss: 0.3845 - val_acc: 0.9082\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9659\n",
      "Epoch 00047: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.1064 - acc: 0.9659 - val_loss: 0.4316 - val_acc: 0.8980\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9745\n",
      "Epoch 00048: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0829 - acc: 0.9745 - val_loss: 0.4470 - val_acc: 0.9043\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9740\n",
      "Epoch 00049: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0849 - acc: 0.9739 - val_loss: 0.4351 - val_acc: 0.9052\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9736\n",
      "Epoch 00050: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0841 - acc: 0.9736 - val_loss: 0.4052 - val_acc: 0.9064\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9746\n",
      "Epoch 00051: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0805 - acc: 0.9746 - val_loss: 0.3907 - val_acc: 0.9052\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9787\n",
      "Epoch 00052: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0709 - acc: 0.9787 - val_loss: 0.4488 - val_acc: 0.9010\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9746\n",
      "Epoch 00053: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0795 - acc: 0.9747 - val_loss: 0.4237 - val_acc: 0.9080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9780\n",
      "Epoch 00054: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0714 - acc: 0.9780 - val_loss: 0.4740 - val_acc: 0.8905\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9732\n",
      "Epoch 00055: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0849 - acc: 0.9731 - val_loss: 0.4843 - val_acc: 0.8901\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9769\n",
      "Epoch 00056: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0729 - acc: 0.9769 - val_loss: 0.3952 - val_acc: 0.9143\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9817\n",
      "Epoch 00057: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0597 - acc: 0.9816 - val_loss: 0.5219 - val_acc: 0.8882\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9750\n",
      "Epoch 00058: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0781 - acc: 0.9750 - val_loss: 0.4698 - val_acc: 0.8956\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9737\n",
      "Epoch 00059: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0863 - acc: 0.9738 - val_loss: 0.3777 - val_acc: 0.9124\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9830\n",
      "Epoch 00060: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0558 - acc: 0.9830 - val_loss: 0.3949 - val_acc: 0.9057\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9829\n",
      "Epoch 00061: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0574 - acc: 0.9829 - val_loss: 0.4296 - val_acc: 0.8970\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9820\n",
      "Epoch 00062: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0583 - acc: 0.9820 - val_loss: 0.5538 - val_acc: 0.8896\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9798\n",
      "Epoch 00063: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0669 - acc: 0.9798 - val_loss: 0.4061 - val_acc: 0.9068\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9786\n",
      "Epoch 00064: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0704 - acc: 0.9786 - val_loss: 0.4329 - val_acc: 0.9012\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9826\n",
      "Epoch 00065: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0564 - acc: 0.9825 - val_loss: 0.4245 - val_acc: 0.9096\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9803\n",
      "Epoch 00066: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0633 - acc: 0.9803 - val_loss: 0.4462 - val_acc: 0.9043\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9842\n",
      "Epoch 00067: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0524 - acc: 0.9842 - val_loss: 0.4929 - val_acc: 0.8919\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9852\n",
      "Epoch 00068: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0500 - acc: 0.9852 - val_loss: 0.5085 - val_acc: 0.8870\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9782\n",
      "Epoch 00069: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0690 - acc: 0.9782 - val_loss: 0.4190 - val_acc: 0.9057\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9846\n",
      "Epoch 00070: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0500 - acc: 0.9846 - val_loss: 0.4849 - val_acc: 0.8991\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9811\n",
      "Epoch 00071: val_loss did not improve from 0.35123\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0610 - acc: 0.9810 - val_loss: 0.4075 - val_acc: 0.9057\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9828\n",
      "Epoch 00072: val_loss improved from 0.35123 to 0.34660, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/072-0.3466.hdf5\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0556 - acc: 0.9828 - val_loss: 0.3466 - val_acc: 0.9255\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9876\n",
      "Epoch 00073: val_loss did not improve from 0.34660\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0424 - acc: 0.9876 - val_loss: 0.3834 - val_acc: 0.9126\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9837\n",
      "Epoch 00074: val_loss improved from 0.34660 to 0.33555, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_6_conv_checkpoint/074-0.3355.hdf5\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0536 - acc: 0.9837 - val_loss: 0.3355 - val_acc: 0.9229\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9869\n",
      "Epoch 00075: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0457 - acc: 0.9869 - val_loss: 0.3694 - val_acc: 0.9238\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9874\n",
      "Epoch 00076: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0427 - acc: 0.9874 - val_loss: 0.3800 - val_acc: 0.9175\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9856\n",
      "Epoch 00077: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0474 - acc: 0.9856 - val_loss: 0.6871 - val_acc: 0.8609\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9865\n",
      "Epoch 00078: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0450 - acc: 0.9865 - val_loss: 0.3830 - val_acc: 0.9208\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9864\n",
      "Epoch 00079: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0446 - acc: 0.9864 - val_loss: 0.4475 - val_acc: 0.9047\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9850\n",
      "Epoch 00080: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0498 - acc: 0.9850 - val_loss: 0.4423 - val_acc: 0.9033\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9848\n",
      "Epoch 00081: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0482 - acc: 0.9848 - val_loss: 0.3555 - val_acc: 0.9266\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9876\n",
      "Epoch 00082: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0426 - acc: 0.9876 - val_loss: 0.4280 - val_acc: 0.9008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9887\n",
      "Epoch 00083: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0375 - acc: 0.9887 - val_loss: 0.4403 - val_acc: 0.9106\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9869\n",
      "Epoch 00084: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0442 - acc: 0.9869 - val_loss: 0.5246 - val_acc: 0.8980\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9877\n",
      "Epoch 00085: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0412 - acc: 0.9876 - val_loss: 0.4875 - val_acc: 0.8982\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9854\n",
      "Epoch 00086: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0480 - acc: 0.9854 - val_loss: 0.3883 - val_acc: 0.9147\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9867\n",
      "Epoch 00087: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0436 - acc: 0.9867 - val_loss: 0.4243 - val_acc: 0.9138\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9896\n",
      "Epoch 00088: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0375 - acc: 0.9896 - val_loss: 0.3955 - val_acc: 0.9208\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9902\n",
      "Epoch 00089: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0330 - acc: 0.9902 - val_loss: 0.4346 - val_acc: 0.9022\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9899\n",
      "Epoch 00090: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0349 - acc: 0.9899 - val_loss: 0.4235 - val_acc: 0.9071\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9870\n",
      "Epoch 00091: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0426 - acc: 0.9870 - val_loss: 0.3813 - val_acc: 0.9217\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9891\n",
      "Epoch 00092: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0365 - acc: 0.9891 - val_loss: 0.4094 - val_acc: 0.9138\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9882\n",
      "Epoch 00093: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0383 - acc: 0.9882 - val_loss: 0.4022 - val_acc: 0.9145\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9886\n",
      "Epoch 00094: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0379 - acc: 0.9886 - val_loss: 0.4470 - val_acc: 0.9054\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9901\n",
      "Epoch 00095: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0346 - acc: 0.9901 - val_loss: 0.4310 - val_acc: 0.9157\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9891\n",
      "Epoch 00096: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0357 - acc: 0.9891 - val_loss: 0.4836 - val_acc: 0.9040\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9900\n",
      "Epoch 00097: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0337 - acc: 0.9899 - val_loss: 0.4844 - val_acc: 0.9022\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9882\n",
      "Epoch 00098: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0399 - acc: 0.9882 - val_loss: 0.4035 - val_acc: 0.9185\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9907\n",
      "Epoch 00099: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0328 - acc: 0.9907 - val_loss: 0.3954 - val_acc: 0.9255\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9913\n",
      "Epoch 00100: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0306 - acc: 0.9913 - val_loss: 0.4942 - val_acc: 0.9008\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9897\n",
      "Epoch 00101: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0364 - acc: 0.9897 - val_loss: 0.4100 - val_acc: 0.9157\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9884\n",
      "Epoch 00102: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0385 - acc: 0.9884 - val_loss: 0.4414 - val_acc: 0.9180\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9924\n",
      "Epoch 00103: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0270 - acc: 0.9924 - val_loss: 0.4329 - val_acc: 0.9094\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9876\n",
      "Epoch 00104: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0392 - acc: 0.9876 - val_loss: 0.4151 - val_acc: 0.9152\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9916\n",
      "Epoch 00105: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0278 - acc: 0.9916 - val_loss: 0.4207 - val_acc: 0.9150\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9853\n",
      "Epoch 00106: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0518 - acc: 0.9853 - val_loss: 0.3916 - val_acc: 0.9185\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9922\n",
      "Epoch 00107: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0263 - acc: 0.9922 - val_loss: 0.3944 - val_acc: 0.9234\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9923\n",
      "Epoch 00108: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0258 - acc: 0.9923 - val_loss: 0.4041 - val_acc: 0.9231\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9908\n",
      "Epoch 00109: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0290 - acc: 0.9908 - val_loss: 0.4397 - val_acc: 0.9126\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9905\n",
      "Epoch 00110: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0327 - acc: 0.9905 - val_loss: 0.4246 - val_acc: 0.9175\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9898\n",
      "Epoch 00111: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0357 - acc: 0.9898 - val_loss: 0.3746 - val_acc: 0.9243\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9919\n",
      "Epoch 00112: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0272 - acc: 0.9919 - val_loss: 0.5662 - val_acc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9900\n",
      "Epoch 00113: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0347 - acc: 0.9900 - val_loss: 0.6407 - val_acc: 0.8772\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9915\n",
      "Epoch 00114: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0288 - acc: 0.9915 - val_loss: 0.5012 - val_acc: 0.8980\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9917\n",
      "Epoch 00115: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0270 - acc: 0.9917 - val_loss: 0.5213 - val_acc: 0.9061\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9920\n",
      "Epoch 00116: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0281 - acc: 0.9920 - val_loss: 0.4851 - val_acc: 0.9110\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9929\n",
      "Epoch 00117: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0263 - acc: 0.9929 - val_loss: 0.5583 - val_acc: 0.8942\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9887\n",
      "Epoch 00118: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0368 - acc: 0.9888 - val_loss: 0.4991 - val_acc: 0.9054\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 00119: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0217 - acc: 0.9936 - val_loss: 0.4410 - val_acc: 0.9222\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9919\n",
      "Epoch 00120: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0268 - acc: 0.9919 - val_loss: 0.4469 - val_acc: 0.9124\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9921\n",
      "Epoch 00121: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0272 - acc: 0.9921 - val_loss: 0.3783 - val_acc: 0.9276\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9939\n",
      "Epoch 00122: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0220 - acc: 0.9939 - val_loss: 0.4252 - val_acc: 0.9213\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9909\n",
      "Epoch 00123: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0306 - acc: 0.9909 - val_loss: 0.5003 - val_acc: 0.9036\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9924\n",
      "Epoch 00124: val_loss did not improve from 0.33555\n",
      "36805/36805 [==============================] - 426s 12ms/sample - loss: 0.0256 - acc: 0.9924 - val_loss: 0.4490 - val_acc: 0.9138\n",
      "\n",
      "1D_CNN_custom_4_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VMX6xz+zm0YS0iC0UBJ6Swg9SFWkWUDlIiiI4gUsoD/Ry73YLigWrgUVLxYULmIBEbAgKIKAAQWFQOggLZCEQBJSSC+78/tjdpMNySabsgRwPs+zz+6ZM+fMe86eM99535kzR0gp0Wg0Go2mIgy1bYBGo9Forg20YGg0Go3GIbRgaDQajcYhtGBoNBqNxiG0YGg0Go3GIbRgaDQajcYhtGBoNBqNxiG0YGg0Go3GIbRgaDQajcYhXGrbgJqkfv36Mjg4uLbN0Gg0mmuGqKioZClloCN5ryvBCA4OZvfu3bVthkaj0VwzCCHOOJpXh6Q0Go1G4xBaMDQajUbjEFowNBqNRuMQ11UfRlkUFBQQFxdHbm5ubZtyTeLh4UHTpk1xdXWtbVM0Gk0tc90LRlxcHHXr1iU4OBghRG2bc00hpeTixYvExcUREhJS2+ZoNJpa5roPSeXm5lKvXj0tFlVACEG9evW0d6bRaAAnCoYQopkQYosQ4rAQ4pAQ4v/KyCOEEAuEECeEEPuFEN1s1t0vhDhu+dxfTVuqs/lfGn3uNBqNFWeGpAqBp6SUe4QQdYEoIcRGKeVhmzwjgDaWT2/gfaC3ECIAmA30AKRl2++klKnOMDQv7xxGoxcuLr7O2L1Go9FcFzjNw5BSJkgp91h+ZwBHgKDLso0ClknFTsBPCNEYGAZslFKmWERiIzDcWbbm55+nsPCSU/adlpbGe++9V6Vtb7nlFtLS0hzOP2fOHN54440qlaXRaDQVcUX6MIQQwUBX4PfLVgUBsTbLcZY0e+lOwoByZGqe8gSjsLCw3G3Xr1+Pn5+fM8zSaDSaSuN0wRBCeAOrgSeklDXejBdCTBVC7BZC7E5KSqrqPpDSXMOWKWbNmsXJkycJDw9n5syZbN26lf79+zNy5Eg6duwIwB133EH37t3p1KkTixYtKto2ODiY5ORkYmJi6NChA1OmTKFTp04MHTqUnJyccsuNjo4mIiKCsLAw7rzzTlJTVTRvwYIFdOzYkbCwMMaNGwfAL7/8Qnh4OOHh4XTt2pWMjAynnAuNRnNt49RhtUIIV5RYfC6lXFNGlnigmc1yU0taPDDosvStZZUhpVwELALo0aNHuW7C8eNPkJkZXSrdZMpCCCMGg0d5m5eJt3c4bdq8bXf9vHnzOHjwINHRqtytW7eyZ88eDh48WDRUdcmSJQQEBJCTk0PPnj0ZPXo09erVu8z24yxfvpyPPvqIu+++m9WrVzNhwgS75U6cOJF3332XgQMH8u9//5sXXniBt99+m3nz5nH69Gnc3d2Lwl1vvPEGCxcupG/fvmRmZuLhUfnzoNForn+cOUpKAIuBI1LK+XayfQdMtIyWigDSpZQJwAZgqBDCXwjhDwy1pDnJVnBWSKosevXqVeK5hgULFtClSxciIiKIjY3l+PHjpbYJCQkhPDwcgO7duxMTE2N3/+np6aSlpTFw4EAA7r//fiIjIwEICwtj/PjxfPbZZ7i4qPZC3759efLJJ1mwYAFpaWlF6RqNRmOLM2uGvsB9wAEhhLVZ/wzQHEBK+QGwHrgFOAFkA5Ms61KEEHOBXZbtXpRSplTXIHueQFbWYYRwxdOzTXWLcAgvL6+i31u3bmXTpk3s2LEDT09PBg0aVOZzD+7u7kW/jUZjhSEpe6xbt47IyEjWrl3Lyy+/zIEDB5g1axa33nor69evp2/fvmzYsIH27dtXaf8ajeb6xWmCIaXcDpQ7iF9KKYFpdtYtAZY4wbQyMADO6cOoW7duuX0C6enp+Pv74+npydGjR9m5c2e1y/T19cXf359t27bRv39/Pv30UwYOHIjZbCY2NpYbb7yRfv36sWLFCjIzM7l48SKhoaGEhoaya9cujh49qgVDo9GUQscesHZ6OyckVa9ePfr27Uvnzp0ZMWIEt956a4n1w4cP54MPPqBDhw60a9eOiIiIGin3k08+4eGHHyY7O5uWLVvyv//9D5PJxIQJE0hPT0dKyeOPP46fnx/PP/88W7ZswWAw0KlTJ0aMGFEjNmg0musL4ayKsjbo0aOHvPwFSkeOHKFDhw7lbpedfRwpC/Dy6uhM865ZHDmHGo3m2kQIESWl7OFI3ut+LilHUP3z149wajQajTPQggGAwWnPYWg0Gs31ghYMwJlPems0Gs31ghYMrCEp7WFoNBpNeWjBAHRISqPRaCpGCwa601uj0WgcQQsGYO3DuFqGGHt7e1cqXaPRaK4EWjCA4tOgw1IajUZjDy0YFL+G1BkexqxZs1i4cGHRsvUlR5mZmQwePJhu3boRGhrKt99+6/A+pZTMnDmTzp07ExoaypdffglAQkICAwYMIDw8nM6dO7Nt2zZMJhMPPPBAUd633nqrxo9Ro9H8NfhrTQ3yxBMQXXp6cxdZgMGcizB6U8H0V6UJD4e37U9vPnbsWJ544gmmTVNTZq1cuZINGzbg4eHB119/jY+PD8nJyURERDBy5EiH3qG9Zs0aoqOj2bdvH8nJyfTs2ZMBAwbwxRdfMGzYMJ599llMJhPZ2dlER0cTHx/PwYMHASr1Bj+NRqOx5a8lGBUiqbRgVEDXrl1JTEzk3LlzJCUl4e/vT7NmzSgoKOCZZ54hMjISg8FAfHw8Fy5coFGjRhXuc/v27dxzzz0YjUYaNmzIwIED2bVrFz179uTBBx+koKCAO+64g/DwcFq2bMmpU6d47LHHuPXWWxk6dGiNHp9Go/nr8NcSDDuegKkghdzcU3h6dsJorFPjxY4ZM4ZVq1Zx/vx5xo4dC8Dnn39OUlISUVFRuLq6EhwcXOa05pVhwIABREZGsm7dOh544AGefPJJJk6cyL59+9iwYQMffPABK1euZMmSKzQJsEajua7QfRhA8WlwziipsWPHsmLFClatWsWYMWMANa15gwYNcHV1ZcuWLZw5c8bh/fXv358vv/wSk8lEUlISkZGR9OrVizNnztCwYUOmTJnC5MmT2bNnD8nJyZjNZkaPHs1LL73Enj17nHKMGo3m+uev5WHYobjT2zmjpDp16kRGRgZBQUE0btwYgPHjx3P77bcTGhpKjx49KvX+iTvvvJMdO3bQpUsXhBC89tprNGrUiE8++YTXX38dV1dXvL29WbZsGfHx8UyaNAmzWR3bq6++6pRj1Gg01z9Om95cCLEEuA1IlFJ2LmP9TGC8ZdEF6AAEWt62FwNkACag0NGpd6s6vXlhYQY5OceoU6ctLi4+jhT1l0JPb67RXL9cLdObLwWG21sppXxdShkupQwHngZ+uew1rDda1jt0INWheGTS1fHgnkaj0VyNOE0wpJSRgKPv4b4HWO4sWypGnQY9n5RGo9HYp9Y7vYUQnihPZLVNsgR+EkJECSGmXgErLN9aMDQajcYeV0On9+3Ar5eFo/pJKeOFEA2AjUKIoxaPpRQWQZkK0Lx58yoZIITVw9AhKY1Go7FHrXsYwDguC0dJKeMt34nA10AvextLKRdJKXtIKXsEBgZW0QQ9l5RGo9FURK0KhhDCFxgIfGuT5iWEqGv9DQwFDjrZEsu39jA0Go3GHk4TDCHEcmAH0E4IESeE+LsQ4mEhxMM22e4EfpJSZtmkNQS2CyH2AX8A66SUPzrLTmWr8zq909LSeO+996q07S233KLnftJoNFcNTuvDkFLe40Cepajht7Zpp4AuzrHKHs7zMKyC8eijj5ZaV1hYiIuL/b9g/fr1NW6PRqPRVJWroQ+j1lHPYQineBizZs3i5MmThIeHM3PmTLZu3Ur//v0ZOXIkHTt2BOCOO+6ge/fudOrUiUWLFhVtGxwcTHJyMjExMXTo0IEpU6bQqVMnhg4dSk5OTqmy1q5dS+/evenatSs333wzFy5cACAzM5NJkyYRGhpKWFgYq1erAWk//vgj3bp1o0uXLgwePLjGj12j0VxfXA2jpK4YdmY3B8BkaosQrhgqKaEVzG7OvHnzOHjwINGWgrdu3cqePXs4ePAgISEhACxZsoSAgABycnLo2bMno0ePpl69eiX2c/z4cZYvX85HH33E3XffzerVq5kwYUKJPP369WPnzp0IIfj444957bXXePPNN5k7dy6+vr4cOHAAgNTUVJKSkpgyZQqRkZGEhISQkuLoIzMajeavyl9KMMqnZqc1L49evXoViQXAggUL+PrrrwGIjY3l+PHjpQQjJCSE8PBwALp3705MTEyp/cbFxTF27FgSEhLIz88vKmPTpk2sWLGiKJ+/vz9r165lwIABRXkCAgJq9Bg1Gs31x19KMMrzBDIzT2E01qVOnRD7mWoILy+vot9bt25l06ZN7NixA09PTwYNGlTmNOfu7u5Fv41GY5khqccee4wnn3ySkSNHsnXrVubMmeMU+zUazV8T3YdRhAFndHrXrVuXjIwMu+vT09Px9/fH09OTo0ePsnPnziqXlZ6eTlBQEACffPJJUfqQIUNKvCY2NTWViIgIIiMjOX36NIAOSWk0mgrRgmFBdXzXfKd3vXr16Nu3L507d2bmzJml1g8fPpzCwkI6dOjArFmziIiIqHJZc+bMYcyYMXTv3p369esXpT/33HOkpqbSuXNnunTpwpYtWwgMDGTRokXcdddddOnSpejFThqNRmMPp01vXhtUdXpzgKysIwhhxNOzrbPMu2bR05trNNcvV8v05tcUysO4fsRTo9FoahotGEUY9PTmGo1GUw5aMIrQHoZGo9GUhxYMC2o+Ke1haDQajT20YBRh0O/D0Gg0mnLQgmHBWcNqNRqN5npBC0YRV0+nt7e3d22boNFoNKXQglGE7vTWaDSa8tCCYcHa6V3T/RizZs0qMS3HnDlzeOONN8jMzGTw4MF069aN0NBQvv3223L2orA3DXpZ05Tbm9Jco9FoqorTJh8UQiwBbgMSpZSdy1g/CPVq1tOWpDVSyhct64YD7wBG4GMp5byasOmJH58g+nzZ85ubzflImYfRWLdS+wxvFM7bw+3Pajh27FieeOIJpk2bBsDKlSvZsGEDHh4efP311/j4+JCcnExERAQjR4609KWUTVnToJvN5jKnKS9rSnONRqOpDs6crXYp8F9gWTl5tkkpb7NNEEIYgYXAECAO2CWE+E5KedhZhqpyQTkXkpqc6rxr164kJiZy7tw5kpKS8Pf3p1mzZhQUFPDMM88QGRmJwWAgPj6eCxcu0KhRI7v7Kmsa9KSkpDKnKS9rSnONRqOpDs58RWukECK4Cpv2Ak5YXtWKEGIFMAqotmCU5wnk5yeSl3cWL68uGAyu1S2qBGPGjGHVqlWcP3++aJK/zz//nKSkJKKionB1dSU4OLjMac2tODoNukaj0TiL2u7D6COE2CeE+EEI0cmSFgTE2uSJs6Q5Gee913vs2LGsWLGCVatWMWbMGEBNRd6gQQNcXV3ZsmULZ86cKXcf9qZBtzdNeVlTmms0Gk11qE3B2AO0kFJ2Ad4FvqnKToQQU4UQu4UQu5OSkqpmSWEhwjKi1hlDazt16kRGRgZBQUE0btwYgPHjx7N7925CQ0NZtmwZ7du3L3cf9qZBtzdNeVlTmms0Gk11cOr05paQ1PdldXqXkTcG6AG0AeZIKYdZ0p8GkFK+WtE+qjy9eVQUpkBfsv3T8PTshNFYp6Ki/lLo6c01muuXa2J6cyFEI2EZEiSE6GWx5SKwC2gjhAgRQrgB44DvnGqM0YgwW4Xz6nh4T6PRaK42nDmsdjkwCKgvhIgDZgOuAFLKD4C/AY8IIQqBHGCcVO5OoRBiOrABNax2iZTykLPsBMBgAItgXC1Pe2s0Gs3VhjNHSd1Twfr/oobdlrVuPbC+Bm0p9/kGW8HQT3uXRE/IqNForNT2KCmn4+HhwcWLF8uv+LSHUSZSSi5evIiHh0dtm6LRaK4CnPng3lVB06ZNiYuLo9wRVBcuIJHkZeXj6iowGj2vnIFXOR4eHjRt2rS2zdBoNFcB171guLq6Fj0FbZcZMzBdTGDb6/vp0OEzGjYcf2WM02g0mmuI6z4k5RCenohs9dS02ayfntZoNJqy0IIB4OWlBUOj0WgqQAsGgJcXZOcAWjA0Go3GHlowADw9ISsb0IKh0Wg09tCCARYPIxskmM15tW2NRqPRXJVowQDVhyElLoUe2sPQaDQaO2jBABWSAlzy3LVgaDQajR20YIAKSQEu+VowNBqNxh5aMMBGMFy1YGg0Go0dtGCATUhKC4ZGo9HYQwsGFHsYWjA0Go3GLlowwEYwDHpYrUaj0dhBCwYUhaSMeS7aw9BoNBo7OE0whBBLhBCJQoiDdtaPF0LsF0IcEEL8JoToYrMuxpIeLYTYXdb2NYrFwzDmGbRgaDQajR2c6WEsBYaXs/40MFBKGQrMBRZdtv5GKWW4oy8nrxbWTu9cLRgajUZjD2e+ojVSCBFczvrfbBZ3ArX3lh6Lh2HIE5jNObVmhkaj0VzNXC19GH8HfrBZlsBPQogoIcTU8jYUQkwVQuwWQuwu96165WHtw8jRkw9qNBqNPWr9jXtCiBtRgtHPJrmflDJeCNEA2CiEOCqljCxreynlIizhrB49epTz4u5ycHEBNzcMuVKPktJoNBo71KqHIYQIAz4GRkkpL1rTpZTxlu9E4Gugl9ON8fLCmKs9DI1Go7FHrQmGEKI5sAa4T0r5p026lxCirvU3MBQoc6RVjeLlhSHXrAVDo9Fo7OC0kJQQYjkwCKgvhIgDZgOuAFLKD4B/A/WA94QQAIWWEVENga8taS7AF1LKH51lZxGenhhyzUiZh5RmhLhaunc0Go3m6sCZo6TuqWD9ZGByGemngC6lt3AyXl4Yc1T/RWFhOq6u/lfcBI1Go7ma0c1oK15eGHMFAPn552rZGI1Go7n60IJhxdMTQ54aZJWXl1DLxmg0Gs3VhxYMK15eGLILAe1haDQaTVlowbDi5YXIyQcgP197GBqNRnM5WjCseHoisnMwGn3Iy9Mehkaj0VyOFgwrXl6QlYWbW2PtYWg0Gk0ZaMGw4ukJWVm4uzXWHoZGo9GUgRYMK15eICXuNNQehkaj0ZSBFgwrlinOPUz1yc8/h5RVm8dQo9Forle0YFixTHHubvLHbM6lsDC9lg3SaDSaqwuHBEMI8X9CCB+hWCyE2COEGOps464oFg/DrcAP0M9iaDQazeU46mE8KKW8hJo51h+4D5jnNKtqA4tguBfWBfSzGBqNRnM5jgqGsHzfAnwqpTxkk3Z9YAlJuRZ4A+iRUhqNRnMZjgpGlBDiJ5RgbLC8r8LsPLNqAYuH4ZpfB9Aehkaj0VyOo9Ob/x0IB05JKbOFEAHAJOeZVQtYBMOYa8boU1d7GBqNRnMZjnoYfYBjUso0IcQE4DmgwmFEQoglQohEIUSZb8yzdKIvEEKcEELsF0J0s1l3vxDiuOVzv4N2Vh1LSIrsbP20t0aj0ZSBo4LxPpAthOgCPAWcBJY5sN1SYHg560cAbSyfqZZysHgws4HeqPd5zxZCOPeNRhYPg6ws3N2b6FFSGo1GcxmOCkahVE+yjQL+K6VcCNStaCMpZSSQUk6WUcAyqdgJ+AkhGgPDgI1SyhQpZSqwkfKFp/rYCIabW2P9TgyNRqO5DEf7MDKEEE+jhtP2F+qF1641UH4QEGuzHGdJs5fuPEqEpJqQn5+AlBLLu8U1Go3mipCfDwYDuJRRO+fkgMkE3t5X3i5wXDDGAveinsc4L4RoDrzuPLMcRwgxFRXOonnz5lXfkYsLuLlZQlKNMZuzMZku4eLiW0OWaq53rLPJVKeNkZkJCQmqYigogI4doY4auIfZDLGxEBhY3L4ByMuDQ4cgOlott2sHTZuq/cTEQFqa2heAnx8EBCgbc3LUtkKoj6cn+PoqZ9toVGmJiarMxERlW3a2Wh8QoL7z8tS+fX2hYUNla0YGpKdDfDycPQsXL6pKsKBA7dNoVPmbN4dGjVTexES13ssLPDxUpVhQAFlZyv6sLJXu6anOs7Xi9PMDf3+1/vx5SE5WNuXnq7x+fqqsunXVvmNj4ehRdSxhYdC5s7LvyBG1v/79oVcv2LcPNmyAkyeL/1Nvb7UvT09VXVirDA8PyM1Vx5CWpvLUq6e2S09XZZlMpT85OWq7wEDo1Elts38/HD6s1vv6qjKlVMuXLqltrP9j06ZQWKjKqFsXjh2r+nXnKA4JhkUkPgd6CiFuA/6QUjrSh1ER8UAzm+WmlrR4YNBl6Vvt2LYIWATQo0eP6k0A5elZ1OkN6lWtWjCuPbKyVMXg5qY+Li6qkjKZVIWXnq5uriNH1M3YsSO0aAHnzsGpU6pi6dYNWrWCM2dU3gMHVIWckAB9+sDNN0PbtupGvXABPvsMVqxQlQ+o8qwVXECAqkzr11f569RR9h05Aqmp0KCByhMbC3FxJY/FwwMGDlQVx9atxfsPClJ2pqSoj/kKDHI3GpXt2dmOlxcYqI7b3R1cXdX5LiyEgwfhyy/Vb1AVoJub2ndOTnGF7OWl1nl5qco1O1u1vj081Hd6ujp+T09o3FiV5+urysrJUetOnVIilpGh8nTooPa3fz/88IM69+3bq/299ZYSKoMBeveGu+9Wv81mVfGnpysbbAUtN1cdX5Mm6lpKT1f/kxDq2ENC1LEYDOocWj916qjjSEhQgn/okBKw225T+0tJUWUaDOrj46NsNRrVtRIbq47TKtZXAocEQwhxN8qj2Ip6YO9dIcRMKeWqapb/HTBdCLEC1cGdLqVMEEJsAF6x6egeCjxdzbIqpuidGE0ANT2Il1d7pxf7V8FkUjdBWpq6mQwGVUm4u6vl1FS1/tgxdTOfPVvc+rV+srJUJR4bq7b38lKt1F691M26dSusXatu6oowWHrwHKn8XF1VK7BRI/jiC/jww5LrPTxg1ChV8VgrRWsFd/GiEpVjx1SllZWlKvyePVWrMjFR5Rk0SFVmzZoVTZ7Mtm2qpZubqyqSiAjVij5+XKXVq6cqpc6dITxcHdOxY6p1HxQEwcGqknG1BJCt5xhUheXuXnwOrGKalaWWzWa1/2bNVEVr/Z/MZtXazc4uFoLUVHUcOTlKFH18VAVq9Y7sXQ+pqSqvm1vF/4EzMJlUBWwlK0tde+3aqfOmKYmjIalngZ5SykQAIUQgsAkoVzCEEMtRnkJ9IUQcauSTK4CU8gNgPephwBNANpZnO6SUKUKIucAuy65elFKW13leM1gEw91deRh6aG35pKYq9z45WbW2MjNVq/nwYVWxBAer1uG+fRAVpVrPjk4C7O6uQhYGg9rGup2Hh/IGbrhBLVsFZPFiVYHVrw/33w99+6rKIC9PfRcWFgtM3brQurXyEEBVvmfPqgouJERVhnv3qpZpcLDK16ZNcaVWUKCO59w5JQCurnDrraqlV9OMHl35bVq3tr+ufv2q22LFYFD/q59fcZqPj/pfKoPRWDP2VAdbsQB1ffTpUzu2XAs4KhgGq1hYuIgDI6yklPdUsF4C0+ysWwIscdC+mqEoJKU8jLy8+CtafG2Sk6Mq+kOHIClJtXit4Y60NLU+P199W1uiycml9+PqqipYIWDTJlWht20L/fqpiiwwUMWcQVXk+fmqUpeyOB7dqpWqoMvq9LNHYSGcPq0qeNdKDscIDVUfK35+Sqzs4eqqWvoazV8NR2/JHy1houWW5bEo7+D6wuJhuLjUxdW1IdnZR2vbompjNqtKPylJVfT5+er3wYNKIKzhndjYkqEZFxdVeQcEqAq0Th11eho3LtlKb99exU/d3VUe2wpbSlWeNezhTFxclMhoNBrn4Win90whxGigryVpkZTya+eZVUt4eammM+Dt3YXMzOhaNsgxTCYV7jl+XIWEjh6FP/+EEyeUEJhMZW/XvLkKwfTvr76to0aaNFGCUN0RxUJcGbH4K2Mym9gZt5NeQb1wNRa7Vpn5mWTmZ5JdkE1Dr4Z4uXlVet8/nviR8EbhNPJuVJMmO42z6WfJzM+kY2DHGtlfXmEeKTkpGA1G3I3u+HqUHXNMykqirntdPFw8aqTcy8kpyGHZvmX4uPswKHgQjes2dko5juCw0y+lXA2sdqIttY+npxqyAHh7hxMX9zZmcwEGQ008clI9pFSCsH077NihPISMjOJhmPn5xXl9fVUY6IYblBBYR+h4eak4vJ+f6iD28am947FHZn4mHi4euBgqEY9yMik5KXi6ejqtQqgqF7MvMm71ODad2kRL/5bMHjgbL1cv3t/9Pj+f/rkon4vBhV5BvRjVbhRP9XkKo0EF7ref3c7srbOZPXA2A1oMKMovpWTWplm89ttrtPJvxS8P/EKQT8WPQaXnpnMw8SB1XFVP97Yz29hwcgP1POvx1rC3qO9Z3GGRW5jL/B3zWbJ3Cd2bdOfO9nfSuUFn8grzMBqMhDUMwyBU1DslJ4V95/fRt3lf3Iwle8ezC7KZt30eqw6v4kjyEQDeHfEu03tNB1Rl/sOJHzifeZ6krCS6N+nO7W1vx8vNi0t5l4g6F0VOYQ5GYSQjP4MTKSf48+KfRJ+P5mDiQQrMBUVlDQoexLSe0whrGMbehL38Hv87m05t4kDiAYLqBrF+/HrCGoZV6j+8nB9P/MiTG56knmc9JnedjJ+HHzM2zOB02umiPD2b9GTxyMWENgwtOpfHko/RpVGXapXtCKK8V5EKITKAsjIIVBfEVVXl9OjRQ+7evbvqO5gwQdXGJ09y4cIXHDkynh499uPtHVrxtjWIVRyOHFHj6A8dgp9+UuEjUKGi8HD17eWlRsO0bKli/x06qJE8FXkHZmkmMSuRhl4Nix5OzCvMY/HexQgELfxaYBAGjiUf43zmeSZ2mUiHwA527JXEpMVwMeci3Rt3L/dhx9Opp1m8dzGTwifRKqBVUfrehL3M3zmfFQdXEOgZyISwCUwKn1RUplmaeX/X++xJ2MOcQXNo5tsMKSVfHf6KkyknGR82nua+quMhMSuREyknyC7IJrsgm6z8LDLzM5FIvN288XH3oW08btBrAAAgAElEQVS9trTyb0V6XjobT27k2MVjTO42mSZ1m5Sw94/4Pxj22TCa+zZny/1bCKhTPHQmLTeNGRtm8PWRr+ndtDdDWg5hVLtRtKlXOjZ2NPko289up1vjbnRu0Jnd53az8tBK4i7F0b1xdyKaRtC/Rf8yhfJM2hli0mLo3bQ3Hi4e5Jvy+SXmF6Z+P5VzGef4V99/8f2f37P3/F4Amvs2Z2LYRIJ8gvBw8eBY8jE2ntpIVEIUq+9ezV0d7gLg5mU3FwnLQ90f4oHwB/Bx9+Gdne+waM8ixnQcw48nfiTIJ4hvxn7DplObWHN0DRFBETzZR1VqVlYfXs2j6x8lMSuxhO1t67UlJi2GQM9Alt2pWsm74nfx5o43OZl6kv7N+3Ps4rFS2zXzaca9ofdyPvM8Xx76ktzCXJr6NOWpPk9xa5tbqe9Zn73n9zJl7RROpZ7i5pY3M6L1CCLPRPLtsW95c+ibuBpceX7L86TnqaiBi8GFQnMhnq6ehPiFcCT5CGZZeohcQ6+GdGnUha6NutLCtwUSSVJWEkv3LSUmLaYon7vRnb7N+zKwxUAWRS3iUt4lvhrzFcNaDyu1z5yCHDad2sSvsb+yJ2EPablpvHzTywxpNQRQ4j9r0yw+3vsx7eu3x2Q2cTzlOADt67fn3RHv4u/hz+bTm5m/cz4pOSm8MOgFUnJSWLJ3CUaDkdgZsaUE1RGEEFFSyh4O5b2e3l1dbcF46CH47jtISCAr6zC7dnWifftPadRoQs0ZaYe0NPj+e/X55Rf1EJIVX1+48Ubod3MqYb3S6NzOE986dfF09Sy1n0JzIadTT7P/wn72XdiHQRiYecPMopDEZ/s/471d77H/wn6yCrIY3WE0y+5chlEYGfPVGNb+ubbUPgUCo8HIE72fYHzYeM6mn+VkykmOXTzGkeQj7L+wn7TcNABmRMzgzaFvIoQgtzCX6PPR9A7qjRCCvMI8IhZHEH0+GqMwcm/ovdR1q8vPp3/m2MVjeLt5MzFsInEZcaw/vp5CcyHDWw9nctfJvPvHu/xy5hcMwoCnqyczb5jJDyd+YGfcTgAMwsBNITdxLuMch5MOO3TO3Y3uFJgLiiqNgDoBLLptEaM7qqFJv579lRGfj8DPw48LWRfo0rALmyZuws3oxtpja3liwxNcyLzA3zr+jQOJB4rK7da4G3d3vJuR7UbSrn473t75Ns/8/Ax5prwiW83SjJvRjaY+TTmVegqA1gGtebb/s4wPHV8UXloavZRH1z1KTmEO7kZ3QhuGcjjpMNkF2TT1acqau9fQM6gnZmlmw4kNSCTDWg0r8iKsmMwm2rzbhiCfILZN2sbR5KN0WNiBZ/s/S25hLm/tfKtE5flMv2d46aaX2H52O8M/H052QXaRjSdTTuLt5s3ojqNxM7hxJv0MG05uoFvjbvx7wL8RQpBvyqdHkx4E+wWzN2Evd6+6mxMpJ4r236F+B94Z/g5DWg0pCqudyziHu4s7ablprDi4gp9O/kQd1zpMCJ1A/xb9+TDqQyLPRJY4rtYBrfn49o8ZGDwQgAJTAeNWj2PNkTUA3NzyZl4d/Crt67enjksdtp/dzoqDK4hJj6F3UG/6NO2Dn4cfZmmmjmsdWvm3oq572bMemcwmNpzcwPnM83Rr3I2OgR2LKui4S3Hc9sVtHEw8yPLRyxnTaQwAp1JP8ezmZ/n+z+/JzM/E1eBK5waduZR3iZOpJ5nSbQouBheWRi8lz5THP2/4J7MHzcbd6E7kmUjiLsUxptOYEkKQlJXE5LWT+e7YdxiFkVHtR/Fw94cZ3HJwkVdWGbRgVJUZM9T4zLVrke+8w74bv8d7xOO0bv1GzRmJ6nz+6SfYvLn4Ya3oaDXSp1EjuOkmNSY/PFyFlGILonnn97dZfnA5+abi2JOfhx/NfJpRx7UOJrOJjPwMTqWeotCsnoYyCANSSjoEdmDZHcv4MOpDPtrzEV0admFgi4G4Gd14c8eb9AzqSaBnIOuOr2PhLQsZ1W4UZ9LPYDKbaFe/HQLB0z8/zeK9i0sch7+HPx0CO9A5sDPdm3Rnb8JePoj6gJk3zGRwyGCmrZ/GydSTTO46mfdufY9/bvwnb//+NotHLuZg4kE+2P0BBmFgQIsBDG89nIldJuLnocZqJmYl8lHURyz4YwGJWYn4uPvw1rC3GBQ8iOnrp/PDiR9o5N2IV256hUHBg1i8dzFfHf6Klv4tGdRiEGENw/B286aOax283bzxdlNzKWTlZ5Gam8rR5KMcSjyEt5s3Q1sNxcfdhwe+fYDd53bTJqAN7i7unEw5STPfZvw88Wf2JuzlrpV30dSnKYlZiWQXZNMxsCOf3PEJPZqoey02PZZVh1ex4tAK/oj/o+gcpeamMrLdSF4Y9AJHko6w9/xewhqGcXvb2/H18CUlJ4VNpzYxb/s89p7fi7+HP92bdMfd6M664+sYFDyIx3s9zraz29iTsIewhmEMDhnMTSE32a3cyuLtnW8zY8MMdk3ZxbJ96nqInRFLA68GnEg5wfGLx7mUd4l6nvW4ueXNRdttO7ONb499y72h99KtcTcOJh5kbuRctsZsxSAMuBvdmdp9KjNvmFmiH8WWjLwMPtn3CY29G9OtcTeC/YIrnHYnJScFN6Nb0X8HEHUuisNJh7mYcxFXgyuTuk4q1XAqMBXwyrZXCG0Yyp3t77xi0/tk5GVwyxe3sCN2ByvHrCSgTgCjV46m0FzIuE7jGN1xNANbDMTdxZ2cghxmb53NG7+9gavRlfvC7mNGxAw6NejkUFlSSn6N/ZUQvxCHwoXloQWjqjz3HLz8ctHihXENOD8rjC5dNlbbtuz8XH6L9ODDD5UXkZurwkkhIaqTOTwcug89xgdxj1DPsx4tfFuQkJnA9rPbOZt+Fk9XTyaFT6J74+7kFOaQnptO3KU44jLiyDflF7W82wS0oU1AG0IbhtIpsBO/xv7K+DXji1z+Z/o9w4s3vljUAv3m6DeMXzOe7IJs3rvlPR7p+YjdY9ibsJdjF48R4hdCS/+W1PesX+JmlFIyff103tv9HqDCEYNaDGLRnkV0a9yNPQl7eKzXYywYsUCdk4JsXA2udisZUPHZn07+RLfG3Wjq07SonIOJBwnxDylRmVSXAlMB83fMJyohigJzAT7uPvzn5v8UdfquOryKF395kYEtBnJb29u4MeRGuyGA2PRYvv/ze3458wvDWw/n/i73V1hxSSlZd3wd3x37jj0JeziVeorHez/O8wOeL+UxVIVLeZdoOr8pN4XcxJaYLdze9nY+u+uzau9XU0xGXgbDPhvGrnPqEbI2AW347p7vaB1Q9sMxMWkxeLp60sCrwZU0swRaMKrKmjXw1FMwfTosXkxGoyz2z83mhhsSHWql5BXm8dXhr/jq8FcEegbSMbATR8+k8M3Rr0kSh2Dn4wREvcG9Y125dWQ+fu3207tZccz/7q/u5vs/v6eZbzPOpp8loE4A/Zr3Y0DzAdwbei/+dao2w/v5zPM8v/l5RrUfxW1tbyu1/lDiIeIuxZUZe60sZmlm7i9zcTO68WSfJ3F3ced/e//H1O+n0jGwI79P/v2q6zz+KzHjxxm8/fvbAPz24G/0aaafUqtp0nPTuWvlXfi4+7B01FK7o6uuFrRg1AR33EHB0T/49YME+vSJx929SbnZF0Ut4rnNz5GUnUTTus1Jy8whUyaB2QBn+9OwThAXGn5Bv2YDuK3tLbz7x7vEZ8SzYPgCHuv9GEeSjtDpvU483e9pXh788nU3U+6fF/+kvmf9Ep3GmivP6dTTtH63NV0adiFqatR1dY1pqkZlBOPqGbt4tRESgsumn0BCZma0XcEwSzOzNs3i9d9eJ6LRIPokPMNP/xlMbo6Brv2SGH+PkfumBdCgAXy+/xamrJ3C9thIBocMpqV/S/656Z8MaTWEl7e9TB3XOjwR8QTAdXcjt63XtrZN0AAh/iEsHbVU9U1dZ9eYxvlowbBHcDAiKwfXdCUY9erdUipLem46j6x7hOUHl9Mx+xF2PfYuQhq57z54/HEIDw8skX982Hh6BfUipzCHsIZhJGQkEPp+KKNXjuZo8lFmRMwg0CuwVDkaTU1yX5f7atsEzTWKFgx7hIQA4HOxCZmZ+0qsklLyxYEv+MfGf3Ah8wIev77C0c2zePghwb/+Vf48RLZj9BvXbcwHt33AmK/G4OHiwT9u+IdTDkWj0WhqAi0Y9rAIhm9qEAk2U4TEX4rnwe8e5KeTP+GT0RO5fC19O/TgnX1q+uvK8reOf2PujXOp71n/mpmCQaPR/DXRgmEPi2CsS8nm1V/+ZGD8ONrXD+WNHW+Qm59P3W3/Je+3h/ngLSNTp1Zv3qXnBjxXQ0ZrNBqN89CCYQ9vb6hfnw+J5VIh/HD8Bz478CXtvfpw6r1lNPFtzVe/l5wWW6PRaK5nKv8c+V+Io50bsc/9Eve1MLLjrr/zfptYjj+zjc5NWrN9uxYLjUbz18KpgiGEGC6EOCaEOCGEmFXG+reEENGWz59CiDSbdSabdd850057fNlRIiTc3rInmzdnMX1iUyJ6Gdm8ufbfFKbRaDRXGqeFpIQQRmAhMASIA3YJIb6TUhbNDCelnGGT/zGgq80ucqSU4c6yryKklKyon8CAswIfeRdPPz2R1q1NrF9vvCqnBddoNBpn40wPoxdwQkp5SkqZD6wARpWT/x6K3+hX6+y/sJ+jhhTGHBA8Mf0BcnK8WLLkVy0WGo3mL4szBSMIiLVZjrOklUII0QIIATbbJHsIIXYLIXYKIe5wnpklsU7jvOLgCowYOHX4H/y+N5CZMx+mYcN1V8oMjUajueq4WkZJjQNWSSltXybaQkoZL4RoCWwWQhyQUp68fEMhxFRgKkDz8p6Yc4DFexYzee1k6nvWJ6cgh37+fXkvew73RJzmjjtiSEs7UfFONBqN5jrFmR5GPNDMZrmpJa0sxnFZOEpKGW/5PgVspWT/hm2+RVLKHlLKHoGB1ZtWY+/5vXi6enJn+zvp06wPLvueB2BexDf4+Q0kI2M3hYWZ1SpDo9ForlWcKRi7gDZCiBAhhBtKFEqNdhJCtAf8gR02af5CCHfL7/pAX8Cx16hVg7hLcbTyb8Wi2xfxSvuN/LxkCE96f0Tz1H34+Q0ATFy69JuzzdBoNJqrEqcJhpSyEJgObACOACullIeEEC8KIUbaZB0HrJAl51nvAOwWQuwDtgDzbEdXOYu4S3FFL+l56ilo0ABmdf4eTp/Gx6cvQriRkvKDs83QaDSaqxKn9mFIKdcD6y9L+/dly3PK2O434Io/Fhd3KY6ujbpy/Dhs2wZvvgl1oxvB5s24GDwJCBhKUtJqWrWar6eG1mg0fzn0k94W8k35JGYl0tSnKestEnfnnUD//hAfD3//O4H+d5KXF0tGxq5atVWjqRbvvANbttS2FZprEC0YFhIyEpDIIsHo0MEy/+DkyTBnDixdSoNpazDku5CUtKq2zdVoqs7s2fDxx7VtheYaRAuGhbhLcQDUc2vK1q1wi/V9SUKoG+yddzB8t45OHzcmKWkV19OrbTV/IfLzIT0dkpJq2xLNNYgWDAtWwYg71JT8fBvBsPL44/DUU9T7KpY620+Tmbn3yhup0VSXixfVtxYMTRXQgmHBKhhRW4Pw9oZ+/crINHcusl0b2r0Gyac+u7IGajQ1gVUotGA4l/x8MJkqzneNoQXDQtylOLxcvdi0zpchQ8DNrYxMdeogln2G+0Xweu4jSj6YrtFcA9gKhg6rOo+bboKHHqptK2ocLRgW4jLiCHRvSlysKB2OsqVXL3Km3UGD7zNJ2fDyFbNPo6kRrIKRnw8ZGbVry/XKsWPw668QGVnbltQ4WjAsxF+KxzVHPbQ3YkT5eeu8tJSCABdcn5mH2VRwBazTaGoI21CUDks5h6++Ut8nTkBWVu3aUsNowbAQdykOY1ZTfH0hqMw5dYsRPr7kzvo7PntzSP90ZvULl1KHBzRXBi0YzmflSnB3V/f0oUO1bU2NogUDMJlNnMs4h0xvWqFYWPF+4l1ygt3xmPMe5ryc6hkwcqR63kOjcTbJycW/tWDUPEeOwIED8PDDann//tq1p4bRggFcyLqASZrISw6iSRPHthGuruTPfYo6ZwrI+O/0qheekwMbNsDmzRXn1WiqS1ISeHoW/9bULF99pZ7dmjkTvLy0YFyPWIfUZsY77mEA+Nw7l6y2Hri+9znSXMURU3/8AQUFEBOjHqjSaJxJUpKaxsD6W1OzrFypxuQHBUFoqBaM6xGrYKSeaeqwhwEgDAYKpt2H56k8Ln39atUK3769+PfBg1Xbh0bjKElJ0KKF8jK0YNQsR46oPou771bLYWEqPHUd9U9qwUCNkAIwpVbOwwDwmTKffH8DvP1W1Qrfvh3q11e/r7PWiAYVooi3996wWiApCQID1UcLRs0SFaW+b75ZfYeFQUoKnDtXdn4pYehQmD//ythXA2jBQHkYrgY3yK5fKQ8DwFDHm5xJQ/HdnkJG1JeV29hkgt9+g9Gjwc9PC8b1RlKSam2+805tW6Iwm9XUIFownMOpU+o7OFh9h4Wpb3v39Z49sHEjvPoq5OY63byaQAsG6qG9AJcgQFTawwDwfmohZlfIe/1fldvwwAG4dElNoR4Wdn0Lxp13wuuv17YVVxZri/NqCTWmpirR0ILhHE6fVn0XHh5quXNn9W3vvl6xQn0nJ8OXlWxs1hJOFQwhxHAhxDEhxAkhxKwy1j8ghEgSQkRbPpNt1t0vhDhu+dzvTDvjLsVRV6qH9irrYQAYm7Qke2Q3/Nae4ULM/8rP/OSTcO+96sbdtk2l9etXHO80mytvwNVOVhZ8+y38cI2/rTA3Vz3B6yi7d6vvq2UsvlUg6tevvmDs3Alvv10zdl0vnDpleSeCBX9/aNasWDBOnVKiDeo+X7lSzXLaoQO8++410dfhNMEQQhiBhcAIoCNwjxCiYxlZv5RShls+H1u2DQBmA72BXsBsIYS/s2yNuxSHe15TDAZo1Khq+/B6ZB4u2ZDyyTRyck6XnSk3FxYtguXL1ev8tm9XF1SLFkowMjLgzJmqH8jVwvz58MUXxcsHD6qb4dix2rOpJli6VIn71q2O5bcKxtmzypOsbawCYfUwEhOrvq85c2DGDFXpaRSnT0PLliXTrA3Bjz6C9u3VHFP5+Upwz56FceNg+nTljf7xR+3YXQmc6WH0Ak5IKU9JKfOBFcAoB7cdBmyUUqZIKVOBjcBwZxgppST+UjyGzKY0bAguVXxprRh0E7JJIxr8VMCRI/dhNheWzrRli2ptt20LTz8NP/5YPC1uRfHOa4l580qGn6Kj1fe5c9f2/EW//66+X3nFsfxRURAQoH4frsQr6fPyYO5cqFev5Ci66nK5YOTkVG3qisxMdS0LAY8+Chcu1JyN1yr5+RAXV9LDgGLBmDpVhaiio+HFF1UIyt0dRo2C++6DunXhv/+tXJlxcVc8IuFMwQgCYm2W4yxplzNaCLFfCLFKCNGsktvWCFFTowg4Mb1K4agijEbE+PsI+EOSE/srcXFljHz47jv1ME9kJDRvXtx/AdCpk7oBr3XBuHhRVUz796uKBYoFA+D48dqxqybYs0e1KDZuhF0VvKb3wgV1Q99zj1p2NCy1Zw+Eh8O//63CF8uXV89mWy4XDNu0yrBpk6og//tf9R8/8sjVF05ZuPDK9h2dOaPOweWCYW0Q/utf6pqZNEl1cn/yiQpH+fgosXjgASUijorvuXPQqpXjjZcaorY7vdcCwVLKMJQX8UlldyCEmCqE2C2E2J1UhYtfCEGnBp1IjWlepQ7vEkyYgCg0EfxHGDEnZlP48EQ15Yd1rqi1a2HYMGjYULny3brBrbeqbb291QVwrQuGNexkNhe72NHR0KBByfVXM4WFsGaN8gLz8lRaTo6q9B99VI1oe7mCmYqtHd5/+5t65sGRyqugQIUoLl2C9evVtfHjj8Xr09Nh2rSq9z1c3odhm1YZvv8efH1hyhTlCX39NaxbVzWbKsuyZUqgHnwQ/vGPst85ce6cCvO89NKVsQlUOApKh6RuuUWd43nzwGhU/T7Nmqn/cty44nzTp6v/f9Eix8pbtUqJ9ptvXtlwp5TSKR+gD7DBZvlp4Oly8huBdMvve4APbdZ9CNxTUZndu3eXVaVePSkffrjKmxcTGipNPcJlUn8Xq0xIuWqVlFFR6vfSpfa3vesuKdu2rQEjapHFi4uPe+5cKQsLpfT0lPKhh6QUQso5c2rbwvJZulTKoKDiY/juO5X+++9qefVqKf/9b/V7/377+3nhBXW8ly5J2b27lEOGVFz2Bx+ULPPdd9Xy8eNq+a231PJ//lO1Y/u//5PSx0f93rFD7Wvdusrtw2SSslEjKe++Wy0XFEjp6yvl1KlVs6kynDwppdGojqFBA2X/li2l833+uVrn6ytlfr7z7ZJSyvffV2XGxlacd8cOKUePljIrq2T68OFSNm4sZV5exfvo10/Khg1Vma+8UjWbLQC7pYP1ujM9jF1AGyFEiBDCDRgHfGebQQjR2GZxJHDE8nsDMFQI4W/p7B5qSXMKeXkqklJtDwNgwgQMu6Opv62QE49CQcdmqnNw+XIVcirvZRthYSpkk51dA4bUEseOqbdPtW8PO3aoKZ6zsyEiQnXuV9bDkFKFF6xj3J1JXJyKNTdpAqtXq/ChtYW/Z4/67t5dva7Xy0vZZY+oKGjXToUbOneuOCSVnQ0vvAB9+8Jtt6m04ZZuuw0b1HlYskQtr15dteOzPrQHVfcw9u6F8+eLbXRxgQEDVJ+Gs3n9ddVKP3wYTp5UfQDffFM6n9WW9PTKjWqrDqdPq+vekbh2RITyEKxzell5/HFISKj4/42PV31b06erdzHMn3/lplF3VFmq8gFuAf4ETgLPWtJeBEZafr8KHAL2AVuA9jbbPgicsHwmOVJeVT2MU6eUUC9eXKXNS3LunJRdukjz4o9lVFQfGb2wjtq5EFL27Vv+tuvXq7xPPiml2VwDxjjIkSNSbtpUcT6zWcqdO6V89FEpw8PVibuckSOl7NRJyilTpPT3l/KLL9Qx7d0r5bBhUnbrVjnb9uxR20+YULntHCE/X7WYrUyfLqWLi5SnT6vl22+XMiREHffkycoNtf4vd9whZYsW9v+nJk2kHD9e/X7tNXUMKSn2bXnlFZVn+/aS6S1bSnnbbVLu3q3Wt2unvs+erfzxDhkiZe/e6nd6utrP669Xbh9z5qhrOSmpOO3NN9W+4uIqb5OjnDsnpbt7SU/mllukDA4u/R+0aiXlTTdJ6eYm5VNPOc8mW8aMqX50wGSSsk0bKSMiys/3zjvqfB89KuVvv6nfb7xR5WKphIfhVMG40p+qCsb27epM/PhjlTa3S25uvNyxI1ieH+GuCpg3r/wNzGYpp01Tef/1L3UBHT0q5bZt1TVEyvvvL9t9l1LdXN7eFbvC06cr2zw81Pdrr5XO07atcreXLFF5Ro2S0tVV7fvxx1U51hv84YelXLiw/DL/8Q+1H09PKTMyKjpSJWJ33qkqxi++UCETewwbJmVoqJQXLkgZH68qpMmTi9f/97+q7D//VEJnG1ZauLB43eWcO6fWvfWWWl63Ti3b+x/371fhk9tvL73u0Uel9PJSAuzuXhwae/vtis/F5YSHK/GRUv0Hbm5S/vOfKmzYvbuqaO+5R8pPPil7e7NZ5bvhhpLpVlH/7LPK22QPs1ldRyNHShkTI+XMmVIaDFKeOFGc58MPVbn79hWnnT2r0ubPV/+vbSW+caOUBw9Wz670dCmTk0und++uyqsuCxYo+6dOlbJPHyUel9+X/fqp69bK4MEqPJWTU6UitWBUki+/lBWGpKtKdvYJ+fv3DeT5Wz1lzumoijcwm1VFCqpytcbSq3MzWuOrTZuqmLotiYnqRgQpf/nF/j4KC6X081M3cFqalJ07S3nzzSXz5OerFvozzyihA7XvLl3UemsFHB8v5YEDxce2YIH9MoOCVEve9hzEx6uW5cqVxXnNZilffVWJmZeXqihAVYK2FYoVa4sdlEd0333Kdluv6cQJWdR6c3WVctas4nXHj6t1ZQne2rVqXWSkWo6JUcsffKAE7K23pPz6a3WDb9ggZd26yiMpS3y+/VYWeajjxqm0zp2l7N+/7HNWHkFBUk6aVHrZKmj9+6sYOhR7WVKqc/vDD6ryKqs1a702/v53+2UnJamyPv3UsRj9V1+psoxG9X96eioxsyUhQZ2XF18sTlu2TBZ5tNY+oGPH1LVj+3+/+Wb5jYmyyMlRFbWnp2r82R5HQICUjzxSuf2VRXq68mRdXdX/DOoasBIXJ4v6B63s2qX+nypGJbRgVBJrX+LFi1XavEIyMvbJyEhvuWdPP2kyOXCRmkyq8nvoIRUnu+EG1dFnexM7Sm6ulM2aqYpTCCmfeKLk+kWLim+kZ56xv59du1Sezz9XyzNmqBZvdnZxniNHVJ5ly9TFGxCglu+/X63fuFEtb94s5XPPKTEZOlSlffRR6TI3b1brli+Xsnlz1SkopdqfNTxTWKjSVq9WaXfcoToeTSYp16xRFXGzZqpysWXiRCXI33wjZR1L2PDBB0vb0Lq12h5KC1RwsPKgLmfcOCU+Vo/IbFZlTZumyrCe77p1VYUYFma/s/TSJVV5gJQ//aTSZs9W/+Xlx1Qeth6FFavHcdddUgYGqgrQKvTvvVeczxpyat5cNT7KqmhHjVLXmJSqQfHYY1IeOlS8fsKE4uNu0qT8BlBurgrFde6sOrpHjFC2l9Wii4hQrXsrkyap685kKo41P/CA+o8HDFCNln79VHqfPipPXp7y3KyDC+wxY4babsAA9d2hg2q8pKVJux53VUhJUfdVfr76X0aPLl43f74sCkfVEFowKsk//qHqPmd2G5w//7ncsgV58uTTld/49GklGH37Vr5VZPUuNmxQLSCDQbWurQwZonWzbpsAAByISURBVCrFvn2l7NHD/n7+8x+1H2sl9cMPxfu18vXXKu2PP9TyrbfKEqEZa7jg/fdVrHbwYFU5DBumKs6YmJJlTp6sKtqsLCVmBoMaQQSqkrCOQCsokLJ9e3UDX35+oqJUZRERUeyynz+vKqDp09Xy5s1S3nhj2YJsDcOBqrxsmTpV/S+2I3E+/VTlfeGFknl791ZlghLLn35Sxzd1qmpVlsfNNysvyyqO+/cXn0dHuXRJluqzGDJEVfIuLsWxfrO5uN/Euty2rbo+yvMMrK2us2eLBb1ZM1WhWq+V555T/XQ9e6pzYVtBJyerUJ6UxQJljRGbzaU9YyuvvipLjE4KDlYhSSudOski7/r8+eL05ctVGNDTszjE6uZmP2xobexYr5m1a5WQP/KI8mZAeUU1zeOPK7tSUtR90KRJ6ZBgNdGCUUnuvVfdI87m6NHJcssW5MWLVegssbrUU6eqStYeOTmqYktOVjd4s2aqJWU2S5maqoZEhoeriy85WVXUs2Ypt/7yzkxbhg6VsmPH4uWsrNKditab11oBvvyyWrb2nZhMqvIeNEilL1qk0s+cUWLw7LPF+8rNVWGOiRPV8qFDsqj/pGFDdSytWyvh+PhjtW7NmrJtt3ofI0aosl58UTrcSrOGl/z8SrcorGETa0f1iRNK4Pr1Ky1cVs/isccq3zI5d65kqMxsVoJ7442O7+PkSVW+7bDue+8tFsPDh4vTp01TFWlOTnH/hPW/skd0tMo3erT6Hj9enYuuXZXYtW9ffN2eO6fWWUXp7NniocxduqiK3NH+gMOH1XZTpqjr7PIQ5/PPq9agtRFjS0yM8kiefFLKFSuUMNarVzI0mJurYtZNmqhjsB0K+/DDSjSsndBRDoScK4vVs//ww+JGW3mh4yqgBaOSDByo7nFnU1iYJf/4o7OMjPSWKSk/V34HM2eqvywsTLUyrRXPrl2qVRcYWFwB2HZO23oBa9cqYRg9WoWBQHkc1s7U5ctLl5uXpyp6a+vKyuDBJTvfHnhAxcCtnD2rbirbzrguXVQ5Li4lOw9vu02JmbW1bq3kbUcidOum0pYsUcvWcJq3t5S9epVfES9cqM5HnTqqQrKGtyoiI0MJ4003lV538aISutmzlRCFhythOXOmdN7oaNW6tx2VVR3mzpVF8XlH2LlT5f/+++K0//s/lXZ5i9Xap7Fhg7rmLv+vysJkKg5Bdu+u/sd164r7xy5vub/+ukr/9FPlBfj4qPM4cKB6xuLAAceOy2xWITXb695227w85eU4wvHjUtavrxoijz+uPBXrMTVvrjwJW86cUYLh66vylDcKrqqYzcpz7tpVjTq85ZYaL0ILRiVp3VrKsWOrtGmlyc2Nl7//3klu3eouk5K+qfwO1q4tFgZ39+KHd7y9lWi89JKqUOfPV63Z558vXZFaXX5v7+Jho4WF6oKcNEm1jh95RD2cVVioWjSgQk62WFs81lBCRETFrd4xY2RRa9+W77+XRSGmtDRlV0hIyZb6N98o+6yVbm6uavmBlD87IMCnT6vyhXBsGLGVd99VYZWy6NVLhTt8fNT5tK2QnUlCgqrIn3yyOO2zz+yXbw3l/f57cdpLL6m0y8eTZ2UpcX3sMVVROlpJjR6ttrP1VtasKds7yctTLXZrKGjzZsfKsMfJk6oz/plnqhdb3r5d3Qfe3qqivuceJZzWcODlPPSQLPJAnYV1yLUQZQ/gqCZaMCqB2ay87xkzKr1plcnPT5a7d/eSW7YYZVxcBcNKyyIhQcWM//lPVYEuWFBxHNwWs1kJAqgWpJUxY1QFfM89sqi19vLLquVnMJRuQVljt598ovbp51fxSJHnnivexpbCQlU5DR6syjca1Rjzili9uuToJUewFw+vCs8/L4s6Qst6LsWZjBmjKrfsbFXRCaGEy3b0xokTKsTk7a3WW8VdStXqHziw7OHKI0aoG8PqBThCfHzpVnh5/PyzumbK8mprk8JCx0UnJkZ5GV27Os8ea8jW+lxPDaMFoxKYTKrRHh1d6U2rRUHBJblv361yyxbksWOPSJPpCk1hUGyAiovaVi7WEBWoYYNjx6qKu0WLsjvETSYVPggPVzFgUPHc8ti+XXkiZQmctcV7+bDBq5mMDNV6t9cCdSbWUWQLF6rO68aNlSj8619qfWysism7/X979x4eV30eePz7zn0kjWZ0A19kG9soYDtgGxvHxIRLDE9JmgWnkEKS0jShDzRJS2l3aaFsd7vZ5El2CU3aLWmTAIUkPISEJo0XlhIu5hIHG5tgCMbYlgWyZMm2JEsaSaO5v/vHOTLji/D4Is+M9H6eR490rvP+ztHMO+f3O+f3CzhtQcdTxz72PEAodHxfRo5XKY7bqXbvve/f5c+p8PLLp/aLTgFLGBUin89qa+vtum4dumXLlZrJFPFg2kTq6nK+5Y/dZ9/f70zDobdjFnroIafed+yD/mSefuzudj7cLrlkcnyQTLR83qnW8XqdY//888630HDYSRYXX+xcWZzILZhjz5kU3tJpJiVLGBWmq+t+XbfOq5s3r9B0+hiNixPt8Evxl15yGvVefnn8bZJJp/78i188skO147Vtm+rw8MntYyr59redt/HY8zU7dzoJZOzZkZN54PNb3zr5J6NN2TuehCHO+pPD8uXLdfPYKGcVprf3F2zdej3h8DwWLnyUmprzSh3Se1SdjhNN+UmlnI7srr32vbGkb7nF6Sb7859/r8NCY8YhIq+q6vKi1rWEUT4GBl5g69bryGYHmD37DmbPvguvN1TqsEyl6etzEsWXvuT0qmvM+ziehFHqAZRMgVjsUi68cBtnnPEZ2tu/yiuvnEt3978efbhXY8bT0AC3327JwpxyljDKTCDQyIIFD7F48TMEAk1s3/4FNm8+j9HRXaUOzRgzxVnCKFN1dau54IJXWLTo56TTPWzZchmJRGupwzLGTGGWMMqYiNDUtIYlS54llxtly5bLGBl5u9RhGWOmqAlNGCJylYhsF5FWEbnjKMv/UkTeEpE3RORZEZlTsCwnIlvcn7WHbzuV1NQsZsmS51BNsXnzYtra7iKbHS51WMaYKWbC7pISES/O8KxXAp04Y3x/WlXfKljncmCjqiZE5IvAZap6vbtsWFVrjuc1K/0uqWNJpbpoa7uDfft+iN/fSF3dlcRil9HUdB1+f32pwzPGVKByuUtqBdCqqm2qmgZ+DFxTuIKqrlPVhDu5AWiewHgqXjA4gwULfsDSpeupq7uCgYF17NhxC5s2nUd//3OlDs8YM8lNZMKYCXQUTHe688ZzE/BkwXRIRDaLyAYRWTPeRiJys7ve5p6enpOLuEJEox9m4cJHuOiiLi64YANeb4TXX7+CXbtuJ5XaW+rwjDGTVFk0eovIHwDLgbsLZs9xL5M+A3xbROYfbVtV/Z6qLlfV5U1NTach2vIhItTWfojly19l+vSb6Oj4Jhs2zOLNN69lYODFUodnjJlkJjJh7AFmFUw3u/MOISJXAHcBV6tqamy+qu5xf7cBzwNLJzDWiub1VnPOOd9nxYrtNDf/BYODL7Jly6W89tpH6On5OZlMX6lDNMZMAhPZ6O3DafRejZMoNgGfUdWtBessBR4DrlLVnQXz64CEqqZEpBF4GbimsMH8aCZ7o3excrlRurvvo6Pjf5NKdQIQDp9NdfV5hMMfIBq9iIaGqxHrH8qYKe94Gr19ExWEqmZF5E+BpwAv8ICqbhWRr+D0jrgWpwqqBvip++G1W1WvBhYA3xWRPM5V0DeOlSzMe7zeMM3Nf8aMGbcwOLieeHwjQ0OvkEi8TV/f43R0ZIjFLqOl5TtUVy8odbjGmAphnQ9OMfl8lr1776et7Q5yuRGmT/9jZs26nXB4bqlDM8aUQLncVmvKkMfjY8aMW1ixYjvTpn2B7u772bixhW3bbmRoaEupwzPGlDG7wpjiUqk9dHR8k66u75PPjxCLXU59/ceJRJYTiSzD54uUOkRjzASy8TDMcctk+unu/j5dXd8lmWxz53qprb2QWOxyamtXEolcSDA4vaRxGmNOLUsY5qSk0z0MDb1KPL6e/v7niMc3AjkAPJ4wIn683gjTpt1Ic/NtBAJnljZgY8wJs4RhTqlcLsHw8GvE46+QTneRz2dIpdrp7f0FHk+QhoZriEYvJhb7CNXV5x+8XTebHWJoaBOx2GWIWHOZMeWoLG6rNZOH11tFNLqKaHTVIfMTiZ10dHyTvr4n6Ol5FIDa2pXMmvXXpFLttLd/lUyml7q6Kzj33AcJBt+vZxhjTLmzKwxz0lSVVKqD3t61dHbeQzL5LgCx2Grq6lbT3v5VPJ4g06ffRCg0j1DoLMLheQSDc/B6Q6g61V1OB8fGmNPJqqRMyeTzWfr6/i9+fz2x2KWAcyWyY8fNDA6uRzVz2BYeIA+Az1eH399EY+MnmT37Dvz+2HG99ujoLkKhuVb9ZcxxsIRhypJqnnS6m9HRd0gmnZ98Po3HE0A1TybTSzL5DgcOPInPV8fMmV8iEJiO1xvB4wki4sPvb6K2diUej//gflOpblpbb6On5yc0NX2KBQt+hMcTKGFJjakc1oZhypKIh2BwptuWcfG46w0NvUZb21/R3v7Voy73+eqor78Kr7eWXG6Ivr4nyOeTNDV9ip6en5LLDbNo0WOIBMhmBxDx4vGE8HhCx9V/lmrumNVkudwI6fQ+wuF5Re/XmEplCcOUnUhkKYsXP00uN0I2O0QuN4RqBtUco6M76e1dS3//L1HN4fVGiMUuZf78u6mq+gBdXfexY8fNrF/fQD6fPGS/Hk8V4fDZVFWdS1PT79HYuAbwMjCwjnj810SjFxONXkoisY22tjs5cOBJmpquY86cv6GmZvEh++rre4Lu7gc4cOBJ8vkU5533BA0NV53Go2TM6WdVUmbS6et7kgMH/h9+fyM+Xx2qefL5JJnMfkZHdzI09BvS6S58vnpEfGQy+w9u6/PVkc0O4PXW0ti4ht7en5HLDdHYuIa5c79GKDSHnTtvZe/eBwgEptHYeC2Dgy+RTLazbNlGqqrOOWpMyeRuBgZeZHS0lVSqg4aGT9DYuKaoK57+/ufIZgdpavrkKTtGxoyxNgxj3odqjv7+Z9m790FAaWr6fWKxSxkYeIHe3l8QDE5n1qzb8fvryWT62bPnH+nouIdcboRAYBrpdDezZ/8NZ531d3g8PpLJdl59dTk+XwMtLf9ELhcnm42TzyfIZgfp63uCeHy9++riVqUNEo1eQlPTtQwMvEA8voFY7FKam2+jtnYF4NxA8O67f8vu3d8AYNq0L9DS8n9IpfawZ8+9ZLN9RCIfIhr9MDU1S627enNCLGEYc4ql073s3v0N+vuf5uyz/566utWHLB8YeJHXX1+NavaIbaurP8gZZ9xAQ8PVVFV9APDS3X0f777738hkeggGZxGJrKC//2lyuTih0HxCoVlkswMMD29h+vSbCQTOoL39awQCZ5JO70PEj99fTzrtDMkbjV7CvHlfRyRAZ+c9HDjwFNHoxTQ2/h7R6CqCwVl4PAFGR9sYGXkDUAKBafj9jYgEEPGQSOxkaGgTyeS7+Hy1+HwxqqoWEo1++Iin+VWVfD6F1xsCIJsdZmDgeTKZHqqrF1FdvQivt3pCzsVESiRaaW29lebmv6S+/opSh3NaWMIwpgQSiZ1uVVcMr7cWr7cKj6dq3A4cs9lhMpl9hELzEBGy2SH27n2QwcEXSaW6yeWGmDXrvzBt2o0AHDjwFO3tXycWu5QZM75IIHAmqVQnvb3/Tnv718hk9gHg9dbS0PC7DA6uJ5XaffD1RIIUDGo5Lr+/kVxuhHx+9OC8UGgukciF1NQsZmTkLfr7nyGT2Yff30QgcCaJxPbDbpkWamoWE41eQm3tRdTULCYcbkE1QybTSzy+kb6+tcTjG6iuXkQ0+hECgelks/1ks3FEPIh48fkaCIfnEQjMQDVNLjeCxxPC728E8gwO/orBwfUEAtOor/8YNTVLDl5pqSqjo62MjLxJJrOfTOYAsdjlRKMrAUgmO9i372EikeXU1a0mkdjG669fQTrdjccT4rzzHj/ii8GR5zBOMrkbv7/hkH7WcrkRwIPXGz7m8QanO55kso1IZMVpv1Ism4QhIlcB/4AzgNJ9qvqNw5YHgR8Ay4A+4HpVfddddidwE04nRreq6lPHej1LGGaqyuVG6O6+DxCmTfsjfL5aVJXh4dcYGXmTZHI32ewA1dULqK5ejMfjJ53eSybTi2oW1SzB4BwikWX4/XXuPkcZHt5CPP5r4vENxOObSKXa8fvPoK7uCqqqziGV6iSV6qK6+oPU119JMDiLkZGtDA+/zuDgr4jHf12QeN575gbA56snGl3FyMhbJJO7TrjsHk/44Gv4fDECgRn4/fUkEjsOaZ8aE41eTDjcwr59PzqY5GpqlpBKdSLiY+HCR9m588uMju6ipeU7BIPNiHhIpTpJJt9hdHQXicR2Rkd3ks32u3sVotFLqKu7nMHB9QwMPA9AJLKM2tqVBALT8PkaEBFyuWHy+TQ+Xwyfr5a+vsfZv/8nqKaJxT5KS8s/EQ7PY3j4DUZHWw/e8BEOz6em5gJA6etbS1/fE/j9jUQiy6ipWUZ19aITSjZlkTDEuR9xB3Al0IkzROunC0fOE5EvAeer6p+IyA3AJ1X1ehFZCDwCrABmAM8AH9CxR4LHYQnDmImVzQ7i9UaKfjgyn0+TSGxjePgNEonteL3V+P2NVFWdS23tRXg8zo2aqVQX2ewgPl8dPl8t4LQ1OTcqtJFOd+HxhN3kkHITXYba2pXU1Cwlk+mlv/8p4vGNpNP7yWR6CIXOIhpdRSSyjEBgOh5PmH37fkhHxz2k0/uYPv2PaW6+lcHBl+jouId8fpTzz/8lVVUtpNP72bLlchKJwwf6FILBmYTD51BV1UIoNJdgcDajozvYv/8REom3CYfPobHxPyHiY2DgJYaHf3PI1drhnI48P0coNJf29v9JLjcMyFEecgXwIOJDNY3ffya53BD5fAKfr55Vq3orOmFcBPydqv6OO30ngKp+vWCdp9x1XnbHAN8LNAF3FK5buN77vaYlDGPMseTzWVQzh1QXOZ+D+UOeu8nlRhkZ+S35fNq9AptJKDQbjyd41P2qKtnswMErtML5+XyCTKYPAK+3BhE/2Ww/mcwBwuH5B6st0+n9dHTcDXiIRC6kunqh+3pCIvE2Q0ObyOWGaWxcQ23tRYCSSGwnldpDff2VJ3Q8yuXBvZlAR8F0J/Ch8dZxxwAfBBrc+RsO29Z6rjPGnDTnqubQjz7nm/mhD2l6veGDd6wVQ0SOSBZj873e6iNuAvD5IoRCsw+ZFwicwfz5dx91/+HwPBoaPn7E/OrqhVRXLyw6zpNR8Z3uiMjNIrJZRDb39PSUOhxjjJm0JjJh7AFmFUw3u/OOuo5bJRXFafwuZlsAVPV7qrpcVZc3NTWdotCNMcYcbiITxiagRUTmikgAuAFYe9g6a4HPuX9fBzynTmXiWuAGEQmKyFygBXhlAmM1xhhzDBPWhuG2Sfwp8BRO5eADqrpVRL4CbFbVtcD9wA9FpBU4gJNUcNf7CfAWkAW+fKw7pIwxxkwse3DPGGOmsOO5S6riG72NMcacHpYwjDHGFMUShjHGmKJMqjYMEekB2k9w80ag9xSGUwqToQwwOcoxGcoAk6Mck6EMMHHlmKOqRT2TMKkSxskQkc3FNvyUq8lQBpgc5ZgMZYDJUY7JUAYoj3JYlZQxxpiiWMIwxhhTFEsY7/leqQM4BSZDGWBylGMylAEmRzkmQxmgDMphbRjGGGOKYlcYxhhjijLlE4aIXCUi20WkVUTuKHU8xRKRWSKyTkTeEpGtIvLn7vx6EXlaRHa6v4/soL/MiIhXRF4Tkcfd6bkistE9J4+6nVeWNRGJichjIvK2iGwTkYsq7VyIyF+4/0tvisgjIhKqhHMhIg+IyH4RebNg3lGPvTj+0S3PGyJyQekif884Zbjb/X96Q0R+LiKxgmV3umXYLiK/c7rinNIJwx1G9l7gY8BC4NPu8LCVIAv8Z1VdCKwEvuzGfgfwrKq2AM+60+Xuz4FtBdP/C/iWqp4N9OOM7V7u/gH4D1U9F1iMU56KORciMhO4FViuqh/E6TD0BirjXDwIXHXYvPGO/cdwer9uAW4G/vk0xXgsD3JkGZ4GPqiq5+MMd30ngPs+vwFY5G7zHSkcKnACTemEgTNmeKuqtqlqGvgxcE2JYyqKqnar6m/cv4dwPqBm4sT/kLvaQ8Ca0kRYHBFpBn4XuM+dFuCjwGPuKpVQhihwCU7vy6hqWlUHqLBzgdN7ddgdm6YK6KYCzoWqvojT23Wh8Y79NcAP1LEBiInI9NMT6fiOVgZV/aWqZt3JDTjjAoFThh+rakpV3wFacT7LJtxUTxhHG0a24oaCFZGzgKXARuBMVe12F+0FzixRWMX6NvBXQN6dbgAGCt4olXBO5gI9wL+6VWv3iUg1FXQuVHUP8E1gN06iGARepfLOxZjxjn2lvue/ADzp/l2yMkz1hFHxRKQG+DfgNlWNFy5zB6Mq29vgROQTwH5VfbXUsZwkH3AB8M+quhQY4bDqpwo4F3U431znAjOAao6sIqlI5X7sj0VE7sKpgn641LFM9YRR9FCw5UhE/DjJ4mFV/Zk7e9/YJbb7e3+p4ivCKuBqEXkXpzrwozhtATG3WgQq45x0Ap2qutGdfgwngVTSubgCeEdVe1Q1A/wM5/xU2rkYM96xr6j3vIj8EfAJ4LP63jMQJSvDVE8YxQwjW5bcuv77gW2q+vcFiwqHvf0c8IvTHVuxVPVOVW1W1bNwjv1zqvpZYB3OkL1Q5mUAUNW9QIeInOPOWo0zWmTFnAucqqiVIlLl/m+NlaGizkWB8Y79WuAP3bulVgKDBVVXZUVErsKprr1aVRMFi0o3hLWqTukf4OM4dyDsAu4qdTzHEffFOJfZbwBb3J+P47QBPAvsBJ4B6ksda5HluQx43P17nvsGaAV+CgRLHV8R8S8BNrvn49+Buko7F8D/AN4G3gR+CAQr4VwAj+C0u2RwrvZuGu/YA4JzZ+Qu4Lc4d4WVaxlacdoqxt7f/1Kw/l1uGbYDHztdcdqT3sYYY4oy1aukjDHGFMkShjHGmKJYwjDGGFMUSxjGGGOKYgnDGGNMUSxhGFMGROSysd56jSlXljCMMcYUxRKGMcdBRP5ARF4RkS0i8l13LI9hEfmWO5bEsyLS5K67REQ2FIxnMDYmw9ki8oyIvC4ivxGR+e7uawrG1HjYfeLamLJhCcOYIonIAuB6YJWqLgFywGdxOurbrKqLgBeA/+5u8gPgr9UZz+C3BfMfBu5V1cXAh3Ge8AWnx+HbcMZmmYfTl5MxZcN37FWMMa7VwDJgk/vlP4zTqV0eeNRd50fAz9wxMmKq+oI7/yHgpyISAWaq6s8BVDUJ4O7vFVXtdKe3AGcBv5r4YhlTHEsYxhRPgIdU9c5DZor87WHrnWh/O6mCv3PY+9OUGauSMqZ4zwLXicgZcHDc6Dk476OxHl0/A/xKVQeBfhH5iDv/RuAFdUZH7BSRNe4+giJSdVpLYcwJsm8wxhRJVd8Skf8K/FJEPDg9i34ZZ8CkFe6y/TjtHOB0q/0vbkJoAz7vzr8R+K6IfMXdx6dOYzGMOWHWW60xJ0lEhlW1ptRxGDPRrErKGGNMUewKwxhjTFHsCsMYY0xRLGEYY4wpiiUMY4wxRbGEYYwxpiiWMIwxxhTFEoYxxpii/H9BTX1i5z7tWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.4405 - acc: 0.8999\n",
      "Loss: 0.4404968751553806 Accuracy: 0.89989614\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1318 - acc: 0.3443\n",
      "Epoch 00001: val_loss improved from inf to 1.76927, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/001-1.7693.hdf5\n",
      "36805/36805 [==============================] - 432s 12ms/sample - loss: 2.1320 - acc: 0.3443 - val_loss: 1.7693 - val_acc: 0.3818\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2394 - acc: 0.6033\n",
      "Epoch 00002: val_loss improved from 1.76927 to 0.81950, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/002-0.8195.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 1.2394 - acc: 0.6033 - val_loss: 0.8195 - val_acc: 0.7645\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9194 - acc: 0.7161\n",
      "Epoch 00003: val_loss improved from 0.81950 to 0.66411, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/003-0.6641.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.9194 - acc: 0.7161 - val_loss: 0.6641 - val_acc: 0.8120\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7373 - acc: 0.7746\n",
      "Epoch 00004: val_loss improved from 0.66411 to 0.58659, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/004-0.5866.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.7376 - acc: 0.7746 - val_loss: 0.5866 - val_acc: 0.8402\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6298 - acc: 0.8107\n",
      "Epoch 00005: val_loss improved from 0.58659 to 0.56573, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/005-0.5657.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.6301 - acc: 0.8107 - val_loss: 0.5657 - val_acc: 0.8351\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.8349\n",
      "Epoch 00006: val_loss improved from 0.56573 to 0.45030, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/006-0.4503.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.5528 - acc: 0.8349 - val_loss: 0.4503 - val_acc: 0.8705\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8571\n",
      "Epoch 00007: val_loss improved from 0.45030 to 0.37679, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/007-0.3768.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.4816 - acc: 0.8571 - val_loss: 0.3768 - val_acc: 0.8924\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8682\n",
      "Epoch 00008: val_loss improved from 0.37679 to 0.37069, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/008-0.3707.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.4348 - acc: 0.8682 - val_loss: 0.3707 - val_acc: 0.8994\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8820\n",
      "Epoch 00009: val_loss did not improve from 0.37069\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.3912 - acc: 0.8819 - val_loss: 0.4352 - val_acc: 0.8717\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8906\n",
      "Epoch 00010: val_loss improved from 0.37069 to 0.35319, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/010-0.3532.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.3596 - acc: 0.8906 - val_loss: 0.3532 - val_acc: 0.9017\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9020\n",
      "Epoch 00011: val_loss improved from 0.35319 to 0.29083, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/011-0.2908.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.3243 - acc: 0.9020 - val_loss: 0.2908 - val_acc: 0.9189\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9096\n",
      "Epoch 00012: val_loss improved from 0.29083 to 0.26659, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/012-0.2666.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2996 - acc: 0.9096 - val_loss: 0.2666 - val_acc: 0.9236\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9131\n",
      "Epoch 00013: val_loss did not improve from 0.26659\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2814 - acc: 0.9131 - val_loss: 0.2745 - val_acc: 0.9231\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9189\n",
      "Epoch 00014: val_loss did not improve from 0.26659\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2621 - acc: 0.9189 - val_loss: 0.2999 - val_acc: 0.9187\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9237\n",
      "Epoch 00015: val_loss did not improve from 0.26659\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2522 - acc: 0.9237 - val_loss: 0.3455 - val_acc: 0.9068\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9305\n",
      "Epoch 00016: val_loss improved from 0.26659 to 0.24918, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/016-0.2492.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2276 - acc: 0.9304 - val_loss: 0.2492 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9328\n",
      "Epoch 00017: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2198 - acc: 0.9328 - val_loss: 0.2662 - val_acc: 0.9285\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9382\n",
      "Epoch 00018: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.2000 - acc: 0.9382 - val_loss: 0.2742 - val_acc: 0.9231\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9417\n",
      "Epoch 00019: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1867 - acc: 0.9417 - val_loss: 0.2737 - val_acc: 0.9250\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9448\n",
      "Epoch 00020: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1800 - acc: 0.9448 - val_loss: 0.2729 - val_acc: 0.9301\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9460\n",
      "Epoch 00021: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1739 - acc: 0.9460 - val_loss: 0.3098 - val_acc: 0.9189\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9498\n",
      "Epoch 00022: val_loss did not improve from 0.24918\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1594 - acc: 0.9498 - val_loss: 0.2655 - val_acc: 0.9311\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9541\n",
      "Epoch 00023: val_loss improved from 0.24918 to 0.23223, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/023-0.2322.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1499 - acc: 0.9540 - val_loss: 0.2322 - val_acc: 0.9390\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9529\n",
      "Epoch 00024: val_loss improved from 0.23223 to 0.21025, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/024-0.2103.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1531 - acc: 0.9529 - val_loss: 0.2103 - val_acc: 0.9408\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9573\n",
      "Epoch 00025: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1347 - acc: 0.9573 - val_loss: 0.2352 - val_acc: 0.9392\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9595\n",
      "Epoch 00026: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1286 - acc: 0.9595 - val_loss: 0.2991 - val_acc: 0.9164\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9596\n",
      "Epoch 00027: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1278 - acc: 0.9596 - val_loss: 0.2626 - val_acc: 0.9320\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9619\n",
      "Epoch 00028: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1208 - acc: 0.9619 - val_loss: 0.2112 - val_acc: 0.9471\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9644\n",
      "Epoch 00029: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1122 - acc: 0.9644 - val_loss: 0.2492 - val_acc: 0.9383\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9667\n",
      "Epoch 00030: val_loss did not improve from 0.21025\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1068 - acc: 0.9667 - val_loss: 0.2199 - val_acc: 0.9434\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9676\n",
      "Epoch 00031: val_loss improved from 0.21025 to 0.19807, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_7_conv_checkpoint/031-0.1981.hdf5\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1033 - acc: 0.9676 - val_loss: 0.1981 - val_acc: 0.9511\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9686\n",
      "Epoch 00032: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.1006 - acc: 0.9686 - val_loss: 0.2274 - val_acc: 0.9443\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9700\n",
      "Epoch 00033: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0951 - acc: 0.9700 - val_loss: 0.2300 - val_acc: 0.9406\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9711\n",
      "Epoch 00034: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0909 - acc: 0.9711 - val_loss: 0.2718 - val_acc: 0.9304\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9728\n",
      "Epoch 00035: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0831 - acc: 0.9728 - val_loss: 0.3104 - val_acc: 0.9203\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9751\n",
      "Epoch 00036: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0785 - acc: 0.9751 - val_loss: 0.2561 - val_acc: 0.9331\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9748\n",
      "Epoch 00037: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0779 - acc: 0.9748 - val_loss: 0.3082 - val_acc: 0.9166\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9737\n",
      "Epoch 00038: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0834 - acc: 0.9737 - val_loss: 0.3172 - val_acc: 0.9273\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9772\n",
      "Epoch 00039: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0712 - acc: 0.9771 - val_loss: 0.2419 - val_acc: 0.9401\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9758\n",
      "Epoch 00040: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0766 - acc: 0.9757 - val_loss: 0.3279 - val_acc: 0.9285\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9769\n",
      "Epoch 00041: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0724 - acc: 0.9769 - val_loss: 0.2222 - val_acc: 0.9441\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9805\n",
      "Epoch 00042: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0615 - acc: 0.9805 - val_loss: 0.2914 - val_acc: 0.9364\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9785\n",
      "Epoch 00043: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0678 - acc: 0.9784 - val_loss: 0.2542 - val_acc: 0.9387\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0631 - acc: 0.9802 - val_loss: 0.2102 - val_acc: 0.9506\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9829\n",
      "Epoch 00045: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0561 - acc: 0.9829 - val_loss: 0.3312 - val_acc: 0.9238\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9836\n",
      "Epoch 00046: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0539 - acc: 0.9836 - val_loss: 0.2737 - val_acc: 0.9366\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9765\n",
      "Epoch 00047: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0710 - acc: 0.9765 - val_loss: 0.2247 - val_acc: 0.9488\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9852\n",
      "Epoch 00048: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0481 - acc: 0.9852 - val_loss: 0.2090 - val_acc: 0.9541\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9849\n",
      "Epoch 00049: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0513 - acc: 0.9849 - val_loss: 0.2629 - val_acc: 0.9364\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9830\n",
      "Epoch 00050: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0538 - acc: 0.9830 - val_loss: 0.2955 - val_acc: 0.9359\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9854\n",
      "Epoch 00051: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0473 - acc: 0.9854 - val_loss: 0.2882 - val_acc: 0.9338\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9851\n",
      "Epoch 00052: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0467 - acc: 0.9851 - val_loss: 0.2756 - val_acc: 0.9376\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9822\n",
      "Epoch 00053: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0582 - acc: 0.9822 - val_loss: 0.2044 - val_acc: 0.9506\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9891\n",
      "Epoch 00054: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0362 - acc: 0.9891 - val_loss: 0.3126 - val_acc: 0.9322\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9860\n",
      "Epoch 00055: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0444 - acc: 0.9860 - val_loss: 0.2250 - val_acc: 0.9464\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9885\n",
      "Epoch 00056: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0389 - acc: 0.9885 - val_loss: 0.2570 - val_acc: 0.9434\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9882\n",
      "Epoch 00057: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0392 - acc: 0.9882 - val_loss: 0.2636 - val_acc: 0.9478\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9883\n",
      "Epoch 00058: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0383 - acc: 0.9883 - val_loss: 0.3406 - val_acc: 0.9324\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9894\n",
      "Epoch 00059: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0349 - acc: 0.9894 - val_loss: 0.2659 - val_acc: 0.9453\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9869\n",
      "Epoch 00060: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0419 - acc: 0.9869 - val_loss: 0.2739 - val_acc: 0.9427\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9882\n",
      "Epoch 00061: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0383 - acc: 0.9882 - val_loss: 0.3360 - val_acc: 0.9206\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9897\n",
      "Epoch 00062: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0339 - acc: 0.9897 - val_loss: 0.3346 - val_acc: 0.9322\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9887\n",
      "Epoch 00063: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0377 - acc: 0.9887 - val_loss: 0.2605 - val_acc: 0.9394\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9887\n",
      "Epoch 00064: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0351 - acc: 0.9887 - val_loss: 0.2321 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9867\n",
      "Epoch 00065: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0405 - acc: 0.9867 - val_loss: 0.2449 - val_acc: 0.9502\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9907\n",
      "Epoch 00066: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0296 - acc: 0.9906 - val_loss: 0.2385 - val_acc: 0.9509\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9896\n",
      "Epoch 00067: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0329 - acc: 0.9896 - val_loss: 0.2373 - val_acc: 0.9478\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9916\n",
      "Epoch 00068: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0282 - acc: 0.9916 - val_loss: 0.2202 - val_acc: 0.9522\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9900\n",
      "Epoch 00069: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.2960 - val_acc: 0.9404\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9908\n",
      "Epoch 00070: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0315 - acc: 0.9908 - val_loss: 0.2679 - val_acc: 0.9488\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9915\n",
      "Epoch 00071: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0282 - acc: 0.9915 - val_loss: 0.2827 - val_acc: 0.9434\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 00072: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0293 - acc: 0.9906 - val_loss: 0.3679 - val_acc: 0.9159\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9932\n",
      "Epoch 00073: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0235 - acc: 0.9932 - val_loss: 0.2194 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9917\n",
      "Epoch 00074: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0283 - acc: 0.9917 - val_loss: 0.2689 - val_acc: 0.9427\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9908\n",
      "Epoch 00075: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0295 - acc: 0.9908 - val_loss: 0.2640 - val_acc: 0.9457\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 00076: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.2511 - val_acc: 0.9560\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9915\n",
      "Epoch 00077: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.3100 - val_acc: 0.9322\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9906\n",
      "Epoch 00078: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0315 - acc: 0.9906 - val_loss: 0.2833 - val_acc: 0.9462\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 00079: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0217 - acc: 0.9936 - val_loss: 0.2347 - val_acc: 0.9532\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9931\n",
      "Epoch 00080: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0222 - acc: 0.9931 - val_loss: 0.2254 - val_acc: 0.9513\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9923\n",
      "Epoch 00081: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 427s 12ms/sample - loss: 0.0251 - acc: 0.9923 - val_loss: 1.2969 - val_acc: 0.7706\n",
      "\n",
      "1D_CNN_custom_4_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.2741 - acc: 0.9279\n",
      "Loss: 0.2740789396883697 Accuracy: 0.9279335\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1919 - acc: 0.3376\n",
      "Epoch 00001: val_loss improved from inf to 1.45830, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/001-1.4583.hdf5\n",
      "36805/36805 [==============================] - 436s 12ms/sample - loss: 2.1918 - acc: 0.3376 - val_loss: 1.4583 - val_acc: 0.5723\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1713 - acc: 0.6261\n",
      "Epoch 00002: val_loss improved from 1.45830 to 0.74672, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/002-0.7467.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 1.1712 - acc: 0.6261 - val_loss: 0.7467 - val_acc: 0.7848\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7990 - acc: 0.7510\n",
      "Epoch 00003: val_loss improved from 0.74672 to 0.48543, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/003-0.4854.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.7992 - acc: 0.7510 - val_loss: 0.4854 - val_acc: 0.8677\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.8127\n",
      "Epoch 00004: val_loss improved from 0.48543 to 0.36621, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/004-0.3662.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.6124 - acc: 0.8128 - val_loss: 0.3662 - val_acc: 0.8949\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8503\n",
      "Epoch 00005: val_loss improved from 0.36621 to 0.32061, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/005-0.3206.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.4925 - acc: 0.8503 - val_loss: 0.3206 - val_acc: 0.9096\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8792\n",
      "Epoch 00006: val_loss did not improve from 0.32061\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.4063 - acc: 0.8792 - val_loss: 0.3346 - val_acc: 0.9087\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8928\n",
      "Epoch 00007: val_loss improved from 0.32061 to 0.27626, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/007-0.2763.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.3559 - acc: 0.8928 - val_loss: 0.2763 - val_acc: 0.9276\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9050\n",
      "Epoch 00008: val_loss improved from 0.27626 to 0.25921, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/008-0.2592.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.3144 - acc: 0.9050 - val_loss: 0.2592 - val_acc: 0.9241\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9151\n",
      "Epoch 00009: val_loss improved from 0.25921 to 0.19857, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/009-0.1986.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2779 - acc: 0.9151 - val_loss: 0.1986 - val_acc: 0.9443\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9229\n",
      "Epoch 00010: val_loss improved from 0.19857 to 0.19707, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/010-0.1971.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2502 - acc: 0.9229 - val_loss: 0.1971 - val_acc: 0.9436\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9302\n",
      "Epoch 00011: val_loss did not improve from 0.19707\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2250 - acc: 0.9302 - val_loss: 0.1998 - val_acc: 0.9450\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9347\n",
      "Epoch 00012: val_loss did not improve from 0.19707\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.2096 - acc: 0.9347 - val_loss: 0.2162 - val_acc: 0.9357\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9424\n",
      "Epoch 00013: val_loss did not improve from 0.19707\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1873 - acc: 0.9425 - val_loss: 0.1986 - val_acc: 0.9418\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9448\n",
      "Epoch 00014: val_loss did not improve from 0.19707\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1757 - acc: 0.9447 - val_loss: 0.2016 - val_acc: 0.9443\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9471\n",
      "Epoch 00015: val_loss did not improve from 0.19707\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1670 - acc: 0.9472 - val_loss: 0.2177 - val_acc: 0.9380\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9538\n",
      "Epoch 00016: val_loss improved from 0.19707 to 0.17184, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/016-0.1718.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1488 - acc: 0.9538 - val_loss: 0.1718 - val_acc: 0.9509\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9573\n",
      "Epoch 00017: val_loss improved from 0.17184 to 0.16201, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/017-0.1620.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1387 - acc: 0.9573 - val_loss: 0.1620 - val_acc: 0.9553\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9585\n",
      "Epoch 00018: val_loss improved from 0.16201 to 0.13799, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/018-0.1380.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1335 - acc: 0.9585 - val_loss: 0.1380 - val_acc: 0.9599\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9603\n",
      "Epoch 00019: val_loss did not improve from 0.13799\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1252 - acc: 0.9603 - val_loss: 0.1652 - val_acc: 0.9506\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9609\n",
      "Epoch 00020: val_loss did not improve from 0.13799\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1268 - acc: 0.9609 - val_loss: 0.1758 - val_acc: 0.9543\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9643\n",
      "Epoch 00021: val_loss did not improve from 0.13799\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1134 - acc: 0.9643 - val_loss: 0.1771 - val_acc: 0.9513\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9688\n",
      "Epoch 00022: val_loss did not improve from 0.13799\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1007 - acc: 0.9687 - val_loss: 0.1404 - val_acc: 0.9602\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9678\n",
      "Epoch 00023: val_loss did not improve from 0.13799\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.1040 - acc: 0.9678 - val_loss: 0.1426 - val_acc: 0.9581\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9700\n",
      "Epoch 00024: val_loss improved from 0.13799 to 0.12493, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_8_conv_checkpoint/024-0.1249.hdf5\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0967 - acc: 0.9700 - val_loss: 0.1249 - val_acc: 0.9672\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9731\n",
      "Epoch 00025: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0873 - acc: 0.9731 - val_loss: 0.1659 - val_acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9709\n",
      "Epoch 00026: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0934 - acc: 0.9708 - val_loss: 0.1472 - val_acc: 0.9560\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9764\n",
      "Epoch 00027: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0782 - acc: 0.9764 - val_loss: 0.1549 - val_acc: 0.9553\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9793\n",
      "Epoch 00028: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0677 - acc: 0.9792 - val_loss: 0.1400 - val_acc: 0.9630\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9746\n",
      "Epoch 00029: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0795 - acc: 0.9746 - val_loss: 0.1353 - val_acc: 0.9639\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9774\n",
      "Epoch 00030: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0730 - acc: 0.9774 - val_loss: 0.1869 - val_acc: 0.9467\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9808\n",
      "Epoch 00031: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0624 - acc: 0.9808 - val_loss: 0.1456 - val_acc: 0.9597\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9785\n",
      "Epoch 00032: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0684 - acc: 0.9785 - val_loss: 0.1708 - val_acc: 0.9562\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9823\n",
      "Epoch 00033: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0575 - acc: 0.9823 - val_loss: 0.1388 - val_acc: 0.9618\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9849\n",
      "Epoch 00034: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0502 - acc: 0.9849 - val_loss: 0.1403 - val_acc: 0.9613\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9843\n",
      "Epoch 00035: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 0.0511 - acc: 0.9843 - val_loss: 0.1636 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9836\n",
      "Epoch 00036: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0529 - acc: 0.9835 - val_loss: 0.1565 - val_acc: 0.9581\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9844\n",
      "Epoch 00037: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 0.0512 - acc: 0.9843 - val_loss: 0.1656 - val_acc: 0.9541\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9839\n",
      "Epoch 00038: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 0.0534 - acc: 0.9839 - val_loss: 0.1588 - val_acc: 0.9606\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9822\n",
      "Epoch 00039: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0578 - acc: 0.9822 - val_loss: 0.1730 - val_acc: 0.9576\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9887\n",
      "Epoch 00040: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0383 - acc: 0.9887 - val_loss: 0.1645 - val_acc: 0.9550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9882\n",
      "Epoch 00041: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0400 - acc: 0.9882 - val_loss: 0.1414 - val_acc: 0.9625\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9877\n",
      "Epoch 00042: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0409 - acc: 0.9876 - val_loss: 0.1562 - val_acc: 0.9595\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9862\n",
      "Epoch 00043: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0446 - acc: 0.9862 - val_loss: 0.1665 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9861\n",
      "Epoch 00044: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0433 - acc: 0.9861 - val_loss: 0.1432 - val_acc: 0.9658\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9910\n",
      "Epoch 00045: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0300 - acc: 0.9910 - val_loss: 0.1412 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9896\n",
      "Epoch 00046: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0343 - acc: 0.9896 - val_loss: 0.2080 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9899\n",
      "Epoch 00047: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0324 - acc: 0.9899 - val_loss: 0.1978 - val_acc: 0.9543\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9908\n",
      "Epoch 00048: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 0.0303 - acc: 0.9908 - val_loss: 0.1536 - val_acc: 0.9602\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0244 - acc: 0.9932 - val_loss: 0.1675 - val_acc: 0.9604\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9912\n",
      "Epoch 00050: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 429s 12ms/sample - loss: 0.0310 - acc: 0.9912 - val_loss: 0.1585 - val_acc: 0.9599\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9906\n",
      "Epoch 00051: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0305 - acc: 0.9906 - val_loss: 0.2305 - val_acc: 0.9474\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 00052: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0275 - acc: 0.9918 - val_loss: 0.1571 - val_acc: 0.9616\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9936\n",
      "Epoch 00053: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0230 - acc: 0.9936 - val_loss: 0.1649 - val_acc: 0.9625\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0266 - acc: 0.9917 - val_loss: 0.1834 - val_acc: 0.9564\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9915\n",
      "Epoch 00055: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0299 - acc: 0.9915 - val_loss: 0.1449 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9913\n",
      "Epoch 00056: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0287 - acc: 0.9913 - val_loss: 0.1466 - val_acc: 0.9634\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9941\n",
      "Epoch 00057: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0199 - acc: 0.9941 - val_loss: 0.1655 - val_acc: 0.9620\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00058: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0201 - acc: 0.9941 - val_loss: 0.2087 - val_acc: 0.9548\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0204 - acc: 0.9942 - val_loss: 0.1710 - val_acc: 0.9611\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9935\n",
      "Epoch 00060: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0220 - acc: 0.9935 - val_loss: 0.2336 - val_acc: 0.9502\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9909\n",
      "Epoch 00061: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0292 - acc: 0.9909 - val_loss: 0.1803 - val_acc: 0.9567\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9936\n",
      "Epoch 00062: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0217 - acc: 0.9935 - val_loss: 0.1764 - val_acc: 0.9597\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9889\n",
      "Epoch 00063: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0360 - acc: 0.9889 - val_loss: 0.1429 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00064: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.1942 - val_acc: 0.9578\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0148 - acc: 0.9960 - val_loss: 0.1911 - val_acc: 0.9637\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958\n",
      "Epoch 00066: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0158 - acc: 0.9958 - val_loss: 0.1508 - val_acc: 0.9634\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9935\n",
      "Epoch 00067: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0211 - acc: 0.9935 - val_loss: 0.1952 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9907\n",
      "Epoch 00068: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0311 - acc: 0.9907 - val_loss: 0.1518 - val_acc: 0.9648\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9962\n",
      "Epoch 00069: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0135 - acc: 0.9962 - val_loss: 0.1636 - val_acc: 0.9651\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9973\n",
      "Epoch 00070: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0107 - acc: 0.9973 - val_loss: 0.1877 - val_acc: 0.9606\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00071: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0182 - acc: 0.9946 - val_loss: 0.2711 - val_acc: 0.9418\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9911\n",
      "Epoch 00072: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0289 - acc: 0.9911 - val_loss: 0.1453 - val_acc: 0.9683\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9963\n",
      "Epoch 00073: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.1841 - val_acc: 0.9625\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9959\n",
      "Epoch 00074: val_loss did not improve from 0.12493\n",
      "36805/36805 [==============================] - 428s 12ms/sample - loss: 0.0145 - acc: 0.9959 - val_loss: 0.1680 - val_acc: 0.9630\n",
      "\n",
      "1D_CNN_custom_4_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 18s 4ms/sample - loss: 0.1530 - acc: 0.9566\n",
      "Loss: 0.15303098773458046 Accuracy: 0.956594\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0073 - acc: 0.3897\n",
      "Epoch 00001: val_loss improved from inf to 1.37891, saving model to model/checkpoint/1D_CNN_custom_4_DO_BN_9_conv_checkpoint/001-1.3789.hdf5\n",
      "36805/36805 [==============================] - 441s 12ms/sample - loss: 2.0074 - acc: 0.3897 - val_loss: 1.3789 - val_acc: 0.6161\n",
      "Epoch 2/500\n",
      " 5056/36805 [===>..........................] - ETA: 5:56 - loss: 1.2059 - acc: 0.6224"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,256,16000,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_12/Adam/gradients/max_pooling1d_54/MaxPool_grad/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6011733534ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n\u001b[1;32m     16\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val_abs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_onehot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                      callbacks = [checkpointer, early_stopping])\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,256,16000,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_12/Adam/gradients/max_pooling1d_54/MaxPool_grad/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
