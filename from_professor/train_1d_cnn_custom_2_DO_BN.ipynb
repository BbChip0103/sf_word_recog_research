{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 256\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                65536016  \n",
      "=================================================================\n",
      "Total params: 65,538,576\n",
      "Trainable params: 65,538,064\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1365248)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                21843984  \n",
      "=================================================================\n",
      "Total params: 22,175,504\n",
      "Trainable params: 22,174,480\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 454912)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                7278608   \n",
      "=================================================================\n",
      "Total params: 7,939,088\n",
      "Trainable params: 7,937,552\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 151552)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2424848   \n",
      "=================================================================\n",
      "Total params: 3,414,288\n",
      "Trainable params: 3,412,240\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 1,557,392\n",
      "Trainable params: 1,555,088\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 1,369,616\n",
      "Trainable params: 1,367,056\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 1,362,064\n",
      "Trainable params: 1,359,248\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 1,415,952\n",
      "Trainable params: 1,412,880\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 256)        1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 256)        327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 256)         327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 64)             41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 1,444,944\n",
      "Trainable params: 1,441,744\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9937 - acc: 0.3809\n",
      "Epoch 00001: val_loss improved from inf to 1.13620, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/001-1.1362.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 1.9937 - acc: 0.3810 - val_loss: 1.1362 - val_acc: 0.6546\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1975 - acc: 0.6233\n",
      "Epoch 00002: val_loss improved from 1.13620 to 0.83174, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/002-0.8317.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 1.1976 - acc: 0.6233 - val_loss: 0.8317 - val_acc: 0.7566\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9447 - acc: 0.7074\n",
      "Epoch 00003: val_loss improved from 0.83174 to 0.82742, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/003-0.8274.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.9447 - acc: 0.7074 - val_loss: 0.8274 - val_acc: 0.7531\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.7488\n",
      "Epoch 00004: val_loss improved from 0.82742 to 0.61836, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/004-0.6184.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.8227 - acc: 0.7488 - val_loss: 0.6184 - val_acc: 0.8195\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7185 - acc: 0.7834\n",
      "Epoch 00005: val_loss improved from 0.61836 to 0.59408, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/005-0.5941.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.7187 - acc: 0.7834 - val_loss: 0.5941 - val_acc: 0.8344\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.8071\n",
      "Epoch 00006: val_loss improved from 0.59408 to 0.56334, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/006-0.5633.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.6503 - acc: 0.8071 - val_loss: 0.5633 - val_acc: 0.8442\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6000 - acc: 0.8191\n",
      "Epoch 00007: val_loss did not improve from 0.56334\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.6000 - acc: 0.8190 - val_loss: 0.6191 - val_acc: 0.8244\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.8356\n",
      "Epoch 00008: val_loss improved from 0.56334 to 0.55544, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/008-0.5554.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.5545 - acc: 0.8356 - val_loss: 0.5554 - val_acc: 0.8300\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8462\n",
      "Epoch 00009: val_loss improved from 0.55544 to 0.45374, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/009-0.4537.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.5135 - acc: 0.8461 - val_loss: 0.4537 - val_acc: 0.8777\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8569\n",
      "Epoch 00010: val_loss did not improve from 0.45374\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.4785 - acc: 0.8569 - val_loss: 0.4843 - val_acc: 0.8614\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8687\n",
      "Epoch 00011: val_loss did not improve from 0.45374\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.4410 - acc: 0.8687 - val_loss: 0.4825 - val_acc: 0.8633\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8744\n",
      "Epoch 00012: val_loss improved from 0.45374 to 0.43642, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/012-0.4364.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.4170 - acc: 0.8743 - val_loss: 0.4364 - val_acc: 0.8847\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8806\n",
      "Epoch 00013: val_loss improved from 0.43642 to 0.43153, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/013-0.4315.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.3910 - acc: 0.8805 - val_loss: 0.4315 - val_acc: 0.8772\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8870\n",
      "Epoch 00014: val_loss improved from 0.43153 to 0.41531, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/014-0.4153.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.3698 - acc: 0.8869 - val_loss: 0.4153 - val_acc: 0.8798\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8936\n",
      "Epoch 00015: val_loss improved from 0.41531 to 0.38639, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/015-0.3864.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.3505 - acc: 0.8935 - val_loss: 0.3864 - val_acc: 0.8924\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8990\n",
      "Epoch 00016: val_loss improved from 0.38639 to 0.38162, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/016-0.3816.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.3296 - acc: 0.8989 - val_loss: 0.3816 - val_acc: 0.8924\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9045\n",
      "Epoch 00017: val_loss did not improve from 0.38162\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.3144 - acc: 0.9044 - val_loss: 0.5479 - val_acc: 0.8437\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.9065\n",
      "Epoch 00018: val_loss improved from 0.38162 to 0.37369, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/018-0.3737.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.3024 - acc: 0.9065 - val_loss: 0.3737 - val_acc: 0.8998\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9125\n",
      "Epoch 00019: val_loss improved from 0.37369 to 0.36790, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/019-0.3679.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.2826 - acc: 0.9124 - val_loss: 0.3679 - val_acc: 0.9001\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9158\n",
      "Epoch 00020: val_loss did not improve from 0.36790\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.2716 - acc: 0.9159 - val_loss: 0.4959 - val_acc: 0.8747\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9226\n",
      "Epoch 00021: val_loss improved from 0.36790 to 0.36673, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/021-0.3667.hdf5\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.2554 - acc: 0.9225 - val_loss: 0.3667 - val_acc: 0.8968\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9232\n",
      "Epoch 00022: val_loss did not improve from 0.36673\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.2531 - acc: 0.9232 - val_loss: 0.3797 - val_acc: 0.8959\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9271\n",
      "Epoch 00023: val_loss improved from 0.36673 to 0.34806, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/023-0.3481.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.2332 - acc: 0.9271 - val_loss: 0.3481 - val_acc: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9285\n",
      "Epoch 00024: val_loss did not improve from 0.34806\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.2237 - acc: 0.9285 - val_loss: 0.3509 - val_acc: 0.9059\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9326\n",
      "Epoch 00025: val_loss did not improve from 0.34806\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.2152 - acc: 0.9326 - val_loss: 0.3861 - val_acc: 0.8949\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9344\n",
      "Epoch 00026: val_loss did not improve from 0.34806\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.2070 - acc: 0.9343 - val_loss: 0.3797 - val_acc: 0.9036\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9376\n",
      "Epoch 00027: val_loss did not improve from 0.34806\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1992 - acc: 0.9375 - val_loss: 0.3862 - val_acc: 0.8994\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9389\n",
      "Epoch 00028: val_loss did not improve from 0.34806\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1939 - acc: 0.9389 - val_loss: 0.3521 - val_acc: 0.9136\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9432\n",
      "Epoch 00029: val_loss improved from 0.34806 to 0.34375, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/029-0.3437.hdf5\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1812 - acc: 0.9432 - val_loss: 0.3437 - val_acc: 0.9082\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9436\n",
      "Epoch 00030: val_loss did not improve from 0.34375\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1779 - acc: 0.9436 - val_loss: 0.3578 - val_acc: 0.9024\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9475\n",
      "Epoch 00031: val_loss improved from 0.34375 to 0.33944, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/031-0.3394.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1670 - acc: 0.9475 - val_loss: 0.3394 - val_acc: 0.9101\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9482\n",
      "Epoch 00032: val_loss did not improve from 0.33944\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1624 - acc: 0.9481 - val_loss: 0.4601 - val_acc: 0.8744\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9498\n",
      "Epoch 00033: val_loss did not improve from 0.33944\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.1605 - acc: 0.9498 - val_loss: 0.4415 - val_acc: 0.8875\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9529\n",
      "Epoch 00034: val_loss did not improve from 0.33944\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1464 - acc: 0.9528 - val_loss: 0.3549 - val_acc: 0.9057\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9562\n",
      "Epoch 00035: val_loss improved from 0.33944 to 0.33442, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/035-0.3344.hdf5\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1419 - acc: 0.9562 - val_loss: 0.3344 - val_acc: 0.9173\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9557\n",
      "Epoch 00036: val_loss improved from 0.33442 to 0.32563, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_6_conv_checkpoint/036-0.3256.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.1384 - acc: 0.9556 - val_loss: 0.3256 - val_acc: 0.9175\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9560\n",
      "Epoch 00037: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1379 - acc: 0.9560 - val_loss: 0.4001 - val_acc: 0.9036\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9601\n",
      "Epoch 00038: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1247 - acc: 0.9600 - val_loss: 0.4459 - val_acc: 0.8854\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9574\n",
      "Epoch 00039: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1333 - acc: 0.9574 - val_loss: 0.4526 - val_acc: 0.8935\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9629\n",
      "Epoch 00040: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1191 - acc: 0.9629 - val_loss: 0.3356 - val_acc: 0.9171\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9637\n",
      "Epoch 00041: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.1162 - acc: 0.9637 - val_loss: 0.3718 - val_acc: 0.9054\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9648\n",
      "Epoch 00042: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1108 - acc: 0.9648 - val_loss: 0.3726 - val_acc: 0.9166\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9664\n",
      "Epoch 00043: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1059 - acc: 0.9664 - val_loss: 0.3960 - val_acc: 0.9061\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9640\n",
      "Epoch 00044: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1123 - acc: 0.9640 - val_loss: 0.3836 - val_acc: 0.9115\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9661\n",
      "Epoch 00045: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1045 - acc: 0.9661 - val_loss: 0.4769 - val_acc: 0.8870\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9667\n",
      "Epoch 00046: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1026 - acc: 0.9667 - val_loss: 0.4713 - val_acc: 0.8838\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9698\n",
      "Epoch 00047: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0969 - acc: 0.9698 - val_loss: 0.3325 - val_acc: 0.9222\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9676\n",
      "Epoch 00048: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.1005 - acc: 0.9675 - val_loss: 0.3817 - val_acc: 0.8991\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9711\n",
      "Epoch 00049: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0913 - acc: 0.9711 - val_loss: 0.3926 - val_acc: 0.9161\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9728\n",
      "Epoch 00050: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0879 - acc: 0.9728 - val_loss: 0.3877 - val_acc: 0.9108\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9720\n",
      "Epoch 00051: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0888 - acc: 0.9720 - val_loss: 0.4428 - val_acc: 0.9017\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9732\n",
      "Epoch 00052: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0834 - acc: 0.9732 - val_loss: 0.4231 - val_acc: 0.9026\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9717\n",
      "Epoch 00053: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0876 - acc: 0.9717 - val_loss: 0.3598 - val_acc: 0.9210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9745\n",
      "Epoch 00054: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0812 - acc: 0.9745 - val_loss: 0.3397 - val_acc: 0.9185\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9749\n",
      "Epoch 00055: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0787 - acc: 0.9748 - val_loss: 0.3983 - val_acc: 0.9085\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9740\n",
      "Epoch 00056: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0844 - acc: 0.9740 - val_loss: 0.4005 - val_acc: 0.9159\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9755\n",
      "Epoch 00057: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0762 - acc: 0.9755 - val_loss: 0.3508 - val_acc: 0.9220\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9759\n",
      "Epoch 00058: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0740 - acc: 0.9758 - val_loss: 0.4302 - val_acc: 0.8994\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9775\n",
      "Epoch 00059: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0728 - acc: 0.9774 - val_loss: 0.3384 - val_acc: 0.9208\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9749\n",
      "Epoch 00060: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0754 - acc: 0.9749 - val_loss: 0.3541 - val_acc: 0.9171\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9785\n",
      "Epoch 00061: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0666 - acc: 0.9785 - val_loss: 0.3862 - val_acc: 0.9133\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9796\n",
      "Epoch 00062: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0638 - acc: 0.9796 - val_loss: 0.3845 - val_acc: 0.9157\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9827\n",
      "Epoch 00063: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0566 - acc: 0.9827 - val_loss: 0.4369 - val_acc: 0.8982\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9780\n",
      "Epoch 00064: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0703 - acc: 0.9780 - val_loss: 0.3694 - val_acc: 0.9157\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9811\n",
      "Epoch 00065: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0619 - acc: 0.9810 - val_loss: 0.3673 - val_acc: 0.9154\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9796\n",
      "Epoch 00066: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0665 - acc: 0.9795 - val_loss: 0.4009 - val_acc: 0.9082\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9799\n",
      "Epoch 00067: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0632 - acc: 0.9798 - val_loss: 0.3754 - val_acc: 0.9178\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9807\n",
      "Epoch 00068: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0612 - acc: 0.9806 - val_loss: 0.3757 - val_acc: 0.9194\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9796\n",
      "Epoch 00069: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0660 - acc: 0.9796 - val_loss: 0.3741 - val_acc: 0.9175\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9798\n",
      "Epoch 00070: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0640 - acc: 0.9798 - val_loss: 0.3624 - val_acc: 0.9201\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9839\n",
      "Epoch 00071: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0509 - acc: 0.9839 - val_loss: 0.4098 - val_acc: 0.9103\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9839\n",
      "Epoch 00072: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0519 - acc: 0.9839 - val_loss: 0.4660 - val_acc: 0.9052\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9839\n",
      "Epoch 00073: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0500 - acc: 0.9839 - val_loss: 0.3701 - val_acc: 0.9208\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9842\n",
      "Epoch 00074: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0500 - acc: 0.9842 - val_loss: 0.3972 - val_acc: 0.9199\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9832\n",
      "Epoch 00075: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0534 - acc: 0.9832 - val_loss: 0.3747 - val_acc: 0.9189\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9858\n",
      "Epoch 00076: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0476 - acc: 0.9858 - val_loss: 0.4662 - val_acc: 0.9040\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9820\n",
      "Epoch 00077: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0563 - acc: 0.9820 - val_loss: 0.4150 - val_acc: 0.9192\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9864\n",
      "Epoch 00078: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0435 - acc: 0.9864 - val_loss: 0.4036 - val_acc: 0.9171\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9849\n",
      "Epoch 00079: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0473 - acc: 0.9849 - val_loss: 0.3713 - val_acc: 0.9206\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9870\n",
      "Epoch 00080: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0446 - acc: 0.9870 - val_loss: 0.4834 - val_acc: 0.8998\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9843\n",
      "Epoch 00081: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0491 - acc: 0.9843 - val_loss: 0.3539 - val_acc: 0.9245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9880\n",
      "Epoch 00082: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0393 - acc: 0.9880 - val_loss: 0.4209 - val_acc: 0.9078\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9868\n",
      "Epoch 00083: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0431 - acc: 0.9868 - val_loss: 0.3819 - val_acc: 0.9210\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9863\n",
      "Epoch 00084: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0424 - acc: 0.9863 - val_loss: 0.3841 - val_acc: 0.9178\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9855\n",
      "Epoch 00085: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0486 - acc: 0.9855 - val_loss: 0.3794 - val_acc: 0.9217\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9876\n",
      "Epoch 00086: val_loss did not improve from 0.32563\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0403 - acc: 0.9876 - val_loss: 0.5141 - val_acc: 0.9029\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 21s 4ms/sample - loss: 0.3842 - acc: 0.8964\n",
      "Loss: 0.38417450881573767 Accuracy: 0.8963655\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1621 - acc: 0.3387\n",
      "Epoch 00001: val_loss improved from inf to 1.16462, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/001-1.1646.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 2.1621 - acc: 0.3386 - val_loss: 1.1646 - val_acc: 0.6550\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2783 - acc: 0.5906\n",
      "Epoch 00002: val_loss improved from 1.16462 to 0.85832, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/002-0.8583.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 1.2784 - acc: 0.5906 - val_loss: 0.8583 - val_acc: 0.7503\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9591 - acc: 0.7001\n",
      "Epoch 00003: val_loss improved from 0.85832 to 0.67380, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/003-0.6738.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.9591 - acc: 0.7001 - val_loss: 0.6738 - val_acc: 0.8153\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7837 - acc: 0.7608\n",
      "Epoch 00004: val_loss improved from 0.67380 to 0.65708, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/004-0.6571.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.7840 - acc: 0.7607 - val_loss: 0.6571 - val_acc: 0.8102\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.7977\n",
      "Epoch 00005: val_loss improved from 0.65708 to 0.58822, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/005-0.5882.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.6704 - acc: 0.7977 - val_loss: 0.5882 - val_acc: 0.8348\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5782 - acc: 0.8257\n",
      "Epoch 00006: val_loss improved from 0.58822 to 0.44019, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/006-0.4402.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.5782 - acc: 0.8256 - val_loss: 0.4402 - val_acc: 0.8775\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8452\n",
      "Epoch 00007: val_loss improved from 0.44019 to 0.41343, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/007-0.4134.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.5130 - acc: 0.8452 - val_loss: 0.4134 - val_acc: 0.8884\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.8596\n",
      "Epoch 00008: val_loss did not improve from 0.41343\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.4647 - acc: 0.8595 - val_loss: 0.4781 - val_acc: 0.8712\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8733\n",
      "Epoch 00009: val_loss improved from 0.41343 to 0.33790, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/009-0.3379.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.4212 - acc: 0.8733 - val_loss: 0.3379 - val_acc: 0.9101\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8869\n",
      "Epoch 00010: val_loss improved from 0.33790 to 0.32608, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/010-0.3261.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.3772 - acc: 0.8869 - val_loss: 0.3261 - val_acc: 0.9110\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8948\n",
      "Epoch 00011: val_loss improved from 0.32608 to 0.28487, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/011-0.2849.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.3528 - acc: 0.8948 - val_loss: 0.2849 - val_acc: 0.9234\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9003\n",
      "Epoch 00012: val_loss did not improve from 0.28487\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.3223 - acc: 0.9003 - val_loss: 0.3297 - val_acc: 0.9052\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9064\n",
      "Epoch 00013: val_loss did not improve from 0.28487\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2969 - acc: 0.9063 - val_loss: 0.3067 - val_acc: 0.9210\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9140\n",
      "Epoch 00014: val_loss improved from 0.28487 to 0.25869, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/014-0.2587.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2834 - acc: 0.9140 - val_loss: 0.2587 - val_acc: 0.9299\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9214\n",
      "Epoch 00015: val_loss did not improve from 0.25869\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2606 - acc: 0.9214 - val_loss: 0.3556 - val_acc: 0.9054\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9226\n",
      "Epoch 00016: val_loss did not improve from 0.25869\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2514 - acc: 0.9226 - val_loss: 0.2882 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9265\n",
      "Epoch 00017: val_loss improved from 0.25869 to 0.25487, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/017-0.2549.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2393 - acc: 0.9265 - val_loss: 0.2549 - val_acc: 0.9348\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9309\n",
      "Epoch 00018: val_loss improved from 0.25487 to 0.23528, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/018-0.2353.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.2207 - acc: 0.9309 - val_loss: 0.2353 - val_acc: 0.9369\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9356\n",
      "Epoch 00019: val_loss did not improve from 0.23528\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.2087 - acc: 0.9356 - val_loss: 0.3429 - val_acc: 0.9138\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9357\n",
      "Epoch 00020: val_loss improved from 0.23528 to 0.23296, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/020-0.2330.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.2033 - acc: 0.9357 - val_loss: 0.2330 - val_acc: 0.9399\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9424\n",
      "Epoch 00021: val_loss did not improve from 0.23296\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1843 - acc: 0.9423 - val_loss: 0.2582 - val_acc: 0.9336\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9433\n",
      "Epoch 00022: val_loss improved from 0.23296 to 0.23295, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/022-0.2329.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1848 - acc: 0.9432 - val_loss: 0.2329 - val_acc: 0.9364\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9429\n",
      "Epoch 00023: val_loss did not improve from 0.23295\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1840 - acc: 0.9429 - val_loss: 0.2406 - val_acc: 0.9394\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9499\n",
      "Epoch 00024: val_loss improved from 0.23295 to 0.21843, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/024-0.2184.hdf5\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.1612 - acc: 0.9500 - val_loss: 0.2184 - val_acc: 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9531\n",
      "Epoch 00025: val_loss did not improve from 0.21843\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.1511 - acc: 0.9531 - val_loss: 0.2589 - val_acc: 0.9304\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9504\n",
      "Epoch 00026: val_loss improved from 0.21843 to 0.19271, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/026-0.1927.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1553 - acc: 0.9504 - val_loss: 0.1927 - val_acc: 0.9511\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9559\n",
      "Epoch 00027: val_loss did not improve from 0.19271\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1428 - acc: 0.9559 - val_loss: 0.2194 - val_acc: 0.9448\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9580\n",
      "Epoch 00028: val_loss improved from 0.19271 to 0.19181, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_7_conv_checkpoint/028-0.1918.hdf5\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1370 - acc: 0.9580 - val_loss: 0.1918 - val_acc: 0.9490\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9592\n",
      "Epoch 00029: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1303 - acc: 0.9592 - val_loss: 0.1984 - val_acc: 0.9464\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9608\n",
      "Epoch 00030: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.1241 - acc: 0.9607 - val_loss: 0.1953 - val_acc: 0.9481\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9626\n",
      "Epoch 00031: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1207 - acc: 0.9626 - val_loss: 0.2590 - val_acc: 0.9362\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9641\n",
      "Epoch 00032: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.1166 - acc: 0.9640 - val_loss: 0.2818 - val_acc: 0.9334\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9618\n",
      "Epoch 00033: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1213 - acc: 0.9618 - val_loss: 0.2378 - val_acc: 0.9418\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9674\n",
      "Epoch 00034: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.1057 - acc: 0.9674 - val_loss: 0.2176 - val_acc: 0.9513\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9674\n",
      "Epoch 00035: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.1034 - acc: 0.9673 - val_loss: 0.2030 - val_acc: 0.9497\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9685\n",
      "Epoch 00036: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1010 - acc: 0.9685 - val_loss: 0.2124 - val_acc: 0.9490\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9663\n",
      "Epoch 00037: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1028 - acc: 0.9663 - val_loss: 0.2172 - val_acc: 0.9469\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9666\n",
      "Epoch 00038: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.1018 - acc: 0.9666 - val_loss: 0.2066 - val_acc: 0.9474\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9724\n",
      "Epoch 00039: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0862 - acc: 0.9724 - val_loss: 0.2663 - val_acc: 0.9390\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9720\n",
      "Epoch 00040: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0891 - acc: 0.9720 - val_loss: 0.2348 - val_acc: 0.9483\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9744\n",
      "Epoch 00041: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0807 - acc: 0.9744 - val_loss: 0.2476 - val_acc: 0.9387\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9774\n",
      "Epoch 00042: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0730 - acc: 0.9773 - val_loss: 0.2120 - val_acc: 0.9474\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9734\n",
      "Epoch 00043: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0843 - acc: 0.9733 - val_loss: 0.2031 - val_acc: 0.9497\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9760\n",
      "Epoch 00044: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0749 - acc: 0.9760 - val_loss: 0.2006 - val_acc: 0.9543\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9795\n",
      "Epoch 00045: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0648 - acc: 0.9795 - val_loss: 0.2447 - val_acc: 0.9467\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9803\n",
      "Epoch 00046: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0638 - acc: 0.9802 - val_loss: 0.2401 - val_acc: 0.9455\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9761\n",
      "Epoch 00047: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0758 - acc: 0.9761 - val_loss: 0.2051 - val_acc: 0.9499\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9837\n",
      "Epoch 00048: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0552 - acc: 0.9837 - val_loss: 0.2792 - val_acc: 0.9387\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9800\n",
      "Epoch 00049: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0627 - acc: 0.9800 - val_loss: 0.2392 - val_acc: 0.9453\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9798\n",
      "Epoch 00050: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0654 - acc: 0.9798 - val_loss: 0.2216 - val_acc: 0.9488\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9817\n",
      "Epoch 00051: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0593 - acc: 0.9816 - val_loss: 0.2750 - val_acc: 0.9311\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9793\n",
      "Epoch 00052: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0650 - acc: 0.9794 - val_loss: 0.2085 - val_acc: 0.9553\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9860\n",
      "Epoch 00053: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0463 - acc: 0.9859 - val_loss: 0.2949 - val_acc: 0.9350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9787\n",
      "Epoch 00054: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0668 - acc: 0.9786 - val_loss: 0.1956 - val_acc: 0.9539\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9842\n",
      "Epoch 00055: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0511 - acc: 0.9842 - val_loss: 0.2192 - val_acc: 0.9471\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9864\n",
      "Epoch 00056: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0455 - acc: 0.9864 - val_loss: 0.2099 - val_acc: 0.9550\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9834\n",
      "Epoch 00057: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0543 - acc: 0.9834 - val_loss: 0.2074 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9856\n",
      "Epoch 00058: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0476 - acc: 0.9856 - val_loss: 0.2390 - val_acc: 0.9453\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9831\n",
      "Epoch 00059: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0553 - acc: 0.9830 - val_loss: 0.1979 - val_acc: 0.9532\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9844\n",
      "Epoch 00060: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0482 - acc: 0.9844 - val_loss: 0.2567 - val_acc: 0.9469\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9849\n",
      "Epoch 00061: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0483 - acc: 0.9849 - val_loss: 0.2597 - val_acc: 0.9385\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9881\n",
      "Epoch 00062: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0397 - acc: 0.9881 - val_loss: 0.2270 - val_acc: 0.9520\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9847\n",
      "Epoch 00063: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0483 - acc: 0.9847 - val_loss: 0.2300 - val_acc: 0.9509\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9889\n",
      "Epoch 00064: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0364 - acc: 0.9889 - val_loss: 0.2160 - val_acc: 0.9548\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9893\n",
      "Epoch 00065: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0362 - acc: 0.9893 - val_loss: 0.2410 - val_acc: 0.9513\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9861\n",
      "Epoch 00066: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0429 - acc: 0.9860 - val_loss: 0.2285 - val_acc: 0.9555\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9874\n",
      "Epoch 00067: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0396 - acc: 0.9874 - val_loss: 0.2417 - val_acc: 0.9495\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9877\n",
      "Epoch 00068: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 378s 10ms/sample - loss: 0.0399 - acc: 0.9877 - val_loss: 0.2707 - val_acc: 0.9495\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888\n",
      "Epoch 00069: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0350 - acc: 0.9888 - val_loss: 0.2115 - val_acc: 0.9550\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9910\n",
      "Epoch 00070: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0301 - acc: 0.9910 - val_loss: 0.2380 - val_acc: 0.9504\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9878\n",
      "Epoch 00071: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0380 - acc: 0.9877 - val_loss: 0.3052 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9883\n",
      "Epoch 00072: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0404 - acc: 0.9883 - val_loss: 0.3510 - val_acc: 0.9262\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9911\n",
      "Epoch 00073: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 379s 10ms/sample - loss: 0.0294 - acc: 0.9911 - val_loss: 0.2419 - val_acc: 0.9513\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9905\n",
      "Epoch 00074: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0316 - acc: 0.9905 - val_loss: 0.2465 - val_acc: 0.9502\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9905\n",
      "Epoch 00075: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0311 - acc: 0.9905 - val_loss: 0.2180 - val_acc: 0.9534\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9857\n",
      "Epoch 00076: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0437 - acc: 0.9857 - val_loss: 0.2370 - val_acc: 0.9518\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9922\n",
      "Epoch 00077: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0256 - acc: 0.9921 - val_loss: 0.2566 - val_acc: 0.9413\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9885\n",
      "Epoch 00078: val_loss did not improve from 0.19181\n",
      "36805/36805 [==============================] - 380s 10ms/sample - loss: 0.0376 - acc: 0.9884 - val_loss: 0.2361 - val_acc: 0.9515\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.2345 - acc: 0.9346\n",
      "Loss: 0.23450478180732312 Accuracy: 0.93457943\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1306 - acc: 0.3609\n",
      "Epoch 00001: val_loss improved from inf to 1.02030, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/001-1.0203.hdf5\n",
      "36805/36805 [==============================] - 391s 11ms/sample - loss: 2.1306 - acc: 0.3609 - val_loss: 1.0203 - val_acc: 0.6983\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.6248\n",
      "Epoch 00002: val_loss improved from 1.02030 to 0.64566, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/002-0.6457.hdf5\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 1.1843 - acc: 0.6248 - val_loss: 0.6457 - val_acc: 0.8160\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8329 - acc: 0.7421\n",
      "Epoch 00003: val_loss improved from 0.64566 to 0.45534, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/003-0.4553.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.8331 - acc: 0.7421 - val_loss: 0.4553 - val_acc: 0.8768\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.8039\n",
      "Epoch 00004: val_loss did not improve from 0.45534\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.6469 - acc: 0.8039 - val_loss: 0.4688 - val_acc: 0.8717\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8390\n",
      "Epoch 00005: val_loss improved from 0.45534 to 0.31893, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/005-0.3189.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.5335 - acc: 0.8390 - val_loss: 0.3189 - val_acc: 0.9117\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8648\n",
      "Epoch 00006: val_loss improved from 0.31893 to 0.26749, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/006-0.2675.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.4420 - acc: 0.8647 - val_loss: 0.2675 - val_acc: 0.9276\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8830\n",
      "Epoch 00007: val_loss improved from 0.26749 to 0.25209, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/007-0.2521.hdf5\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.3850 - acc: 0.8829 - val_loss: 0.2521 - val_acc: 0.9287\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8936\n",
      "Epoch 00008: val_loss did not improve from 0.25209\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.3445 - acc: 0.8936 - val_loss: 0.3106 - val_acc: 0.9110\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.9064\n",
      "Epoch 00009: val_loss improved from 0.25209 to 0.20935, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/009-0.2094.hdf5\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.3056 - acc: 0.9063 - val_loss: 0.2094 - val_acc: 0.9406\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9138\n",
      "Epoch 00010: val_loss did not improve from 0.20935\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.2799 - acc: 0.9138 - val_loss: 0.2343 - val_acc: 0.9345\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9240\n",
      "Epoch 00011: val_loss improved from 0.20935 to 0.16133, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/011-0.1613.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.2468 - acc: 0.9240 - val_loss: 0.1613 - val_acc: 0.9525\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9291\n",
      "Epoch 00012: val_loss did not improve from 0.16133\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.2298 - acc: 0.9291 - val_loss: 0.2183 - val_acc: 0.9336\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9354\n",
      "Epoch 00013: val_loss did not improve from 0.16133\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.2101 - acc: 0.9353 - val_loss: 0.2310 - val_acc: 0.9306\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9388\n",
      "Epoch 00014: val_loss did not improve from 0.16133\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1979 - acc: 0.9387 - val_loss: 0.1625 - val_acc: 0.9522\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9407\n",
      "Epoch 00015: val_loss did not improve from 0.16133\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.1902 - acc: 0.9407 - val_loss: 0.1733 - val_acc: 0.9520\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9481\n",
      "Epoch 00016: val_loss did not improve from 0.16133\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.1711 - acc: 0.9481 - val_loss: 0.1950 - val_acc: 0.9425\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9470\n",
      "Epoch 00017: val_loss improved from 0.16133 to 0.15484, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/017-0.1548.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1711 - acc: 0.9470 - val_loss: 0.1548 - val_acc: 0.9546\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9542\n",
      "Epoch 00018: val_loss did not improve from 0.15484\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1460 - acc: 0.9542 - val_loss: 0.1687 - val_acc: 0.9527\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9564\n",
      "Epoch 00019: val_loss improved from 0.15484 to 0.14115, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/019-0.1412.hdf5\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1402 - acc: 0.9564 - val_loss: 0.1412 - val_acc: 0.9604\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9583\n",
      "Epoch 00020: val_loss improved from 0.14115 to 0.14079, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/020-0.1408.hdf5\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.1350 - acc: 0.9583 - val_loss: 0.1408 - val_acc: 0.9581\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9585\n",
      "Epoch 00021: val_loss improved from 0.14079 to 0.13242, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/021-0.1324.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1297 - acc: 0.9585 - val_loss: 0.1324 - val_acc: 0.9625\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9649\n",
      "Epoch 00022: val_loss improved from 0.13242 to 0.12618, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/022-0.1262.hdf5\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1146 - acc: 0.9650 - val_loss: 0.1262 - val_acc: 0.9627\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9668\n",
      "Epoch 00023: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.1056 - acc: 0.9668 - val_loss: 0.1293 - val_acc: 0.9632\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9661\n",
      "Epoch 00024: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.1075 - acc: 0.9661 - val_loss: 0.1479 - val_acc: 0.9597\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9706\n",
      "Epoch 00025: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0939 - acc: 0.9705 - val_loss: 0.1485 - val_acc: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9684\n",
      "Epoch 00026: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0994 - acc: 0.9684 - val_loss: 0.1382 - val_acc: 0.9625\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9710\n",
      "Epoch 00027: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0891 - acc: 0.9710 - val_loss: 0.1494 - val_acc: 0.9613\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9745\n",
      "Epoch 00028: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0820 - acc: 0.9745 - val_loss: 0.1316 - val_acc: 0.9655\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9745\n",
      "Epoch 00029: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0802 - acc: 0.9745 - val_loss: 0.1997 - val_acc: 0.9467\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9738\n",
      "Epoch 00030: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0852 - acc: 0.9738 - val_loss: 0.1504 - val_acc: 0.9606\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9755\n",
      "Epoch 00031: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0774 - acc: 0.9755 - val_loss: 0.1659 - val_acc: 0.9541\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9762\n",
      "Epoch 00032: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0752 - acc: 0.9762 - val_loss: 0.1389 - val_acc: 0.9604\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9806\n",
      "Epoch 00033: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0635 - acc: 0.9805 - val_loss: 0.1538 - val_acc: 0.9578\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9779\n",
      "Epoch 00034: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0712 - acc: 0.9779 - val_loss: 0.1300 - val_acc: 0.9651\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9799\n",
      "Epoch 00035: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0638 - acc: 0.9799 - val_loss: 0.1343 - val_acc: 0.9662\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9796\n",
      "Epoch 00036: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0644 - acc: 0.9796 - val_loss: 0.1683 - val_acc: 0.9550\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9840\n",
      "Epoch 00037: val_loss did not improve from 0.12618\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0519 - acc: 0.9840 - val_loss: 0.1418 - val_acc: 0.9644\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9822\n",
      "Epoch 00038: val_loss improved from 0.12618 to 0.12434, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/038-0.1243.hdf5\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0563 - acc: 0.9822 - val_loss: 0.1243 - val_acc: 0.9667\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9843\n",
      "Epoch 00039: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0513 - acc: 0.9843 - val_loss: 0.1284 - val_acc: 0.9669\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9841\n",
      "Epoch 00040: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0522 - acc: 0.9840 - val_loss: 0.1577 - val_acc: 0.9611\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9809\n",
      "Epoch 00041: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0587 - acc: 0.9809 - val_loss: 0.1558 - val_acc: 0.9588\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9842\n",
      "Epoch 00042: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0509 - acc: 0.9842 - val_loss: 0.1591 - val_acc: 0.9616\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9846\n",
      "Epoch 00043: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0499 - acc: 0.9846 - val_loss: 0.1412 - val_acc: 0.9655\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9876\n",
      "Epoch 00044: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0411 - acc: 0.9875 - val_loss: 0.2137 - val_acc: 0.9474\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9834\n",
      "Epoch 00045: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0515 - acc: 0.9834 - val_loss: 0.1464 - val_acc: 0.9613\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9864\n",
      "Epoch 00046: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0432 - acc: 0.9864 - val_loss: 0.1514 - val_acc: 0.9620\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0354 - acc: 0.9895 - val_loss: 0.1622 - val_acc: 0.9606\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9894\n",
      "Epoch 00048: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0359 - acc: 0.9894 - val_loss: 0.1719 - val_acc: 0.9618\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9899\n",
      "Epoch 00049: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0330 - acc: 0.9899 - val_loss: 0.1366 - val_acc: 0.9639\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9857\n",
      "Epoch 00050: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0440 - acc: 0.9857 - val_loss: 0.1318 - val_acc: 0.9672\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9912\n",
      "Epoch 00051: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0300 - acc: 0.9912 - val_loss: 0.1572 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9897\n",
      "Epoch 00052: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0328 - acc: 0.9897 - val_loss: 0.1279 - val_acc: 0.9686\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9898\n",
      "Epoch 00053: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0327 - acc: 0.9898 - val_loss: 0.1273 - val_acc: 0.9679\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9870\n",
      "Epoch 00054: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0418 - acc: 0.9870 - val_loss: 0.1542 - val_acc: 0.9623\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9928\n",
      "Epoch 00055: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0252 - acc: 0.9928 - val_loss: 0.1392 - val_acc: 0.9660\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9922\n",
      "Epoch 00056: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0256 - acc: 0.9921 - val_loss: 0.1477 - val_acc: 0.9623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9883\n",
      "Epoch 00057: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0397 - acc: 0.9883 - val_loss: 0.1395 - val_acc: 0.9653\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9890\n",
      "Epoch 00058: val_loss did not improve from 0.12434\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0356 - acc: 0.9891 - val_loss: 0.1302 - val_acc: 0.9667\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9933\n",
      "Epoch 00059: val_loss improved from 0.12434 to 0.12034, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_8_conv_checkpoint/059-0.1203.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0229 - acc: 0.9933 - val_loss: 0.1203 - val_acc: 0.9679\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9926\n",
      "Epoch 00060: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0249 - acc: 0.9926 - val_loss: 0.1364 - val_acc: 0.9637\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9927\n",
      "Epoch 00061: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0242 - acc: 0.9927 - val_loss: 0.1763 - val_acc: 0.9613\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9930\n",
      "Epoch 00062: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.1348 - val_acc: 0.9662\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9900\n",
      "Epoch 00063: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 0.1533 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9942\n",
      "Epoch 00064: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0204 - acc: 0.9942 - val_loss: 0.2595 - val_acc: 0.9497\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9914\n",
      "Epoch 00065: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0265 - acc: 0.9914 - val_loss: 0.1394 - val_acc: 0.9655\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9923\n",
      "Epoch 00066: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0255 - acc: 0.9922 - val_loss: 0.1780 - val_acc: 0.9560\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9900\n",
      "Epoch 00067: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 0.1333 - val_acc: 0.9667\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9928\n",
      "Epoch 00068: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0250 - acc: 0.9928 - val_loss: 0.1337 - val_acc: 0.9688\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9925\n",
      "Epoch 00069: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0247 - acc: 0.9925 - val_loss: 0.1426 - val_acc: 0.9690\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 00070: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.1312 - val_acc: 0.9676\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9954\n",
      "Epoch 00071: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0175 - acc: 0.9954 - val_loss: 0.1435 - val_acc: 0.9676\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00072: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0191 - acc: 0.9945 - val_loss: 0.1445 - val_acc: 0.9676\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9894\n",
      "Epoch 00073: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0343 - acc: 0.9894 - val_loss: 0.1653 - val_acc: 0.9662\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0192 - acc: 0.9945 - val_loss: 0.1658 - val_acc: 0.9646\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9920\n",
      "Epoch 00075: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0245 - acc: 0.9919 - val_loss: 0.1427 - val_acc: 0.9679\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9932\n",
      "Epoch 00076: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0214 - acc: 0.9932 - val_loss: 0.1661 - val_acc: 0.9655\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9956\n",
      "Epoch 00077: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0151 - acc: 0.9956 - val_loss: 0.1470 - val_acc: 0.9716\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9937\n",
      "Epoch 00078: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.1578 - val_acc: 0.9653\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9914\n",
      "Epoch 00079: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0285 - acc: 0.9914 - val_loss: 0.1257 - val_acc: 0.9683\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9968\n",
      "Epoch 00080: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0120 - acc: 0.9967 - val_loss: 0.1770 - val_acc: 0.9623\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9898\n",
      "Epoch 00081: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 383s 10ms/sample - loss: 0.0335 - acc: 0.9898 - val_loss: 0.1531 - val_acc: 0.9634\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9926\n",
      "Epoch 00082: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0250 - acc: 0.9926 - val_loss: 0.1327 - val_acc: 0.9709\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9963\n",
      "Epoch 00083: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.1390 - val_acc: 0.9686\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00084: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1366 - val_acc: 0.9693\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9967\n",
      "Epoch 00085: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0118 - acc: 0.9967 - val_loss: 0.1445 - val_acc: 0.9697\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9968\n",
      "Epoch 00086: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0125 - acc: 0.9968 - val_loss: 0.1559 - val_acc: 0.9679\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9941\n",
      "Epoch 00087: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0190 - acc: 0.9941 - val_loss: 0.1468 - val_acc: 0.9679\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9947\n",
      "Epoch 00088: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0163 - acc: 0.9947 - val_loss: 0.1433 - val_acc: 0.9679\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00089: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1637 - val_acc: 0.9660\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9945\n",
      "Epoch 00090: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0181 - acc: 0.9945 - val_loss: 0.1637 - val_acc: 0.9625\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9964\n",
      "Epoch 00091: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0134 - acc: 0.9963 - val_loss: 0.1635 - val_acc: 0.9660\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9925\n",
      "Epoch 00092: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0240 - acc: 0.9925 - val_loss: 0.1748 - val_acc: 0.9658\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9938\n",
      "Epoch 00093: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0199 - acc: 0.9938 - val_loss: 0.1416 - val_acc: 0.9669\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9974\n",
      "Epoch 00094: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 387s 11ms/sample - loss: 0.0106 - acc: 0.9973 - val_loss: 0.1468 - val_acc: 0.9667\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9939\n",
      "Epoch 00095: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 381s 10ms/sample - loss: 0.0201 - acc: 0.9938 - val_loss: 0.1303 - val_acc: 0.9727\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9922\n",
      "Epoch 00096: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0245 - acc: 0.9921 - val_loss: 0.1355 - val_acc: 0.9700\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9953\n",
      "Epoch 00097: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0148 - acc: 0.9953 - val_loss: 0.1479 - val_acc: 0.9700\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9978\n",
      "Epoch 00098: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0079 - acc: 0.9978 - val_loss: 0.1511 - val_acc: 0.9695\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9943\n",
      "Epoch 00099: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.1389 - val_acc: 0.9686\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9938\n",
      "Epoch 00100: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0197 - acc: 0.9938 - val_loss: 0.1526 - val_acc: 0.9702\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 00101: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0094 - acc: 0.9976 - val_loss: 0.1528 - val_acc: 0.9679\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9937\n",
      "Epoch 00102: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0211 - acc: 0.9937 - val_loss: 0.1559 - val_acc: 0.9658\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9955\n",
      "Epoch 00103: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0143 - acc: 0.9955 - val_loss: 0.1520 - val_acc: 0.9674\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9957\n",
      "Epoch 00104: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.1358 - val_acc: 0.9711\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9979\n",
      "Epoch 00105: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 382s 10ms/sample - loss: 0.0078 - acc: 0.9979 - val_loss: 0.1568 - val_acc: 0.9686\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9933\n",
      "Epoch 00106: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0212 - acc: 0.9933 - val_loss: 0.1388 - val_acc: 0.9693\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9968\n",
      "Epoch 00107: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0094 - acc: 0.9968 - val_loss: 0.1807 - val_acc: 0.9632\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0094 - acc: 0.9973 - val_loss: 0.1661 - val_acc: 0.9641\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 00109: val_loss did not improve from 0.12034\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0101 - acc: 0.9969 - val_loss: 0.1743 - val_acc: 0.9665\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.1596 - acc: 0.9605\n",
      "Loss: 0.15961949999133745 Accuracy: 0.96054\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0675 - acc: 0.3835\n",
      "Epoch 00001: val_loss improved from inf to 0.86604, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/001-0.8660.hdf5\n",
      "36805/36805 [==============================] - 394s 11ms/sample - loss: 2.0674 - acc: 0.3835 - val_loss: 0.8660 - val_acc: 0.7531\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0545 - acc: 0.6713\n",
      "Epoch 00002: val_loss improved from 0.86604 to 0.59124, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/002-0.5912.hdf5\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 1.0548 - acc: 0.6712 - val_loss: 0.5912 - val_acc: 0.8472\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7722\n",
      "Epoch 00003: val_loss improved from 0.59124 to 0.48139, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/003-0.4814.hdf5\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.7559 - acc: 0.7722 - val_loss: 0.4814 - val_acc: 0.8644\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8221\n",
      "Epoch 00004: val_loss improved from 0.48139 to 0.34465, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/004-0.3447.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.5946 - acc: 0.8220 - val_loss: 0.3447 - val_acc: 0.9052\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8536\n",
      "Epoch 00005: val_loss improved from 0.34465 to 0.32411, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/005-0.3241.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.4940 - acc: 0.8535 - val_loss: 0.3241 - val_acc: 0.9145\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8765\n",
      "Epoch 00006: val_loss improved from 0.32411 to 0.31722, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/006-0.3172.hdf5\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.4249 - acc: 0.8764 - val_loss: 0.3172 - val_acc: 0.9164\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8886\n",
      "Epoch 00007: val_loss improved from 0.31722 to 0.21128, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/007-0.2113.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.3797 - acc: 0.8886 - val_loss: 0.2113 - val_acc: 0.9420\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.9020\n",
      "Epoch 00008: val_loss did not improve from 0.21128\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.3332 - acc: 0.9019 - val_loss: 0.2223 - val_acc: 0.9399\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9106\n",
      "Epoch 00009: val_loss did not improve from 0.21128\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.3085 - acc: 0.9106 - val_loss: 0.2380 - val_acc: 0.9331\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9173\n",
      "Epoch 00010: val_loss improved from 0.21128 to 0.17969, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/010-0.1797.hdf5\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.2830 - acc: 0.9173 - val_loss: 0.1797 - val_acc: 0.9485\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9268\n",
      "Epoch 00011: val_loss did not improve from 0.17969\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.2504 - acc: 0.9267 - val_loss: 0.2075 - val_acc: 0.9401\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9317\n",
      "Epoch 00012: val_loss improved from 0.17969 to 0.14821, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/012-0.1482.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.2331 - acc: 0.9316 - val_loss: 0.1482 - val_acc: 0.9557\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9355\n",
      "Epoch 00013: val_loss improved from 0.14821 to 0.14207, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/013-0.1421.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.2168 - acc: 0.9355 - val_loss: 0.1421 - val_acc: 0.9595\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9417\n",
      "Epoch 00014: val_loss did not improve from 0.14207\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.2034 - acc: 0.9417 - val_loss: 0.1557 - val_acc: 0.9567\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9441\n",
      "Epoch 00015: val_loss improved from 0.14207 to 0.12612, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/015-0.1261.hdf5\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1934 - acc: 0.9441 - val_loss: 0.1261 - val_acc: 0.9616\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9480\n",
      "Epoch 00016: val_loss did not improve from 0.12612\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1770 - acc: 0.9480 - val_loss: 0.1279 - val_acc: 0.9630\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9482\n",
      "Epoch 00017: val_loss did not improve from 0.12612\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1737 - acc: 0.9482 - val_loss: 0.1436 - val_acc: 0.9583\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9516\n",
      "Epoch 00018: val_loss did not improve from 0.12612\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1633 - acc: 0.9516 - val_loss: 0.1274 - val_acc: 0.9630\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9540\n",
      "Epoch 00019: val_loss did not improve from 0.12612\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1520 - acc: 0.9539 - val_loss: 0.1348 - val_acc: 0.9611\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9555\n",
      "Epoch 00020: val_loss improved from 0.12612 to 0.11660, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/020-0.1166.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1515 - acc: 0.9554 - val_loss: 0.1166 - val_acc: 0.9660\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9584\n",
      "Epoch 00021: val_loss did not improve from 0.11660\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1397 - acc: 0.9584 - val_loss: 0.1283 - val_acc: 0.9606\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9604\n",
      "Epoch 00022: val_loss did not improve from 0.11660\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1319 - acc: 0.9604 - val_loss: 0.1454 - val_acc: 0.9602\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9645\n",
      "Epoch 00023: val_loss did not improve from 0.11660\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.1208 - acc: 0.9645 - val_loss: 0.1254 - val_acc: 0.9639\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9675\n",
      "Epoch 00024: val_loss did not improve from 0.11660\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1098 - acc: 0.9675 - val_loss: 0.1237 - val_acc: 0.9641\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9657\n",
      "Epoch 00025: val_loss did not improve from 0.11660\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.1133 - acc: 0.9657 - val_loss: 0.1181 - val_acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9695\n",
      "Epoch 00026: val_loss improved from 0.11660 to 0.11602, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/026-0.1160.hdf5\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.1047 - acc: 0.9694 - val_loss: 0.1160 - val_acc: 0.9653\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9711\n",
      "Epoch 00027: val_loss improved from 0.11602 to 0.10906, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/027-0.1091.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0988 - acc: 0.9710 - val_loss: 0.1091 - val_acc: 0.9669\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9717\n",
      "Epoch 00028: val_loss did not improve from 0.10906\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0933 - acc: 0.9717 - val_loss: 0.1135 - val_acc: 0.9639\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9716\n",
      "Epoch 00029: val_loss did not improve from 0.10906\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0944 - acc: 0.9716 - val_loss: 0.1315 - val_acc: 0.9632\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9743\n",
      "Epoch 00030: val_loss did not improve from 0.10906\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0837 - acc: 0.9743 - val_loss: 0.1178 - val_acc: 0.9639\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9710\n",
      "Epoch 00031: val_loss improved from 0.10906 to 0.09596, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/031-0.0960.hdf5\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0946 - acc: 0.9709 - val_loss: 0.0960 - val_acc: 0.9709\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9750\n",
      "Epoch 00032: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0826 - acc: 0.9750 - val_loss: 0.1027 - val_acc: 0.9672\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9777\n",
      "Epoch 00033: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0757 - acc: 0.9777 - val_loss: 0.1134 - val_acc: 0.9662\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9773\n",
      "Epoch 00034: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0756 - acc: 0.9773 - val_loss: 0.1037 - val_acc: 0.9688\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9774\n",
      "Epoch 00035: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0777 - acc: 0.9774 - val_loss: 0.1092 - val_acc: 0.9658\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9787\n",
      "Epoch 00036: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 390s 11ms/sample - loss: 0.0710 - acc: 0.9788 - val_loss: 0.1130 - val_acc: 0.9667\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9825\n",
      "Epoch 00037: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0598 - acc: 0.9825 - val_loss: 0.1110 - val_acc: 0.9658\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9809\n",
      "Epoch 00038: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0650 - acc: 0.9809 - val_loss: 0.1192 - val_acc: 0.9658\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9776\n",
      "Epoch 00039: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0737 - acc: 0.9775 - val_loss: 0.1090 - val_acc: 0.9697\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9810\n",
      "Epoch 00040: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0630 - acc: 0.9810 - val_loss: 0.1172 - val_acc: 0.9658\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9833\n",
      "Epoch 00041: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0574 - acc: 0.9833 - val_loss: 0.1077 - val_acc: 0.9672\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9860\n",
      "Epoch 00042: val_loss did not improve from 0.09596\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0487 - acc: 0.9860 - val_loss: 0.1041 - val_acc: 0.9686\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9816\n",
      "Epoch 00043: val_loss improved from 0.09596 to 0.08783, saving model to model/checkpoint/1D_CNN_custom_2_DO_BN_9_conv_checkpoint/043-0.0878.hdf5\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0593 - acc: 0.9815 - val_loss: 0.0878 - val_acc: 0.9727\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9832\n",
      "Epoch 00044: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0571 - acc: 0.9832 - val_loss: 0.1058 - val_acc: 0.9704\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9862\n",
      "Epoch 00045: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0458 - acc: 0.9862 - val_loss: 0.1205 - val_acc: 0.9625\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9861\n",
      "Epoch 00046: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0487 - acc: 0.9860 - val_loss: 0.1187 - val_acc: 0.9660\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9839\n",
      "Epoch 00047: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0531 - acc: 0.9839 - val_loss: 0.0899 - val_acc: 0.9739\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9889\n",
      "Epoch 00048: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0386 - acc: 0.9888 - val_loss: 0.1246 - val_acc: 0.9637\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9822\n",
      "Epoch 00049: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0570 - acc: 0.9822 - val_loss: 0.1108 - val_acc: 0.9683\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9888\n",
      "Epoch 00050: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0388 - acc: 0.9888 - val_loss: 0.1293 - val_acc: 0.9646\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9875\n",
      "Epoch 00051: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0423 - acc: 0.9874 - val_loss: 0.1302 - val_acc: 0.9625\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9862\n",
      "Epoch 00052: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0454 - acc: 0.9861 - val_loss: 0.1199 - val_acc: 0.9632\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9870\n",
      "Epoch 00053: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0443 - acc: 0.9870 - val_loss: 0.1050 - val_acc: 0.9688\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9897\n",
      "Epoch 00054: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0353 - acc: 0.9897 - val_loss: 0.1044 - val_acc: 0.9723\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9905\n",
      "Epoch 00055: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0334 - acc: 0.9904 - val_loss: 0.1115 - val_acc: 0.9700\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9879\n",
      "Epoch 00056: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0406 - acc: 0.9879 - val_loss: 0.1058 - val_acc: 0.9711\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9893\n",
      "Epoch 00057: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0342 - acc: 0.9893 - val_loss: 0.1375 - val_acc: 0.9588\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9916\n",
      "Epoch 00058: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0307 - acc: 0.9916 - val_loss: 0.1074 - val_acc: 0.9706\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9903\n",
      "Epoch 00059: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0326 - acc: 0.9902 - val_loss: 0.1472 - val_acc: 0.9646\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9846\n",
      "Epoch 00060: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0536 - acc: 0.9846 - val_loss: 0.1156 - val_acc: 0.9674\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9902\n",
      "Epoch 00061: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0332 - acc: 0.9902 - val_loss: 0.1049 - val_acc: 0.9706\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9935\n",
      "Epoch 00062: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0249 - acc: 0.9934 - val_loss: 0.1032 - val_acc: 0.9711\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9890\n",
      "Epoch 00063: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0365 - acc: 0.9890 - val_loss: 0.1417 - val_acc: 0.9658\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9904\n",
      "Epoch 00064: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0328 - acc: 0.9903 - val_loss: 0.1165 - val_acc: 0.9706\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9880\n",
      "Epoch 00065: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0400 - acc: 0.9880 - val_loss: 0.0932 - val_acc: 0.9767\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9924\n",
      "Epoch 00066: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0278 - acc: 0.9924 - val_loss: 0.1027 - val_acc: 0.9734\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9946\n",
      "Epoch 00067: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0204 - acc: 0.9946 - val_loss: 0.1192 - val_acc: 0.9695\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9930\n",
      "Epoch 00068: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0241 - acc: 0.9930 - val_loss: 0.1134 - val_acc: 0.9720\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9938\n",
      "Epoch 00069: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0234 - acc: 0.9938 - val_loss: 0.1192 - val_acc: 0.9686\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9937\n",
      "Epoch 00070: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0235 - acc: 0.9937 - val_loss: 0.1488 - val_acc: 0.9602\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9912\n",
      "Epoch 00071: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0310 - acc: 0.9911 - val_loss: 0.1338 - val_acc: 0.9672\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9909\n",
      "Epoch 00072: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0309 - acc: 0.9909 - val_loss: 0.1156 - val_acc: 0.9713\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9886\n",
      "Epoch 00073: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0381 - acc: 0.9886 - val_loss: 0.1108 - val_acc: 0.9693\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9949\n",
      "Epoch 00074: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0187 - acc: 0.9949 - val_loss: 0.1186 - val_acc: 0.9709\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9935\n",
      "Epoch 00075: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0225 - acc: 0.9935 - val_loss: 0.1121 - val_acc: 0.9723\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9915\n",
      "Epoch 00076: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0287 - acc: 0.9916 - val_loss: 0.1229 - val_acc: 0.9713\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9958\n",
      "Epoch 00077: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0165 - acc: 0.9958 - val_loss: 0.1227 - val_acc: 0.9711\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9911\n",
      "Epoch 00078: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0295 - acc: 0.9911 - val_loss: 0.1218 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9895\n",
      "Epoch 00079: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0344 - acc: 0.9895 - val_loss: 0.1004 - val_acc: 0.9739\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9961\n",
      "Epoch 00080: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0162 - acc: 0.9960 - val_loss: 0.1081 - val_acc: 0.9720\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9924\n",
      "Epoch 00081: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0262 - acc: 0.9924 - val_loss: 0.1090 - val_acc: 0.9718\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0139 - acc: 0.9965 - val_loss: 0.1276 - val_acc: 0.9686\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9938\n",
      "Epoch 00083: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0211 - acc: 0.9938 - val_loss: 0.1199 - val_acc: 0.9702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9954\n",
      "Epoch 00084: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0174 - acc: 0.9954 - val_loss: 0.1142 - val_acc: 0.9709\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9938\n",
      "Epoch 00085: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 10ms/sample - loss: 0.0212 - acc: 0.9938 - val_loss: 0.1176 - val_acc: 0.9681\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9948\n",
      "Epoch 00086: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0183 - acc: 0.9948 - val_loss: 0.1331 - val_acc: 0.9704\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9942\n",
      "Epoch 00087: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 385s 10ms/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.1356 - val_acc: 0.9665\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9937\n",
      "Epoch 00088: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0213 - acc: 0.9937 - val_loss: 0.1076 - val_acc: 0.9748\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9947\n",
      "Epoch 00089: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 386s 11ms/sample - loss: 0.0177 - acc: 0.9947 - val_loss: 0.1281 - val_acc: 0.9697\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9952\n",
      "Epoch 00090: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 389s 11ms/sample - loss: 0.0166 - acc: 0.9951 - val_loss: 0.1318 - val_acc: 0.9681\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9913\n",
      "Epoch 00091: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 384s 10ms/sample - loss: 0.0291 - acc: 0.9913 - val_loss: 0.1081 - val_acc: 0.9716\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9945\n",
      "Epoch 00092: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 392s 11ms/sample - loss: 0.0196 - acc: 0.9945 - val_loss: 0.1009 - val_acc: 0.9760\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9939\n",
      "Epoch 00093: val_loss did not improve from 0.08783\n",
      "36805/36805 [==============================] - 388s 11ms/sample - loss: 0.0211 - acc: 0.9939 - val_loss: 0.1170 - val_acc: 0.9693\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.1256 - acc: 0.9628\n",
      "Loss: 0.12563233519551323 Accuracy: 0.9628245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "    #         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1369616     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,369,616\n",
      "Trainable params: 1,367,056\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.3842 - acc: 0.8964\n",
      "Loss: 0.38417450881573767 Accuracy: 0.8963655\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1362064     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,362,064\n",
      "Trainable params: 1,359,248\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 23s 5ms/sample - loss: 0.2345 - acc: 0.9346\n",
      "Loss: 0.23450478180732312 Accuracy: 0.93457943\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1415952     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,415,952\n",
      "Trainable params: 1,412,880\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 22s 5ms/sample - loss: 0.1596 - acc: 0.9605\n",
      "Loss: 0.15961949999133745 Accuracy: 0.96054\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1444944     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,444,944\n",
      "Trainable params: 1,441,744\n",
      "Non-trainable params: 3,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 24s 5ms/sample - loss: 0.1256 - acc: 0.9628\n",
      "Loss: 0.12563233519551323 Accuracy: 0.9628245\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1369616     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,369,616\n",
      "Trainable params: 1,367,056\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 24s 5ms/sample - loss: 0.5755 - acc: 0.8791\n",
      "Loss: 0.5754671365920133 Accuracy: 0.87912774\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_51_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_51_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1362064     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,362,064\n",
      "Trainable params: 1,359,248\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 24s 5ms/sample - loss: 0.3053 - acc: 0.9300\n",
      "Loss: 0.30527937733573085 Accuracy: 0.9300104\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_58_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_58_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1415952     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,415,952\n",
      "Trainable params: 1,412,880\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 25s 5ms/sample - loss: 0.2175 - acc: 0.9537\n",
      "Loss: 0.2175073031510148 Accuracy: 0.9536864\n",
      "\n",
      "1D_CNN_custom_2_DO_BN_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_66_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_66_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1444944     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,444,944\n",
      "Trainable params: 1,441,744\n",
      "Non-trainable params: 3,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 24s 5ms/sample - loss: 0.1480 - acc: 0.9637\n",
      "Loss: 0.14797008283035434 Accuracy: 0.96365523\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
