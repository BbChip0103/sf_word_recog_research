{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_075_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_075_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4450 - acc: 0.2366\n",
      "Epoch 00001: val_loss improved from inf to 2.27126, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/001-2.2713.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 2.4446 - acc: 0.2368 - val_loss: 2.2713 - val_acc: 0.3173\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0350 - acc: 0.3959\n",
      "Epoch 00002: val_loss improved from 2.27126 to 2.16076, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/002-2.1608.hdf5\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 2.0350 - acc: 0.3959 - val_loss: 2.1608 - val_acc: 0.3380\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7773 - acc: 0.4826\n",
      "Epoch 00003: val_loss improved from 2.16076 to 2.12478, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/003-2.1248.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.7773 - acc: 0.4827 - val_loss: 2.1248 - val_acc: 0.3466\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5774 - acc: 0.5411\n",
      "Epoch 00004: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.5779 - acc: 0.5409 - val_loss: 2.1424 - val_acc: 0.3315\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4177 - acc: 0.5921\n",
      "Epoch 00005: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 758us/sample - loss: 1.4177 - acc: 0.5920 - val_loss: 2.1461 - val_acc: 0.3443\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2816 - acc: 0.6344\n",
      "Epoch 00006: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.2811 - acc: 0.6346 - val_loss: 2.1701 - val_acc: 0.3366\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1703 - acc: 0.6651\n",
      "Epoch 00007: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 1.1700 - acc: 0.6652 - val_loss: 2.2004 - val_acc: 0.3431\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6985\n",
      "Epoch 00008: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 1.0711 - acc: 0.6982 - val_loss: 2.2250 - val_acc: 0.3459\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9852 - acc: 0.7244\n",
      "Epoch 00009: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 760us/sample - loss: 0.9852 - acc: 0.7244 - val_loss: 2.2783 - val_acc: 0.3438\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9127 - acc: 0.7428\n",
      "Epoch 00010: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.9130 - acc: 0.7428 - val_loss: 2.3168 - val_acc: 0.3359\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8427 - acc: 0.7673\n",
      "Epoch 00011: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.8425 - acc: 0.7675 - val_loss: 2.3557 - val_acc: 0.3382\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7837\n",
      "Epoch 00012: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.7827 - acc: 0.7836 - val_loss: 2.3697 - val_acc: 0.3464\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7299 - acc: 0.8004\n",
      "Epoch 00013: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.7298 - acc: 0.8005 - val_loss: 2.4278 - val_acc: 0.3468\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.8168\n",
      "Epoch 00014: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.6795 - acc: 0.8168 - val_loss: 2.4823 - val_acc: 0.3410\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6394 - acc: 0.8288\n",
      "Epoch 00015: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.6394 - acc: 0.8287 - val_loss: 2.5451 - val_acc: 0.3350\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5986 - acc: 0.8378\n",
      "Epoch 00016: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5986 - acc: 0.8378 - val_loss: 2.5713 - val_acc: 0.3338\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.8506\n",
      "Epoch 00017: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.5625 - acc: 0.8506 - val_loss: 2.6055 - val_acc: 0.3399\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8595\n",
      "Epoch 00018: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.5323 - acc: 0.8595 - val_loss: 2.6591 - val_acc: 0.3403\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.8685\n",
      "Epoch 00019: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.5002 - acc: 0.8683 - val_loss: 2.7013 - val_acc: 0.3413\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8745\n",
      "Epoch 00020: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.4763 - acc: 0.8745 - val_loss: 2.7416 - val_acc: 0.3424\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.8832\n",
      "Epoch 00021: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.4469 - acc: 0.8831 - val_loss: 2.7935 - val_acc: 0.3340\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8896\n",
      "Epoch 00022: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.4272 - acc: 0.8897 - val_loss: 2.7957 - val_acc: 0.3424\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8937\n",
      "Epoch 00023: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.4074 - acc: 0.8936 - val_loss: 2.8825 - val_acc: 0.3380\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.9029\n",
      "Epoch 00024: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3795 - acc: 0.9029 - val_loss: 2.9168 - val_acc: 0.3475\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.9055\n",
      "Epoch 00025: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.3662 - acc: 0.9056 - val_loss: 2.9644 - val_acc: 0.3284\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.9082\n",
      "Epoch 00026: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.3483 - acc: 0.9082 - val_loss: 2.9912 - val_acc: 0.3324\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.9130\n",
      "Epoch 00027: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.3373 - acc: 0.9129 - val_loss: 3.0233 - val_acc: 0.3373\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9164\n",
      "Epoch 00028: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.3244 - acc: 0.9164 - val_loss: 3.0518 - val_acc: 0.3345\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9219\n",
      "Epoch 00029: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.3054 - acc: 0.9219 - val_loss: 3.1191 - val_acc: 0.3378\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9278\n",
      "Epoch 00030: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2888 - acc: 0.9279 - val_loss: 3.1535 - val_acc: 0.3329\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9288\n",
      "Epoch 00031: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.2802 - acc: 0.9288 - val_loss: 3.1856 - val_acc: 0.3359\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9308\n",
      "Epoch 00032: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2757 - acc: 0.9308 - val_loss: 3.2242 - val_acc: 0.3382\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9344\n",
      "Epoch 00033: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.2605 - acc: 0.9344 - val_loss: 3.2730 - val_acc: 0.3319\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9355\n",
      "Epoch 00034: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.2548 - acc: 0.9355 - val_loss: 3.2833 - val_acc: 0.3420\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9374\n",
      "Epoch 00035: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.2438 - acc: 0.9374 - val_loss: 3.3296 - val_acc: 0.3373\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9410\n",
      "Epoch 00036: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 734us/sample - loss: 0.2351 - acc: 0.9410 - val_loss: 3.3595 - val_acc: 0.3413\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9443\n",
      "Epoch 00037: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.2237 - acc: 0.9443 - val_loss: 3.4048 - val_acc: 0.3385\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9468\n",
      "Epoch 00038: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.2149 - acc: 0.9469 - val_loss: 3.4490 - val_acc: 0.3340\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9459\n",
      "Epoch 00039: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.2149 - acc: 0.9459 - val_loss: 3.4573 - val_acc: 0.3420\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9490\n",
      "Epoch 00040: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.2053 - acc: 0.9490 - val_loss: 3.4970 - val_acc: 0.3415\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9492\n",
      "Epoch 00041: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.2037 - acc: 0.9492 - val_loss: 3.5465 - val_acc: 0.3333\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9534\n",
      "Epoch 00042: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1927 - acc: 0.9534 - val_loss: 3.5420 - val_acc: 0.3347\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9527\n",
      "Epoch 00043: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1883 - acc: 0.9527 - val_loss: 3.5937 - val_acc: 0.3347\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9553\n",
      "Epoch 00044: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1801 - acc: 0.9553 - val_loss: 3.6216 - val_acc: 0.3368\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9567\n",
      "Epoch 00045: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1764 - acc: 0.9567 - val_loss: 3.6588 - val_acc: 0.3433\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9568\n",
      "Epoch 00046: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1742 - acc: 0.9568 - val_loss: 3.6816 - val_acc: 0.3380\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9597\n",
      "Epoch 00047: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1658 - acc: 0.9597 - val_loss: 3.6930 - val_acc: 0.3354\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9605\n",
      "Epoch 00048: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1647 - acc: 0.9604 - val_loss: 3.7083 - val_acc: 0.3366\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9613\n",
      "Epoch 00049: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1605 - acc: 0.9613 - val_loss: 3.7383 - val_acc: 0.3389\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9607\n",
      "Epoch 00050: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1585 - acc: 0.9607 - val_loss: 3.8113 - val_acc: 0.3296\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9626\n",
      "Epoch 00051: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1492 - acc: 0.9626 - val_loss: 3.8480 - val_acc: 0.3357\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9652\n",
      "Epoch 00052: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1433 - acc: 0.9652 - val_loss: 3.8327 - val_acc: 0.3312\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9654\n",
      "Epoch 00053: val_loss did not improve from 2.12478\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.1430 - acc: 0.9654 - val_loss: 3.8579 - val_acc: 0.3352\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 277us/sample - loss: 2.1406 - acc: 0.3267\n",
      "Loss: 2.140593708267093 Accuracy: 0.32668743\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2879 - acc: 0.2913\n",
      "Epoch 00001: val_loss improved from inf to 1.99661, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/001-1.9966.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.2878 - acc: 0.2913 - val_loss: 1.9966 - val_acc: 0.4011\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8218 - acc: 0.4547\n",
      "Epoch 00002: val_loss improved from 1.99661 to 1.79420, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/002-1.7942.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.8219 - acc: 0.4546 - val_loss: 1.7942 - val_acc: 0.4673\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5910 - acc: 0.5233\n",
      "Epoch 00003: val_loss improved from 1.79420 to 1.76711, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/003-1.7671.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.5911 - acc: 0.5233 - val_loss: 1.7671 - val_acc: 0.4610\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4261 - acc: 0.5732\n",
      "Epoch 00004: val_loss improved from 1.76711 to 1.76516, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/004-1.7652.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.4261 - acc: 0.5733 - val_loss: 1.7652 - val_acc: 0.4689\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2890 - acc: 0.6147\n",
      "Epoch 00005: val_loss improved from 1.76516 to 1.75349, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/005-1.7535.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2890 - acc: 0.6148 - val_loss: 1.7535 - val_acc: 0.4614\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1822 - acc: 0.6451\n",
      "Epoch 00006: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1823 - acc: 0.6450 - val_loss: 1.7857 - val_acc: 0.4507\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0941 - acc: 0.6692\n",
      "Epoch 00007: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0942 - acc: 0.6692 - val_loss: 1.8470 - val_acc: 0.4365\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0110 - acc: 0.6931\n",
      "Epoch 00008: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0110 - acc: 0.6931 - val_loss: 1.8197 - val_acc: 0.4573\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9349 - acc: 0.7169\n",
      "Epoch 00009: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9350 - acc: 0.7168 - val_loss: 1.8715 - val_acc: 0.4552\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8591 - acc: 0.7401\n",
      "Epoch 00010: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8591 - acc: 0.7401 - val_loss: 1.8914 - val_acc: 0.4633\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8063 - acc: 0.7573\n",
      "Epoch 00011: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8063 - acc: 0.7573 - val_loss: 1.9310 - val_acc: 0.4545\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7722\n",
      "Epoch 00012: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7513 - acc: 0.7722 - val_loss: 1.9407 - val_acc: 0.4598\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6988 - acc: 0.7891\n",
      "Epoch 00013: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6989 - acc: 0.7891 - val_loss: 1.9682 - val_acc: 0.4605\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.8028\n",
      "Epoch 00014: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6500 - acc: 0.8028 - val_loss: 2.0288 - val_acc: 0.4575\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6173 - acc: 0.8106\n",
      "Epoch 00015: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6173 - acc: 0.8106 - val_loss: 2.0412 - val_acc: 0.4591\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.8212\n",
      "Epoch 00016: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5779 - acc: 0.8212 - val_loss: 2.0837 - val_acc: 0.4503\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8355\n",
      "Epoch 00017: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5408 - acc: 0.8355 - val_loss: 2.1024 - val_acc: 0.4591\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.8438\n",
      "Epoch 00018: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5100 - acc: 0.8439 - val_loss: 2.1364 - val_acc: 0.4659\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8526\n",
      "Epoch 00019: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4822 - acc: 0.8526 - val_loss: 2.1786 - val_acc: 0.4587\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8587\n",
      "Epoch 00020: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4579 - acc: 0.8587 - val_loss: 2.2242 - val_acc: 0.4577\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8674\n",
      "Epoch 00021: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4285 - acc: 0.8674 - val_loss: 2.2281 - val_acc: 0.4619\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8749\n",
      "Epoch 00022: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4062 - acc: 0.8749 - val_loss: 2.2673 - val_acc: 0.4652\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8821\n",
      "Epoch 00023: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3821 - acc: 0.8821 - val_loss: 2.3205 - val_acc: 0.4696\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8858\n",
      "Epoch 00024: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3660 - acc: 0.8858 - val_loss: 2.3275 - val_acc: 0.4724\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8927\n",
      "Epoch 00025: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3523 - acc: 0.8927 - val_loss: 2.3224 - val_acc: 0.4747\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8957\n",
      "Epoch 00026: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3363 - acc: 0.8957 - val_loss: 2.4079 - val_acc: 0.4684\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9026\n",
      "Epoch 00027: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3191 - acc: 0.9026 - val_loss: 2.3813 - val_acc: 0.4775\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9079\n",
      "Epoch 00028: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2989 - acc: 0.9079 - val_loss: 2.4710 - val_acc: 0.4670\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9110\n",
      "Epoch 00029: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2925 - acc: 0.9110 - val_loss: 2.4726 - val_acc: 0.4745\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9172\n",
      "Epoch 00030: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2735 - acc: 0.9172 - val_loss: 2.4412 - val_acc: 0.4801\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9166\n",
      "Epoch 00031: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2742 - acc: 0.9166 - val_loss: 2.4954 - val_acc: 0.4759\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9243\n",
      "Epoch 00032: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2555 - acc: 0.9243 - val_loss: 2.5498 - val_acc: 0.4808\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9253\n",
      "Epoch 00033: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2467 - acc: 0.9253 - val_loss: 2.5461 - val_acc: 0.4743\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9290\n",
      "Epoch 00034: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2376 - acc: 0.9291 - val_loss: 2.5617 - val_acc: 0.4691\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9307\n",
      "Epoch 00035: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2305 - acc: 0.9306 - val_loss: 2.6618 - val_acc: 0.4624\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9334\n",
      "Epoch 00036: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2238 - acc: 0.9334 - val_loss: 2.5923 - val_acc: 0.4799\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9354\n",
      "Epoch 00037: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2176 - acc: 0.9354 - val_loss: 2.6026 - val_acc: 0.4817\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9385\n",
      "Epoch 00038: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2060 - acc: 0.9385 - val_loss: 2.6560 - val_acc: 0.4829\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9400\n",
      "Epoch 00039: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2019 - acc: 0.9400 - val_loss: 2.6444 - val_acc: 0.4836\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1936 - acc: 0.9420\n",
      "Epoch 00040: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1936 - acc: 0.9420 - val_loss: 2.6595 - val_acc: 0.4896\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9448\n",
      "Epoch 00041: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1873 - acc: 0.9448 - val_loss: 2.7077 - val_acc: 0.4799\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9456\n",
      "Epoch 00042: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1824 - acc: 0.9456 - val_loss: 2.7319 - val_acc: 0.4885\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9458\n",
      "Epoch 00043: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1811 - acc: 0.9458 - val_loss: 2.7619 - val_acc: 0.4810\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9488\n",
      "Epoch 00044: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1764 - acc: 0.9488 - val_loss: 2.7408 - val_acc: 0.4908\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9527\n",
      "Epoch 00045: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1672 - acc: 0.9527 - val_loss: 2.7472 - val_acc: 0.4829\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9521\n",
      "Epoch 00046: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1663 - acc: 0.9521 - val_loss: 2.7410 - val_acc: 0.4892\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9518\n",
      "Epoch 00047: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1632 - acc: 0.9518 - val_loss: 2.8249 - val_acc: 0.4885\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9547\n",
      "Epoch 00048: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1600 - acc: 0.9547 - val_loss: 2.7824 - val_acc: 0.4903\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9527\n",
      "Epoch 00049: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1614 - acc: 0.9527 - val_loss: 2.7987 - val_acc: 0.4885\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9569\n",
      "Epoch 00050: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1482 - acc: 0.9569 - val_loss: 2.8234 - val_acc: 0.4955\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9607\n",
      "Epoch 00051: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1382 - acc: 0.9607 - val_loss: 2.8476 - val_acc: 0.4959\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9584\n",
      "Epoch 00052: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1423 - acc: 0.9584 - val_loss: 2.8566 - val_acc: 0.4903\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9602\n",
      "Epoch 00053: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1426 - acc: 0.9602 - val_loss: 2.8745 - val_acc: 0.4915\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9607\n",
      "Epoch 00054: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1364 - acc: 0.9607 - val_loss: 2.8721 - val_acc: 0.4948\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9635\n",
      "Epoch 00055: val_loss did not improve from 1.75349\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1292 - acc: 0.9635 - val_loss: 2.9411 - val_acc: 0.4812\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 468us/sample - loss: 1.8014 - acc: 0.4496\n",
      "Loss: 1.8014415986812746 Accuracy: 0.44963655\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2197 - acc: 0.3011\n",
      "Epoch 00001: val_loss improved from inf to 1.80300, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/001-1.8030.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.2196 - acc: 0.3011 - val_loss: 1.8030 - val_acc: 0.4545\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6413 - acc: 0.4965\n",
      "Epoch 00002: val_loss improved from 1.80300 to 1.53133, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/002-1.5313.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.6412 - acc: 0.4965 - val_loss: 1.5313 - val_acc: 0.5386\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4191 - acc: 0.5609\n",
      "Epoch 00003: val_loss improved from 1.53133 to 1.44475, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/003-1.4448.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4190 - acc: 0.5610 - val_loss: 1.4448 - val_acc: 0.5586\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2863 - acc: 0.6045\n",
      "Epoch 00004: val_loss improved from 1.44475 to 1.39868, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/004-1.3987.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2863 - acc: 0.6046 - val_loss: 1.3987 - val_acc: 0.5693\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1831 - acc: 0.6351\n",
      "Epoch 00005: val_loss improved from 1.39868 to 1.36817, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/005-1.3682.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1832 - acc: 0.6350 - val_loss: 1.3682 - val_acc: 0.5816\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1072 - acc: 0.6582\n",
      "Epoch 00006: val_loss improved from 1.36817 to 1.34143, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/006-1.3414.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1073 - acc: 0.6582 - val_loss: 1.3414 - val_acc: 0.5935\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0328 - acc: 0.6781\n",
      "Epoch 00007: val_loss did not improve from 1.34143\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0329 - acc: 0.6781 - val_loss: 1.3514 - val_acc: 0.5870\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6968\n",
      "Epoch 00008: val_loss improved from 1.34143 to 1.33188, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/008-1.3319.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9703 - acc: 0.6968 - val_loss: 1.3319 - val_acc: 0.6010\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.7097\n",
      "Epoch 00009: val_loss did not improve from 1.33188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9198 - acc: 0.7097 - val_loss: 1.3338 - val_acc: 0.6019\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8606 - acc: 0.7303\n",
      "Epoch 00010: val_loss improved from 1.33188 to 1.30512, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/010-1.3051.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8606 - acc: 0.7304 - val_loss: 1.3051 - val_acc: 0.6042\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7432\n",
      "Epoch 00011: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8072 - acc: 0.7432 - val_loss: 1.3180 - val_acc: 0.6068\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7587\n",
      "Epoch 00012: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7645 - acc: 0.7587 - val_loss: 1.3415 - val_acc: 0.6035\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7209 - acc: 0.7710\n",
      "Epoch 00013: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7210 - acc: 0.7710 - val_loss: 1.3339 - val_acc: 0.6138\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.7798\n",
      "Epoch 00014: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6885 - acc: 0.7798 - val_loss: 1.3285 - val_acc: 0.6145\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.7892\n",
      "Epoch 00015: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6563 - acc: 0.7892 - val_loss: 1.3214 - val_acc: 0.6294\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.7998\n",
      "Epoch 00016: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6243 - acc: 0.7998 - val_loss: 1.3402 - val_acc: 0.6233\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5975 - acc: 0.8082\n",
      "Epoch 00017: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5974 - acc: 0.8082 - val_loss: 1.3297 - val_acc: 0.6275\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8165\n",
      "Epoch 00018: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5695 - acc: 0.8165 - val_loss: 1.3568 - val_acc: 0.6252\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8285\n",
      "Epoch 00019: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5386 - acc: 0.8285 - val_loss: 1.3508 - val_acc: 0.6327\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8342\n",
      "Epoch 00020: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5138 - acc: 0.8342 - val_loss: 1.3948 - val_acc: 0.6157\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8428\n",
      "Epoch 00021: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4929 - acc: 0.8428 - val_loss: 1.3682 - val_acc: 0.6417\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.8469\n",
      "Epoch 00022: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4740 - acc: 0.8469 - val_loss: 1.4268 - val_acc: 0.6313\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8532\n",
      "Epoch 00023: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4556 - acc: 0.8532 - val_loss: 1.3832 - val_acc: 0.6364\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4386 - acc: 0.8598\n",
      "Epoch 00024: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4385 - acc: 0.8598 - val_loss: 1.4021 - val_acc: 0.6355\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8637\n",
      "Epoch 00025: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4178 - acc: 0.8636 - val_loss: 1.3822 - val_acc: 0.6485\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8668\n",
      "Epoch 00026: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4125 - acc: 0.8668 - val_loss: 1.4102 - val_acc: 0.6399\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8717\n",
      "Epoch 00027: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3934 - acc: 0.8717 - val_loss: 1.4162 - val_acc: 0.6403\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8758\n",
      "Epoch 00028: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3811 - acc: 0.8758 - val_loss: 1.4108 - val_acc: 0.6448\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8791\n",
      "Epoch 00029: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3708 - acc: 0.8791 - val_loss: 1.4415 - val_acc: 0.6438\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8842\n",
      "Epoch 00030: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3539 - acc: 0.8842 - val_loss: 1.4555 - val_acc: 0.6532\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8890\n",
      "Epoch 00031: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3407 - acc: 0.8890 - val_loss: 1.4357 - val_acc: 0.6541\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8932\n",
      "Epoch 00032: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3334 - acc: 0.8932 - val_loss: 1.4660 - val_acc: 0.6494\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8969\n",
      "Epoch 00033: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3203 - acc: 0.8969 - val_loss: 1.4485 - val_acc: 0.6499\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8993\n",
      "Epoch 00034: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3087 - acc: 0.8993 - val_loss: 1.4597 - val_acc: 0.6562\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9026\n",
      "Epoch 00035: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3026 - acc: 0.9026 - val_loss: 1.4469 - val_acc: 0.6571\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9056\n",
      "Epoch 00036: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2961 - acc: 0.9056 - val_loss: 1.4743 - val_acc: 0.6543\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9068\n",
      "Epoch 00037: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2915 - acc: 0.9068 - val_loss: 1.5048 - val_acc: 0.6527\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9114\n",
      "Epoch 00038: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2757 - acc: 0.9114 - val_loss: 1.4810 - val_acc: 0.6548\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9115\n",
      "Epoch 00039: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2735 - acc: 0.9115 - val_loss: 1.5061 - val_acc: 0.6525\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9113\n",
      "Epoch 00040: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2702 - acc: 0.9112 - val_loss: 1.4905 - val_acc: 0.6636\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9139\n",
      "Epoch 00041: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2673 - acc: 0.9140 - val_loss: 1.5089 - val_acc: 0.6606\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9178\n",
      "Epoch 00042: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2549 - acc: 0.9178 - val_loss: 1.4992 - val_acc: 0.6655\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9199\n",
      "Epoch 00043: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2467 - acc: 0.9199 - val_loss: 1.5254 - val_acc: 0.6653\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9258\n",
      "Epoch 00044: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2360 - acc: 0.9258 - val_loss: 1.5541 - val_acc: 0.6604\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9231\n",
      "Epoch 00045: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2387 - acc: 0.9230 - val_loss: 1.5385 - val_acc: 0.6578\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9240\n",
      "Epoch 00046: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2342 - acc: 0.9241 - val_loss: 1.5389 - val_acc: 0.6618\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9275\n",
      "Epoch 00047: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2268 - acc: 0.9275 - val_loss: 1.5345 - val_acc: 0.6657\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9289\n",
      "Epoch 00048: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2242 - acc: 0.9289 - val_loss: 1.5318 - val_acc: 0.6683\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9307\n",
      "Epoch 00049: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2176 - acc: 0.9307 - val_loss: 1.5538 - val_acc: 0.6625\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9297\n",
      "Epoch 00050: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2153 - acc: 0.9297 - val_loss: 1.5611 - val_acc: 0.6594\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9337\n",
      "Epoch 00051: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2077 - acc: 0.9338 - val_loss: 1.5469 - val_acc: 0.6676\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9329\n",
      "Epoch 00052: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2132 - acc: 0.9329 - val_loss: 1.5250 - val_acc: 0.6720\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9344\n",
      "Epoch 00053: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2054 - acc: 0.9344 - val_loss: 1.5922 - val_acc: 0.6569\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9351\n",
      "Epoch 00054: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2043 - acc: 0.9351 - val_loss: 1.5589 - val_acc: 0.6692\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9392\n",
      "Epoch 00055: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1935 - acc: 0.9392 - val_loss: 1.5885 - val_acc: 0.6697\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9398\n",
      "Epoch 00056: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1905 - acc: 0.9398 - val_loss: 1.5629 - val_acc: 0.6744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9408\n",
      "Epoch 00057: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1891 - acc: 0.9408 - val_loss: 1.5735 - val_acc: 0.6709\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9433\n",
      "Epoch 00058: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1816 - acc: 0.9433 - val_loss: 1.6326 - val_acc: 0.6606\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9424\n",
      "Epoch 00059: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1854 - acc: 0.9424 - val_loss: 1.5611 - val_acc: 0.6804\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9458\n",
      "Epoch 00060: val_loss did not improve from 1.30512\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1737 - acc: 0.9458 - val_loss: 1.5579 - val_acc: 0.6795\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 547us/sample - loss: 1.3782 - acc: 0.5736\n",
      "Loss: 1.3782013468529453 Accuracy: 0.5736241\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2086 - acc: 0.2812\n",
      "Epoch 00001: val_loss improved from inf to 1.72490, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/001-1.7249.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.2086 - acc: 0.2812 - val_loss: 1.7249 - val_acc: 0.4689\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6479 - acc: 0.4679\n",
      "Epoch 00002: val_loss improved from 1.72490 to 1.49909, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/002-1.4991.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.6478 - acc: 0.4680 - val_loss: 1.4991 - val_acc: 0.5302\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4416 - acc: 0.5433\n",
      "Epoch 00003: val_loss improved from 1.49909 to 1.36669, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/003-1.3667.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4424 - acc: 0.5433 - val_loss: 1.3667 - val_acc: 0.5782\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3079 - acc: 0.5884\n",
      "Epoch 00004: val_loss improved from 1.36669 to 1.23814, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/004-1.2381.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3079 - acc: 0.5884 - val_loss: 1.2381 - val_acc: 0.6138\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2036 - acc: 0.6251\n",
      "Epoch 00005: val_loss improved from 1.23814 to 1.15063, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/005-1.1506.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2037 - acc: 0.6251 - val_loss: 1.1506 - val_acc: 0.6455\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1196 - acc: 0.6538\n",
      "Epoch 00006: val_loss improved from 1.15063 to 1.07747, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/006-1.0775.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1195 - acc: 0.6538 - val_loss: 1.0775 - val_acc: 0.6702\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0461 - acc: 0.6790\n",
      "Epoch 00007: val_loss improved from 1.07747 to 1.07540, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/007-1.0754.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0462 - acc: 0.6790 - val_loss: 1.0754 - val_acc: 0.6727\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9895 - acc: 0.6957\n",
      "Epoch 00008: val_loss improved from 1.07540 to 0.99875, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/008-0.9987.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9895 - acc: 0.6956 - val_loss: 0.9987 - val_acc: 0.6962\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.7150\n",
      "Epoch 00009: val_loss improved from 0.99875 to 0.97823, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/009-0.9782.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9286 - acc: 0.7150 - val_loss: 0.9782 - val_acc: 0.6988\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7302\n",
      "Epoch 00010: val_loss improved from 0.97823 to 0.97331, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/010-0.9733.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8779 - acc: 0.7302 - val_loss: 0.9733 - val_acc: 0.7053\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8360 - acc: 0.7434\n",
      "Epoch 00011: val_loss improved from 0.97331 to 0.91886, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/011-0.9189.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8360 - acc: 0.7434 - val_loss: 0.9189 - val_acc: 0.7240\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7564\n",
      "Epoch 00012: val_loss improved from 0.91886 to 0.88638, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/012-0.8864.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7960 - acc: 0.7564 - val_loss: 0.8864 - val_acc: 0.7338\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7686\n",
      "Epoch 00013: val_loss improved from 0.88638 to 0.88407, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/013-0.8841.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7559 - acc: 0.7686 - val_loss: 0.8841 - val_acc: 0.7379\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7150 - acc: 0.7808\n",
      "Epoch 00014: val_loss did not improve from 0.88407\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7149 - acc: 0.7808 - val_loss: 0.8934 - val_acc: 0.7291\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.7893\n",
      "Epoch 00015: val_loss improved from 0.88407 to 0.84092, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/015-0.8409.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6842 - acc: 0.7893 - val_loss: 0.8409 - val_acc: 0.7538\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6587 - acc: 0.7956\n",
      "Epoch 00016: val_loss did not improve from 0.84092\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6587 - acc: 0.7956 - val_loss: 0.8488 - val_acc: 0.7494\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6284 - acc: 0.8057\n",
      "Epoch 00017: val_loss did not improve from 0.84092\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6284 - acc: 0.8057 - val_loss: 0.8457 - val_acc: 0.7538\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.8106\n",
      "Epoch 00018: val_loss did not improve from 0.84092\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6026 - acc: 0.8106 - val_loss: 0.8480 - val_acc: 0.7489\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5774 - acc: 0.8199\n",
      "Epoch 00019: val_loss improved from 0.84092 to 0.81517, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/019-0.8152.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5774 - acc: 0.8199 - val_loss: 0.8152 - val_acc: 0.7629\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5607 - acc: 0.8254\n",
      "Epoch 00020: val_loss improved from 0.81517 to 0.81200, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/020-0.8120.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5607 - acc: 0.8254 - val_loss: 0.8120 - val_acc: 0.7626\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8343\n",
      "Epoch 00021: val_loss did not improve from 0.81200\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5320 - acc: 0.8343 - val_loss: 0.8133 - val_acc: 0.7615\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.8360\n",
      "Epoch 00022: val_loss did not improve from 0.81200\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5102 - acc: 0.8360 - val_loss: 0.8277 - val_acc: 0.7624\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8412\n",
      "Epoch 00023: val_loss improved from 0.81200 to 0.80680, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/023-0.8068.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4955 - acc: 0.8412 - val_loss: 0.8068 - val_acc: 0.7678\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8470\n",
      "Epoch 00024: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4797 - acc: 0.8470 - val_loss: 0.8083 - val_acc: 0.7703\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8524\n",
      "Epoch 00025: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4660 - acc: 0.8523 - val_loss: 0.8217 - val_acc: 0.7633\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8533\n",
      "Epoch 00026: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4530 - acc: 0.8533 - val_loss: 0.8114 - val_acc: 0.7724\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8601\n",
      "Epoch 00027: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4375 - acc: 0.8601 - val_loss: 0.8118 - val_acc: 0.7710\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8666\n",
      "Epoch 00028: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4187 - acc: 0.8666 - val_loss: 0.8362 - val_acc: 0.7687\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8672\n",
      "Epoch 00029: val_loss did not improve from 0.80680\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4145 - acc: 0.8672 - val_loss: 0.8449 - val_acc: 0.7671\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3972 - acc: 0.8727\n",
      "Epoch 00030: val_loss improved from 0.80680 to 0.78897, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/030-0.7890.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3973 - acc: 0.8726 - val_loss: 0.7890 - val_acc: 0.7792\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8736\n",
      "Epoch 00031: val_loss did not improve from 0.78897\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3928 - acc: 0.8736 - val_loss: 0.8281 - val_acc: 0.7647\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8761\n",
      "Epoch 00032: val_loss did not improve from 0.78897\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3823 - acc: 0.8761 - val_loss: 0.8481 - val_acc: 0.7687\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.8762\n",
      "Epoch 00033: val_loss did not improve from 0.78897\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3764 - acc: 0.8762 - val_loss: 0.8041 - val_acc: 0.7759\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8813\n",
      "Epoch 00034: val_loss improved from 0.78897 to 0.78722, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/034-0.7872.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3658 - acc: 0.8813 - val_loss: 0.7872 - val_acc: 0.7822\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8890\n",
      "Epoch 00035: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3470 - acc: 0.8890 - val_loss: 0.8233 - val_acc: 0.7815\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8894\n",
      "Epoch 00036: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3434 - acc: 0.8894 - val_loss: 0.8226 - val_acc: 0.7761\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8898\n",
      "Epoch 00037: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3402 - acc: 0.8898 - val_loss: 0.8204 - val_acc: 0.7785\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8908\n",
      "Epoch 00038: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3317 - acc: 0.8908 - val_loss: 0.8264 - val_acc: 0.7803\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8944\n",
      "Epoch 00039: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3217 - acc: 0.8944 - val_loss: 0.8014 - val_acc: 0.7836\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8946\n",
      "Epoch 00040: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3223 - acc: 0.8946 - val_loss: 0.8195 - val_acc: 0.7820\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8968\n",
      "Epoch 00041: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3136 - acc: 0.8968 - val_loss: 0.8309 - val_acc: 0.7864\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8963\n",
      "Epoch 00042: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3109 - acc: 0.8963 - val_loss: 0.8201 - val_acc: 0.7836\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9027\n",
      "Epoch 00043: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2990 - acc: 0.9026 - val_loss: 0.8194 - val_acc: 0.7866\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9002\n",
      "Epoch 00044: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3011 - acc: 0.9002 - val_loss: 0.8269 - val_acc: 0.7873\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9027\n",
      "Epoch 00045: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2968 - acc: 0.9027 - val_loss: 0.8146 - val_acc: 0.7943\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9072\n",
      "Epoch 00046: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2800 - acc: 0.9072 - val_loss: 0.8126 - val_acc: 0.7852\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9068\n",
      "Epoch 00047: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2816 - acc: 0.9069 - val_loss: 0.8216 - val_acc: 0.7897\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9093\n",
      "Epoch 00048: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2735 - acc: 0.9093 - val_loss: 0.8345 - val_acc: 0.7852\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9097\n",
      "Epoch 00049: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2731 - acc: 0.9097 - val_loss: 0.8675 - val_acc: 0.7831\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9120\n",
      "Epoch 00050: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2675 - acc: 0.9119 - val_loss: 0.8237 - val_acc: 0.7906\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9139\n",
      "Epoch 00051: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2612 - acc: 0.9139 - val_loss: 0.8331 - val_acc: 0.7929\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9127\n",
      "Epoch 00052: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2652 - acc: 0.9127 - val_loss: 0.8280 - val_acc: 0.7897\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9156\n",
      "Epoch 00053: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2567 - acc: 0.9156 - val_loss: 0.8490 - val_acc: 0.7864\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9148\n",
      "Epoch 00054: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2567 - acc: 0.9148 - val_loss: 0.8628 - val_acc: 0.7859\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9154\n",
      "Epoch 00055: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2528 - acc: 0.9154 - val_loss: 0.8261 - val_acc: 0.7966\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9163\n",
      "Epoch 00056: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2533 - acc: 0.9163 - val_loss: 0.8403 - val_acc: 0.7899\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9185\n",
      "Epoch 00057: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2492 - acc: 0.9185 - val_loss: 0.8390 - val_acc: 0.7950\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9189\n",
      "Epoch 00058: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2419 - acc: 0.9190 - val_loss: 0.8139 - val_acc: 0.8004\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9202\n",
      "Epoch 00059: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2385 - acc: 0.9202 - val_loss: 0.8774 - val_acc: 0.7906\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9216\n",
      "Epoch 00060: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2372 - acc: 0.9215 - val_loss: 0.8476 - val_acc: 0.7971\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2372 - acc: 0.9220 - val_loss: 0.8106 - val_acc: 0.7999\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9220\n",
      "Epoch 00062: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2352 - acc: 0.9220 - val_loss: 0.8320 - val_acc: 0.7978\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9260\n",
      "Epoch 00063: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2253 - acc: 0.9260 - val_loss: 0.8840 - val_acc: 0.7883\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9235\n",
      "Epoch 00064: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2281 - acc: 0.9235 - val_loss: 0.8466 - val_acc: 0.7959\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9259\n",
      "Epoch 00065: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2230 - acc: 0.9259 - val_loss: 0.8440 - val_acc: 0.7950\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9260\n",
      "Epoch 00066: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2250 - acc: 0.9260 - val_loss: 0.8322 - val_acc: 0.7969\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9252\n",
      "Epoch 00067: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2268 - acc: 0.9252 - val_loss: 0.8059 - val_acc: 0.8060\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9301\n",
      "Epoch 00068: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2122 - acc: 0.9301 - val_loss: 0.8290 - val_acc: 0.8043\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9273\n",
      "Epoch 00069: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2166 - acc: 0.9272 - val_loss: 0.8219 - val_acc: 0.8039\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9316\n",
      "Epoch 00070: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2086 - acc: 0.9316 - val_loss: 0.8444 - val_acc: 0.8008\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9297\n",
      "Epoch 00071: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2122 - acc: 0.9297 - val_loss: 0.8345 - val_acc: 0.7987\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9300\n",
      "Epoch 00072: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2121 - acc: 0.9300 - val_loss: 0.8431 - val_acc: 0.8097\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9305\n",
      "Epoch 00073: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2095 - acc: 0.9305 - val_loss: 0.8316 - val_acc: 0.8025\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9338\n",
      "Epoch 00074: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2051 - acc: 0.9338 - val_loss: 0.8297 - val_acc: 0.8046\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9329\n",
      "Epoch 00075: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2019 - acc: 0.9329 - val_loss: 0.8250 - val_acc: 0.8123\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9330\n",
      "Epoch 00076: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2020 - acc: 0.9330 - val_loss: 0.8366 - val_acc: 0.8069\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9330\n",
      "Epoch 00077: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2006 - acc: 0.9330 - val_loss: 0.8309 - val_acc: 0.8102\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9351\n",
      "Epoch 00078: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1968 - acc: 0.9351 - val_loss: 0.8667 - val_acc: 0.8022\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9365\n",
      "Epoch 00079: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1933 - acc: 0.9366 - val_loss: 0.8622 - val_acc: 0.8050\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9351\n",
      "Epoch 00080: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1908 - acc: 0.9351 - val_loss: 0.8469 - val_acc: 0.8078\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9375\n",
      "Epoch 00081: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1895 - acc: 0.9375 - val_loss: 0.8437 - val_acc: 0.8032\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9365\n",
      "Epoch 00082: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1929 - acc: 0.9366 - val_loss: 0.8426 - val_acc: 0.8018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9334\n",
      "Epoch 00083: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1987 - acc: 0.9334 - val_loss: 0.8086 - val_acc: 0.8097\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9378\n",
      "Epoch 00084: val_loss did not improve from 0.78722\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1884 - acc: 0.9378 - val_loss: 0.8163 - val_acc: 0.8090\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 566us/sample - loss: 0.8739 - acc: 0.7533\n",
      "Loss: 0.8739451072173946 Accuracy: 0.75327104\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2048 - acc: 0.2775\n",
      "Epoch 00001: val_loss improved from inf to 1.65095, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/001-1.6509.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.2047 - acc: 0.2775 - val_loss: 1.6509 - val_acc: 0.4794\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5833 - acc: 0.4867\n",
      "Epoch 00002: val_loss improved from 1.65095 to 1.37802, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/002-1.3780.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.5834 - acc: 0.4867 - val_loss: 1.3780 - val_acc: 0.5614\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3771 - acc: 0.5595\n",
      "Epoch 00003: val_loss improved from 1.37802 to 1.20215, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/003-1.2021.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3770 - acc: 0.5595 - val_loss: 1.2021 - val_acc: 0.6266\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2183 - acc: 0.6151\n",
      "Epoch 00004: val_loss improved from 1.20215 to 1.06222, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/004-1.0622.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2182 - acc: 0.6151 - val_loss: 1.0622 - val_acc: 0.6837\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1058 - acc: 0.6544\n",
      "Epoch 00005: val_loss improved from 1.06222 to 1.02589, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/005-1.0259.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1059 - acc: 0.6543 - val_loss: 1.0259 - val_acc: 0.6853\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0199 - acc: 0.6807\n",
      "Epoch 00006: val_loss improved from 1.02589 to 0.92924, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/006-0.9292.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0199 - acc: 0.6806 - val_loss: 0.9292 - val_acc: 0.7237\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.7060\n",
      "Epoch 00007: val_loss improved from 0.92924 to 0.91048, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/007-0.9105.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9492 - acc: 0.7060 - val_loss: 0.9105 - val_acc: 0.7319\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8856 - acc: 0.7259\n",
      "Epoch 00008: val_loss improved from 0.91048 to 0.82670, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/008-0.8267.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8855 - acc: 0.7259 - val_loss: 0.8267 - val_acc: 0.7601\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7442\n",
      "Epoch 00009: val_loss improved from 0.82670 to 0.79711, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/009-0.7971.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8324 - acc: 0.7442 - val_loss: 0.7971 - val_acc: 0.7703\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7879 - acc: 0.7598\n",
      "Epoch 00010: val_loss improved from 0.79711 to 0.76133, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/010-0.7613.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7880 - acc: 0.7598 - val_loss: 0.7613 - val_acc: 0.7738\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7512 - acc: 0.7701\n",
      "Epoch 00011: val_loss did not improve from 0.76133\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7513 - acc: 0.7701 - val_loss: 0.7671 - val_acc: 0.7747\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7810\n",
      "Epoch 00012: val_loss improved from 0.76133 to 0.72115, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/012-0.7211.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7165 - acc: 0.7810 - val_loss: 0.7211 - val_acc: 0.7911\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6811 - acc: 0.7914\n",
      "Epoch 00013: val_loss did not improve from 0.72115\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6811 - acc: 0.7914 - val_loss: 0.7450 - val_acc: 0.7883\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6498 - acc: 0.8039\n",
      "Epoch 00014: val_loss improved from 0.72115 to 0.71359, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/014-0.7136.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6499 - acc: 0.8039 - val_loss: 0.7136 - val_acc: 0.7999\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6215 - acc: 0.8107\n",
      "Epoch 00015: val_loss improved from 0.71359 to 0.69265, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/015-0.6927.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6215 - acc: 0.8107 - val_loss: 0.6927 - val_acc: 0.8036\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.8182\n",
      "Epoch 00016: val_loss improved from 0.69265 to 0.66724, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/016-0.6672.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5992 - acc: 0.8182 - val_loss: 0.6672 - val_acc: 0.8109\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.8220\n",
      "Epoch 00017: val_loss improved from 0.66724 to 0.64790, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/017-0.6479.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5820 - acc: 0.8220 - val_loss: 0.6479 - val_acc: 0.8118\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5552 - acc: 0.8308\n",
      "Epoch 00018: val_loss improved from 0.64790 to 0.63520, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/018-0.6352.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5552 - acc: 0.8308 - val_loss: 0.6352 - val_acc: 0.8216\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8368\n",
      "Epoch 00019: val_loss improved from 0.63520 to 0.62894, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/019-0.6289.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5368 - acc: 0.8368 - val_loss: 0.6289 - val_acc: 0.8220\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8399\n",
      "Epoch 00020: val_loss did not improve from 0.62894\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5190 - acc: 0.8399 - val_loss: 0.6338 - val_acc: 0.8183\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.8448\n",
      "Epoch 00021: val_loss did not improve from 0.62894\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4973 - acc: 0.8448 - val_loss: 0.6619 - val_acc: 0.8143\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8507\n",
      "Epoch 00022: val_loss improved from 0.62894 to 0.61831, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/022-0.6183.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4822 - acc: 0.8508 - val_loss: 0.6183 - val_acc: 0.8265\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4699 - acc: 0.8546\n",
      "Epoch 00023: val_loss improved from 0.61831 to 0.60264, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/023-0.6026.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4699 - acc: 0.8546 - val_loss: 0.6026 - val_acc: 0.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8599\n",
      "Epoch 00024: val_loss did not improve from 0.60264\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4474 - acc: 0.8599 - val_loss: 0.6589 - val_acc: 0.8171\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8631\n",
      "Epoch 00025: val_loss did not improve from 0.60264\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4408 - acc: 0.8631 - val_loss: 0.6072 - val_acc: 0.8314\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4244 - acc: 0.8678\n",
      "Epoch 00026: val_loss improved from 0.60264 to 0.60255, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/026-0.6026.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4244 - acc: 0.8678 - val_loss: 0.6026 - val_acc: 0.8355\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8746\n",
      "Epoch 00027: val_loss did not improve from 0.60255\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4078 - acc: 0.8746 - val_loss: 0.6156 - val_acc: 0.8288\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8762\n",
      "Epoch 00028: val_loss did not improve from 0.60255\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3940 - acc: 0.8762 - val_loss: 0.6079 - val_acc: 0.8337\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8788\n",
      "Epoch 00029: val_loss improved from 0.60255 to 0.59609, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/029-0.5961.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3817 - acc: 0.8788 - val_loss: 0.5961 - val_acc: 0.8390\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8843\n",
      "Epoch 00030: val_loss did not improve from 0.59609\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3697 - acc: 0.8843 - val_loss: 0.6100 - val_acc: 0.8348\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8867\n",
      "Epoch 00031: val_loss did not improve from 0.59609\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3614 - acc: 0.8867 - val_loss: 0.6510 - val_acc: 0.8213\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8856\n",
      "Epoch 00032: val_loss did not improve from 0.59609\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3577 - acc: 0.8856 - val_loss: 0.6627 - val_acc: 0.8206\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8886\n",
      "Epoch 00033: val_loss improved from 0.59609 to 0.58352, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/033-0.5835.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3515 - acc: 0.8886 - val_loss: 0.5835 - val_acc: 0.8446\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8957\n",
      "Epoch 00034: val_loss improved from 0.58352 to 0.58229, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/034-0.5823.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3286 - acc: 0.8957 - val_loss: 0.5823 - val_acc: 0.8416\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8939\n",
      "Epoch 00035: val_loss did not improve from 0.58229\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3314 - acc: 0.8939 - val_loss: 0.5842 - val_acc: 0.8458\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8974\n",
      "Epoch 00036: val_loss did not improve from 0.58229\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3212 - acc: 0.8974 - val_loss: 0.6291 - val_acc: 0.8344\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8994\n",
      "Epoch 00037: val_loss did not improve from 0.58229\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3110 - acc: 0.8994 - val_loss: 0.6097 - val_acc: 0.8411\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9023\n",
      "Epoch 00038: val_loss improved from 0.58229 to 0.57528, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/038-0.5753.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3031 - acc: 0.9024 - val_loss: 0.5753 - val_acc: 0.8516\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9026\n",
      "Epoch 00039: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3006 - acc: 0.9026 - val_loss: 0.5822 - val_acc: 0.8453\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9074\n",
      "Epoch 00040: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2905 - acc: 0.9074 - val_loss: 0.6074 - val_acc: 0.8425\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9078\n",
      "Epoch 00041: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2862 - acc: 0.9078 - val_loss: 0.5990 - val_acc: 0.8491\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9083\n",
      "Epoch 00042: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2834 - acc: 0.9083 - val_loss: 0.6009 - val_acc: 0.8416\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9113\n",
      "Epoch 00043: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2737 - acc: 0.9113 - val_loss: 0.5937 - val_acc: 0.8470\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2686 - acc: 0.9145\n",
      "Epoch 00044: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2686 - acc: 0.9145 - val_loss: 0.6138 - val_acc: 0.8477\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9154\n",
      "Epoch 00045: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2624 - acc: 0.9154 - val_loss: 0.5857 - val_acc: 0.8546\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9146\n",
      "Epoch 00046: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2583 - acc: 0.9146 - val_loss: 0.5871 - val_acc: 0.8516\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9193\n",
      "Epoch 00047: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2483 - acc: 0.9193 - val_loss: 0.5770 - val_acc: 0.8574\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9192\n",
      "Epoch 00048: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2530 - acc: 0.9192 - val_loss: 0.5843 - val_acc: 0.8565\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9201\n",
      "Epoch 00049: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2432 - acc: 0.9201 - val_loss: 0.5990 - val_acc: 0.8519\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9227\n",
      "Epoch 00050: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2395 - acc: 0.9228 - val_loss: 0.5965 - val_acc: 0.8595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9201\n",
      "Epoch 00051: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2449 - acc: 0.9201 - val_loss: 0.6397 - val_acc: 0.8484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9253\n",
      "Epoch 00052: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2354 - acc: 0.9253 - val_loss: 0.5781 - val_acc: 0.8581\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9250\n",
      "Epoch 00053: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2318 - acc: 0.9250 - val_loss: 0.5811 - val_acc: 0.8602\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9259\n",
      "Epoch 00054: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2236 - acc: 0.9259 - val_loss: 0.6134 - val_acc: 0.8591\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9261\n",
      "Epoch 00055: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2247 - acc: 0.9261 - val_loss: 0.5968 - val_acc: 0.8530\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9297\n",
      "Epoch 00056: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2177 - acc: 0.9297 - val_loss: 0.5762 - val_acc: 0.8623\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9301\n",
      "Epoch 00057: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2163 - acc: 0.9301 - val_loss: 0.6224 - val_acc: 0.8500\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9293\n",
      "Epoch 00058: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2153 - acc: 0.9293 - val_loss: 0.6065 - val_acc: 0.8572\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9310\n",
      "Epoch 00059: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2133 - acc: 0.9310 - val_loss: 0.5824 - val_acc: 0.8635\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9332\n",
      "Epoch 00060: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2037 - acc: 0.9331 - val_loss: 0.6088 - val_acc: 0.8577\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9328\n",
      "Epoch 00061: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2075 - acc: 0.9328 - val_loss: 0.6187 - val_acc: 0.8628\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9340\n",
      "Epoch 00062: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2033 - acc: 0.9340 - val_loss: 0.6024 - val_acc: 0.8644\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9365\n",
      "Epoch 00063: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1940 - acc: 0.9365 - val_loss: 0.6164 - val_acc: 0.8586\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9368\n",
      "Epoch 00064: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1953 - acc: 0.9368 - val_loss: 0.5858 - val_acc: 0.8637\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9368\n",
      "Epoch 00065: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1952 - acc: 0.9368 - val_loss: 0.6063 - val_acc: 0.8677\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9351\n",
      "Epoch 00066: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1979 - acc: 0.9351 - val_loss: 0.5877 - val_acc: 0.8630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9384\n",
      "Epoch 00067: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1888 - acc: 0.9384 - val_loss: 0.5816 - val_acc: 0.8612\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9393\n",
      "Epoch 00068: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1869 - acc: 0.9393 - val_loss: 0.6204 - val_acc: 0.8621\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9383\n",
      "Epoch 00069: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1863 - acc: 0.9383 - val_loss: 0.5970 - val_acc: 0.8656\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9403\n",
      "Epoch 00070: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1825 - acc: 0.9403 - val_loss: 0.6083 - val_acc: 0.8570\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9431\n",
      "Epoch 00071: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1761 - acc: 0.9431 - val_loss: 0.6242 - val_acc: 0.8635\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9412\n",
      "Epoch 00072: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1849 - acc: 0.9412 - val_loss: 0.6045 - val_acc: 0.8609\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9449\n",
      "Epoch 00073: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1695 - acc: 0.9449 - val_loss: 0.6034 - val_acc: 0.8642\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9417\n",
      "Epoch 00074: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1772 - acc: 0.9417 - val_loss: 0.6762 - val_acc: 0.8495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9400\n",
      "Epoch 00075: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1823 - acc: 0.9400 - val_loss: 0.5940 - val_acc: 0.8635\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9438\n",
      "Epoch 00076: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1707 - acc: 0.9438 - val_loss: 0.5897 - val_acc: 0.8661\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9462\n",
      "Epoch 00077: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1666 - acc: 0.9462 - val_loss: 0.5963 - val_acc: 0.8647\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9448\n",
      "Epoch 00078: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1692 - acc: 0.9448 - val_loss: 0.6155 - val_acc: 0.8609\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9465\n",
      "Epoch 00079: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1670 - acc: 0.9465 - val_loss: 0.6144 - val_acc: 0.8693\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9468\n",
      "Epoch 00080: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1612 - acc: 0.9468 - val_loss: 0.6297 - val_acc: 0.8684\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9471\n",
      "Epoch 00081: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1630 - acc: 0.9472 - val_loss: 0.6383 - val_acc: 0.8647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9474\n",
      "Epoch 00082: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1649 - acc: 0.9474 - val_loss: 0.6156 - val_acc: 0.8635\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9461\n",
      "Epoch 00083: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1648 - acc: 0.9460 - val_loss: 0.6001 - val_acc: 0.8661\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9471\n",
      "Epoch 00084: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1618 - acc: 0.9470 - val_loss: 0.6043 - val_acc: 0.8658\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9472\n",
      "Epoch 00085: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1591 - acc: 0.9472 - val_loss: 0.6018 - val_acc: 0.8609\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9500\n",
      "Epoch 00086: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1552 - acc: 0.9500 - val_loss: 0.6064 - val_acc: 0.8700\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9497\n",
      "Epoch 00087: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1521 - acc: 0.9497 - val_loss: 0.6020 - val_acc: 0.8640\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9510\n",
      "Epoch 00088: val_loss did not improve from 0.57528\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1538 - acc: 0.9510 - val_loss: 0.6546 - val_acc: 0.8677\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 617us/sample - loss: 0.6760 - acc: 0.8116\n",
      "Loss: 0.6760014045151842 Accuracy: 0.8116303\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3947 - acc: 0.2121\n",
      "Epoch 00001: val_loss improved from inf to 1.70095, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/001-1.7010.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.3947 - acc: 0.2121 - val_loss: 1.7010 - val_acc: 0.4736\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6621 - acc: 0.4557\n",
      "Epoch 00002: val_loss improved from 1.70095 to 1.38410, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/002-1.3841.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.6621 - acc: 0.4557 - val_loss: 1.3841 - val_acc: 0.5625\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4596 - acc: 0.5213\n",
      "Epoch 00003: val_loss improved from 1.38410 to 1.23027, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/003-1.2303.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4596 - acc: 0.5213 - val_loss: 1.2303 - val_acc: 0.6194\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3304 - acc: 0.5662\n",
      "Epoch 00004: val_loss improved from 1.23027 to 1.13576, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/004-1.1358.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3303 - acc: 0.5662 - val_loss: 1.1358 - val_acc: 0.6536\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2201 - acc: 0.6124\n",
      "Epoch 00005: val_loss improved from 1.13576 to 0.99521, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/005-0.9952.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2200 - acc: 0.6125 - val_loss: 0.9952 - val_acc: 0.7037\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.6545\n",
      "Epoch 00006: val_loss improved from 0.99521 to 0.92267, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/006-0.9227.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0994 - acc: 0.6544 - val_loss: 0.9227 - val_acc: 0.7331\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.6916\n",
      "Epoch 00007: val_loss improved from 0.92267 to 0.81265, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/007-0.8127.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9992 - acc: 0.6916 - val_loss: 0.8127 - val_acc: 0.7622\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.7151\n",
      "Epoch 00008: val_loss improved from 0.81265 to 0.77887, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/008-0.7789.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9267 - acc: 0.7152 - val_loss: 0.7789 - val_acc: 0.7815\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8585 - acc: 0.7398\n",
      "Epoch 00009: val_loss improved from 0.77887 to 0.70795, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/009-0.7079.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8585 - acc: 0.7398 - val_loss: 0.7079 - val_acc: 0.7997\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.7561\n",
      "Epoch 00010: val_loss improved from 0.70795 to 0.66547, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/010-0.6655.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8017 - acc: 0.7561 - val_loss: 0.6655 - val_acc: 0.8153\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.7689\n",
      "Epoch 00011: val_loss improved from 0.66547 to 0.63734, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/011-0.6373.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7646 - acc: 0.7689 - val_loss: 0.6373 - val_acc: 0.8160\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7193 - acc: 0.7829\n",
      "Epoch 00012: val_loss improved from 0.63734 to 0.62249, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/012-0.6225.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7192 - acc: 0.7829 - val_loss: 0.6225 - val_acc: 0.8246\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.7936\n",
      "Epoch 00013: val_loss improved from 0.62249 to 0.57829, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/013-0.5783.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6856 - acc: 0.7936 - val_loss: 0.5783 - val_acc: 0.8381\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.8018\n",
      "Epoch 00014: val_loss improved from 0.57829 to 0.57628, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/014-0.5763.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6502 - acc: 0.8018 - val_loss: 0.5763 - val_acc: 0.8330\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6267 - acc: 0.8118\n",
      "Epoch 00015: val_loss improved from 0.57628 to 0.53276, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/015-0.5328.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6266 - acc: 0.8118 - val_loss: 0.5328 - val_acc: 0.8465\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.8204\n",
      "Epoch 00016: val_loss did not improve from 0.53276\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6048 - acc: 0.8204 - val_loss: 0.5490 - val_acc: 0.8479\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8259\n",
      "Epoch 00017: val_loss improved from 0.53276 to 0.49239, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/017-0.4924.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5747 - acc: 0.8259 - val_loss: 0.4924 - val_acc: 0.8614\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.8346\n",
      "Epoch 00018: val_loss improved from 0.49239 to 0.48407, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/018-0.4841.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5492 - acc: 0.8346 - val_loss: 0.4841 - val_acc: 0.8693\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8410\n",
      "Epoch 00019: val_loss improved from 0.48407 to 0.46898, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/019-0.4690.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5339 - acc: 0.8410 - val_loss: 0.4690 - val_acc: 0.8705\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8439\n",
      "Epoch 00020: val_loss improved from 0.46898 to 0.44408, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/020-0.4441.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5235 - acc: 0.8440 - val_loss: 0.4441 - val_acc: 0.8796\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8442\n",
      "Epoch 00021: val_loss did not improve from 0.44408\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5044 - acc: 0.8443 - val_loss: 0.4512 - val_acc: 0.8726\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8532\n",
      "Epoch 00022: val_loss did not improve from 0.44408\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4867 - acc: 0.8532 - val_loss: 0.4525 - val_acc: 0.8749\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8590\n",
      "Epoch 00023: val_loss improved from 0.44408 to 0.44222, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/023-0.4422.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4749 - acc: 0.8590 - val_loss: 0.4422 - val_acc: 0.8772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8627\n",
      "Epoch 00024: val_loss improved from 0.44222 to 0.40144, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/024-0.4014.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4563 - acc: 0.8627 - val_loss: 0.4014 - val_acc: 0.8908\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.8684\n",
      "Epoch 00025: val_loss did not improve from 0.40144\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4396 - acc: 0.8683 - val_loss: 0.4067 - val_acc: 0.8856\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8693\n",
      "Epoch 00026: val_loss did not improve from 0.40144\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4348 - acc: 0.8693 - val_loss: 0.4060 - val_acc: 0.8868\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8725\n",
      "Epoch 00027: val_loss did not improve from 0.40144\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4236 - acc: 0.8725 - val_loss: 0.4022 - val_acc: 0.8926\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8753\n",
      "Epoch 00028: val_loss improved from 0.40144 to 0.37924, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/028-0.3792.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4111 - acc: 0.8753 - val_loss: 0.3792 - val_acc: 0.8931\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8801\n",
      "Epoch 00029: val_loss did not improve from 0.37924\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3957 - acc: 0.8801 - val_loss: 0.3826 - val_acc: 0.8956\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8811\n",
      "Epoch 00030: val_loss improved from 0.37924 to 0.37121, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/030-0.3712.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3921 - acc: 0.8810 - val_loss: 0.3712 - val_acc: 0.8991\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8843\n",
      "Epoch 00031: val_loss did not improve from 0.37121\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3824 - acc: 0.8843 - val_loss: 0.3762 - val_acc: 0.8933\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8870\n",
      "Epoch 00032: val_loss did not improve from 0.37121\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3698 - acc: 0.8870 - val_loss: 0.3751 - val_acc: 0.8994\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8864\n",
      "Epoch 00033: val_loss improved from 0.37121 to 0.35637, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/033-0.3564.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3708 - acc: 0.8864 - val_loss: 0.3564 - val_acc: 0.9008\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8913\n",
      "Epoch 00034: val_loss did not improve from 0.35637\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3522 - acc: 0.8913 - val_loss: 0.3686 - val_acc: 0.9029\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8924\n",
      "Epoch 00035: val_loss improved from 0.35637 to 0.33414, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/035-0.3341.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3505 - acc: 0.8925 - val_loss: 0.3341 - val_acc: 0.9094\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8950\n",
      "Epoch 00036: val_loss did not improve from 0.33414\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3424 - acc: 0.8950 - val_loss: 0.3615 - val_acc: 0.9052\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8980\n",
      "Epoch 00037: val_loss did not improve from 0.33414\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3286 - acc: 0.8980 - val_loss: 0.3462 - val_acc: 0.9075\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8971\n",
      "Epoch 00038: val_loss did not improve from 0.33414\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3322 - acc: 0.8971 - val_loss: 0.3392 - val_acc: 0.9103\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9001\n",
      "Epoch 00039: val_loss improved from 0.33414 to 0.33065, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/039-0.3307.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3217 - acc: 0.9001 - val_loss: 0.3307 - val_acc: 0.9129\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9004\n",
      "Epoch 00040: val_loss did not improve from 0.33065\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3171 - acc: 0.9004 - val_loss: 0.3353 - val_acc: 0.9092\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9049\n",
      "Epoch 00041: val_loss did not improve from 0.33065\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3075 - acc: 0.9048 - val_loss: 0.3359 - val_acc: 0.9131\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9064\n",
      "Epoch 00042: val_loss did not improve from 0.33065\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3016 - acc: 0.9065 - val_loss: 0.3396 - val_acc: 0.9138\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9069\n",
      "Epoch 00043: val_loss improved from 0.33065 to 0.32290, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/043-0.3229.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2978 - acc: 0.9069 - val_loss: 0.3229 - val_acc: 0.9168\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9092\n",
      "Epoch 00044: val_loss did not improve from 0.32290\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2922 - acc: 0.9092 - val_loss: 0.3299 - val_acc: 0.9157\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9112\n",
      "Epoch 00045: val_loss improved from 0.32290 to 0.31736, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/045-0.3174.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2883 - acc: 0.9112 - val_loss: 0.3174 - val_acc: 0.9175\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9127\n",
      "Epoch 00046: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2830 - acc: 0.9127 - val_loss: 0.3584 - val_acc: 0.9015\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9122\n",
      "Epoch 00047: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2818 - acc: 0.9122 - val_loss: 0.3181 - val_acc: 0.9194\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9160\n",
      "Epoch 00048: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2708 - acc: 0.9160 - val_loss: 0.3225 - val_acc: 0.9189\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9148\n",
      "Epoch 00049: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2692 - acc: 0.9148 - val_loss: 0.3353 - val_acc: 0.9147\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9155\n",
      "Epoch 00050: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2665 - acc: 0.9156 - val_loss: 0.3288 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9174\n",
      "Epoch 00051: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2616 - acc: 0.9174 - val_loss: 0.3228 - val_acc: 0.9215\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9191\n",
      "Epoch 00052: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2565 - acc: 0.9191 - val_loss: 0.3266 - val_acc: 0.9178\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9205\n",
      "Epoch 00053: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2529 - acc: 0.9206 - val_loss: 0.3315 - val_acc: 0.9138\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9218\n",
      "Epoch 00054: val_loss improved from 0.31736 to 0.31219, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/054-0.3122.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2465 - acc: 0.9219 - val_loss: 0.3122 - val_acc: 0.9194\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9235\n",
      "Epoch 00055: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2416 - acc: 0.9235 - val_loss: 0.3178 - val_acc: 0.9227\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9229\n",
      "Epoch 00056: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2418 - acc: 0.9229 - val_loss: 0.3253 - val_acc: 0.9215\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9242\n",
      "Epoch 00057: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2403 - acc: 0.9242 - val_loss: 0.3130 - val_acc: 0.9217\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9248\n",
      "Epoch 00058: val_loss improved from 0.31219 to 0.30102, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/058-0.3010.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2373 - acc: 0.9248 - val_loss: 0.3010 - val_acc: 0.9220\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9259\n",
      "Epoch 00059: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2311 - acc: 0.9259 - val_loss: 0.3235 - val_acc: 0.9201\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9257\n",
      "Epoch 00060: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2307 - acc: 0.9257 - val_loss: 0.3241 - val_acc: 0.9145\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9274\n",
      "Epoch 00061: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2284 - acc: 0.9273 - val_loss: 0.3178 - val_acc: 0.9203\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9300\n",
      "Epoch 00062: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2246 - acc: 0.9300 - val_loss: 0.3132 - val_acc: 0.9208\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9290\n",
      "Epoch 00063: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2238 - acc: 0.9290 - val_loss: 0.3128 - val_acc: 0.9199\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9307\n",
      "Epoch 00064: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2171 - acc: 0.9307 - val_loss: 0.3293 - val_acc: 0.9215\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9307\n",
      "Epoch 00065: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2121 - acc: 0.9307 - val_loss: 0.3088 - val_acc: 0.9236\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9315\n",
      "Epoch 00066: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2129 - acc: 0.9314 - val_loss: 0.3187 - val_acc: 0.9243\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9340\n",
      "Epoch 00067: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2098 - acc: 0.9340 - val_loss: 0.3169 - val_acc: 0.9227\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9330\n",
      "Epoch 00068: val_loss improved from 0.30102 to 0.29583, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/068-0.2958.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2060 - acc: 0.9330 - val_loss: 0.2958 - val_acc: 0.9266\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9348\n",
      "Epoch 00069: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2013 - acc: 0.9348 - val_loss: 0.3099 - val_acc: 0.9222\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9364\n",
      "Epoch 00070: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2035 - acc: 0.9364 - val_loss: 0.3068 - val_acc: 0.9236\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9361\n",
      "Epoch 00071: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1955 - acc: 0.9361 - val_loss: 0.3177 - val_acc: 0.9276\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9378\n",
      "Epoch 00072: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1944 - acc: 0.9378 - val_loss: 0.3499 - val_acc: 0.9171\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9374\n",
      "Epoch 00073: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1940 - acc: 0.9375 - val_loss: 0.3086 - val_acc: 0.9257\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9359\n",
      "Epoch 00074: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1933 - acc: 0.9359 - val_loss: 0.3225 - val_acc: 0.9229\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9380\n",
      "Epoch 00075: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1929 - acc: 0.9380 - val_loss: 0.3006 - val_acc: 0.9259\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1912 - acc: 0.9370\n",
      "Epoch 00076: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1913 - acc: 0.9370 - val_loss: 0.2968 - val_acc: 0.9245\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9388\n",
      "Epoch 00077: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1917 - acc: 0.9388 - val_loss: 0.3010 - val_acc: 0.9259\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9391\n",
      "Epoch 00078: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1841 - acc: 0.9391 - val_loss: 0.3103 - val_acc: 0.9271\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9424\n",
      "Epoch 00079: val_loss did not improve from 0.29583\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1810 - acc: 0.9424 - val_loss: 0.3056 - val_acc: 0.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9420\n",
      "Epoch 00080: val_loss improved from 0.29583 to 0.29366, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/080-0.2937.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1828 - acc: 0.9420 - val_loss: 0.2937 - val_acc: 0.9259\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9428\n",
      "Epoch 00081: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1754 - acc: 0.9428 - val_loss: 0.2949 - val_acc: 0.9317\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9430\n",
      "Epoch 00082: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1759 - acc: 0.9430 - val_loss: 0.3027 - val_acc: 0.9273\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9430\n",
      "Epoch 00083: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1732 - acc: 0.9430 - val_loss: 0.3206 - val_acc: 0.9234\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9446\n",
      "Epoch 00084: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1732 - acc: 0.9447 - val_loss: 0.3092 - val_acc: 0.9294\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9443\n",
      "Epoch 00085: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1675 - acc: 0.9443 - val_loss: 0.3310 - val_acc: 0.9280\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9439\n",
      "Epoch 00086: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1698 - acc: 0.9439 - val_loss: 0.3074 - val_acc: 0.9308\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9449\n",
      "Epoch 00087: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1696 - acc: 0.9450 - val_loss: 0.3054 - val_acc: 0.9292\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9467\n",
      "Epoch 00088: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1619 - acc: 0.9467 - val_loss: 0.3001 - val_acc: 0.9264\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9463\n",
      "Epoch 00089: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1650 - acc: 0.9463 - val_loss: 0.3254 - val_acc: 0.9292\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9456\n",
      "Epoch 00090: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1659 - acc: 0.9456 - val_loss: 0.3069 - val_acc: 0.9276\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9461\n",
      "Epoch 00091: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1598 - acc: 0.9461 - val_loss: 0.3082 - val_acc: 0.9259\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9476\n",
      "Epoch 00092: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1608 - acc: 0.9476 - val_loss: 0.3147 - val_acc: 0.9280\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9498\n",
      "Epoch 00093: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1547 - acc: 0.9498 - val_loss: 0.2940 - val_acc: 0.9306\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9470\n",
      "Epoch 00094: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1606 - acc: 0.9470 - val_loss: 0.2998 - val_acc: 0.9313\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9504\n",
      "Epoch 00095: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1518 - acc: 0.9504 - val_loss: 0.3153 - val_acc: 0.9255\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9479\n",
      "Epoch 00096: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1561 - acc: 0.9479 - val_loss: 0.2977 - val_acc: 0.9292\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9493\n",
      "Epoch 00097: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1534 - acc: 0.9493 - val_loss: 0.3072 - val_acc: 0.9257\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9504\n",
      "Epoch 00098: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1486 - acc: 0.9504 - val_loss: 0.3213 - val_acc: 0.9236\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9505\n",
      "Epoch 00099: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1491 - acc: 0.9505 - val_loss: 0.3386 - val_acc: 0.9241\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9523\n",
      "Epoch 00100: val_loss did not improve from 0.29366\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1451 - acc: 0.9523 - val_loss: 0.3208 - val_acc: 0.9304\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9505\n",
      "Epoch 00101: val_loss improved from 0.29366 to 0.28990, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/101-0.2899.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1471 - acc: 0.9506 - val_loss: 0.2899 - val_acc: 0.9292\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9524\n",
      "Epoch 00102: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1427 - acc: 0.9524 - val_loss: 0.3101 - val_acc: 0.9297\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9516\n",
      "Epoch 00103: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1443 - acc: 0.9516 - val_loss: 0.3346 - val_acc: 0.9273\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9511\n",
      "Epoch 00104: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1459 - acc: 0.9511 - val_loss: 0.3058 - val_acc: 0.9287\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9527\n",
      "Epoch 00105: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1421 - acc: 0.9527 - val_loss: 0.3123 - val_acc: 0.9306\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9540\n",
      "Epoch 00106: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1407 - acc: 0.9541 - val_loss: 0.3145 - val_acc: 0.9264\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9531\n",
      "Epoch 00107: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1398 - acc: 0.9531 - val_loss: 0.3302 - val_acc: 0.9278\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9545\n",
      "Epoch 00108: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1348 - acc: 0.9545 - val_loss: 0.3029 - val_acc: 0.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9546\n",
      "Epoch 00109: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1390 - acc: 0.9546 - val_loss: 0.3032 - val_acc: 0.9362\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9575\n",
      "Epoch 00110: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1328 - acc: 0.9575 - val_loss: 0.3114 - val_acc: 0.9301\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9561\n",
      "Epoch 00111: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1338 - acc: 0.9561 - val_loss: 0.3251 - val_acc: 0.9324\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9563\n",
      "Epoch 00112: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1326 - acc: 0.9563 - val_loss: 0.3162 - val_acc: 0.9317\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9588\n",
      "Epoch 00113: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1289 - acc: 0.9588 - val_loss: 0.3086 - val_acc: 0.9327\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9567\n",
      "Epoch 00114: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1281 - acc: 0.9567 - val_loss: 0.3123 - val_acc: 0.9336\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9579\n",
      "Epoch 00115: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1258 - acc: 0.9579 - val_loss: 0.3181 - val_acc: 0.9327\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9580\n",
      "Epoch 00116: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1291 - acc: 0.9580 - val_loss: 0.2982 - val_acc: 0.9352\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9578\n",
      "Epoch 00117: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1244 - acc: 0.9578 - val_loss: 0.3136 - val_acc: 0.9299\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9582\n",
      "Epoch 00118: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1238 - acc: 0.9582 - val_loss: 0.2950 - val_acc: 0.9362\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9595\n",
      "Epoch 00119: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1255 - acc: 0.9595 - val_loss: 0.2952 - val_acc: 0.9315\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9588\n",
      "Epoch 00120: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1270 - acc: 0.9588 - val_loss: 0.3017 - val_acc: 0.9311\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9615\n",
      "Epoch 00121: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1187 - acc: 0.9616 - val_loss: 0.3221 - val_acc: 0.9331\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9599\n",
      "Epoch 00122: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1216 - acc: 0.9598 - val_loss: 0.3103 - val_acc: 0.9324\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9590\n",
      "Epoch 00123: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1232 - acc: 0.9590 - val_loss: 0.2902 - val_acc: 0.9324\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9593\n",
      "Epoch 00124: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1248 - acc: 0.9593 - val_loss: 0.3167 - val_acc: 0.9311\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9602\n",
      "Epoch 00125: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1191 - acc: 0.9602 - val_loss: 0.3005 - val_acc: 0.9331\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9606\n",
      "Epoch 00126: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1198 - acc: 0.9606 - val_loss: 0.3222 - val_acc: 0.9320\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9599\n",
      "Epoch 00127: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1216 - acc: 0.9599 - val_loss: 0.3280 - val_acc: 0.9294\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9595\n",
      "Epoch 00128: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1210 - acc: 0.9595 - val_loss: 0.3149 - val_acc: 0.9320\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9618\n",
      "Epoch 00129: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1147 - acc: 0.9618 - val_loss: 0.3278 - val_acc: 0.9313\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9623\n",
      "Epoch 00130: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1136 - acc: 0.9623 - val_loss: 0.3185 - val_acc: 0.9294\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9629\n",
      "Epoch 00131: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1122 - acc: 0.9629 - val_loss: 0.3437 - val_acc: 0.9290\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9632\n",
      "Epoch 00132: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1120 - acc: 0.9632 - val_loss: 0.3199 - val_acc: 0.9331\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9630\n",
      "Epoch 00133: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1166 - acc: 0.9630 - val_loss: 0.3072 - val_acc: 0.9315\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9626\n",
      "Epoch 00134: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1129 - acc: 0.9626 - val_loss: 0.2979 - val_acc: 0.9320\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9611\n",
      "Epoch 00135: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1170 - acc: 0.9611 - val_loss: 0.3188 - val_acc: 0.9311\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9629\n",
      "Epoch 00136: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1104 - acc: 0.9629 - val_loss: 0.3083 - val_acc: 0.9317\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9642\n",
      "Epoch 00137: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1072 - acc: 0.9642 - val_loss: 0.3067 - val_acc: 0.9306\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9649\n",
      "Epoch 00138: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1058 - acc: 0.9649 - val_loss: 0.3304 - val_acc: 0.9329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9652\n",
      "Epoch 00139: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1058 - acc: 0.9652 - val_loss: 0.3366 - val_acc: 0.9311\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9637\n",
      "Epoch 00140: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1098 - acc: 0.9637 - val_loss: 0.2956 - val_acc: 0.9373\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9640\n",
      "Epoch 00141: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1085 - acc: 0.9640 - val_loss: 0.3128 - val_acc: 0.9341\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9654\n",
      "Epoch 00142: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1019 - acc: 0.9654 - val_loss: 0.3094 - val_acc: 0.9348\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9641\n",
      "Epoch 00143: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1064 - acc: 0.9641 - val_loss: 0.3298 - val_acc: 0.9301\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9661\n",
      "Epoch 00144: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1029 - acc: 0.9661 - val_loss: 0.3022 - val_acc: 0.9341\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9657\n",
      "Epoch 00145: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1050 - acc: 0.9657 - val_loss: 0.3169 - val_acc: 0.9348\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9654\n",
      "Epoch 00146: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1079 - acc: 0.9654 - val_loss: 0.3001 - val_acc: 0.9355\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9649\n",
      "Epoch 00147: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1042 - acc: 0.9650 - val_loss: 0.3157 - val_acc: 0.9357\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9647\n",
      "Epoch 00148: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1062 - acc: 0.9647 - val_loss: 0.2987 - val_acc: 0.9366\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9665\n",
      "Epoch 00149: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1004 - acc: 0.9664 - val_loss: 0.2963 - val_acc: 0.9327\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9679\n",
      "Epoch 00150: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0975 - acc: 0.9679 - val_loss: 0.3060 - val_acc: 0.9348\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9663\n",
      "Epoch 00151: val_loss did not improve from 0.28990\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0980 - acc: 0.9663 - val_loss: 0.3096 - val_acc: 0.9362\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4VNX9+PH3mX0mkz0QQgIERBbZwo7iQsV93xCt1qqtttXaL7Vff1JrLV9rq7W2WrdabV1oXR/UitVWpUJRCwIiq4AsYQuB7JNl9pnz++OELJBAgAyBzOf1PPPMcu/c+7k3k/O555x7z1Vaa4QQQggAS1cHIIQQ4tghSUEIIUQTSQpCCCGaSFIQQgjRRJKCEEKIJpIUhBBCNJGkIIQQoknCkoJSqo9Sar5S6iul1Fql1P+0Mc8UpZRPKbWi8XFfouIRQghxcLYELjsK/ERrvVwplQp8oZT6SGv91T7zfaK1viiBcQghhOighCUFrXUpUNr4uk4ptQ7IB/ZNCockJydHFxYWHnmAQgiRRL744osKrXWPg82XyJpCE6VUITAa+LyNyScrpVYCu4D/1VqvbeP7twK3AvTt25dly5YlLlghhOiGlFLbOjJfwjualVJe4E1ghta6dp/Jy4F+WutRwBPA39tahtb6Wa31OK31uB49DprohBBCHKaEJgWllB2TEF7WWr+173Stda3Wur7x9fuAXSmVk8iYhBBCtC+RZx8p4C/AOq3179uZp1fjfCilJjTGU5momIQQQhxYIvsUJgPfAlYrpVY0fnYP0BdAa/0McBXwA6VUFAgA1+jDGMs7Eomwc+dOgsFg50SehFwuFwUFBdjt9q4ORQjRhRJ59tGngDrIPE8CTx7punbu3ElqaiqFhYU0VjzEIdBaU1lZyc6dO+nfv39XhyOE6ELd4ormYDBIdna2JITDpJQiOztbalpCiO6RFABJCEdI9p8QArpRUjiYWCxAKFRCPB7p6lCEEOKYlTRJIR4PEA6XonXnJ4Wamhqefvrpw/ruBRdcQE1NTYfnnzVrFo888shhrUsIIQ4maZKCUns39ZBPbjqoAyWFaDR6wO++//77ZGRkdHpMQghxOJImKezdVK3jnb7kmTNnsnnzZoqKirjrrrtYsGABp512GpdccgknnXQSAJdddhljx45l2LBhPPvss03fLSwspKKigq1btzJ06FBuueUWhg0bxjnnnEMgEDjgelesWMGkSZMYOXIkl19+OdXV1QA8/vjjnHTSSYwcOZJrrrkGgP/85z8UFRVRVFTE6NGjqaur6/T9IIQ4/h2VsY+Opo0bZ1Bfv2K/z7WOEY/7sVjcKHVom+31FnHiiY+1O/2hhx5izZo1rFhh1rtgwQKWL1/OmjVrmk7xfP7558nKyiIQCDB+/HiuvPJKsrOz94l9I6+++irPPfccV199NW+++SbXX399u+u94YYbeOKJJzjjjDO47777+L//+z8ee+wxHnroIYqLi3E6nU1NU4888ghPPfUUkydPpr6+HpfLdUj7QAiRHJKmpnC0T66ZMGFCq3P+H3/8cUaNGsWkSZPYsWMHGzdu3O87/fv3p6ioCICxY8eydevWdpfv8/moqanhjDPOAODb3/42CxcuBGDkyJFcd911/O1vf8NmMwlw8uTJ3HnnnTz++OPU1NQ0fS6EEC11u5KhvSP6WCyA378Wl2sAdntWwuNISUlper1gwQLmzZvHokWL8Hg8TJkypc1rApxOZ9Nrq9V60Oaj9rz33nssXLiQd999l1/96lesXr2amTNncuGFF/L+++8zefJkPvjgA4YMGXJYyxdCdF9JVFPYu6md36eQmpp6wDZ6n89HZmYmHo+H9evXs3jx4iNeZ3p6OpmZmXzyyScA/PWvf+WMM84gHo+zY8cOvvGNb/Cb3/wGn89HfX09mzdvZsSIEdx9992MHz+e9evXH3EMQojup9vVFNpn2o8OY2ilg8rOzmby5MkMHz6c888/nwsvvLDV9PPOO49nnnmGoUOHMnjwYCZNmtQp633ppZf4/ve/j9/vZ8CAAbzwwgvEYjGuv/56fD4fWmt+9KMfkZGRwc9//nPmz5+PxWJh2LBhnH/++Z0SgxCie1GJKCQTady4cXrfm+ysW7eOoUOHHvB78XiUhoYVOJ19cDhyExnicasj+1EIcXxSSn2htR53sPmSrvkoEaekCiFEd5E0SaF5wNbjq2YkhBBHU9IkBTPgm5KaghBCHEDSJAXDQiLOPhJCiO4iqZKC6VeQ5iMhhGhPUiUFaT4SQogDS7KkcOw0H3m93kP6XAghjoakSgpKqYRcvCaEEN1FUiWFRNUUZs6cyVNPPdX0fu+NcOrr65k6dSpjxoxhxIgRvPPOOx1eptaau+66i+HDhzNixAhef/11AEpLSzn99NMpKipi+PDhfPLJJ8RiMW688cameR999NFO30YhRHLofsNczJgBK/YfOhvAFfebFxbPoS2zqAgea3/o7OnTpzNjxgxuv/12AN544w0++OADXC4Xb7/9NmlpaVRUVDBp0iQuueSSDt0P+a233mLFihWsXLmSiooKxo8fz+mnn84rr7zCueeey89+9jNisRh+v58VK1ZQUlLCmjVrAA7pTm5CCNFS90sKB6QgAc1Ho0ePpqysjF27dlFeXk5mZiZ9+vQhEolwzz33sHDhQiwWCyUlJezZs4devXoddJmffvop1157LVarldzcXM444wyWLl3K+PHjufnmm4lEIlx22WUUFRUxYMAAtmzZwh133MGFF17IOeec0+nbKIRIDt0vKRzgiD4c2EQ8HiIlZVinr3batGnMmTOH3bt3M336dABefvllysvL+eKLL7Db7RQWFrY5ZPahOP3001m4cCHvvfceN954I3feeSc33HADK1eu5IMPPuCZZ57hjTfe4Pnnn++MzRJCJJmk61NI1Cmp06dP57XXXmPOnDlMmzYNMENm9+zZE7vdzvz589m2bVuHl3faaafx+uuvE4vFKC8vZ+HChUyYMIFt27aRm5vLLbfcwne/+12WL19ORUUF8XicK6+8kgceeIDly5cnZBuFEN1f96spHFDiLl4bNmwYdXV15Ofnk5eXB8B1113HxRdfzIgRIxg3btwh3dTm8ssvZ9GiRYwaNQqlFA8//DC9evXipZde4re//S12ux2v18vs2bMpKSnhpptuIh43Ce/BBx9MyDYKIbq/pBk6GyAY3EY0Wo3XW5So8I5rMnS2EN2XDJ3dpsQ1HwkhRHeQVElBxj4SQogDS6qkYO6poOWqZiGEaEeSJYW9mytNSEII0ZakSgp7rySWfgUhhGhbUiWF5s2V5iMhhGhLUiUF09Hc+TWFmpoann766cP67gUXXCBjFQkhjhkJSwpKqT5KqflKqa+UUmuVUv/TxjxKKfW4UmqTUmqVUmpMouJpXGPjc+fWFA6UFKLR6AG/+/7775ORkdGp8QghxOFKZE0hCvxEa30SMAm4XSl10j7znA+c2Pi4FfhjAuMhUR3NM2fOZPPmzRQVFXHXXXexYMECTjvtNC655BJOOsls8mWXXcbYsWMZNmwYzz77bNN3CwsLqaioYOvWrQwdOpRbbrmFYcOGcc455xAIBPZb17vvvsvEiRMZPXo0Z511Fnv27AGgvr6em266iREjRjBy5EjefPNNAP71r38xZswYRo0axdSpUzt1u4UQ3U/ChrnQWpcCpY2v65RS64B84KsWs10KzNbmHNHFSqkMpVRe43cPywFGzkZrL/H4YCwWJx0YvbrJQUbO5qGHHmLNmjWsaFzxggULWL58OWvWrKF///4APP/882RlZREIBBg/fjxXXnkl2dnZrZazceNGXn31VZ577jmuvvpq3nzzTa6//vpW85x66qksXrwYpRR//vOfefjhh/nd737HL3/5S9LT01m9ejUA1dXVlJeXc8stt7Bw4UL69+9PVVVVxzdaCJGUjsrYR0qpQmA08Pk+k/KBHS3e72z8rFVSUErdiqlJ0Ldv3yOJ5Ai+e2gmTJjQlBAAHn/8cd5++20AduzYwcaNG/dLCv3796eoyAzBMXbsWLZu3brfcnfu3Mn06dMpLS0lHA43rWPevHm89tprTfNlZmby7rvvcvrppzfNk5WV1anbKITofhKeFJRSXuBNYIbWuvZwlqG1fhZ4FszYRwea90BH9LFYEL9/Ay7XQOz2xLbjp6SkNL1esGAB8+bNY9GiRXg8HqZMmdLmENpOp7PptdVqbbP56I477uDOO+/kkksuYcGCBcyaNSsh8QshklNCzz5SStkxCeFlrfVbbcxSAvRp8b6g8bMESUyfQmpqKnV1de1O9/l8ZGZm4vF4WL9+PYsXLz7sdfl8PvLz8wF46aWXmj4/++yzW90StLq6mkmTJrFw4UKKi4sBpPlICHFQiTz7SAF/AdZprX/fzmxzgRsaz0KaBPiOpD+hAzE1vurcpJCdnc3kyZMZPnw4d911137TzzvvPKLRKEOHDmXmzJlMmjTpsNc1a9Yspk2bxtixY8nJyWn6/N5776W6uprhw4czatQo5s+fT48ePXj22We54oorGDVqVNPNf4QQoj0JGzpbKXUq8AmwmuZS+B6gL4DW+pnGxPEkcB7gB27SWi9rY3FNjmTo7Hg8TEPDKpzOfjgcPQ5xi7o/GTpbiO6ro0NnJ/Lso085SM9u41lHtycqhv3J2EdCCHEgSXZF896xj2SYCyGEaEtSJQWpKQghxIElVVIwNQWFJAUhhGhbUiUFwyLNR0II0Y6kSwqmtiA1BSGEaEvSJQVTU+j6pOD1ers6BCGE2E9SJgWpKQghRNuSLikopTq9T2HmzJmthpiYNWsWjzzyCPX19UydOpUxY8YwYsQI3nnnnYMuq70httsaAru94bKFEOJwHZVRUo+mGf+awYrd7YydDcRifpRSWCzuDi+zqFcRj53X/kh706dPZ8aMGdx+u7kO74033uCDDz7A5XLx9ttvk5aWRkVFBZMmTeKSSy5pMdzG/toaYjsej7c5BHZbw2ULIcSR6HZJ4WBMedy5NYXRo0dTVlbGrl27KC8vJzMzkz59+hCJRLjnnntYuHAhFouFkpIS9uzZQ69evdpdVltDbJeXl7c5BHZbw2ULIcSR6HZJ4UBH9AB+/9doHSMlpXPH+Jk2bRpz5sxh9+7dTQPPvfzyy5SXl/PFF19gt9spLCxsc8jsvTo6xLYQQiRKEvYpJKajefr06bz22mvMmTOHadOmAWaY6549e2K325k/fz7btm074DLaG2K7vSGw2xouWwghjkTSJYVEXbw2bNgw6urqyM/PJy8vD4DrrruOZcuWMWLECGbPns2QIUMOuIz2hthubwjstobLFkKII5GwobMT5UiGzgYIBIqJxerwekcmIrzjmgydLUT31dGhs5OuppCo5iMhhOgOki4pHCtXNAshxLGo2ySFjjaDmWsEjq8ms6PheGtGFEIkRrdICi6Xi8rKyg4WbBZASyHYgtaayspKXC5XV4cihOhi3eI6hYKCAnbu3El5eflB541GfUSjNTidXzX2LwgwibWgoKCrwxBCdLFukRTsdnvT1b7t+uADuOsuSp+9kg3BWUyeXIHdnn10AhRCiONE8hwqRyKwejW2yhAA8bhcKSyEEPtKnqSQbWoF1powIElBCCHaknxJwWdqCrFYoCujEUKIY1LSJQVbTQSAWMzXldEIIcQxKXmSQkYGKIW91tzLIBjc0cUBCSHEsSd5koLVCpmZ2HxRAEIhSQpCCLGv5EkKANnZWKrrsNkyCIW2d3U0QghxzEm6pEBlJU5nH4JBSQpCCLGv5EoKOTmNSaGv1BSEEKINyZUUGmsKLldfqSkIIUQbkjIpOJ19iUariMUaujoiIYQ4piRfUmhowKXM7TLltFQhhGgt+ZIC4GpIA5B+BSGE2EdyJoV6N4D0KwghxD4SlhSUUs8rpcqUUmvamT5FKeVTSq1ofNyXqFiaNCYFe60VsEhNQQgh9pHI+ym8CDwJzD7APJ9orS9KYAytNSYFS7UPZ35vqSkIIcQ+ElZT0FovBKoStfzD0pgUmq9VkI5mIYRoqav7FE5WSq1USv1TKTWsvZmUUrcqpZYppZZ15Jab7WqVFPpI85EQQuyjK5PCcqCf1noU8ATw9/Zm1Fo/q7Uep7Ue16NHj8Nfo8sFHk+LC9h2oHX88JcnhBDdTJclBa11rda6vvH1+4BdKZWT8BW3uIBN6xCRyBHUPIQQopvpsqSglOqllFKNryc0xlKZ8BU3jn/kcvUFIBjcmvBVCiHE8SJhZx8ppV4FpgA5SqmdwC8AO4DW+hngKuAHSqkoEACu0VrrRMXTpLGm4PEMBaChYS1paRMTvlohhDgeJCwpaK2vPcj0JzGnrB5d2dmwfTtu9wlYLB7q61ce9RCEEOJY1dVnHx19jTUFpSykpIygoWFVV0ckhBDHjORMCtXVEI/j9Y6ivn4lR6PVSgghjgfJmRTicaipwesdRTRaTSi0s6ujEkKIY0JyJgWAykpSUkYCSBOSEEI0Sr6k0KuXeS4pwes1SUE6m4UQwki+pDB4sHnesAGbLQ2Xq78kBSGEaJR8SSE/3wx1sWEDACkpI6X5SAghGiVfUrBYYNCgpqTg9Y7C7/+aWCzQxYEJIUTXS76kAKYJaf16wCQFiNPQsLprYxJCiGNAh5KCUup/lFJpyviLUmq5UuqcRAeXMIMHw9atEAqRmjoWgLq6ZV0bkxBCHAM6WlO4WWtdC5wDZALfAh5KWFSJNmSIuVZh0yaczr7Y7T2prf28q6MSQogu19GkoBqfLwD+qrVe2+Kz40+LM5CUUqSlTZSkIIQQdDwpfKGU+hCTFD5QSqUCx+/daQYNMs+N/QppaRMJBDYQidR0YVBCCNH1OpoUvgPMBMZrrf2YIbBvSlhUieb1mlNTG89ASk01Q2fX1S3tyqiEEKLLdTQpnAxs0FrXKKWuB+4FfIkL6ygYPLhFUhgHIE1IQoik19Gk8EfAr5QaBfwE2AzMTlhUR8OQIab5SGvs9gw8niHU1UlSEEIkt44mhWjjXdEuBZ7UWj8FpCYurKNg8GDw+aCsDDBNSLW1S2QYbSFEUutoUqhTSv0Ucyrqe0opC4231jxuDTW342SVGeIiLW0ikUgZweC2LgxKCCG6VkeTwnQghLleYTdQAPw2YVEdDRMngtUKCxcCkJY2AYC6uiVdGZUQQnSpDiWFxkTwMpCulLoICGqtj+8+hbQ0GDsWFiwAICVlOErZqa//smvjEkKILtTRYS6uBpYA04Crgc+VUlclMrCjYsoU+Pxz8PuxWJykpAynrm55V0clhBBdpqPNRz/DXKPwba31DcAE4OeJC+somTIFIhFYtAgAr3cM9fXLpbNZCJG0OpoULFrrshbvKw/hu8euyZNNv0JjE1Jq6hgikQq5Z7MQImnZOjjfv5RSHwCvNr6fDryfmJCOorQ0GDOmKSl4vWMAqK9fjsvVpwsDE0KIrtHRjua7gGeBkY2PZ7XWdycysKOmRb+CuWezRfoVhBBJq8NNQFrrN7XWdzY+3k5kUEfVGWeYfoUlS7BaPXg8Q6mvl6QghEhOB0wKSqk6pVRtG486pVTt0QoyoYqKzPOaNYDpV5CaghAiWR0wKWitU7XWaW08UrXWaUcryITq3RvS02HtWsD0K4TDuwiFdndxYEIIcfQd/2cQHSmlYNiwpqSQmrq3s/mLroxKCCG6hCQFaE4KWpOaOhalbPh8/+3qqIQQ4qiTpAAmKVRVwZ49WK0peL1j8Pk+7eqohBDiqJOkACYpQFMTUnr6qdTVLSEeD3VhUEIIcfRJUoA2k0I8HqSuTvoVhBDJRZICQK9ekJnZKikA0oQkhEg6CUsKSqnnlVJlSqk17UxXSqnHlVKblFKrlFJjEhXLQe1zBpLD0QO3e7AkBSFE0klkTeFF4LwDTD8fOLHxcSvmPtBdp8UZSGBqCz7fp2gd79KwhBDiaEpYUtBaLwSqDjDLpcBsbSwGMpRSeYmK56CGDYOaGigtBUxSiEar8fvXdVlIQghxtHVln0I+sKPF+52Nn+1HKXWrUmqZUmpZeXl5YqLZ29m8YgUAGRlnAFBV9VFi1ieEEMeg46KjWWv9rNZ6nNZ6XI8ePRKzkpNPhpQUmDsXALe7PykpI6moeDMx6xNCiGNQR++nkAglQMubFhQ0ftY13G648EJ4+2146imwWunR40q2bp1FKFSK09l1LVtCJLu9N0NUCoJB2LEDYjFzS5S0NHM8p1TzvLEYRKMHf+7IPEqZ4dFcLvD5TCtzdTXU1kI4bB6RSOt4lIJ43MSy9zkahcpK8wBzfy+brfmx973VCnV1Zh3V1WZ9Tqc5QfLii+GKKxK7r7syKcwFfqiUeg2YCPi01qVdGA9ceSW88QZ89hmcfnpjUvgFFRVvk59/W5eGJsTBaG0Kk4oK87qgACwWKC6G+vrmsR/r6poffr8ptEIhU1j5/abwSUkxF/nX1oLDYQolp9MUWLW1zQVjfb0pLJ1O87q2trkw3bkTSkrMtLQ0E0s0Cnv2mK47rxd69jQFaChkCtdQyHy/ZWHa0GBiBbOsYHD/bbdYTGyxmPnesazlvtibfCKR1vNYLObvkJkJGRlmv1RXw8CBiY8vYUlBKfUqMAXIUUrtBH4B2AG01s9g7tx2AbAJ8AM3JSqWDrvgAvMLnzMHTj8dj+ck3O7BlJe/KUkhCe09SgRTCPp85rXF0vywWptf752vqsr8E4dC5oi2uBgCAVNY7S209j1a3fex77TaWlNoh8Ot17/3EQyaZNCycFHKTIvFErePHA4TE5h9kZZmnq1WyM+HwkIzvbbWFPA2GwwZYu5t1dBgEoRSzUlnb+KxWMznSoHHA6mpZh2hkElsffqYddfWmofPZ/bTvkfcLZ8PNO1A34nFzDoCAbPuvYV1WpqJ1+EAu93EWlur2VlRQ6ojHavF0rQde38rmZlm3rbE4xAMRwmEImR4XVitqtV0rTXReJTGYjRhEpYUtNbXHmS6Bm5P1PoPi9cL554Lb70Fjz2Gsljo0eMqtm9/iHC4Aocjp6sjFI1iMXNU29DQXFAGg6aQ2b3bPNfXmwLFYmmu7rcsaBsazJFsVRXYvXXolN0ES/tTVWGjqgr8fg1pO8G7G0rHgLaalTt9EPaa9yoGKWUQc0IoFRz14K4Gd1XTQ7l82MO52Or7o7PXE++9CGskA0fNCFy+EbgCJ4C3lHDeQrAHcMTTwVlLJGs7VmXBFckjvU8vRnp6YXWGqLNsJ6aj2GOZOKJZ2KNZ2O3gyqrEnl6BSqmkPlbJrpoKVMzNpLzT6JfRh9WlG6jx19EnZSCFGQPIS8/Gm2IKrpCqYXNsPtsCa9ERD0RduNxRXC5FpiOXFJVFtb+WmlAVIUsVyh5gRO9BnJQ7iEAkRImvjC/LPmfpriVsrtrM7vrd9Os1ioF9TiUcC7OnYQ8WZcFhdRCOhdkR8eNvfNitdmI2FzsDVZTWldInvQ8T8yey3bedBVsXkOJIYWzeWPK8editdsrDdXxWv4doJIoz1Ykr04XT6sRlc+GyuQhEApT5ywjHwngdXrLd2RSkFWCz2CipLaE+Uk+KPcU8HM3PgUiA1WWrKa0uJdOVSYo9haiOorXGbXPj8DoIhUPESmN4K73YLDb2NOyhJlhDhisDi7KwcNtCtvm24bK56Jvel7iOE9dxhuYMZWjOUHbU7uDryq+pD9cTioUIRUOtnuONp8Bnu7MZ2mMoWmsqA5VU+iupClQx89SZPHDmAwn931J6b2PdcWLcuHF62bJliVvB3/4G3/oWLF4MEydSV/clX3wxhkGDnqV371sSt97jRCweIxAN4I/42eHbwXbfduxWOx67h7pQHb6Qj3NPOJdcby7hWJgnlzzJ5rJd1NbYyXX2Y0TPkQz0FhGq97ChfDOvl/yGumA96ZGheBpOwl4zlIqGSkqsn1JtX4PftZm41Y81mgpxB7F4nFg8hiYOMQf4c6C6P+yYDKE06LcQsjaCNQxKQ8QD2gJOH8pTg3LVgKsG7fSBLYgtmo7VYiXoMN1ZlriT1OgAsAUJWMoJUw9AjuUEJqVezbrgx2wOfY4VG15LD+ri5cSJHtI+dNvcrQqAvQXlvhQKzeH/fzqtTiLxSNN69mVVVlKdqUTjURrCDUe0LgCbxUZRryIGZw+mh6cHS3ctZUnJElIcKeSm5AIQioVwWp147B48dg9uu5toPIo/4ifTlUkvby82V29maclScr25TO0/lWA0yJe7v6QqUEUkFiHFkULPlJ44rA5C0RDBaJBQzDwHo0GcVie53lwcVgf14Xoq/BWUNZQBkOXOwuvw4o/4aQg3EIgGWm1DnjePgrQCfCEf9eF67BY7SikCkQChWAiXzYVVWWmINBCKhsj15pLpyqQmWEMgGuDkgpOZVDCJsoYytvu2Y7PYiOs4a8vXsr5iPX3S+jAkZwjprnScVqd52Jw4rI6m1zaLjeLqYtZXrsdusZPtySbbnU2WO4sz+5/JWQPOOqy/j1LqC631uIP+HQ9r6d3Zueea54ULYeJEvN4i3O4TKSt7rVskhVg8RnWwmuLqYj7a8hGr9qxiYNZATsg8gQp/BTtrd7Kzbic7a3dSUltCub+cvul9KUwvZHPlNrbVbibOgRttVSAb6/yHiI36Czp/MYQ9ppC2RmEDELPD7lHQawXE7dDQEzJehRTMo5E71I/s+EAc9CKk64lbwtgsVmxWGzarFW0J4FdrqYz9gyiPAGDBQm9PIR6nC6U0gUiAqI6R5c4g051BhqsPGa4RZLgycFqd1ARrCMfDDM4eTC9vL9aVr2Nz9WZSHClkubIYkjMEj93Dn774E//Y+SAjeo5g1tBZhGIhdtfvppe3F33S+hCKhagL1ZHqTCXTlUmWO6vpkeZMY1fdLrZUb2FA5gBG540mHAvzVflXrN6zmjVla8hPy+cbhd8gy52FL+Qj1ZFKflo+WmvKGsrYXb+b3fW7sVvt9Evvh8PqoDpYTVWgikq/6bnM8eSQ7ck2z+5sPHYP9eF6PtvxGWUNZQzOHkyaM41NVZsorilmT/0eakO12K12Ml2ZTCmcwrje4wjHwgSiAewWO3EdZ3f9bqqD1aQ708lyZ5GalrnRAAAgAElEQVTpzsRusbOhcgObqjbhsXvIcmcxoucI3HZ3q9+C1hqlWjeDdERcx1Gow/puW0LREDEdw2P37LeevQnCZrGR7cnulPW15XD3xdEmNYW2nHACjB5t+haA4uL72LbtAU4+uaRLzkKK6ziRWASnzdnq8521O9lQsYFwLExtqJbimmKKq4sprikmEA1wSsEp9MvoxyfbP2F56XLKG8qpCda0OiLMdfajPLSTOKbh2RZNwx0twBMpIO7LJ1CVTcSzjUjKVuLVfaBiKAQzIOqC2ny8sb6kpmm8WfVkeFLxpkVY3ecOKhzLcOhUzgk8z5SeVzF4sKZGb2dNxUo2hj5jY/C/DMkcwYwx9zK4d29s7gY21aznq/KvSHOmMbnvZHI8HWuuC0aDLNu1jPpwPScXnEy6K73zdn4LVYEqstxZCVm2EInW0ZqCJIW2XHstfPqp6SUEGhq+YunSYQwc+AcKCn7U6avzR/wA+x3FaK35x9f/4Ccf/oQt1VsYmTuSwoxCGiINbKraxJbqLfstK8fdg1xnIdGwjU3+ZcSI4Izk4a06BVe0FwSyKduaTaQqD7adDg25YA1B2k5yU3uQn5PWdJpdQYHp0HO7TWfa4MEwalTz+IHp6abzbF+RWISXV7/MqX1PZWDWUThdQghxUJIUjsRjj8GPf2x6IXv3BmDp0lFYrSmMGXPkd2TbWLmRTVWbqA3V8q/N/+KNtW8Qi8c4s/+ZFGYUUlpfSmldqWnCqSthSM4QLh18KZ/vWMb26lLcllTSVD69wqdhLSuibJeTPTtSKV1XSG2Ft3lFdj8pPffQL6OQnj0UsZgpxIuKTOG+93S3Pn3MmSJOZ/sxCyGOb9KncCQmTjTPS5bAZZcB0LPntRQX/5RAYCtud+EhLzIYDfLHpX/kxZUvsmrPqqbPvQ4v1424jhR7Cu9vep8lJUvIS80j15PHcM9UTo6cTPrK7/DR83ZWrNj/HGy3G/r3hxMK4azp5nVhoXnu399DRkZ/joNmTCHEMUKSQltGjzYnE3/+eYukMJ3i4p+yZ89sCgvv69BiVu9ZzfqK9eyu382jix+luKaYkwtO5g/n/YHxvceT6kxlQOYArHEPdXVwd9GjLFwIL74I//538/nfOTkwYgTce6959nrNUX5hYfPFP0II0RkkKbTF5TLtK59/3vSR292fzMxz2LXrWfr2vQeLpfWuqwvV8ewXz+KwOhiRO4Lnlj/HK6tfaZo+rMcwPrz+Q84+4WzWr4f33oCPPoI1a2DXrubL+ME05/zwh+YCn0mTIFHDPQkhxL4kKbRn4kR46SWaGuKB/PzbWbPmUior59KjhxmApD5czyurX+G++fexp2FP09edVic/O+1nTB82HY81nbJNBXz6loWfvg5fNN7l86ST4KyzTFNPZqa5enLQIPjGN9ruwBVCiESTpNCeCRPMwHjr1sHw4QBkZ19IyJLPTz/6CVHPXGqCNXy05SP8ET8nF5zM3GvnkufN48vdXzIqt4iSr/ry5H1mOKWaGrPY0aPh0UfhqqvM2T1CCHEskaTQnlNPNY31TzwBf/oTAO9+/R7fXVxLZaCOgrQwbnsq1424jm+N/Ban9j0VpRQ1NVD2SR8ufxq+/NIMs3D55XDppWaReTLYqhDiGCZJoT0DBsBPfgKPPEL0ogu427GQ3y/+PSN7DuPXwzYxqd8Ehg9/u2n2NWvg/vvhnXdMB/GwYfDHP8J11zUP5iWEEMc6SQoH8sADlC94n+lzpzG/IMIPx/+Q3537O3aXPMaWLXdTXv4W9fVX8ItfmCGT0tLgttvMtW/jx8tZQUKI448khQNYVrmaK6ZVUV4b4SV1BTdc8AQABQV3sm7dB/zgBzXMnauxWhV33QV33w1ZMgqCEOI4JkmhHYt2LOLM2WfSM6Unny4bydjSTXCfGZ7517+28bvffUgwqLn66sU88sjJ5Ld5d2khhDi+SFJoQ0ltCVe8cQW9U3uz6DuL6Mkr8OMf8+Xft/Gte/uxdi1Mn27l29++m5SUx8nJ2Qz07uqwhRDiiFm6OoBjTTAa5Io3rqAuVMc717xDz5SecNVV/JPzmHRVPlVV8M9/wmuvwZQp30frGNu2JfamF0IIcbRIUmhBa81t793GkpIlzL58NsN7musTPvyqgMvV3xnu+JpVq+C888z8bnd/8vK+S2npcwQCxV0YuRBCdA5JCi08tfQpXljxAj8//edcMdRcsfzxx+YagyF5Pj4MnEZO9cZW3+nX716UsrNx4x0cbyPOCiHEviQpNFq9ZzUz/jWDiwddzKwpswBz87WLLzb33Pno/SjZVh88/nir7zmdvRkw4EGqqt5j9+7nuyByIYToPJIUGj346YO47W5evOxFLMrCxo1w4YXQt68ZsbTHqN5wyy3wzDPw9detvpuffwcZGd9g06YZBAJbu2YDhBCiE0hSADZXbeb1ta/zg3E/IMudhdZw661mULqPPoLc3MYZZ80yI6jOnNnq+0pZGDz4eUCxfv2N6HZulC6EEMc6SQrAb//7W2wWGzMmzQDghRdgwQJ4+OF9Bq3LzTUJ4e23TfWhBbe7kIEDH8Xn+w87d7ZuYhJCiONF0ieFktoSXlzxIjeOupHeqb0pL4f//V847TT47nfb+MKPf2zGt77mGihufcZRr143k5V1IcXFP6WhYd3R2QAhhOhESZ0UtNbc8u4tKKW4+9S7Afj1r8HnM10Hlrb2jscD774L0ajphfb5miYppRg8+M9YrV5WrToHv39jGwsQQohjV1InheeWP8c/N/2Th896mAGZA9i2DZ5+Gm66ydwAp12DBsGcObBhA4wcCf/4R9Mkp7MXo0bNIx4PsWLF6TQ0rE38hgghRCdJ2qSw3bedOz+4k6n9p3L7hNsB04+sFPziFx1YwNSp8J//mHGxL77Y3Hehkdc7iqKiBYBixYop1NWtSMQmCCFEp0vapPDE508QjAb5yyV/waIsrF0Ls2ebeyP36dPBhZxyCixfDmefbTLJ3turASkpJ1FUtBCLxc3Kld+gtnZpYjZECCE6UVImhUAkwPMrnufyoZfTL6MfAPfeC14v/PSnh7gwhwN++1uTEH7zm1aTPJ6BjB79CTZbBmvWXEootLuTtkAIIRIjKZPCG2vfoCpQxW3jbgNg8WL4+9/hrrsgO/swFjhqFHzzm/CHP8AXX7SqMbhc/Rg+fC7RqI+vvrqaeDzSSVshhBCdLymTwlNLn2JIzhCmFE5Ba3PpQc+eMGPGESz0/vshFoNx4yAzEx58sGmS1zuCwYP/jM/3CZs2/UjGSBJCHLOSLiksL13O0l1LuW3cbSil+OAD01/885+b5qPDNmAArFxp7st5zjkmSezY0TQ5N/da+vS5m127nmHr1llHvB1CCJEISXeTnfe+fg+F4psjvkk8bvoQCgvNsBZHbMgQ85g8GQYPNqczPfaY6WuYMoUBUx8kEqlg27b7ATPCqsVi74QVCyFE50i6pDCveB5j8saQ7cnmtddgxQr4619Nf3GnKSw0pzE99pgZDmPbNpgzB7VuHYMGPYPWYbZtu5/Kync58cSnSE8/uRNXLoQQhy+hzUdKqfOUUhuUUpuUUjPbmH6jUqpcKbWi8dHWwBKdpj5cz6IdizhrwFlEIuaMoxEj4NprE7Cye+6BjAwzqt6Pf2wudPv3v7FYbAwdOpthw+YQCu3iyy9PYfnyyVRVzUtAEEIIcWgSlhSUUlbgKeB84CTgWqVUW9cJv661Lmp8/DlR8QB8su0TIvEIZw04izffhM2b4YEHTLnd6bKzYf16WLvWjJ2Rk2Mul27Uo8eVTJy4kYED/0A4vItVq86muPgXaB1LQDBCCNExiawpTAA2aa23aK3DwGvApQlc30HN2zIPp9XJ5D6T+dvfzAioF12UwBX26GGG2na54DvfgXfeadX5bLOlUlDwI8aPX0tu7g1s23Y/q1dfRCRSmcCghBCifYlMCvnAjhbvdzZ+tq8rlVKrlFJzlFJtXkuslLpVKbVMKbWsvLz8sAOaVzyPU/ueSn2Nm3/9C667rp1B7xLh+98HreHGG+G99yDSfL2C1ephyJAXOfHEP1Jd/W+WLRtLbe3nRykwIYRo1tWnpL4LFGqtRwIfAS+1NZPW+lmt9Tit9bgePXoc1or21O9h1Z5VTO0/lddfN5cUXH/94Qd+yAoL4aGHTM/2RReZgfQ+/bRpslKK/PzvM3r0J0CM5csnsWbNFTKgnhDiqEpkUigBWh75FzR+1kRrXam1DjW+/TMwNlHBfFz8MQBnDTiLl182FyEPH56otbXj//0/KC01I6wGAuamDT/7malBNEpLm8i4cavp1+8XTbWG0tIXjnKgQohklciksBQ4USnVXynlAK4B5racQSmV1+LtJUDC7kxz9gln88oVr5DWMIbFi49yLaElhwOuvBLWrIGbbzad0PffbxLD+vUwdy721/9B/5KzmDhhI+npk9mw4WY2bPgesZi/i4IWQiSLhF2noLWOKqV+CHwAWIHntdZrlVL3A8u01nOBHymlLgGiQBVwY6LiyfHkcO2Ia3nlFfP+/PMTtaYO8nrhuecgHjcXuf3lL606oQEco0Yx8oFfUjx0PDt2/AafbyEnnvg06emnYLE4uyZuIUS3po63cXjGjRunly1bdtjf/93vzO02q6vNZQRdLhqF2283CeHii2H8eEhLg4ULzeirO3bAhg1UpWxg/fpvEw7vQik76emT6dv3HjIzz0Ip1dVbIYQ4ximlvtBajzvYfEl3RfPu3eYM0fT0ro6kkc0Gf/rT/p8PGgRnnWWGzZg5k6yXX2bChK+orp5Hbe1SyspeZtWqc0hLm0xh4SwyM6dKchBCHLGkSwqlpdCrl7nD2jGvsNCM5/3AA3D11dhiMXrsLKGHo5D+GQ9RnrOO4voXWbXqbNLTz2DQoGdISRnS1VELIY5jSdd8NHWqOfHnv//txKASqaHBDK5XUtLmZN2ngLK/3shG61PEYn4KCmaQljYRr7cIt7v/0Y1VCHHMkuajdpSWwtChXR3FIUhJgbffhkWLYOJEOOEEc+FbWRmsXo363/8ld/pzZM7+M7u2PopvyW/YMRawQEbGmeTn/5CcnEtRqqsvSRFCHA+SMimceWZXR3GIxo83j5by8szFFuPGwZln4jj3SgobJ8UG96fu3P7EVi/CsfNj6lO82AYW4f/FjajefcnMmIqqqjLjMQkhRAtJdfgYDJo7Zfbq1dWRdKIhQ2DpUpg929wt6JVXsNpTyHj8Y7K29sJRMJK4NYrjH5/infJdKn55DsHhOei8PPjkE7OM8nJ4+GFT++gsW7aYM6uEEMeVpEoKu3eb57y8A8933MnPh299C04/3YwDvnIl+HyoLVtwfryS1CU1BD5+GXtaAYMeA2qqCWdrwteex+ZFNxM9/3S4+2446SR46SUIh48sntWrzdlTnXLnomOEzwdPPmn6eET3dJz1ryZKUiWF0lLz3O2Swr4sFnOtQ9NbJ96Tv4ll+Wp45x0Cy99j+69HYd/lp+/5L2Bdvp4tP/ISKkwzA/bl5KCnTYO33oJQCHbuhH/9C95809RIfvELM+rrokX7r1trc7PrWAxeeAGWLGk9fe1a2LTp+PoH1Nps7x13wK9+1dXRdL4lS8zdAYPBtqfH4+ZGUd2V1nDbbXDiiea3eajicfjyS/P/EY93fnxHm9b6uHqMHTtWH64339QatF6+/LAX0b3cdZfWoOt+ebP+8sspev489LqHc3XpJR4dykRr0HGb1ey0lg+LRWuv1zzPnKn1PfdofdppWv/f/2n917+aeX79a6179dJ64kStw2Gtv/hC60suaV5Gv35a33yz1i+/bB7/7/9pfe65Wufna33FFVpXVzfH6fdrPXeu1h9/rHU83nobKiu1fu8988f997+1DgT2385YTOsnn9T6mmu0Lis79P30xz+amPv00drp1Lq42Hy+bdv+8RyOWKz59e7dWk+ZovWPf9z685YWL9Z6yZLOWe9DD2ltbfwbjxql9ddft54nGtX6uuvM9Fde2X8Z8bjWr76q9eTJ5m/clkBA6zlztF637shj3svnM7+Lg6mv1/qdd7T+yU+0vu8+rauqzO/xT3/S+u67tV61Sus77zTb53RqXVCg9aZNHY9j7lyte/Ro/l3/+c9tz7dkidbnnKP10qUdX3Ynw4wkcdAytssL+UN9HElSeOops8W7dh32IrqXaFTrlSu1jsd1PB7Xe/a8oVesOFt/9dUNesPa7+lVj3j09mnoTf/j0Bv+PErv/uheHflqudbBoNY1NVp/61tmh1qtWg8f3vyPcdJJWkciWr/0knnvcJjntDStf/lL84e44gqtMzKav+NwaF1UpPW0aVrbbFoPHmwSy0UXae3xNM83ebLW8+eb+DduNMmlZcJyubT+xjdMIfDMM1o//7z5ZwStldL6hBNMErn1VrOOiRPNduxNFvG4KThiMVOgPP64Wea552q9fbvWbrfW552n9QUXmGXedlv7hXdZmdah0IH/Bh9/rHXv3iaO2bO1HjiwuZD+3vf2X/a8eWZfWSxaP/xwc1LassUUdP/4x/7reO89sx3RaPNny5drfcopZj3Tpmn9+utaZ2VpnZ6u9YYNZp5YTOvvftfMU1Bg9sOyZVqXlppC/vHHmxO9xWIOArZta/37eustrQcMaP77FBVp/ZvfmHg3bjQF9l13aX3mmVo/91zz3+Dvf9d68+a299l//6t1To4pjH/7W/N32uuJJ7T+1a/Mfi8u1vrEE5t/X0ppnZlp9vHemPfGdccdWq9YoXV2tta5uVo/+6z5De/1xz9qPWaM2ea6OvPZ3Lla2+3m89mzzf7s2dMkrGDQbNunn2r92mvmdwNm2Vu3mmR0ww3m76+11uXlWv/sZwdOGjU15iDoMElSaMO995rfQcv/DdG+SKRWl5a+qDdsuF0vWTJcz5+P/s9/UvTKlRfq7dsf0ZWVH+nQlx/r+N4f6uLFWn/zm1ovWmTex2LmH/7HP9b6xRfND7+laNQcXa5aZY7e9vrPf8w/PWg9aJDWP/iB1h9+aP4xe/c2n595pimEsrNNQbhihSn8ZszQevx4c9S39x/e7Tbf/e9/m4/qXC6tL77YJAyXyxQe8+Y1F/YOh6kNgakF7dljYps1y3yWmqr1ZZeZ19dcY7ahocEcDb/wgtZnnNG8nPHjTWEVDDZv49atpoZlsZhtLCw082dmmjh/+lPzvrDQFDY33WQKwJQUk4CvuspM79nTxNIyMf7sZ6ZgLS42f4+9n59xhimopk836+3Rw/xdWiaW7GytR440NbXrrzffu/des/19+5p91XJdHo8p5FeuNAll6FBTe7z1VvP32XuQ8PbbWj/6qNYTJrT+PpiCtX//5iQ7dap5nZ6u9bvvNu+zqiqT+JxOU7CffbaZb8gQk0Cefrp18und2xx4zJ1raisrVpi/7/jxZrnl5Vr//vfmQGVv8l29WuuTTzbLOPFErX/3O1OLBa3z8prjzcszBy/jxzfXapcuNdNvuMEkipbbOGGCOZhJTze14b2JXymtb7yx+Xdps5naW0WF+Z+YM0fr8883yWTv3/YwSVJow3e+Y36n4tDF43Ht8y3WGzb8QC9ePEjPn0/T47PP8vWaNdP11q0P6vLyuToY7ISqWH19c0Hckt9vCpeePc0/5tq1bX8/HNZ6505TMLZsiiouNgVLyyOuzz4zR8lgEsE995iC4HvfM0d6LQUCJsGUlpr3Dz20fyEH5uh41iyTFMePN5/l55sEM3Ro83zXXWeOPMNhrd94o/koPR43hdw115iaT3Z283J37TLTn3nGJIsZM7T+wx+0XrOm+ch+b1K1202z3gsvNNe4MjNNXC33y17//GfzETWYAnNv0li50iTCBx80zSFlZa1rMvPmmf1ot5tEdcUVZptaJnytTfPMY4+ZWtxnn5lkGo1q/T//05xwf/tbrUePbi70Bw5sPrI/5ZTmA4wPPzSxZmaa6RddZJoSe/Qwv49Vq9r+fRxIPG5qOHuTA5gkF4mYhH333WY///CH++/Dm25q3scvv6z1++9r/be/mW3cu49SU01htH27+Y2B1uPGaf3JJ6bW1rKZFkwyvukmk3w///zQt6dRR5NCUl3RfOGFprN5+fJODioJhcN7aGj4ioaGNdTW/hef7zNCoeZRXp3OfrhcfbBa08jKOo+8vJuxWlM6L4Bg0Jzy6vV2zvLWr4dXXoEf/ODQz0RYv950NG7eDH37mhsojRzZfFs/reHjj+HRR8HvNycBTJoE06aZixE7Qmuz/Px8cLsPPN8TT5hL9idPhgsuaF7H5s3mzLBzzz3wMu6/Hx58EJ5/3pzNdjR99JG5urSgwAw9cP/9Jm6LxZzRdu65Zt+1vLH611/DJZeY+6J/+KG54LOuznT6HukgZ2vWmM7nSy/t2Ng4FRXw2GPwve9BnzZvJGniannLx02bzJA2Npv5+82bZ07I2LMHTj7ZFFydcCP5jl7RnFRJYcwYc43C++93clACgGjUR0PDWmprP6e2djGRSBnh8B78/nXYbNmkp0/G4eiFw5GLw9ELj2cIqaljsdmOldEJRZNg0IwcebyIxcxzJxSe3ZUMc9GG3bth9OiujqL7stnSSU8/hfT0U1p9XlPzKSUlj+P3r29MFuXA3oMR1ZgcJpCWNoHU1Al4vSOxWBxHPX7RwvGUEECSQSdKmqQQi5naWLe/RuEYlJFxKhkZpza9j8ejRCLlNDSsprZ2CXV1S6iq+id79phbdCvlwOsd3ZQkXK4+aK2xWBzYbBk4HLnYbFkyVLgQCZA0SaG83DTlSVLoehaLDaczD6czj6yscwBzwkMotKMpSdTWLqG09HlKSp5ocxlWayoez0lkZk7F6y3CYnFgsaQ0LrcPNltam98TQhxY0iSFvVczd6txj7oRpRQuV19crr707HkVAFrHaGj4ikikAlBoHSISqSYc3k0wWExd3TK2b/8NENtveQ5Hb1yuflgsHhyOXLKyziEz8xycTjkqEOJAkiYpdNtxj7oxpax4vSMOOE80WkswuBWtY8RitYRCpQSDW/H71xMK7SQeD1BT8zFlZebm3E5nXzyeIWgdARRu9wDc7kF4PINwuwfj8ZyIUtI+LZJX0iQFi8WMNF1Q0NWRiM5ks6Xh9Y484Dxaa+rrV1JT8zG1tUsIBjdjsbjQOkpFxTuNHd+GxZKC2z2QaLSaeDxAVta55ORcht3eE4vFicXiavVQyoHF4kApB0pZpZ9DHPeS6pRUIdoSidQQCGzE7/+KurrlBAKbsduzgTiVle8RjVZ3cEmqMUm4SE8/lR49riQ1dQxOZz+0DhONVuNyFWKxOBO5OUK0SU5JFaKD7PYM7PbxpKWNp1evb7eaFo9HqK//klisnng8uM8jQDweQesw8Xi46TkaraGq6p9UVb2337rMxXzn43TmEY9HsNuzcDr7NF3Y53YPxOsdjcViJx4PEY+HUcqCxeKRWog4KiQpCHEAFoudtLQJh/w9rTUNDavx+zcQDG7DYnFhtXrx+T6lquo9YrEGlLIRjfqA1sMtWywpWCyOVjUUp7OAnJwrcTrzCQa3EIlUEIv5cTh6kpFxJm73QOJxPzZbVmNzmoVgcAvxeISUlCFHuBdEMpHmIyG6UDweIRwuJR4PNp5ttQaf7xO0juJw5GG1eojHI9TWLqKq6gO0DmGzZeFw5GKxeAgGtxKNVrZaps2WidWaQii0EwCvdzSpqeNpaFhLNFqJw5GP290fr3csHs8gQKOUE49nCA5HDrGYn3g8hN2e2QV7RCSKDHMhRDcTizU0NjllNH2mdZz6+lWEw6WNiWAH1dUfE4s1kJFxBlpH2L17NsHgZlJShmO35xIOl+D3f000WrXfOpRyonUIAIcjD7f7BGKxeiKRaqLRKuLxIG73CbjdgxqTjxebLRWbLYOUlFGkpAwnFNpGILCZlJSReL0jUSqp7uV1zJKkIIRol9aaYHArwWAxStmIxfz4/V8RCu3Cbs9GKRsNDWsIBrdis6Vjs2Vit2ehlINAYCOBwEai0VpisTpisTq0bvt+3DZbBkrZiccDjU1oqbhc/fF4hmC3Z2GxeLDbc3A4ehKJVBAIbMHhyCU1dSwORy+UchKP+4lGq4lEqohGa/B4BuH1jmnVx2L6cqqJRKqx27NwOHoerV153JCOZiFEu5RSuN39cbv7N32WnX3eYS8vGvVRV7ccv/8rXK5CXK7+1NUtw+f7L0opLBY38XiIaLSGQGATZWWvNPan7HtQamHfPpa2uN2DGvtXthIO7yEe9+8zfTBu94DGEwBCjZ32ocZmsSwyMr5Bauo4rFYvVqsHi8XT9ByLNVBfv5xweA9u94nY7Vn4/euJRCrJyDiD1NSx3fpaFqkpCCG6hNaaeDxEJFJOOLwHuz0bl6sv4fBu6uqWN14rEsJicWO3Z2GzZWKzpVFbu5iysteJxepxuQpxOHphs2Vht2dis2USCpXg8y0kFCptvLbEPJRyYrE4CIV2Ulu7hLauhO8IqzUNt/tEXK5+jTGlAhbi8QB+/zoCgS1YrR5stmwyMk4jLW0yodA2/P4NOBx5uFyFRKM+wuESQqESwuEynM483O4TG/dHJR7PIDIyzsDtHozF0jnH7tJ8JIQQ7YhGawkENhGPBxo71v3EYn5isQYsFjtebxEOR28CgU1EIpWkpAzFak2luvrf+HyfEgxuIRjcTjRaQyxWh+mst+PxDG48EyxEKFRCXd2SpqY1iyWFeLyhRRSWxqHkexIKlTRdRGmxeFrUfKw4nflYLA7i8Qi9e3+ffv1mHtY2S/OREEK0w2ZLIzV1zEHnczh6tHqfm3stubkdv/FQNFpLff2XuFwDcDoLiMXqCYW2Y7Nl4nDktmqGikZrG6+StxMMbqGm5hMCgU2EQtvQOoZSdtzuDt6U6QhIUhBCiASx2dLIyDijxftUbLZh7b1rSLkAAAe4SURBVM67lznDK/EJoC1yrpgQQogmkhSEEEI0kaQghBCiSUKTglLqPKXUBqXUJqXUfl3mSimnUur1xumfK6UKExmPEEKIA0tYUlCmW/0p4HzgJOBapdRJ+8z2HaBaaz0QeBT4TaLiEUIIcXCJrClMADZprbdorcPAa8Cl+8xzKfBS4+s5wFQl4wMLIUSXSWRSyAd2tHi/s/GzNufR5goPH5D9/9u71xg5qzqO49+fVCulhgVbUNuGFmy8QKRgY6qoIWAUkFBeQGyoeE14gxEMiVqrGHlnNFZNlEsALdCgAYs2BBVYSA2JpezW3mhBFkHcpthFAUUDcvn54pwZH4Zddtyyc04z/0+y2XkuM/ntf/eZs3Pmmf/T+UCSLpA0JGlobGysc3MIIYTXyAHxRrPtq2wvtb107ty5k98hhBDClEznh9f2AAsay/PzuvH2GZU0AzgU+BuvYnh4+AlJf55ipjnAE1O8b69Exv1Xez6oP2Pt+aD+jLXlO6qbnaZzULgPWCxpEenJfwVwXsc+G4BPA78HzgHu8iTNmGxP+aWCpKFuen+UFBn3X+35oP6MteeD+jPWnm8i0zYo2H5B0heA3wIHAdfavl/SZcCQ7Q3ANcD1kkaAv5MGjhBCCIVMa+8j27cBt3Wsu7Rx+1ng3OnMEEIIoXsHxBvNr6GrSgfoQmTcf7Xng/oz1p4P6s9Ye75xHXDXUwghhDB9+u2VQgghhFfRN4PCZH2YCuRZIOluSbsk3S/porz+cEl3SHoofz+sgqwHSfqDpFvz8qLcq2ok9656Q+F8A5JulvSApN2S3l9THSV9Kf+Od0q6UdIbS9dQ0rWS9kna2Vg3bs2U/DBn3S5p8qvTTF/G7+Tf83ZJt0gaaGxblTM+KOljJfI1tl0iyZLm5OUiNZyKvhgUuuzD1GsvAJfYfjewDLgwZ/oqMGh7MTCYl0u7CNjdWP42sCb3rHqS1MOqpB8Av7H9TuB4UtYq6ihpHvBFYKnt40hn4q2gfA1/CpzWsW6imp0OLM5fFwCXF8x4B3Cc7fcAfwRWAeRjZwVwbL7Pj9W8rFnv8iFpAfBR4LHG6lI1/L/1xaBAd32Yesr2Xttb8u1/kp7I5vHyflBrgbPLJEwkzQc+DlydlwWcQupVBYUzSjoU+DDp9GZs/8f2U9RVxxnAwfkDmrOAvRSuoe3fkU4Db5qoZsuB65xsAgYkvbVERtu3u3XRY9hE+lBsK+PPbD9n+xFghHTc9zRftgb4MtB8w7ZIDaeiXwaFbvowFZNbhp8A3AscaXtv3vQ4cGShWC3fJ/2Bv5SX3ww81TgwS9dyETAG/CRPcV0t6RAqqaPtPcB3Sf817iX19xqmrhq2TFSzWo+fzwG/zreryChpObDH9raOTVXk60a/DArVkjQb+AVwse1/NLflT3cXOz1M0pnAPtvDpTJ0YQZwInC57ROAf9ExVVSyjnlefjlp8HobcAjjTDnUpvTf3mQkrSZNwa4rnaVF0izga8Clk+1bs34ZFLrpw9Rzkl5PGhDW2V6fV/+19bIyf99XKh9wEnCWpEdJU26nkObvB/JUCJSv5SgwavvevHwzaZCopY4fAR6xPWb7eWA9qa411bBloppVdfxI+gxwJrCy0RanhozHkAb/bfmYmQ9skfSWSvJ1pV8GhXYfpnyWxwpS36Vi8tz8NcBu299rbGr1gyJ//1Wvs7XYXmV7vu2FpJrdZXslcDepVxWUz/g48BdJ78irTgV2UU8dHwOWSZqVf+etfNXUsGGimm0APpXPoFkGPN2YZuopSaeRpjPPsv3vxqYNwAqlqzkuIr2hu7mX2WzvsH2E7YX5mBkFTsx/o9XUcFK2++ILOIN0tsLDwOoK8nyQ9PJ8O7A1f51BmrMfBB4C7gQOL5015z0ZuDXfPpp0wI0ANwEzC2dbAgzlWv4SOKymOgLfAh4AdgLXAzNL1xC4kfQex/OkJ6/PT1QzQKSz9x4GdpDOpCqVcYQ0N986Zq5o7L86Z3wQOL1Evo7tjwJzStZwKl/xieYQQght/TJ9FEIIoQsxKIQQQmiLQSGEEEJbDAohhBDaYlAIIYTQFoNCCD0k6WTlbrMh1CgGhRBCCG0xKIQwDkmflLRZ0lZJVypdU+IZSWvytREGJc3N+y6RtKnR4791HYK3S7pT0jZJWyQdkx9+tv53/Yd1+ZPOIVQhBoUQOkh6F/AJ4CTbS4AXgZWkZnZDto8FNgLfzHe5DviKU4//HY3164Af2T4e+ADp06+QOuJeTLq2x9GkXkghVGHG5LuE0HdOBd4L3Jf/iT+Y1BzuJeDneZ8bgPX5eg4Dtjfm9WuBmyS9CZhn+xYA288C5MfbbHs0L28FFgL3TP+PFcLkYlAI4ZUErLW96mUrpW907DfVHjHPNW6/SByHoSIxfRTCKw0C50g6AtrXLj6KdLy0OpueB9xj+2ngSUkfyuvPBzY6XU1vVNLZ+TFm5n77IVQt/kMJoYPtXZK+Dtwu6XWkLpgXki7g8768bR/pfQdIbaavyE/6fwI+m9efD1wp6bL8GOf28McIYUqiS2oIXZL0jO3ZpXOEMJ1i+iiEEEJbvFIIIYTQFq8UQgghtMWgEEIIoS0GhRBCCG0xKIQQQmiLQSGEEEJbDAohhBDa/gtL7oX0oCExxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 609us/sample - loss: 0.3295 - acc: 0.9061\n",
      "Loss: 0.3295267518683262 Accuracy: 0.9061267\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4773 - acc: 0.1800\n",
      "Epoch 00001: val_loss improved from inf to 1.81089, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/001-1.8109.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 2.4772 - acc: 0.1800 - val_loss: 1.8109 - val_acc: 0.4556\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7602 - acc: 0.4166\n",
      "Epoch 00002: val_loss improved from 1.81089 to 1.33956, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/002-1.3396.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7601 - acc: 0.4166 - val_loss: 1.3396 - val_acc: 0.5875\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4696 - acc: 0.5145\n",
      "Epoch 00003: val_loss improved from 1.33956 to 1.16357, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/003-1.1636.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4696 - acc: 0.5145 - val_loss: 1.1636 - val_acc: 0.6459\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2954 - acc: 0.5788\n",
      "Epoch 00004: val_loss did not improve from 1.16357\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2955 - acc: 0.5788 - val_loss: 1.1937 - val_acc: 0.6191\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1684 - acc: 0.6266\n",
      "Epoch 00005: val_loss improved from 1.16357 to 0.84012, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/005-0.8401.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.1684 - acc: 0.6265 - val_loss: 0.8401 - val_acc: 0.7661\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0283 - acc: 0.6764\n",
      "Epoch 00006: val_loss improved from 0.84012 to 0.75795, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/006-0.7579.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0284 - acc: 0.6764 - val_loss: 0.7579 - val_acc: 0.7834\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9186 - acc: 0.7151\n",
      "Epoch 00007: val_loss did not improve from 0.75795\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9185 - acc: 0.7152 - val_loss: 0.7827 - val_acc: 0.7631\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7432\n",
      "Epoch 00008: val_loss improved from 0.75795 to 0.58507, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/008-0.5851.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8332 - acc: 0.7431 - val_loss: 0.5851 - val_acc: 0.8330\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7608 - acc: 0.7659\n",
      "Epoch 00009: val_loss improved from 0.58507 to 0.51551, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/009-0.5155.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7608 - acc: 0.7659 - val_loss: 0.5155 - val_acc: 0.8563\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7025 - acc: 0.7853\n",
      "Epoch 00010: val_loss improved from 0.51551 to 0.49258, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/010-0.4926.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7025 - acc: 0.7853 - val_loss: 0.4926 - val_acc: 0.8600\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.8022\n",
      "Epoch 00011: val_loss improved from 0.49258 to 0.47360, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/011-0.4736.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6490 - acc: 0.8022 - val_loss: 0.4736 - val_acc: 0.8626\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.8112\n",
      "Epoch 00012: val_loss improved from 0.47360 to 0.46843, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/012-0.4684.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6125 - acc: 0.8112 - val_loss: 0.4684 - val_acc: 0.8721\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8262\n",
      "Epoch 00013: val_loss improved from 0.46843 to 0.38519, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/013-0.3852.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5706 - acc: 0.8262 - val_loss: 0.3852 - val_acc: 0.8917\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8320\n",
      "Epoch 00014: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5494 - acc: 0.8321 - val_loss: 0.3861 - val_acc: 0.8912\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8377\n",
      "Epoch 00015: val_loss improved from 0.38519 to 0.33346, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/015-0.3335.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5191 - acc: 0.8377 - val_loss: 0.3335 - val_acc: 0.9080\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8462\n",
      "Epoch 00016: val_loss did not improve from 0.33346\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5016 - acc: 0.8462 - val_loss: 0.3726 - val_acc: 0.9010\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4889 - acc: 0.8510\n",
      "Epoch 00017: val_loss improved from 0.33346 to 0.31192, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/017-0.3119.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4889 - acc: 0.8510 - val_loss: 0.3119 - val_acc: 0.9161\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8598\n",
      "Epoch 00018: val_loss improved from 0.31192 to 0.29971, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/018-0.2997.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4596 - acc: 0.8597 - val_loss: 0.2997 - val_acc: 0.9208\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8650\n",
      "Epoch 00019: val_loss improved from 0.29971 to 0.29827, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/019-0.2983.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4397 - acc: 0.8650 - val_loss: 0.2983 - val_acc: 0.9182\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8707\n",
      "Epoch 00020: val_loss improved from 0.29827 to 0.27564, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/020-0.2756.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4140 - acc: 0.8707 - val_loss: 0.2756 - val_acc: 0.9252\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8744\n",
      "Epoch 00021: val_loss improved from 0.27564 to 0.27388, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/021-0.2739.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4074 - acc: 0.8744 - val_loss: 0.2739 - val_acc: 0.9264\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8789\n",
      "Epoch 00022: val_loss improved from 0.27388 to 0.26724, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/022-0.2672.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4006 - acc: 0.8789 - val_loss: 0.2672 - val_acc: 0.9264\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8808\n",
      "Epoch 00023: val_loss improved from 0.26724 to 0.25394, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/023-0.2539.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3883 - acc: 0.8808 - val_loss: 0.2539 - val_acc: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8862\n",
      "Epoch 00024: val_loss improved from 0.25394 to 0.25013, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/024-0.2501.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3725 - acc: 0.8862 - val_loss: 0.2501 - val_acc: 0.9304\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8881\n",
      "Epoch 00025: val_loss did not improve from 0.25013\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3626 - acc: 0.8881 - val_loss: 0.2510 - val_acc: 0.9322\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8899\n",
      "Epoch 00026: val_loss improved from 0.25013 to 0.24135, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/026-0.2413.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3549 - acc: 0.8900 - val_loss: 0.2413 - val_acc: 0.9329\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8929\n",
      "Epoch 00027: val_loss improved from 0.24135 to 0.22908, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/027-0.2291.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3433 - acc: 0.8929 - val_loss: 0.2291 - val_acc: 0.9385\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8959\n",
      "Epoch 00028: val_loss did not improve from 0.22908\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3348 - acc: 0.8959 - val_loss: 0.2348 - val_acc: 0.9355\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9004\n",
      "Epoch 00029: val_loss improved from 0.22908 to 0.21981, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/029-0.2198.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3227 - acc: 0.9004 - val_loss: 0.2198 - val_acc: 0.9406\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.9001\n",
      "Epoch 00030: val_loss did not improve from 0.21981\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3184 - acc: 0.9001 - val_loss: 0.2198 - val_acc: 0.9411\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9007\n",
      "Epoch 00031: val_loss did not improve from 0.21981\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3139 - acc: 0.9007 - val_loss: 0.2463 - val_acc: 0.9313\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9038\n",
      "Epoch 00032: val_loss improved from 0.21981 to 0.21348, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/032-0.2135.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3050 - acc: 0.9038 - val_loss: 0.2135 - val_acc: 0.9390\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9074\n",
      "Epoch 00033: val_loss did not improve from 0.21348\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3001 - acc: 0.9074 - val_loss: 0.2337 - val_acc: 0.9322\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9079\n",
      "Epoch 00034: val_loss did not improve from 0.21348\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2952 - acc: 0.9079 - val_loss: 0.2143 - val_acc: 0.9418\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9097\n",
      "Epoch 00035: val_loss improved from 0.21348 to 0.20770, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/035-0.2077.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2876 - acc: 0.9097 - val_loss: 0.2077 - val_acc: 0.9462\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9124\n",
      "Epoch 00036: val_loss did not improve from 0.20770\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2770 - acc: 0.9124 - val_loss: 0.2104 - val_acc: 0.9401\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9149\n",
      "Epoch 00037: val_loss improved from 0.20770 to 0.19112, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/037-0.1911.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2725 - acc: 0.9149 - val_loss: 0.1911 - val_acc: 0.9471\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9143\n",
      "Epoch 00038: val_loss did not improve from 0.19112\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2685 - acc: 0.9143 - val_loss: 0.2169 - val_acc: 0.9385\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9152\n",
      "Epoch 00039: val_loss did not improve from 0.19112\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2654 - acc: 0.9151 - val_loss: 0.2017 - val_acc: 0.9450\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9147\n",
      "Epoch 00040: val_loss improved from 0.19112 to 0.18742, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/040-0.1874.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2650 - acc: 0.9147 - val_loss: 0.1874 - val_acc: 0.9485\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9192\n",
      "Epoch 00041: val_loss improved from 0.18742 to 0.18661, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/041-0.1866.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2539 - acc: 0.9192 - val_loss: 0.1866 - val_acc: 0.9488\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9223\n",
      "Epoch 00042: val_loss did not improve from 0.18661\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2450 - acc: 0.9222 - val_loss: 0.1950 - val_acc: 0.9471\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9208\n",
      "Epoch 00043: val_loss did not improve from 0.18661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2490 - acc: 0.9207 - val_loss: 0.1931 - val_acc: 0.9471\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9243\n",
      "Epoch 00044: val_loss did not improve from 0.18661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2433 - acc: 0.9243 - val_loss: 0.1913 - val_acc: 0.9460\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9241\n",
      "Epoch 00045: val_loss did not improve from 0.18661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2388 - acc: 0.9241 - val_loss: 0.1910 - val_acc: 0.9474\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9277\n",
      "Epoch 00046: val_loss improved from 0.18661 to 0.17646, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/046-0.1765.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2271 - acc: 0.9277 - val_loss: 0.1765 - val_acc: 0.9520\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9262\n",
      "Epoch 00047: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2278 - acc: 0.9262 - val_loss: 0.2652 - val_acc: 0.9252\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9279\n",
      "Epoch 00048: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2256 - acc: 0.9279 - val_loss: 0.1793 - val_acc: 0.9504\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9295\n",
      "Epoch 00049: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2180 - acc: 0.9295 - val_loss: 0.1957 - val_acc: 0.9469\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9308\n",
      "Epoch 00050: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2180 - acc: 0.9307 - val_loss: 0.1791 - val_acc: 0.9527\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9286\n",
      "Epoch 00051: val_loss improved from 0.17646 to 0.17429, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/051-0.1743.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2193 - acc: 0.9286 - val_loss: 0.1743 - val_acc: 0.9502\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9329\n",
      "Epoch 00052: val_loss did not improve from 0.17429\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2055 - acc: 0.9329 - val_loss: 0.1776 - val_acc: 0.9509\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9340\n",
      "Epoch 00053: val_loss improved from 0.17429 to 0.16767, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/053-0.1677.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2080 - acc: 0.9340 - val_loss: 0.1677 - val_acc: 0.9539\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9343\n",
      "Epoch 00054: val_loss did not improve from 0.16767\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2031 - acc: 0.9343 - val_loss: 0.1739 - val_acc: 0.9536\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9355\n",
      "Epoch 00055: val_loss did not improve from 0.16767\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2021 - acc: 0.9355 - val_loss: 0.1691 - val_acc: 0.9543\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9347\n",
      "Epoch 00056: val_loss did not improve from 0.16767\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1974 - acc: 0.9347 - val_loss: 0.1710 - val_acc: 0.9548\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9365\n",
      "Epoch 00057: val_loss did not improve from 0.16767\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1990 - acc: 0.9365 - val_loss: 0.1711 - val_acc: 0.9546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9383\n",
      "Epoch 00058: val_loss did not improve from 0.16767\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1905 - acc: 0.9383 - val_loss: 0.1727 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9400\n",
      "Epoch 00059: val_loss improved from 0.16767 to 0.16271, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/059-0.1627.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1884 - acc: 0.9400 - val_loss: 0.1627 - val_acc: 0.9574\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9382\n",
      "Epoch 00060: val_loss did not improve from 0.16271\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1860 - acc: 0.9382 - val_loss: 0.1754 - val_acc: 0.9481\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9406\n",
      "Epoch 00061: val_loss did not improve from 0.16271\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1863 - acc: 0.9406 - val_loss: 0.1807 - val_acc: 0.9522\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9402\n",
      "Epoch 00062: val_loss improved from 0.16271 to 0.15964, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/062-0.1596.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1827 - acc: 0.9402 - val_loss: 0.1596 - val_acc: 0.9576\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9414\n",
      "Epoch 00063: val_loss did not improve from 0.15964\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1786 - acc: 0.9414 - val_loss: 0.1733 - val_acc: 0.9525\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9418\n",
      "Epoch 00064: val_loss did not improve from 0.15964\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1825 - acc: 0.9418 - val_loss: 0.1837 - val_acc: 0.9550\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9436\n",
      "Epoch 00065: val_loss did not improve from 0.15964\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1742 - acc: 0.9435 - val_loss: 0.1621 - val_acc: 0.9513\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9439\n",
      "Epoch 00066: val_loss did not improve from 0.15964\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1715 - acc: 0.9439 - val_loss: 0.1634 - val_acc: 0.9550\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9454\n",
      "Epoch 00067: val_loss improved from 0.15964 to 0.15671, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/067-0.1567.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1664 - acc: 0.9454 - val_loss: 0.1567 - val_acc: 0.9576\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9438\n",
      "Epoch 00068: val_loss did not improve from 0.15671\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1681 - acc: 0.9438 - val_loss: 0.1627 - val_acc: 0.9564\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9445\n",
      "Epoch 00069: val_loss did not improve from 0.15671\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1699 - acc: 0.9445 - val_loss: 0.1590 - val_acc: 0.9581\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9459\n",
      "Epoch 00070: val_loss did not improve from 0.15671\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1615 - acc: 0.9459 - val_loss: 0.1739 - val_acc: 0.9539\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9453\n",
      "Epoch 00071: val_loss improved from 0.15671 to 0.15267, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/071-0.1527.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1655 - acc: 0.9453 - val_loss: 0.1527 - val_acc: 0.9571\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9470\n",
      "Epoch 00072: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1590 - acc: 0.9470 - val_loss: 0.1749 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9467\n",
      "Epoch 00073: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1622 - acc: 0.9467 - val_loss: 0.1574 - val_acc: 0.9576\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9478\n",
      "Epoch 00074: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1560 - acc: 0.9478 - val_loss: 0.1610 - val_acc: 0.9569\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9510\n",
      "Epoch 00075: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1511 - acc: 0.9510 - val_loss: 0.1559 - val_acc: 0.9585\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9495\n",
      "Epoch 00076: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1472 - acc: 0.9495 - val_loss: 0.1773 - val_acc: 0.9541\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9494\n",
      "Epoch 00077: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1530 - acc: 0.9494 - val_loss: 0.1579 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9505\n",
      "Epoch 00078: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1507 - acc: 0.9505 - val_loss: 0.1647 - val_acc: 0.9590\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9508\n",
      "Epoch 00079: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1520 - acc: 0.9508 - val_loss: 0.1557 - val_acc: 0.9576\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9539\n",
      "Epoch 00080: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1388 - acc: 0.9539 - val_loss: 0.1571 - val_acc: 0.9595\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9495\n",
      "Epoch 00081: val_loss did not improve from 0.15267\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1473 - acc: 0.9495 - val_loss: 0.1631 - val_acc: 0.9555\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9533\n",
      "Epoch 00082: val_loss improved from 0.15267 to 0.15019, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/082-0.1502.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1418 - acc: 0.9533 - val_loss: 0.1502 - val_acc: 0.9632\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9538\n",
      "Epoch 00083: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1422 - acc: 0.9538 - val_loss: 0.1622 - val_acc: 0.9564\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9541\n",
      "Epoch 00084: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1387 - acc: 0.9541 - val_loss: 0.1559 - val_acc: 0.9590\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9528\n",
      "Epoch 00085: val_loss did not improve from 0.15019\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1413 - acc: 0.9528 - val_loss: 0.1651 - val_acc: 0.9574\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9552\n",
      "Epoch 00086: val_loss improved from 0.15019 to 0.14820, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/086-0.1482.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1383 - acc: 0.9552 - val_loss: 0.1482 - val_acc: 0.9604\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9571\n",
      "Epoch 00087: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1323 - acc: 0.9571 - val_loss: 0.1535 - val_acc: 0.9604\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9565\n",
      "Epoch 00088: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1300 - acc: 0.9566 - val_loss: 0.1524 - val_acc: 0.9611\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9568\n",
      "Epoch 00089: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1287 - acc: 0.9567 - val_loss: 0.1510 - val_acc: 0.9595\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9550\n",
      "Epoch 00090: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1334 - acc: 0.9550 - val_loss: 0.1547 - val_acc: 0.9599\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9579\n",
      "Epoch 00091: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1259 - acc: 0.9578 - val_loss: 0.1692 - val_acc: 0.9578\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9572\n",
      "Epoch 00092: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1274 - acc: 0.9572 - val_loss: 0.1711 - val_acc: 0.9590\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9579\n",
      "Epoch 00093: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1268 - acc: 0.9579 - val_loss: 0.1629 - val_acc: 0.9560\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9589\n",
      "Epoch 00094: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1240 - acc: 0.9589 - val_loss: 0.1576 - val_acc: 0.9588\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9585\n",
      "Epoch 00095: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1278 - acc: 0.9585 - val_loss: 0.1629 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9590\n",
      "Epoch 00096: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1215 - acc: 0.9589 - val_loss: 0.1727 - val_acc: 0.9590\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9589\n",
      "Epoch 00097: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1225 - acc: 0.9589 - val_loss: 0.1523 - val_acc: 0.9625\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9580\n",
      "Epoch 00098: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1189 - acc: 0.9580 - val_loss: 0.1602 - val_acc: 0.9592\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9603\n",
      "Epoch 00099: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1151 - acc: 0.9603 - val_loss: 0.1680 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9610\n",
      "Epoch 00100: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1169 - acc: 0.9610 - val_loss: 0.1559 - val_acc: 0.9576\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9622\n",
      "Epoch 00101: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1131 - acc: 0.9622 - val_loss: 0.1653 - val_acc: 0.9597\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9619\n",
      "Epoch 00102: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1128 - acc: 0.9619 - val_loss: 0.1606 - val_acc: 0.9597\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9623\n",
      "Epoch 00103: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1119 - acc: 0.9623 - val_loss: 0.1526 - val_acc: 0.9592\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9614\n",
      "Epoch 00104: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1147 - acc: 0.9614 - val_loss: 0.1548 - val_acc: 0.9602\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9620\n",
      "Epoch 00105: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1111 - acc: 0.9620 - val_loss: 0.1636 - val_acc: 0.9602\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9620\n",
      "Epoch 00106: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1112 - acc: 0.9620 - val_loss: 0.1646 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9621\n",
      "Epoch 00107: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1103 - acc: 0.9622 - val_loss: 0.1621 - val_acc: 0.9588\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9635\n",
      "Epoch 00108: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1070 - acc: 0.9635 - val_loss: 0.1598 - val_acc: 0.9620\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9640\n",
      "Epoch 00109: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1063 - acc: 0.9640 - val_loss: 0.1552 - val_acc: 0.9588\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9645\n",
      "Epoch 00110: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1061 - acc: 0.9645 - val_loss: 0.1600 - val_acc: 0.9613\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9639\n",
      "Epoch 00111: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1074 - acc: 0.9639 - val_loss: 0.1642 - val_acc: 0.9606\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9642\n",
      "Epoch 00112: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1044 - acc: 0.9642 - val_loss: 0.1613 - val_acc: 0.9599\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9654\n",
      "Epoch 00113: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1033 - acc: 0.9654 - val_loss: 0.1545 - val_acc: 0.9627\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9648\n",
      "Epoch 00114: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1052 - acc: 0.9648 - val_loss: 0.1591 - val_acc: 0.9604\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9644\n",
      "Epoch 00115: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1025 - acc: 0.9644 - val_loss: 0.1484 - val_acc: 0.9637\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9664\n",
      "Epoch 00116: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0986 - acc: 0.9664 - val_loss: 0.1486 - val_acc: 0.9630\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9661\n",
      "Epoch 00117: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0991 - acc: 0.9661 - val_loss: 0.1577 - val_acc: 0.9606\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9673\n",
      "Epoch 00118: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0974 - acc: 0.9673 - val_loss: 0.1669 - val_acc: 0.9602\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9670\n",
      "Epoch 00119: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0968 - acc: 0.9670 - val_loss: 0.1531 - val_acc: 0.9618\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9686\n",
      "Epoch 00120: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0948 - acc: 0.9686 - val_loss: 0.1546 - val_acc: 0.9588\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9674\n",
      "Epoch 00121: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0958 - acc: 0.9674 - val_loss: 0.1742 - val_acc: 0.9597\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9676\n",
      "Epoch 00122: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0939 - acc: 0.9676 - val_loss: 0.1671 - val_acc: 0.9576\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9684\n",
      "Epoch 00123: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0958 - acc: 0.9684 - val_loss: 0.1564 - val_acc: 0.9604\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9676\n",
      "Epoch 00124: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0930 - acc: 0.9676 - val_loss: 0.1588 - val_acc: 0.9623\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9691\n",
      "Epoch 00125: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0903 - acc: 0.9691 - val_loss: 0.1529 - val_acc: 0.9606\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9700\n",
      "Epoch 00126: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0907 - acc: 0.9700 - val_loss: 0.1665 - val_acc: 0.9604\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9688\n",
      "Epoch 00127: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0894 - acc: 0.9688 - val_loss: 0.1865 - val_acc: 0.9576\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9697\n",
      "Epoch 00128: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0884 - acc: 0.9697 - val_loss: 0.1916 - val_acc: 0.9576\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9698\n",
      "Epoch 00129: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0894 - acc: 0.9697 - val_loss: 0.1774 - val_acc: 0.9606\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9702\n",
      "Epoch 00130: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0853 - acc: 0.9702 - val_loss: 0.1793 - val_acc: 0.9606\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9708\n",
      "Epoch 00131: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0849 - acc: 0.9708 - val_loss: 0.1676 - val_acc: 0.9583\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9707\n",
      "Epoch 00132: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0885 - acc: 0.9707 - val_loss: 0.1645 - val_acc: 0.9620\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9693\n",
      "Epoch 00133: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0882 - acc: 0.9693 - val_loss: 0.1734 - val_acc: 0.9583\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9727\n",
      "Epoch 00134: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0805 - acc: 0.9727 - val_loss: 0.1743 - val_acc: 0.9599\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9713\n",
      "Epoch 00135: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0859 - acc: 0.9713 - val_loss: 0.1653 - val_acc: 0.9571\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9696\n",
      "Epoch 00136: val_loss did not improve from 0.14820\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0886 - acc: 0.9697 - val_loss: 0.1584 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 667us/sample - loss: 0.1871 - acc: 0.9450\n",
      "Loss: 0.18708164378118664 Accuracy: 0.94496363\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4701 - acc: 0.1823\n",
      "Epoch 00001: val_loss improved from inf to 1.72356, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/001-1.7236.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 2.4701 - acc: 0.1823 - val_loss: 1.7236 - val_acc: 0.4822\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7407 - acc: 0.4269\n",
      "Epoch 00002: val_loss improved from 1.72356 to 1.22502, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/002-1.2250.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.7407 - acc: 0.4270 - val_loss: 1.2250 - val_acc: 0.6303\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3890 - acc: 0.5394\n",
      "Epoch 00003: val_loss improved from 1.22502 to 0.98386, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/003-0.9839.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3889 - acc: 0.5394 - val_loss: 0.9839 - val_acc: 0.7049\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1832 - acc: 0.6067\n",
      "Epoch 00004: val_loss improved from 0.98386 to 0.81413, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/004-0.8141.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1833 - acc: 0.6067 - val_loss: 0.8141 - val_acc: 0.7685\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0292 - acc: 0.6614\n",
      "Epoch 00005: val_loss improved from 0.81413 to 0.67563, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/005-0.6756.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0292 - acc: 0.6614 - val_loss: 0.6756 - val_acc: 0.8001\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8977 - acc: 0.7081\n",
      "Epoch 00006: val_loss improved from 0.67563 to 0.54704, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/006-0.5470.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8976 - acc: 0.7081 - val_loss: 0.5470 - val_acc: 0.8463\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7964 - acc: 0.7430\n",
      "Epoch 00007: val_loss improved from 0.54704 to 0.48195, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/007-0.4819.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7964 - acc: 0.7430 - val_loss: 0.4819 - val_acc: 0.8598\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7058 - acc: 0.7745\n",
      "Epoch 00008: val_loss improved from 0.48195 to 0.40320, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/008-0.4032.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7058 - acc: 0.7745 - val_loss: 0.4032 - val_acc: 0.8852\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6358 - acc: 0.7976\n",
      "Epoch 00009: val_loss improved from 0.40320 to 0.38550, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/009-0.3855.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6357 - acc: 0.7976 - val_loss: 0.3855 - val_acc: 0.8912\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.8151\n",
      "Epoch 00010: val_loss improved from 0.38550 to 0.33215, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/010-0.3322.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5836 - acc: 0.8151 - val_loss: 0.3322 - val_acc: 0.9024\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8302\n",
      "Epoch 00011: val_loss improved from 0.33215 to 0.29559, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/011-0.2956.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5352 - acc: 0.8302 - val_loss: 0.2956 - val_acc: 0.9152\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8432\n",
      "Epoch 00012: val_loss improved from 0.29559 to 0.27488, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/012-0.2749.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4946 - acc: 0.8433 - val_loss: 0.2749 - val_acc: 0.9224\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.8558\n",
      "Epoch 00013: val_loss improved from 0.27488 to 0.25143, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/013-0.2514.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4649 - acc: 0.8558 - val_loss: 0.2514 - val_acc: 0.9315\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4308 - acc: 0.8644\n",
      "Epoch 00014: val_loss improved from 0.25143 to 0.24940, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/014-0.2494.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4307 - acc: 0.8644 - val_loss: 0.2494 - val_acc: 0.9278\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8733\n",
      "Epoch 00015: val_loss improved from 0.24940 to 0.21214, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/015-0.2121.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4070 - acc: 0.8734 - val_loss: 0.2121 - val_acc: 0.9434\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8780\n",
      "Epoch 00016: val_loss improved from 0.21214 to 0.20658, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/016-0.2066.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3890 - acc: 0.8780 - val_loss: 0.2066 - val_acc: 0.9450\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8851\n",
      "Epoch 00017: val_loss did not improve from 0.20658\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3678 - acc: 0.8851 - val_loss: 0.2173 - val_acc: 0.9383\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.8905\n",
      "Epoch 00018: val_loss improved from 0.20658 to 0.19723, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/018-0.1972.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3513 - acc: 0.8905 - val_loss: 0.1972 - val_acc: 0.9474\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8964\n",
      "Epoch 00019: val_loss improved from 0.19723 to 0.18810, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/019-0.1881.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3346 - acc: 0.8965 - val_loss: 0.1881 - val_acc: 0.9453\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8985\n",
      "Epoch 00020: val_loss improved from 0.18810 to 0.17292, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/020-0.1729.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3193 - acc: 0.8985 - val_loss: 0.1729 - val_acc: 0.9522\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9025\n",
      "Epoch 00021: val_loss did not improve from 0.17292\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3092 - acc: 0.9025 - val_loss: 0.1730 - val_acc: 0.9485\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9095\n",
      "Epoch 00022: val_loss improved from 0.17292 to 0.15658, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/022-0.1566.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2958 - acc: 0.9095 - val_loss: 0.1566 - val_acc: 0.9564\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9101\n",
      "Epoch 00023: val_loss did not improve from 0.15658\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2891 - acc: 0.9101 - val_loss: 0.1639 - val_acc: 0.9515\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9147\n",
      "Epoch 00024: val_loss did not improve from 0.15658\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2783 - acc: 0.9147 - val_loss: 0.1626 - val_acc: 0.9525\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9176\n",
      "Epoch 00025: val_loss did not improve from 0.15658\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2652 - acc: 0.9176 - val_loss: 0.1728 - val_acc: 0.9462\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9177\n",
      "Epoch 00026: val_loss improved from 0.15658 to 0.14755, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/026-0.1476.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2641 - acc: 0.9178 - val_loss: 0.1476 - val_acc: 0.9569\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9207\n",
      "Epoch 00027: val_loss did not improve from 0.14755\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2529 - acc: 0.9207 - val_loss: 0.1482 - val_acc: 0.9557\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9240\n",
      "Epoch 00028: val_loss improved from 0.14755 to 0.14395, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/028-0.1440.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2403 - acc: 0.9240 - val_loss: 0.1440 - val_acc: 0.9590\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9250\n",
      "Epoch 00029: val_loss did not improve from 0.14395\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2402 - acc: 0.9250 - val_loss: 0.1552 - val_acc: 0.9546\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9268\n",
      "Epoch 00030: val_loss did not improve from 0.14395\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2314 - acc: 0.9269 - val_loss: 0.1592 - val_acc: 0.9541\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9272\n",
      "Epoch 00031: val_loss improved from 0.14395 to 0.13968, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/031-0.1397.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2271 - acc: 0.9272 - val_loss: 0.1397 - val_acc: 0.9599\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9296\n",
      "Epoch 00032: val_loss did not improve from 0.13968\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2259 - acc: 0.9297 - val_loss: 0.1509 - val_acc: 0.9588\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9329\n",
      "Epoch 00033: val_loss improved from 0.13968 to 0.13407, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/033-0.1341.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2151 - acc: 0.9329 - val_loss: 0.1341 - val_acc: 0.9574\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9332\n",
      "Epoch 00034: val_loss improved from 0.13407 to 0.13298, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/034-0.1330.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2098 - acc: 0.9332 - val_loss: 0.1330 - val_acc: 0.9609\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9344\n",
      "Epoch 00035: val_loss did not improve from 0.13298\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2052 - acc: 0.9344 - val_loss: 0.1330 - val_acc: 0.9618\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9374\n",
      "Epoch 00036: val_loss improved from 0.13298 to 0.12244, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/036-0.1224.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1975 - acc: 0.9374 - val_loss: 0.1224 - val_acc: 0.9639\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9368\n",
      "Epoch 00037: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1968 - acc: 0.9368 - val_loss: 0.1266 - val_acc: 0.9634\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9395\n",
      "Epoch 00038: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1921 - acc: 0.9395 - val_loss: 0.1351 - val_acc: 0.9592\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9410\n",
      "Epoch 00039: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1868 - acc: 0.9410 - val_loss: 0.1234 - val_acc: 0.9653\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9426\n",
      "Epoch 00040: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1811 - acc: 0.9426 - val_loss: 0.1331 - val_acc: 0.9623\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9426\n",
      "Epoch 00041: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1815 - acc: 0.9426 - val_loss: 0.1322 - val_acc: 0.9618\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9447\n",
      "Epoch 00042: val_loss did not improve from 0.12244\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1755 - acc: 0.9446 - val_loss: 0.1236 - val_acc: 0.9641\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9450\n",
      "Epoch 00043: val_loss improved from 0.12244 to 0.12050, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/043-0.1205.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1738 - acc: 0.9450 - val_loss: 0.1205 - val_acc: 0.9646\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9473\n",
      "Epoch 00044: val_loss did not improve from 0.12050\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1650 - acc: 0.9473 - val_loss: 0.1250 - val_acc: 0.9655\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9462\n",
      "Epoch 00045: val_loss improved from 0.12050 to 0.11607, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/045-0.1161.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1685 - acc: 0.9462 - val_loss: 0.1161 - val_acc: 0.9679\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9473\n",
      "Epoch 00046: val_loss did not improve from 0.11607\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1629 - acc: 0.9473 - val_loss: 0.1366 - val_acc: 0.9592\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9492\n",
      "Epoch 00047: val_loss did not improve from 0.11607\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1579 - acc: 0.9492 - val_loss: 0.1451 - val_acc: 0.9618\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9511\n",
      "Epoch 00048: val_loss did not improve from 0.11607\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1504 - acc: 0.9511 - val_loss: 0.1217 - val_acc: 0.9634\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9508\n",
      "Epoch 00049: val_loss did not improve from 0.11607\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1507 - acc: 0.9508 - val_loss: 0.1304 - val_acc: 0.9611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9517\n",
      "Epoch 00050: val_loss improved from 0.11607 to 0.10931, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/050-0.1093.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1487 - acc: 0.9517 - val_loss: 0.1093 - val_acc: 0.9702\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9526\n",
      "Epoch 00051: val_loss did not improve from 0.10931\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1459 - acc: 0.9526 - val_loss: 0.1310 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9538\n",
      "Epoch 00052: val_loss improved from 0.10931 to 0.10816, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/052-0.1082.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1429 - acc: 0.9538 - val_loss: 0.1082 - val_acc: 0.9693\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9542\n",
      "Epoch 00053: val_loss did not improve from 0.10816\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1392 - acc: 0.9542 - val_loss: 0.1192 - val_acc: 0.9604\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9527\n",
      "Epoch 00054: val_loss improved from 0.10816 to 0.10593, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/054-0.1059.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1419 - acc: 0.9527 - val_loss: 0.1059 - val_acc: 0.9676\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9554\n",
      "Epoch 00055: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1386 - acc: 0.9554 - val_loss: 0.1179 - val_acc: 0.9653\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9576\n",
      "Epoch 00056: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1287 - acc: 0.9576 - val_loss: 0.1255 - val_acc: 0.9669\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9564\n",
      "Epoch 00057: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1341 - acc: 0.9564 - val_loss: 0.1117 - val_acc: 0.9660\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9579\n",
      "Epoch 00058: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1306 - acc: 0.9579 - val_loss: 0.1112 - val_acc: 0.9662\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9602\n",
      "Epoch 00059: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1242 - acc: 0.9602 - val_loss: 0.1172 - val_acc: 0.9655\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9602\n",
      "Epoch 00060: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1206 - acc: 0.9602 - val_loss: 0.1292 - val_acc: 0.9653\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9594\n",
      "Epoch 00061: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1229 - acc: 0.9594 - val_loss: 0.1074 - val_acc: 0.9681\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9596\n",
      "Epoch 00062: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1229 - acc: 0.9597 - val_loss: 0.1091 - val_acc: 0.9674\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9617\n",
      "Epoch 00063: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1146 - acc: 0.9617 - val_loss: 0.1219 - val_acc: 0.9627\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9619\n",
      "Epoch 00064: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1178 - acc: 0.9619 - val_loss: 0.1211 - val_acc: 0.9662\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9627\n",
      "Epoch 00065: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1139 - acc: 0.9627 - val_loss: 0.1140 - val_acc: 0.9648\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9630\n",
      "Epoch 00066: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1114 - acc: 0.9630 - val_loss: 0.1204 - val_acc: 0.9679\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9633\n",
      "Epoch 00067: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1112 - acc: 0.9633 - val_loss: 0.1206 - val_acc: 0.9662\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9634\n",
      "Epoch 00068: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1076 - acc: 0.9634 - val_loss: 0.1158 - val_acc: 0.9693\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1060 - acc: 0.9650 - val_loss: 0.1096 - val_acc: 0.9669\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9652\n",
      "Epoch 00070: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1037 - acc: 0.9652 - val_loss: 0.1113 - val_acc: 0.9667\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9646\n",
      "Epoch 00071: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1057 - acc: 0.9646 - val_loss: 0.1292 - val_acc: 0.9672\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9667\n",
      "Epoch 00072: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1005 - acc: 0.9667 - val_loss: 0.1172 - val_acc: 0.9704\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9673\n",
      "Epoch 00073: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0986 - acc: 0.9673 - val_loss: 0.1076 - val_acc: 0.9713\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9673\n",
      "Epoch 00074: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0970 - acc: 0.9673 - val_loss: 0.1321 - val_acc: 0.9618\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9674\n",
      "Epoch 00075: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0976 - acc: 0.9674 - val_loss: 0.1289 - val_acc: 0.9665\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9688\n",
      "Epoch 00076: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0952 - acc: 0.9688 - val_loss: 0.1183 - val_acc: 0.9683\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9693\n",
      "Epoch 00077: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0916 - acc: 0.9693 - val_loss: 0.1179 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9699\n",
      "Epoch 00078: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0913 - acc: 0.9699 - val_loss: 0.1250 - val_acc: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9689\n",
      "Epoch 00079: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0931 - acc: 0.9689 - val_loss: 0.1232 - val_acc: 0.9674\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9712\n",
      "Epoch 00080: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0881 - acc: 0.9713 - val_loss: 0.1070 - val_acc: 0.9688\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9704\n",
      "Epoch 00081: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0883 - acc: 0.9704 - val_loss: 0.1227 - val_acc: 0.9672\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9713\n",
      "Epoch 00082: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0865 - acc: 0.9713 - val_loss: 0.1187 - val_acc: 0.9672\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9708\n",
      "Epoch 00083: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0873 - acc: 0.9708 - val_loss: 0.1193 - val_acc: 0.9709\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9712\n",
      "Epoch 00084: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0840 - acc: 0.9713 - val_loss: 0.1144 - val_acc: 0.9688\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9715\n",
      "Epoch 00085: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0811 - acc: 0.9715 - val_loss: 0.1079 - val_acc: 0.9700\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9715\n",
      "Epoch 00086: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0844 - acc: 0.9715 - val_loss: 0.1325 - val_acc: 0.9611\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9733\n",
      "Epoch 00087: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0789 - acc: 0.9733 - val_loss: 0.1232 - val_acc: 0.9674\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9735\n",
      "Epoch 00088: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0780 - acc: 0.9735 - val_loss: 0.1202 - val_acc: 0.9697\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9736\n",
      "Epoch 00089: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0792 - acc: 0.9736 - val_loss: 0.1193 - val_acc: 0.9681\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9743\n",
      "Epoch 00090: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0784 - acc: 0.9742 - val_loss: 0.1160 - val_acc: 0.9693\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9751\n",
      "Epoch 00091: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0744 - acc: 0.9751 - val_loss: 0.1228 - val_acc: 0.9672\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9752\n",
      "Epoch 00092: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0742 - acc: 0.9752 - val_loss: 0.1288 - val_acc: 0.9662\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9749\n",
      "Epoch 00093: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0739 - acc: 0.9749 - val_loss: 0.1170 - val_acc: 0.9695\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9757\n",
      "Epoch 00094: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0717 - acc: 0.9757 - val_loss: 0.1303 - val_acc: 0.9683\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9764\n",
      "Epoch 00095: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0711 - acc: 0.9764 - val_loss: 0.1321 - val_acc: 0.9683\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9749\n",
      "Epoch 00096: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0752 - acc: 0.9749 - val_loss: 0.1240 - val_acc: 0.9683\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9770\n",
      "Epoch 00097: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0703 - acc: 0.9770 - val_loss: 0.1223 - val_acc: 0.9690\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9767\n",
      "Epoch 00098: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0662 - acc: 0.9767 - val_loss: 0.1306 - val_acc: 0.9672\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9767\n",
      "Epoch 00099: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0679 - acc: 0.9767 - val_loss: 0.1336 - val_acc: 0.9662\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9760\n",
      "Epoch 00100: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0707 - acc: 0.9760 - val_loss: 0.1283 - val_acc: 0.9688\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9773\n",
      "Epoch 00101: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0680 - acc: 0.9773 - val_loss: 0.1243 - val_acc: 0.9669\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9777\n",
      "Epoch 00102: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0669 - acc: 0.9777 - val_loss: 0.1222 - val_acc: 0.9683\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9786\n",
      "Epoch 00103: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0649 - acc: 0.9786 - val_loss: 0.1389 - val_acc: 0.9683\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9789\n",
      "Epoch 00104: val_loss did not improve from 0.10593\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0621 - acc: 0.9789 - val_loss: 0.1354 - val_acc: 0.9688\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.1568 - acc: 0.9539\n",
      "Loss: 0.1568435341142977 Accuracy: 0.9538941\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2397 - acc: 0.2702\n",
      "Epoch 00001: val_loss improved from inf to 1.31218, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/001-1.3122.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 2.2396 - acc: 0.2703 - val_loss: 1.3122 - val_acc: 0.5884\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2745 - acc: 0.5849\n",
      "Epoch 00002: val_loss improved from 1.31218 to 1.01367, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/002-1.0137.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2744 - acc: 0.5849 - val_loss: 1.0137 - val_acc: 0.6713\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0040 - acc: 0.6753\n",
      "Epoch 00003: val_loss improved from 1.01367 to 0.62860, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/003-0.6286.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0040 - acc: 0.6753 - val_loss: 0.6286 - val_acc: 0.8109\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8345 - acc: 0.7321\n",
      "Epoch 00004: val_loss improved from 0.62860 to 0.51982, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/004-0.5198.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8344 - acc: 0.7321 - val_loss: 0.5198 - val_acc: 0.8472\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7752\n",
      "Epoch 00005: val_loss improved from 0.51982 to 0.47580, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/005-0.4758.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7142 - acc: 0.7752 - val_loss: 0.4758 - val_acc: 0.8593\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.8027\n",
      "Epoch 00006: val_loss improved from 0.47580 to 0.40148, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/006-0.4015.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6244 - acc: 0.8027 - val_loss: 0.4015 - val_acc: 0.8758\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8251\n",
      "Epoch 00007: val_loss improved from 0.40148 to 0.33379, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/007-0.3338.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5496 - acc: 0.8251 - val_loss: 0.3338 - val_acc: 0.9052\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.8447\n",
      "Epoch 00008: val_loss improved from 0.33379 to 0.32098, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/008-0.3210.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4965 - acc: 0.8447 - val_loss: 0.3210 - val_acc: 0.9036\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8590\n",
      "Epoch 00009: val_loss improved from 0.32098 to 0.26202, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/009-0.2620.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4476 - acc: 0.8590 - val_loss: 0.2620 - val_acc: 0.9206\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8675\n",
      "Epoch 00010: val_loss did not improve from 0.26202\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4187 - acc: 0.8675 - val_loss: 0.2634 - val_acc: 0.9150\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8802\n",
      "Epoch 00011: val_loss improved from 0.26202 to 0.26164, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/011-0.2616.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3840 - acc: 0.8802 - val_loss: 0.2616 - val_acc: 0.9201\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8913\n",
      "Epoch 00012: val_loss improved from 0.26164 to 0.21395, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/012-0.2139.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3521 - acc: 0.8913 - val_loss: 0.2139 - val_acc: 0.9380\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8966\n",
      "Epoch 00013: val_loss did not improve from 0.21395\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3290 - acc: 0.8966 - val_loss: 0.2238 - val_acc: 0.9304\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.9024\n",
      "Epoch 00014: val_loss improved from 0.21395 to 0.18933, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/014-0.1893.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3143 - acc: 0.9024 - val_loss: 0.1893 - val_acc: 0.9413\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9104\n",
      "Epoch 00015: val_loss improved from 0.18933 to 0.18294, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/015-0.1829.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2916 - acc: 0.9104 - val_loss: 0.1829 - val_acc: 0.9443\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9145\n",
      "Epoch 00016: val_loss improved from 0.18294 to 0.17394, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/016-0.1739.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2716 - acc: 0.9145 - val_loss: 0.1739 - val_acc: 0.9453\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9173\n",
      "Epoch 00017: val_loss improved from 0.17394 to 0.16497, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/017-0.1650.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2612 - acc: 0.9173 - val_loss: 0.1650 - val_acc: 0.9509\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9241\n",
      "Epoch 00018: val_loss improved from 0.16497 to 0.14363, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/018-0.1436.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2423 - acc: 0.9241 - val_loss: 0.1436 - val_acc: 0.9597\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9271\n",
      "Epoch 00019: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2301 - acc: 0.9271 - val_loss: 0.1557 - val_acc: 0.9532\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9305\n",
      "Epoch 00020: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2201 - acc: 0.9305 - val_loss: 0.1510 - val_acc: 0.9562\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9331\n",
      "Epoch 00021: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2120 - acc: 0.9331 - val_loss: 0.1441 - val_acc: 0.9557\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9347\n",
      "Epoch 00022: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2052 - acc: 0.9347 - val_loss: 0.1539 - val_acc: 0.9539\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9377\n",
      "Epoch 00023: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1957 - acc: 0.9377 - val_loss: 0.1559 - val_acc: 0.9590\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9416\n",
      "Epoch 00024: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1830 - acc: 0.9416 - val_loss: 0.1544 - val_acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9427\n",
      "Epoch 00025: val_loss did not improve from 0.14363\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1810 - acc: 0.9427 - val_loss: 0.1446 - val_acc: 0.9590\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9448\n",
      "Epoch 00026: val_loss improved from 0.14363 to 0.12812, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/026-0.1281.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1712 - acc: 0.9448 - val_loss: 0.1281 - val_acc: 0.9625\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9480\n",
      "Epoch 00027: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1645 - acc: 0.9480 - val_loss: 0.1301 - val_acc: 0.9625\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9488\n",
      "Epoch 00028: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1572 - acc: 0.9488 - val_loss: 0.1403 - val_acc: 0.9592\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9521\n",
      "Epoch 00029: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1508 - acc: 0.9521 - val_loss: 0.1409 - val_acc: 0.9595\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9507\n",
      "Epoch 00030: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1519 - acc: 0.9507 - val_loss: 0.1508 - val_acc: 0.9557\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9542\n",
      "Epoch 00031: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1406 - acc: 0.9542 - val_loss: 0.1450 - val_acc: 0.9611\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9552\n",
      "Epoch 00032: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1382 - acc: 0.9552 - val_loss: 0.1283 - val_acc: 0.9616\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9585\n",
      "Epoch 00033: val_loss did not improve from 0.12812\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1283 - acc: 0.9585 - val_loss: 0.1391 - val_acc: 0.9611\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9603\n",
      "Epoch 00034: val_loss improved from 0.12812 to 0.12720, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/034-0.1272.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1234 - acc: 0.9603 - val_loss: 0.1272 - val_acc: 0.9606\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9615\n",
      "Epoch 00035: val_loss did not improve from 0.12720\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1169 - acc: 0.9615 - val_loss: 0.1317 - val_acc: 0.9616\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9621\n",
      "Epoch 00036: val_loss did not improve from 0.12720\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1176 - acc: 0.9621 - val_loss: 0.1395 - val_acc: 0.9627\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9590\n",
      "Epoch 00037: val_loss improved from 0.12720 to 0.11858, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/037-0.1186.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1252 - acc: 0.9591 - val_loss: 0.1186 - val_acc: 0.9653\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9640\n",
      "Epoch 00038: val_loss did not improve from 0.11858\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1106 - acc: 0.9640 - val_loss: 0.1237 - val_acc: 0.9632\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9669\n",
      "Epoch 00039: val_loss did not improve from 0.11858\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1032 - acc: 0.9669 - val_loss: 0.1309 - val_acc: 0.9625\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9671\n",
      "Epoch 00040: val_loss did not improve from 0.11858\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1027 - acc: 0.9671 - val_loss: 0.1324 - val_acc: 0.9625\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9668\n",
      "Epoch 00041: val_loss did not improve from 0.11858\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1010 - acc: 0.9668 - val_loss: 0.1266 - val_acc: 0.9648\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9689\n",
      "Epoch 00042: val_loss improved from 0.11858 to 0.11815, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/042-0.1182.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0944 - acc: 0.9689 - val_loss: 0.1182 - val_acc: 0.9669\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9691\n",
      "Epoch 00043: val_loss did not improve from 0.11815\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0918 - acc: 0.9691 - val_loss: 0.1321 - val_acc: 0.9627\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9702\n",
      "Epoch 00044: val_loss did not improve from 0.11815\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0898 - acc: 0.9702 - val_loss: 0.1239 - val_acc: 0.9669\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9712\n",
      "Epoch 00045: val_loss did not improve from 0.11815\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0875 - acc: 0.9713 - val_loss: 0.1394 - val_acc: 0.9625\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9721\n",
      "Epoch 00046: val_loss improved from 0.11815 to 0.11182, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/046-0.1118.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0857 - acc: 0.9721 - val_loss: 0.1118 - val_acc: 0.9704\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9732\n",
      "Epoch 00047: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0809 - acc: 0.9732 - val_loss: 0.1221 - val_acc: 0.9641\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9717\n",
      "Epoch 00048: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0837 - acc: 0.9717 - val_loss: 0.1398 - val_acc: 0.9669\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9741\n",
      "Epoch 00049: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0767 - acc: 0.9741 - val_loss: 0.1762 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9752\n",
      "Epoch 00050: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0762 - acc: 0.9752 - val_loss: 0.1317 - val_acc: 0.9655\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9760\n",
      "Epoch 00051: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0725 - acc: 0.9760 - val_loss: 0.1525 - val_acc: 0.9611\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9746\n",
      "Epoch 00052: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0736 - acc: 0.9746 - val_loss: 0.1352 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9759\n",
      "Epoch 00053: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0735 - acc: 0.9759 - val_loss: 0.1425 - val_acc: 0.9639\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9776\n",
      "Epoch 00054: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0692 - acc: 0.9776 - val_loss: 0.1459 - val_acc: 0.9646\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9765\n",
      "Epoch 00055: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0704 - acc: 0.9765 - val_loss: 0.1266 - val_acc: 0.9665\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9786\n",
      "Epoch 00056: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0624 - acc: 0.9786 - val_loss: 0.1355 - val_acc: 0.9658\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9789\n",
      "Epoch 00057: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0644 - acc: 0.9789 - val_loss: 0.1278 - val_acc: 0.9690\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9783\n",
      "Epoch 00058: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0635 - acc: 0.9783 - val_loss: 0.1338 - val_acc: 0.9674\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9802\n",
      "Epoch 00059: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0591 - acc: 0.9802 - val_loss: 0.1460 - val_acc: 0.9641\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9787\n",
      "Epoch 00060: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0608 - acc: 0.9787 - val_loss: 0.1298 - val_acc: 0.9676\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9815\n",
      "Epoch 00061: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0562 - acc: 0.9815 - val_loss: 0.1295 - val_acc: 0.9662\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9799\n",
      "Epoch 00062: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0582 - acc: 0.9799 - val_loss: 0.1294 - val_acc: 0.9697\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9814\n",
      "Epoch 00063: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0559 - acc: 0.9814 - val_loss: 0.1184 - val_acc: 0.9669\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9825\n",
      "Epoch 00064: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0547 - acc: 0.9825 - val_loss: 0.1453 - val_acc: 0.9651\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9818\n",
      "Epoch 00065: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0534 - acc: 0.9819 - val_loss: 0.1240 - val_acc: 0.9681\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9836\n",
      "Epoch 00066: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0486 - acc: 0.9836 - val_loss: 0.1385 - val_acc: 0.9662\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9836\n",
      "Epoch 00067: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0486 - acc: 0.9836 - val_loss: 0.1339 - val_acc: 0.9676\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9832\n",
      "Epoch 00068: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0482 - acc: 0.9832 - val_loss: 0.1308 - val_acc: 0.9688\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9833\n",
      "Epoch 00069: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0495 - acc: 0.9833 - val_loss: 0.1375 - val_acc: 0.9686\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9843\n",
      "Epoch 00070: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0454 - acc: 0.9843 - val_loss: 0.1502 - val_acc: 0.9646\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9847\n",
      "Epoch 00071: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0456 - acc: 0.9847 - val_loss: 0.1371 - val_acc: 0.9669\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9849\n",
      "Epoch 00072: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0462 - acc: 0.9849 - val_loss: 0.1336 - val_acc: 0.9690\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9845\n",
      "Epoch 00073: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0463 - acc: 0.9845 - val_loss: 0.1324 - val_acc: 0.9667\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9856\n",
      "Epoch 00074: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0417 - acc: 0.9856 - val_loss: 0.1366 - val_acc: 0.9660\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9851\n",
      "Epoch 00075: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0450 - acc: 0.9851 - val_loss: 0.1237 - val_acc: 0.9720\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9856\n",
      "Epoch 00076: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0409 - acc: 0.9856 - val_loss: 0.1351 - val_acc: 0.9676\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9880\n",
      "Epoch 00077: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0372 - acc: 0.9880 - val_loss: 0.1367 - val_acc: 0.9700\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9860\n",
      "Epoch 00078: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0403 - acc: 0.9860 - val_loss: 0.1484 - val_acc: 0.9686\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9869\n",
      "Epoch 00079: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0381 - acc: 0.9869 - val_loss: 0.1377 - val_acc: 0.9690\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9868\n",
      "Epoch 00080: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0379 - acc: 0.9868 - val_loss: 0.1420 - val_acc: 0.9711\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9869\n",
      "Epoch 00081: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0376 - acc: 0.9869 - val_loss: 0.1197 - val_acc: 0.9720\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9885\n",
      "Epoch 00082: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0356 - acc: 0.9885 - val_loss: 0.1412 - val_acc: 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9882\n",
      "Epoch 00083: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0357 - acc: 0.9882 - val_loss: 0.1699 - val_acc: 0.9662\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9880\n",
      "Epoch 00084: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0369 - acc: 0.9880 - val_loss: 0.1300 - val_acc: 0.9709\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9880\n",
      "Epoch 00085: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0360 - acc: 0.9880 - val_loss: 0.1457 - val_acc: 0.9706\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9874\n",
      "Epoch 00086: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0371 - acc: 0.9874 - val_loss: 0.1546 - val_acc: 0.9709\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9893\n",
      "Epoch 00087: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0322 - acc: 0.9893 - val_loss: 0.1472 - val_acc: 0.9676\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9892\n",
      "Epoch 00088: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0322 - acc: 0.9892 - val_loss: 0.1557 - val_acc: 0.9686\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9887\n",
      "Epoch 00089: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0342 - acc: 0.9888 - val_loss: 0.1680 - val_acc: 0.9672\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9893\n",
      "Epoch 00090: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0312 - acc: 0.9893 - val_loss: 0.1645 - val_acc: 0.9700\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9889\n",
      "Epoch 00091: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0324 - acc: 0.9889 - val_loss: 0.1514 - val_acc: 0.9681\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9889\n",
      "Epoch 00092: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0324 - acc: 0.9889 - val_loss: 0.1628 - val_acc: 0.9667\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9896\n",
      "Epoch 00093: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0302 - acc: 0.9896 - val_loss: 0.1645 - val_acc: 0.9688\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9905\n",
      "Epoch 00094: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0284 - acc: 0.9905 - val_loss: 0.1485 - val_acc: 0.9716\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9903\n",
      "Epoch 00095: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0299 - acc: 0.9903 - val_loss: 0.1454 - val_acc: 0.9718\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9908\n",
      "Epoch 00096: val_loss did not improve from 0.11182\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0278 - acc: 0.9908 - val_loss: 0.1580 - val_acc: 0.9653\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 666us/sample - loss: 0.1723 - acc: 0.9522\n",
      "Loss: 0.17226057962687572 Accuracy: 0.9522326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_075_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 394us/sample - loss: 2.1406 - acc: 0.3267\n",
      "Loss: 2.140593708267093 Accuracy: 0.32668743\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 555us/sample - loss: 1.8014 - acc: 0.4496\n",
      "Loss: 1.8014415986812746 Accuracy: 0.44963655\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 620us/sample - loss: 1.3782 - acc: 0.5736\n",
      "Loss: 1.3782013468529453 Accuracy: 0.5736241\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 670us/sample - loss: 0.8739 - acc: 0.7533\n",
      "Loss: 0.8739451072173946 Accuracy: 0.75327104\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 696us/sample - loss: 0.6760 - acc: 0.8116\n",
      "Loss: 0.6760014045151842 Accuracy: 0.8116303\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 718us/sample - loss: 0.3295 - acc: 0.9061\n",
      "Loss: 0.3295267518683262 Accuracy: 0.9061267\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 699us/sample - loss: 0.1871 - acc: 0.9450\n",
      "Loss: 0.18708164378118664 Accuracy: 0.94496363\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.1568 - acc: 0.9539\n",
      "Loss: 0.1568435341142977 Accuracy: 0.9538941\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 735us/sample - loss: 0.1723 - acc: 0.9522\n",
      "Loss: 0.17226057962687572 Accuracy: 0.9522326\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 465us/sample - loss: 3.9341 - acc: 0.3194\n",
      "Loss: 3.9341378256539317 Accuracy: 0.3194185\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 607us/sample - loss: 3.1506 - acc: 0.4619\n",
      "Loss: 3.150581363626482 Accuracy: 0.46188992\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 681us/sample - loss: 1.6772 - acc: 0.6430\n",
      "Loss: 1.677210448736466 Accuracy: 0.64299065\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 715us/sample - loss: 0.9493 - acc: 0.7796\n",
      "Loss: 0.9492817862135351 Accuracy: 0.77964693\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 714us/sample - loss: 0.7412 - acc: 0.8351\n",
      "Loss: 0.7411662958368953 Accuracy: 0.8350986\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 748us/sample - loss: 0.3375 - acc: 0.9153\n",
      "Loss: 0.3374960224112246 Accuracy: 0.9152648\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 840us/sample - loss: 0.2065 - acc: 0.9472\n",
      "Loss: 0.20650995333146147 Accuracy: 0.94724816\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 799us/sample - loss: 0.1644 - acc: 0.9589\n",
      "Loss: 0.16439500707535312 Accuracy: 0.9588785\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 807us/sample - loss: 0.2270 - acc: 0.9553\n",
      "Loss: 0.22700213228012933 Accuracy: 0.9553479\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
