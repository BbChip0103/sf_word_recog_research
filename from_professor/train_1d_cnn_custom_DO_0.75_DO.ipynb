{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_075_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_075_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4769 - acc: 0.2231\n",
      "Epoch 00001: val_loss improved from inf to 2.30056, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/001-2.3006.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 2.4769 - acc: 0.2231 - val_loss: 2.3006 - val_acc: 0.2821\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0581 - acc: 0.3920\n",
      "Epoch 00002: val_loss improved from 2.30056 to 2.17512, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/002-2.1751.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 2.0582 - acc: 0.3920 - val_loss: 2.1751 - val_acc: 0.3235\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7870 - acc: 0.4780\n",
      "Epoch 00003: val_loss improved from 2.17512 to 2.13879, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/003-2.1388.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 1.7870 - acc: 0.4780 - val_loss: 2.1388 - val_acc: 0.3417\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5838 - acc: 0.5384\n",
      "Epoch 00004: val_loss improved from 2.13879 to 2.12853, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_1_conv_checkpoint/004-2.1285.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 1.5837 - acc: 0.5385 - val_loss: 2.1285 - val_acc: 0.3473\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4196 - acc: 0.5883\n",
      "Epoch 00005: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 1.4197 - acc: 0.5883 - val_loss: 2.1697 - val_acc: 0.3312\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2914 - acc: 0.6292\n",
      "Epoch 00006: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.2913 - acc: 0.6293 - val_loss: 2.1694 - val_acc: 0.3503\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1805 - acc: 0.6623\n",
      "Epoch 00007: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 1.1805 - acc: 0.6623 - val_loss: 2.2142 - val_acc: 0.3364\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0822 - acc: 0.6938\n",
      "Epoch 00008: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.0821 - acc: 0.6938 - val_loss: 2.2323 - val_acc: 0.3431\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0031 - acc: 0.7171\n",
      "Epoch 00009: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 1.0031 - acc: 0.7171 - val_loss: 2.2921 - val_acc: 0.3371\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.7400\n",
      "Epoch 00010: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.9269 - acc: 0.7400 - val_loss: 2.3588 - val_acc: 0.3322\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7612\n",
      "Epoch 00011: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.8605 - acc: 0.7612 - val_loss: 2.3561 - val_acc: 0.3359\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8029 - acc: 0.7794\n",
      "Epoch 00012: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.8029 - acc: 0.7794 - val_loss: 2.3811 - val_acc: 0.3406\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7499 - acc: 0.7927\n",
      "Epoch 00013: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.7498 - acc: 0.7928 - val_loss: 2.4547 - val_acc: 0.3324\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6980 - acc: 0.8093\n",
      "Epoch 00014: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.6981 - acc: 0.8093 - val_loss: 2.4847 - val_acc: 0.3326\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6568 - acc: 0.8228\n",
      "Epoch 00015: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.6567 - acc: 0.8228 - val_loss: 2.5425 - val_acc: 0.3347\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8349\n",
      "Epoch 00016: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.6171 - acc: 0.8349 - val_loss: 2.5706 - val_acc: 0.3385\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.8454\n",
      "Epoch 00017: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.5792 - acc: 0.8453 - val_loss: 2.6100 - val_acc: 0.3350\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5507 - acc: 0.8538\n",
      "Epoch 00018: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.5507 - acc: 0.8538 - val_loss: 2.6346 - val_acc: 0.3389\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8612\n",
      "Epoch 00019: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5203 - acc: 0.8612 - val_loss: 2.6839 - val_acc: 0.3422\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8723\n",
      "Epoch 00020: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4912 - acc: 0.8724 - val_loss: 2.7534 - val_acc: 0.3357\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8808\n",
      "Epoch 00021: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4611 - acc: 0.8808 - val_loss: 2.7884 - val_acc: 0.3373\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8836\n",
      "Epoch 00022: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.4438 - acc: 0.8836 - val_loss: 2.8427 - val_acc: 0.3345\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8900\n",
      "Epoch 00023: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4206 - acc: 0.8900 - val_loss: 2.8874 - val_acc: 0.3345\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8960\n",
      "Epoch 00024: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4032 - acc: 0.8959 - val_loss: 2.8986 - val_acc: 0.3382\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.9054\n",
      "Epoch 00025: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.3736 - acc: 0.9054 - val_loss: 2.9531 - val_acc: 0.3336\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.9068\n",
      "Epoch 00026: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3600 - acc: 0.9068 - val_loss: 2.9877 - val_acc: 0.3359\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.9104\n",
      "Epoch 00027: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3476 - acc: 0.9104 - val_loss: 3.0464 - val_acc: 0.3443\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.9164\n",
      "Epoch 00028: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3351 - acc: 0.9163 - val_loss: 3.0473 - val_acc: 0.3329\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.9189\n",
      "Epoch 00029: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.3190 - acc: 0.9190 - val_loss: 3.1002 - val_acc: 0.3371\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9230\n",
      "Epoch 00030: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3062 - acc: 0.9230 - val_loss: 3.1438 - val_acc: 0.3382\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9267\n",
      "Epoch 00031: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2901 - acc: 0.9266 - val_loss: 3.1976 - val_acc: 0.3336\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9298\n",
      "Epoch 00032: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2778 - acc: 0.9297 - val_loss: 3.2036 - val_acc: 0.3359\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9331\n",
      "Epoch 00033: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.2688 - acc: 0.9331 - val_loss: 3.2863 - val_acc: 0.3340\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9327\n",
      "Epoch 00034: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2627 - acc: 0.9328 - val_loss: 3.2641 - val_acc: 0.3326\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9369\n",
      "Epoch 00035: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2521 - acc: 0.9369 - val_loss: 3.3259 - val_acc: 0.3296\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9392\n",
      "Epoch 00036: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2422 - acc: 0.9391 - val_loss: 3.3383 - val_acc: 0.3380\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9426\n",
      "Epoch 00037: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2312 - acc: 0.9425 - val_loss: 3.3739 - val_acc: 0.3331\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9433\n",
      "Epoch 00038: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2283 - acc: 0.9434 - val_loss: 3.4099 - val_acc: 0.3366\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9466\n",
      "Epoch 00039: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2204 - acc: 0.9466 - val_loss: 3.4486 - val_acc: 0.3343\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9484\n",
      "Epoch 00040: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2118 - acc: 0.9484 - val_loss: 3.4753 - val_acc: 0.3303\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9496\n",
      "Epoch 00041: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2036 - acc: 0.9495 - val_loss: 3.5010 - val_acc: 0.3364\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9504\n",
      "Epoch 00042: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2000 - acc: 0.9504 - val_loss: 3.5337 - val_acc: 0.3371\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9520\n",
      "Epoch 00043: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1913 - acc: 0.9519 - val_loss: 3.5538 - val_acc: 0.3347\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9555\n",
      "Epoch 00044: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1834 - acc: 0.9555 - val_loss: 3.6099 - val_acc: 0.3324\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9558\n",
      "Epoch 00045: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1812 - acc: 0.9558 - val_loss: 3.6413 - val_acc: 0.3317\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9565\n",
      "Epoch 00046: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1780 - acc: 0.9565 - val_loss: 3.6797 - val_acc: 0.3289\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9583\n",
      "Epoch 00047: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1706 - acc: 0.9583 - val_loss: 3.7066 - val_acc: 0.3345\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9599\n",
      "Epoch 00048: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1672 - acc: 0.9599 - val_loss: 3.7195 - val_acc: 0.3350\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 00049: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1620 - acc: 0.9616 - val_loss: 3.7719 - val_acc: 0.3312\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9623\n",
      "Epoch 00050: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1578 - acc: 0.9623 - val_loss: 3.7944 - val_acc: 0.3301\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9627\n",
      "Epoch 00051: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1574 - acc: 0.9627 - val_loss: 3.8178 - val_acc: 0.3336\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9635\n",
      "Epoch 00052: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1524 - acc: 0.9635 - val_loss: 3.8307 - val_acc: 0.3317\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9639\n",
      "Epoch 00053: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1484 - acc: 0.9639 - val_loss: 3.8227 - val_acc: 0.3396\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9667\n",
      "Epoch 00054: val_loss did not improve from 2.12853\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1431 - acc: 0.9667 - val_loss: 3.8824 - val_acc: 0.3287\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 288us/sample - loss: 2.1545 - acc: 0.3225\n",
      "Loss: 2.154451996168491 Accuracy: 0.32253376\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2736 - acc: 0.2959\n",
      "Epoch 00001: val_loss improved from inf to 1.97886, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/001-1.9789.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 2.2735 - acc: 0.2959 - val_loss: 1.9789 - val_acc: 0.4002\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8155 - acc: 0.4528\n",
      "Epoch 00002: val_loss improved from 1.97886 to 1.80297, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/002-1.8030.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.8157 - acc: 0.4528 - val_loss: 1.8030 - val_acc: 0.4386\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6011 - acc: 0.5188\n",
      "Epoch 00003: val_loss improved from 1.80297 to 1.73865, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/003-1.7386.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.6009 - acc: 0.5189 - val_loss: 1.7386 - val_acc: 0.4642\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4453 - acc: 0.5696\n",
      "Epoch 00004: val_loss improved from 1.73865 to 1.73611, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/004-1.7361.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.4453 - acc: 0.5696 - val_loss: 1.7361 - val_acc: 0.4687\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3138 - acc: 0.6096\n",
      "Epoch 00005: val_loss improved from 1.73611 to 1.72297, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_2_conv_checkpoint/005-1.7230.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.3139 - acc: 0.6096 - val_loss: 1.7230 - val_acc: 0.4736\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2016 - acc: 0.6398\n",
      "Epoch 00006: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.2015 - acc: 0.6398 - val_loss: 1.7534 - val_acc: 0.4642\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1074 - acc: 0.6671\n",
      "Epoch 00007: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.1073 - acc: 0.6672 - val_loss: 1.7700 - val_acc: 0.4708\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0246 - acc: 0.6916\n",
      "Epoch 00008: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.0247 - acc: 0.6916 - val_loss: 1.8021 - val_acc: 0.4584\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9430 - acc: 0.7156\n",
      "Epoch 00009: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.9431 - acc: 0.7156 - val_loss: 1.8132 - val_acc: 0.4731\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8780 - acc: 0.7354\n",
      "Epoch 00010: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.8780 - acc: 0.7354 - val_loss: 1.8513 - val_acc: 0.4689\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7536\n",
      "Epoch 00011: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.8148 - acc: 0.7536 - val_loss: 1.8519 - val_acc: 0.4694\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7534 - acc: 0.7735\n",
      "Epoch 00012: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.7533 - acc: 0.7735 - val_loss: 1.8922 - val_acc: 0.4619\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7056 - acc: 0.7854- ETA: 1s - loss\n",
      "Epoch 00013: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.7056 - acc: 0.7853 - val_loss: 1.9229 - val_acc: 0.4638\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6567 - acc: 0.8051\n",
      "Epoch 00014: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.6567 - acc: 0.8052 - val_loss: 1.9361 - val_acc: 0.4752\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6116 - acc: 0.8150- ETA: 1s - loss: \n",
      "Epoch 00015: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.6117 - acc: 0.8150 - val_loss: 1.9728 - val_acc: 0.4649\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5753 - acc: 0.8255\n",
      "Epoch 00016: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.5752 - acc: 0.8255 - val_loss: 2.0442 - val_acc: 0.4733\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8375\n",
      "Epoch 00017: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.5340 - acc: 0.8375 - val_loss: 2.0503 - val_acc: 0.4701\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8475\n",
      "Epoch 00018: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.5009 - acc: 0.8475 - val_loss: 2.0766 - val_acc: 0.4694\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8550\n",
      "Epoch 00019: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.4717 - acc: 0.8550 - val_loss: 2.0940 - val_acc: 0.4740\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8664\n",
      "Epoch 00020: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.4411 - acc: 0.8664 - val_loss: 2.1553 - val_acc: 0.4794\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8707\n",
      "Epoch 00021: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.4231 - acc: 0.8706 - val_loss: 2.1681 - val_acc: 0.4771\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8805\n",
      "Epoch 00022: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3969 - acc: 0.8805 - val_loss: 2.2344 - val_acc: 0.4750\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8845\n",
      "Epoch 00023: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3784 - acc: 0.8845 - val_loss: 2.2430 - val_acc: 0.4766\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8924\n",
      "Epoch 00024: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3564 - acc: 0.8924 - val_loss: 2.2795 - val_acc: 0.4764\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8986\n",
      "Epoch 00025: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3365 - acc: 0.8985 - val_loss: 2.3260 - val_acc: 0.4766\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9002\n",
      "Epoch 00026: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3262 - acc: 0.9001 - val_loss: 2.3291 - val_acc: 0.4778\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9081\n",
      "Epoch 00027: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3082 - acc: 0.9081 - val_loss: 2.3907 - val_acc: 0.4747\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9114\n",
      "Epoch 00028: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2971 - acc: 0.9114 - val_loss: 2.3612 - val_acc: 0.4808\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9143\n",
      "Epoch 00029: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2836 - acc: 0.9143 - val_loss: 2.4072 - val_acc: 0.4785\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9177\n",
      "Epoch 00030: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2727 - acc: 0.9177 - val_loss: 2.4254 - val_acc: 0.4831\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9211\n",
      "Epoch 00031: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2622 - acc: 0.9210 - val_loss: 2.4530 - val_acc: 0.4840\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9247\n",
      "Epoch 00032: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2503 - acc: 0.9247 - val_loss: 2.4493 - val_acc: 0.4836\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9260\n",
      "Epoch 00033: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2439 - acc: 0.9260 - val_loss: 2.4754 - val_acc: 0.4882\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9300\n",
      "Epoch 00034: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2352 - acc: 0.9299 - val_loss: 2.5397 - val_acc: 0.4847\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9320\n",
      "Epoch 00035: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2239 - acc: 0.9320 - val_loss: 2.5268 - val_acc: 0.4866\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9377\n",
      "Epoch 00036: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2114 - acc: 0.9377 - val_loss: 2.5594 - val_acc: 0.4922\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9367\n",
      "Epoch 00037: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2112 - acc: 0.9367 - val_loss: 2.6032 - val_acc: 0.4922\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9405\n",
      "Epoch 00038: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2000 - acc: 0.9404 - val_loss: 2.6285 - val_acc: 0.4917\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9423\n",
      "Epoch 00039: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1965 - acc: 0.9423 - val_loss: 2.6384 - val_acc: 0.4910\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9440\n",
      "Epoch 00040: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1926 - acc: 0.9440 - val_loss: 2.6327 - val_acc: 0.4922\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9468\n",
      "Epoch 00041: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1824 - acc: 0.9468 - val_loss: 2.6523 - val_acc: 0.4962\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9495\n",
      "Epoch 00042: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1725 - acc: 0.9495 - val_loss: 2.6726 - val_acc: 0.4957\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9485\n",
      "Epoch 00043: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1762 - acc: 0.9485 - val_loss: 2.7383 - val_acc: 0.4964\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9517\n",
      "Epoch 00044: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1710 - acc: 0.9517 - val_loss: 2.7020 - val_acc: 0.4976\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9496\n",
      "Epoch 00045: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1693 - acc: 0.9497 - val_loss: 2.7501 - val_acc: 0.4922\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9524\n",
      "Epoch 00046: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1626 - acc: 0.9524 - val_loss: 2.7594 - val_acc: 0.4948\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9555\n",
      "Epoch 00047: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1535 - acc: 0.9555 - val_loss: 2.7645 - val_acc: 0.4976\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9586\n",
      "Epoch 00048: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1477 - acc: 0.9586 - val_loss: 2.7995 - val_acc: 0.5022\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9583\n",
      "Epoch 00049: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1494 - acc: 0.9583 - val_loss: 2.8118 - val_acc: 0.4962\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9579\n",
      "Epoch 00050: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1471 - acc: 0.9579 - val_loss: 2.8418 - val_acc: 0.5015\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9590\n",
      "Epoch 00051: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1444 - acc: 0.9590 - val_loss: 2.8430 - val_acc: 0.4941\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9588\n",
      "Epoch 00052: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1414 - acc: 0.9587 - val_loss: 2.8289 - val_acc: 0.4922\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9591\n",
      "Epoch 00053: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1401 - acc: 0.9591 - val_loss: 2.8465 - val_acc: 0.4964\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9632\n",
      "Epoch 00054: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1316 - acc: 0.9632 - val_loss: 2.8574 - val_acc: 0.4990\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9609\n",
      "Epoch 00055: val_loss did not improve from 1.72297\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1351 - acc: 0.9609 - val_loss: 2.8918 - val_acc: 0.5031\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 473us/sample - loss: 1.7904 - acc: 0.4538\n",
      "Loss: 1.7904148946051037 Accuracy: 0.45379025\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2082 - acc: 0.3095\n",
      "Epoch 00001: val_loss improved from inf to 1.78154, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/001-1.7815.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.2082 - acc: 0.3095 - val_loss: 1.7815 - val_acc: 0.4542\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6315 - acc: 0.4962\n",
      "Epoch 00002: val_loss improved from 1.78154 to 1.53582, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/002-1.5358.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.6314 - acc: 0.4962 - val_loss: 1.5358 - val_acc: 0.5311\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4107 - acc: 0.5666\n",
      "Epoch 00003: val_loss improved from 1.53582 to 1.48463, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/003-1.4846.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4108 - acc: 0.5666 - val_loss: 1.4846 - val_acc: 0.5369\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2835 - acc: 0.6025\n",
      "Epoch 00004: val_loss improved from 1.48463 to 1.40510, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/004-1.4051.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2834 - acc: 0.6025 - val_loss: 1.4051 - val_acc: 0.5695\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1842 - acc: 0.6324\n",
      "Epoch 00005: val_loss improved from 1.40510 to 1.36065, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/005-1.3606.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1842 - acc: 0.6324 - val_loss: 1.3606 - val_acc: 0.5849\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1042 - acc: 0.6568\n",
      "Epoch 00006: val_loss improved from 1.36065 to 1.31837, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/006-1.3184.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1042 - acc: 0.6568 - val_loss: 1.3184 - val_acc: 0.5942\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0231 - acc: 0.6830\n",
      "Epoch 00007: val_loss did not improve from 1.31837\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0230 - acc: 0.6830 - val_loss: 1.3522 - val_acc: 0.5861\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9625 - acc: 0.6996\n",
      "Epoch 00008: val_loss did not improve from 1.31837\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9625 - acc: 0.6996 - val_loss: 1.3336 - val_acc: 0.5910\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8987 - acc: 0.7193\n",
      "Epoch 00009: val_loss improved from 1.31837 to 1.31238, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/009-1.3124.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8987 - acc: 0.7194 - val_loss: 1.3124 - val_acc: 0.6115\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.7345\n",
      "Epoch 00010: val_loss improved from 1.31238 to 1.29989, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_3_conv_checkpoint/010-1.2999.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8444 - acc: 0.7345 - val_loss: 1.2999 - val_acc: 0.6140\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7972 - acc: 0.7481\n",
      "Epoch 00011: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7973 - acc: 0.7480 - val_loss: 1.3394 - val_acc: 0.6103\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7546 - acc: 0.7629\n",
      "Epoch 00012: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7547 - acc: 0.7629 - val_loss: 1.3219 - val_acc: 0.6094\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.7756\n",
      "Epoch 00013: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7081 - acc: 0.7755 - val_loss: 1.3205 - val_acc: 0.6191\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6752 - acc: 0.7851\n",
      "Epoch 00014: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6752 - acc: 0.7851 - val_loss: 1.3168 - val_acc: 0.6254\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.7937\n",
      "Epoch 00015: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6459 - acc: 0.7937 - val_loss: 1.3213 - val_acc: 0.6247\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6077 - acc: 0.8077\n",
      "Epoch 00016: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6077 - acc: 0.8077 - val_loss: 1.3293 - val_acc: 0.6212\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8123\n",
      "Epoch 00017: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5885 - acc: 0.8123 - val_loss: 1.3436 - val_acc: 0.6306\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8204\n",
      "Epoch 00018: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5541 - acc: 0.8203 - val_loss: 1.3459 - val_acc: 0.6299\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.8259\n",
      "Epoch 00019: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5340 - acc: 0.8259 - val_loss: 1.3507 - val_acc: 0.6341\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5136 - acc: 0.8369\n",
      "Epoch 00020: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5136 - acc: 0.8369 - val_loss: 1.4044 - val_acc: 0.6203\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8428\n",
      "Epoch 00021: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4906 - acc: 0.8427 - val_loss: 1.3659 - val_acc: 0.6366\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8485\n",
      "Epoch 00022: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4692 - acc: 0.8485 - val_loss: 1.3933 - val_acc: 0.6275\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8532\n",
      "Epoch 00023: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4508 - acc: 0.8533 - val_loss: 1.4029 - val_acc: 0.6303\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8583\n",
      "Epoch 00024: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4343 - acc: 0.8584 - val_loss: 1.4077 - val_acc: 0.6352\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8661\n",
      "Epoch 00025: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4130 - acc: 0.8661 - val_loss: 1.3886 - val_acc: 0.6462\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8692\n",
      "Epoch 00026: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4024 - acc: 0.8693 - val_loss: 1.4087 - val_acc: 0.6417\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8777\n",
      "Epoch 00027: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3830 - acc: 0.8777 - val_loss: 1.4172 - val_acc: 0.6438\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8785\n",
      "Epoch 00028: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3700 - acc: 0.8784 - val_loss: 1.4296 - val_acc: 0.6506\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8855\n",
      "Epoch 00029: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3583 - acc: 0.8855 - val_loss: 1.4220 - val_acc: 0.6459\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8898\n",
      "Epoch 00030: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3442 - acc: 0.8898 - val_loss: 1.4427 - val_acc: 0.6473\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8915\n",
      "Epoch 00031: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3356 - acc: 0.8915 - val_loss: 1.4421 - val_acc: 0.6476\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8929\n",
      "Epoch 00032: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3255 - acc: 0.8929 - val_loss: 1.4616 - val_acc: 0.6459\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8980\n",
      "Epoch 00033: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3149 - acc: 0.8980 - val_loss: 1.4461 - val_acc: 0.6518\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9015\n",
      "Epoch 00034: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3027 - acc: 0.9014 - val_loss: 1.4197 - val_acc: 0.6585\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9028\n",
      "Epoch 00035: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3022 - acc: 0.9028 - val_loss: 1.4569 - val_acc: 0.6564\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9070\n",
      "Epoch 00036: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2847 - acc: 0.9070 - val_loss: 1.4837 - val_acc: 0.6553\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9079\n",
      "Epoch 00037: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2839 - acc: 0.9079 - val_loss: 1.4706 - val_acc: 0.6487\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9127\n",
      "Epoch 00038: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2727 - acc: 0.9127 - val_loss: 1.4598 - val_acc: 0.6583\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9127\n",
      "Epoch 00039: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2682 - acc: 0.9126 - val_loss: 1.4905 - val_acc: 0.6560\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9161\n",
      "Epoch 00040: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2586 - acc: 0.9161 - val_loss: 1.4972 - val_acc: 0.6618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9180\n",
      "Epoch 00041: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2524 - acc: 0.9180 - val_loss: 1.4940 - val_acc: 0.6594\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9183\n",
      "Epoch 00042: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2483 - acc: 0.9184 - val_loss: 1.5075 - val_acc: 0.6599\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9232\n",
      "Epoch 00043: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2368 - acc: 0.9232 - val_loss: 1.4864 - val_acc: 0.6641\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9240\n",
      "Epoch 00044: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2339 - acc: 0.9240 - val_loss: 1.5218 - val_acc: 0.6667\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9251\n",
      "Epoch 00045: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2272 - acc: 0.9251 - val_loss: 1.5320 - val_acc: 0.6592\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9262\n",
      "Epoch 00046: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2273 - acc: 0.9262 - val_loss: 1.5096 - val_acc: 0.6671\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9290\n",
      "Epoch 00047: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2217 - acc: 0.9291 - val_loss: 1.5185 - val_acc: 0.6608\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9303\n",
      "Epoch 00048: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2182 - acc: 0.9303 - val_loss: 1.5271 - val_acc: 0.6641\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9308\n",
      "Epoch 00049: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2152 - acc: 0.9308 - val_loss: 1.5075 - val_acc: 0.6643\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9319\n",
      "Epoch 00050: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2083 - acc: 0.9319 - val_loss: 1.5363 - val_acc: 0.6599\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9348\n",
      "Epoch 00051: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2005 - acc: 0.9348 - val_loss: 1.5634 - val_acc: 0.6606\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9362\n",
      "Epoch 00052: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1992 - acc: 0.9362 - val_loss: 1.5306 - val_acc: 0.6739\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9370\n",
      "Epoch 00053: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1961 - acc: 0.9370 - val_loss: 1.5538 - val_acc: 0.6632\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9371\n",
      "Epoch 00054: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1939 - acc: 0.9372 - val_loss: 1.5128 - val_acc: 0.6748\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9392\n",
      "Epoch 00055: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1913 - acc: 0.9392 - val_loss: 1.5323 - val_acc: 0.6753\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9424\n",
      "Epoch 00056: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1800 - acc: 0.9424 - val_loss: 1.5564 - val_acc: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9413\n",
      "Epoch 00057: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1806 - acc: 0.9413 - val_loss: 1.5393 - val_acc: 0.6741\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9427\n",
      "Epoch 00058: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1758 - acc: 0.9427 - val_loss: 1.5680 - val_acc: 0.6720\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9438\n",
      "Epoch 00059: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1747 - acc: 0.9438 - val_loss: 1.5392 - val_acc: 0.6825\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9452\n",
      "Epoch 00060: val_loss did not improve from 1.29989\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1706 - acc: 0.9452 - val_loss: 1.5600 - val_acc: 0.6746\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 524us/sample - loss: 1.3719 - acc: 0.5796\n",
      "Loss: 1.3718986924934982 Accuracy: 0.57964694\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2091 - acc: 0.2881\n",
      "Epoch 00001: val_loss improved from inf to 1.67273, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/001-1.6727.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.2090 - acc: 0.2882 - val_loss: 1.6727 - val_acc: 0.4812\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6237 - acc: 0.4785\n",
      "Epoch 00002: val_loss improved from 1.67273 to 1.43742, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/002-1.4374.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.6239 - acc: 0.4785 - val_loss: 1.4374 - val_acc: 0.5663\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4119 - acc: 0.5552\n",
      "Epoch 00003: val_loss improved from 1.43742 to 1.28943, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/003-1.2894.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4118 - acc: 0.5552 - val_loss: 1.2894 - val_acc: 0.6091\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2703 - acc: 0.6029\n",
      "Epoch 00004: val_loss improved from 1.28943 to 1.18379, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/004-1.1838.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2703 - acc: 0.6029 - val_loss: 1.1838 - val_acc: 0.6355\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1755 - acc: 0.6383\n",
      "Epoch 00005: val_loss improved from 1.18379 to 1.11061, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/005-1.1106.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1755 - acc: 0.6383 - val_loss: 1.1106 - val_acc: 0.6639\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0903 - acc: 0.6609\n",
      "Epoch 00006: val_loss improved from 1.11061 to 1.09107, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/006-1.0911.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0903 - acc: 0.6609 - val_loss: 1.0911 - val_acc: 0.6683\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0245 - acc: 0.6830\n",
      "Epoch 00007: val_loss improved from 1.09107 to 1.02209, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/007-1.0221.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0244 - acc: 0.6830 - val_loss: 1.0221 - val_acc: 0.6883\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.7038\n",
      "Epoch 00008: val_loss improved from 1.02209 to 1.01809, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/008-1.0181.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9666 - acc: 0.7038 - val_loss: 1.0181 - val_acc: 0.6960\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9131 - acc: 0.7187\n",
      "Epoch 00009: val_loss improved from 1.01809 to 0.97341, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/009-0.9734.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9130 - acc: 0.7187 - val_loss: 0.9734 - val_acc: 0.6974\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.7343\n",
      "Epoch 00010: val_loss improved from 0.97341 to 0.96677, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/010-0.9668.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8658 - acc: 0.7344 - val_loss: 0.9668 - val_acc: 0.7119\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.7487\n",
      "Epoch 00011: val_loss improved from 0.96677 to 0.93737, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/011-0.9374.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8164 - acc: 0.7487 - val_loss: 0.9374 - val_acc: 0.7188\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.7595\n",
      "Epoch 00012: val_loss improved from 0.93737 to 0.88742, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/012-0.8874.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7788 - acc: 0.7595 - val_loss: 0.8874 - val_acc: 0.7361\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7724\n",
      "Epoch 00013: val_loss improved from 0.88742 to 0.87613, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/013-0.8761.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7394 - acc: 0.7724 - val_loss: 0.8761 - val_acc: 0.7372\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7008 - acc: 0.7829\n",
      "Epoch 00014: val_loss improved from 0.87613 to 0.86456, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/014-0.8646.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7008 - acc: 0.7829 - val_loss: 0.8646 - val_acc: 0.7435\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.7921\n",
      "Epoch 00015: val_loss improved from 0.86456 to 0.85564, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/015-0.8556.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6701 - acc: 0.7921 - val_loss: 0.8556 - val_acc: 0.7456\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.8020\n",
      "Epoch 00016: val_loss did not improve from 0.85564\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6377 - acc: 0.8020 - val_loss: 0.8747 - val_acc: 0.7440\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6134 - acc: 0.8076\n",
      "Epoch 00017: val_loss improved from 0.85564 to 0.83267, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/017-0.8327.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6134 - acc: 0.8076 - val_loss: 0.8327 - val_acc: 0.7563\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5870 - acc: 0.8193\n",
      "Epoch 00018: val_loss did not improve from 0.83267\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5870 - acc: 0.8193 - val_loss: 0.8553 - val_acc: 0.7405\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8246\n",
      "Epoch 00019: val_loss improved from 0.83267 to 0.82442, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/019-0.8244.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5651 - acc: 0.8246 - val_loss: 0.8244 - val_acc: 0.7619\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.8310\n",
      "Epoch 00020: val_loss did not improve from 0.82442\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5430 - acc: 0.8310 - val_loss: 0.8379 - val_acc: 0.7538\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.8372\n",
      "Epoch 00021: val_loss did not improve from 0.82442\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5209 - acc: 0.8372 - val_loss: 0.8569 - val_acc: 0.7468\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8397\n",
      "Epoch 00022: val_loss did not improve from 0.82442\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5090 - acc: 0.8397 - val_loss: 0.8376 - val_acc: 0.7598\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8442\n",
      "Epoch 00023: val_loss improved from 0.82442 to 0.82008, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/023-0.8201.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4857 - acc: 0.8442 - val_loss: 0.8201 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8496\n",
      "Epoch 00024: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4764 - acc: 0.8496 - val_loss: 0.8395 - val_acc: 0.7591\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8542\n",
      "Epoch 00025: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4574 - acc: 0.8542 - val_loss: 0.8647 - val_acc: 0.7563\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8580\n",
      "Epoch 00026: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4413 - acc: 0.8580 - val_loss: 0.8216 - val_acc: 0.7701\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8622\n",
      "Epoch 00027: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4316 - acc: 0.8622 - val_loss: 0.8209 - val_acc: 0.7724\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8649\n",
      "Epoch 00028: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4170 - acc: 0.8649 - val_loss: 0.8339 - val_acc: 0.7678\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4044 - acc: 0.8695\n",
      "Epoch 00029: val_loss did not improve from 0.82008\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4044 - acc: 0.8694 - val_loss: 0.8290 - val_acc: 0.7703\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8728\n",
      "Epoch 00030: val_loss improved from 0.82008 to 0.81544, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/030-0.8154.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3978 - acc: 0.8728 - val_loss: 0.8154 - val_acc: 0.7817\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8749\n",
      "Epoch 00031: val_loss improved from 0.81544 to 0.81386, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/031-0.8139.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3879 - acc: 0.8749 - val_loss: 0.8139 - val_acc: 0.7801\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8798\n",
      "Epoch 00032: val_loss did not improve from 0.81386\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3742 - acc: 0.8798 - val_loss: 0.8149 - val_acc: 0.7754\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8811\n",
      "Epoch 00033: val_loss improved from 0.81386 to 0.80831, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_4_conv_checkpoint/033-0.8083.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3668 - acc: 0.8811 - val_loss: 0.8083 - val_acc: 0.7834\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8855\n",
      "Epoch 00034: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3521 - acc: 0.8855 - val_loss: 0.8146 - val_acc: 0.7820\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8861\n",
      "Epoch 00035: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3487 - acc: 0.8860 - val_loss: 0.8284 - val_acc: 0.7834\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8885\n",
      "Epoch 00036: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3402 - acc: 0.8885 - val_loss: 0.8129 - val_acc: 0.7827\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8877\n",
      "Epoch 00037: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3379 - acc: 0.8877 - val_loss: 0.8457 - val_acc: 0.7787\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8940\n",
      "Epoch 00038: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3316 - acc: 0.8940 - val_loss: 0.8481 - val_acc: 0.7736\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8941\n",
      "Epoch 00039: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3270 - acc: 0.8941 - val_loss: 0.8288 - val_acc: 0.7785\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8941\n",
      "Epoch 00040: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3207 - acc: 0.8941 - val_loss: 0.8296 - val_acc: 0.7873\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8973\n",
      "Epoch 00041: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3114 - acc: 0.8974 - val_loss: 0.8344 - val_acc: 0.7801\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.8984\n",
      "Epoch 00042: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3070 - acc: 0.8984 - val_loss: 0.8251 - val_acc: 0.7941\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.9009\n",
      "Epoch 00043: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2980 - acc: 0.9009 - val_loss: 0.8642 - val_acc: 0.7836\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9021\n",
      "Epoch 00044: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3001 - acc: 0.9021 - val_loss: 0.8249 - val_acc: 0.7855\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9043\n",
      "Epoch 00045: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2875 - acc: 0.9043 - val_loss: 0.8313 - val_acc: 0.7943\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9079\n",
      "Epoch 00046: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2823 - acc: 0.9079 - val_loss: 0.8390 - val_acc: 0.7911\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9065\n",
      "Epoch 00047: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2822 - acc: 0.9065 - val_loss: 0.8841 - val_acc: 0.7775\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9065\n",
      "Epoch 00048: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2847 - acc: 0.9065 - val_loss: 0.8434 - val_acc: 0.7892\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9110\n",
      "Epoch 00049: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2709 - acc: 0.9110 - val_loss: 0.8389 - val_acc: 0.7992\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9102\n",
      "Epoch 00050: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2702 - acc: 0.9103 - val_loss: 0.8363 - val_acc: 0.7908\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9108\n",
      "Epoch 00051: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2681 - acc: 0.9109 - val_loss: 0.8250 - val_acc: 0.7925\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9144\n",
      "Epoch 00052: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2608 - acc: 0.9144 - val_loss: 0.8606 - val_acc: 0.7952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9153\n",
      "Epoch 00053: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2549 - acc: 0.9153 - val_loss: 0.8282 - val_acc: 0.7934\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9149\n",
      "Epoch 00054: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2521 - acc: 0.9149 - val_loss: 0.8542 - val_acc: 0.7943\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9180\n",
      "Epoch 00055: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2476 - acc: 0.9180 - val_loss: 0.8264 - val_acc: 0.8013\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9169\n",
      "Epoch 00056: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2474 - acc: 0.9169 - val_loss: 0.8536 - val_acc: 0.7920\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9172\n",
      "Epoch 00057: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2476 - acc: 0.9172 - val_loss: 0.8270 - val_acc: 0.7941\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9199\n",
      "Epoch 00058: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2411 - acc: 0.9198 - val_loss: 0.8962 - val_acc: 0.7859\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9166\n",
      "Epoch 00059: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2502 - acc: 0.9166 - val_loss: 0.8573 - val_acc: 0.7929\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9213\n",
      "Epoch 00060: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2411 - acc: 0.9213 - val_loss: 0.8555 - val_acc: 0.8022\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2334 - acc: 0.9220 - val_loss: 0.8548 - val_acc: 0.7994\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9236\n",
      "Epoch 00062: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2344 - acc: 0.9236 - val_loss: 0.8333 - val_acc: 0.8060\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9216\n",
      "Epoch 00063: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2369 - acc: 0.9216 - val_loss: 0.8434 - val_acc: 0.8020\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9242\n",
      "Epoch 00064: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2337 - acc: 0.9241 - val_loss: 0.8757 - val_acc: 0.7964\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9242\n",
      "Epoch 00065: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2218 - acc: 0.9242 - val_loss: 0.8434 - val_acc: 0.7985\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9261\n",
      "Epoch 00066: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2262 - acc: 0.9260 - val_loss: 0.8390 - val_acc: 0.7985\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9283\n",
      "Epoch 00067: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2203 - acc: 0.9283 - val_loss: 0.8610 - val_acc: 0.7911\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9280\n",
      "Epoch 00068: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2187 - acc: 0.9280 - val_loss: 0.8483 - val_acc: 0.7955\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9287\n",
      "Epoch 00069: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2134 - acc: 0.9288 - val_loss: 0.8326 - val_acc: 0.8050\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9278\n",
      "Epoch 00070: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2187 - acc: 0.9278 - val_loss: 0.8494 - val_acc: 0.8029\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9300\n",
      "Epoch 00071: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2110 - acc: 0.9300 - val_loss: 0.8707 - val_acc: 0.8018\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2093 - acc: 0.9318\n",
      "Epoch 00072: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2094 - acc: 0.9317 - val_loss: 0.8401 - val_acc: 0.8081\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9307\n",
      "Epoch 00073: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2110 - acc: 0.9307 - val_loss: 0.8476 - val_acc: 0.8036\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9325\n",
      "Epoch 00074: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2059 - acc: 0.9325 - val_loss: 0.8886 - val_acc: 0.7964\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9295\n",
      "Epoch 00075: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2075 - acc: 0.9295 - val_loss: 0.8686 - val_acc: 0.8006\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9336\n",
      "Epoch 00076: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2015 - acc: 0.9337 - val_loss: 0.8709 - val_acc: 0.7978\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9329\n",
      "Epoch 00077: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2035 - acc: 0.9329 - val_loss: 0.8318 - val_acc: 0.8088\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9333\n",
      "Epoch 00078: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2025 - acc: 0.9333 - val_loss: 0.8468 - val_acc: 0.8006\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9349\n",
      "Epoch 00079: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2000 - acc: 0.9349 - val_loss: 0.8675 - val_acc: 0.8048\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9362\n",
      "Epoch 00080: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1979 - acc: 0.9362 - val_loss: 0.8682 - val_acc: 0.8039\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9359\n",
      "Epoch 00081: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1940 - acc: 0.9359 - val_loss: 0.8591 - val_acc: 0.8029\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9337\n",
      "Epoch 00082: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2008 - acc: 0.9337 - val_loss: 0.8399 - val_acc: 0.8092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9369\n",
      "Epoch 00083: val_loss did not improve from 0.80831\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1927 - acc: 0.9369 - val_loss: 0.8753 - val_acc: 0.8062\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 553us/sample - loss: 0.9202 - acc: 0.7502\n",
      "Loss: 0.9201614695917408 Accuracy: 0.75015575\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2681 - acc: 0.2548\n",
      "Epoch 00001: val_loss improved from inf to 1.69461, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/001-1.6946.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.2680 - acc: 0.2548 - val_loss: 1.6946 - val_acc: 0.4512\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5905 - acc: 0.4902\n",
      "Epoch 00002: val_loss improved from 1.69461 to 1.37278, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/002-1.3728.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5904 - acc: 0.4902 - val_loss: 1.3728 - val_acc: 0.5812\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.5754\n",
      "Epoch 00003: val_loss improved from 1.37278 to 1.21618, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/003-1.2162.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3525 - acc: 0.5754 - val_loss: 1.2162 - val_acc: 0.6417\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2118 - acc: 0.6226\n",
      "Epoch 00004: val_loss improved from 1.21618 to 1.07024, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/004-1.0702.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2117 - acc: 0.6226 - val_loss: 1.0702 - val_acc: 0.6799\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1035 - acc: 0.6577\n",
      "Epoch 00005: val_loss improved from 1.07024 to 0.97370, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/005-0.9737.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1035 - acc: 0.6577 - val_loss: 0.9737 - val_acc: 0.7119\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0130 - acc: 0.6865\n",
      "Epoch 00006: val_loss did not improve from 0.97370\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0130 - acc: 0.6864 - val_loss: 0.9762 - val_acc: 0.7032\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9455 - acc: 0.7108\n",
      "Epoch 00007: val_loss improved from 0.97370 to 0.86217, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/007-0.8622.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9454 - acc: 0.7108 - val_loss: 0.8622 - val_acc: 0.7475\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8883 - acc: 0.7275\n",
      "Epoch 00008: val_loss improved from 0.86217 to 0.85221, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/008-0.8522.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8884 - acc: 0.7275 - val_loss: 0.8522 - val_acc: 0.7480\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8381 - acc: 0.7447\n",
      "Epoch 00009: val_loss did not improve from 0.85221\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8381 - acc: 0.7447 - val_loss: 0.8529 - val_acc: 0.7480\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7585\n",
      "Epoch 00010: val_loss improved from 0.85221 to 0.80562, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/010-0.8056.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7896 - acc: 0.7585 - val_loss: 0.8056 - val_acc: 0.7636\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7720\n",
      "Epoch 00011: val_loss improved from 0.80562 to 0.74219, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/011-0.7422.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7469 - acc: 0.7720 - val_loss: 0.7422 - val_acc: 0.7866\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7171 - acc: 0.7834\n",
      "Epoch 00012: val_loss improved from 0.74219 to 0.71731, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/012-0.7173.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7171 - acc: 0.7834 - val_loss: 0.7173 - val_acc: 0.7908\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6852 - acc: 0.7933\n",
      "Epoch 00013: val_loss improved from 0.71731 to 0.70582, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/013-0.7058.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6853 - acc: 0.7933 - val_loss: 0.7058 - val_acc: 0.7980\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6496 - acc: 0.8042\n",
      "Epoch 00014: val_loss improved from 0.70582 to 0.69194, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/014-0.6919.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6496 - acc: 0.8042 - val_loss: 0.6919 - val_acc: 0.8008\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.8087\n",
      "Epoch 00015: val_loss improved from 0.69194 to 0.66166, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/015-0.6617.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6256 - acc: 0.8087 - val_loss: 0.6617 - val_acc: 0.8081\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8186\n",
      "Epoch 00016: val_loss improved from 0.66166 to 0.63565, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/016-0.6357.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6028 - acc: 0.8186 - val_loss: 0.6357 - val_acc: 0.8167\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8253\n",
      "Epoch 00017: val_loss did not improve from 0.63565\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5736 - acc: 0.8252 - val_loss: 0.6617 - val_acc: 0.8153\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8302\n",
      "Epoch 00018: val_loss did not improve from 0.63565\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5545 - acc: 0.8302 - val_loss: 0.6363 - val_acc: 0.8148\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5381 - acc: 0.8363\n",
      "Epoch 00019: val_loss did not improve from 0.63565\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5380 - acc: 0.8364 - val_loss: 0.6582 - val_acc: 0.8174\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.8431\n",
      "Epoch 00020: val_loss improved from 0.63565 to 0.61405, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/020-0.6140.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5176 - acc: 0.8431 - val_loss: 0.6140 - val_acc: 0.8269\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4988 - acc: 0.8473\n",
      "Epoch 00021: val_loss improved from 0.61405 to 0.60377, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/021-0.6038.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4987 - acc: 0.8473 - val_loss: 0.6038 - val_acc: 0.8334\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8510\n",
      "Epoch 00022: val_loss did not improve from 0.60377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4871 - acc: 0.8511 - val_loss: 0.6155 - val_acc: 0.8325\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8583\n",
      "Epoch 00023: val_loss did not improve from 0.60377\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4643 - acc: 0.8583 - val_loss: 0.6291 - val_acc: 0.8260\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8610\n",
      "Epoch 00024: val_loss improved from 0.60377 to 0.60161, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/024-0.6016.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4534 - acc: 0.8609 - val_loss: 0.6016 - val_acc: 0.8325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4358 - acc: 0.8678\n",
      "Epoch 00025: val_loss did not improve from 0.60161\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4357 - acc: 0.8678 - val_loss: 0.6056 - val_acc: 0.8290\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8690\n",
      "Epoch 00026: val_loss improved from 0.60161 to 0.59118, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/026-0.5912.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4233 - acc: 0.8690 - val_loss: 0.5912 - val_acc: 0.8400\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8715\n",
      "Epoch 00027: val_loss did not improve from 0.59118\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4153 - acc: 0.8715 - val_loss: 0.6351 - val_acc: 0.8272\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8732\n",
      "Epoch 00028: val_loss improved from 0.59118 to 0.57700, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/028-0.5770.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4030 - acc: 0.8732 - val_loss: 0.5770 - val_acc: 0.8425\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8789\n",
      "Epoch 00029: val_loss did not improve from 0.57700\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3876 - acc: 0.8790 - val_loss: 0.6071 - val_acc: 0.8395\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8810\n",
      "Epoch 00030: val_loss improved from 0.57700 to 0.56688, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/030-0.5669.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3791 - acc: 0.8810 - val_loss: 0.5669 - val_acc: 0.8456\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8834\n",
      "Epoch 00031: val_loss did not improve from 0.56688\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3714 - acc: 0.8834 - val_loss: 0.5872 - val_acc: 0.8393\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8886\n",
      "Epoch 00032: val_loss improved from 0.56688 to 0.55588, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/032-0.5559.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3571 - acc: 0.8886 - val_loss: 0.5559 - val_acc: 0.8509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8922\n",
      "Epoch 00033: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3471 - acc: 0.8922 - val_loss: 0.5715 - val_acc: 0.8495\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8933\n",
      "Epoch 00034: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3369 - acc: 0.8933 - val_loss: 0.5955 - val_acc: 0.8446\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8953\n",
      "Epoch 00035: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3293 - acc: 0.8953 - val_loss: 0.5749 - val_acc: 0.8530\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8958\n",
      "Epoch 00036: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3279 - acc: 0.8958 - val_loss: 0.5859 - val_acc: 0.8486\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9000\n",
      "Epoch 00037: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3141 - acc: 0.9000 - val_loss: 0.5663 - val_acc: 0.8523\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9037\n",
      "Epoch 00038: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3064 - acc: 0.9037 - val_loss: 0.5625 - val_acc: 0.8553\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9015\n",
      "Epoch 00039: val_loss did not improve from 0.55588\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3001 - acc: 0.9015 - val_loss: 0.5820 - val_acc: 0.8509\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9030\n",
      "Epoch 00040: val_loss improved from 0.55588 to 0.54927, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_5_conv_checkpoint/040-0.5493.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3020 - acc: 0.9030 - val_loss: 0.5493 - val_acc: 0.8591\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9056\n",
      "Epoch 00041: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2906 - acc: 0.9056 - val_loss: 0.5965 - val_acc: 0.8495\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9067\n",
      "Epoch 00042: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2858 - acc: 0.9067 - val_loss: 0.5780 - val_acc: 0.8558\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9100\n",
      "Epoch 00043: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2807 - acc: 0.9100 - val_loss: 0.5811 - val_acc: 0.8572\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9120\n",
      "Epoch 00044: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2706 - acc: 0.9121 - val_loss: 0.5846 - val_acc: 0.8495\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9144\n",
      "Epoch 00045: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2642 - acc: 0.9144 - val_loss: 0.5801 - val_acc: 0.8539\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9143\n",
      "Epoch 00046: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2602 - acc: 0.9143 - val_loss: 0.6335 - val_acc: 0.8400\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9145\n",
      "Epoch 00047: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2608 - acc: 0.9145 - val_loss: 0.5617 - val_acc: 0.8556\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9179\n",
      "Epoch 00048: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2513 - acc: 0.9179 - val_loss: 0.5623 - val_acc: 0.8579\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9194\n",
      "Epoch 00049: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2472 - acc: 0.9194 - val_loss: 0.5937 - val_acc: 0.8560\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9215\n",
      "Epoch 00050: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2435 - acc: 0.9215 - val_loss: 0.5816 - val_acc: 0.8567\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9223\n",
      "Epoch 00051: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2373 - acc: 0.9222 - val_loss: 0.5757 - val_acc: 0.8572\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9196\n",
      "Epoch 00052: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2425 - acc: 0.9197 - val_loss: 0.5730 - val_acc: 0.8586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9237\n",
      "Epoch 00053: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2318 - acc: 0.9237 - val_loss: 0.5927 - val_acc: 0.8528\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9249\n",
      "Epoch 00054: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2280 - acc: 0.9248 - val_loss: 0.5776 - val_acc: 0.8602\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9272\n",
      "Epoch 00055: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2268 - acc: 0.9272 - val_loss: 0.5866 - val_acc: 0.8633\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9263\n",
      "Epoch 00056: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2224 - acc: 0.9263 - val_loss: 0.5764 - val_acc: 0.8551\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9269\n",
      "Epoch 00057: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2171 - acc: 0.9269 - val_loss: 0.6046 - val_acc: 0.8523\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9275\n",
      "Epoch 00058: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2182 - acc: 0.9275 - val_loss: 0.6071 - val_acc: 0.8572\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9269\n",
      "Epoch 00059: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2190 - acc: 0.9269 - val_loss: 0.6218 - val_acc: 0.8546\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9293\n",
      "Epoch 00060: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2127 - acc: 0.9293 - val_loss: 0.5656 - val_acc: 0.8633\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9332\n",
      "Epoch 00061: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2036 - acc: 0.9331 - val_loss: 0.5871 - val_acc: 0.8598\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9300\n",
      "Epoch 00062: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2133 - acc: 0.9300 - val_loss: 0.5712 - val_acc: 0.8612\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9331\n",
      "Epoch 00063: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2005 - acc: 0.9331 - val_loss: 0.5633 - val_acc: 0.8635\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9333\n",
      "Epoch 00064: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2016 - acc: 0.9333 - val_loss: 0.5799 - val_acc: 0.8612\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9356\n",
      "Epoch 00065: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1971 - acc: 0.9356 - val_loss: 0.5776 - val_acc: 0.8602\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9358\n",
      "Epoch 00066: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1985 - acc: 0.9358 - val_loss: 0.5761 - val_acc: 0.8651\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9354\n",
      "Epoch 00067: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1968 - acc: 0.9354 - val_loss: 0.5714 - val_acc: 0.8633\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9349\n",
      "Epoch 00068: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1958 - acc: 0.9349 - val_loss: 0.6219 - val_acc: 0.8535\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9371\n",
      "Epoch 00069: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1925 - acc: 0.9371 - val_loss: 0.5832 - val_acc: 0.8661\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9386\n",
      "Epoch 00070: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1877 - acc: 0.9386 - val_loss: 0.5590 - val_acc: 0.8675\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9406\n",
      "Epoch 00071: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1785 - acc: 0.9406 - val_loss: 0.6118 - val_acc: 0.8586\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9377\n",
      "Epoch 00072: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1861 - acc: 0.9377 - val_loss: 0.5829 - val_acc: 0.8691\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9407\n",
      "Epoch 00073: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1779 - acc: 0.9407 - val_loss: 0.5748 - val_acc: 0.8663\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9407\n",
      "Epoch 00074: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1778 - acc: 0.9407 - val_loss: 0.6173 - val_acc: 0.8612\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9407\n",
      "Epoch 00075: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1803 - acc: 0.9407 - val_loss: 0.5809 - val_acc: 0.8637\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9426\n",
      "Epoch 00076: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1752 - acc: 0.9426 - val_loss: 0.5904 - val_acc: 0.8675\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9402\n",
      "Epoch 00077: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1791 - acc: 0.9403 - val_loss: 0.5783 - val_acc: 0.8689\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9448\n",
      "Epoch 00078: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1712 - acc: 0.9448 - val_loss: 0.5767 - val_acc: 0.8651\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9438\n",
      "Epoch 00079: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1703 - acc: 0.9438 - val_loss: 0.5647 - val_acc: 0.8730\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9430\n",
      "Epoch 00080: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1732 - acc: 0.9430 - val_loss: 0.5872 - val_acc: 0.8712\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9450\n",
      "Epoch 00081: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1697 - acc: 0.9450 - val_loss: 0.5779 - val_acc: 0.8675\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9449\n",
      "Epoch 00082: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1711 - acc: 0.9449 - val_loss: 0.6007 - val_acc: 0.8626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9461\n",
      "Epoch 00083: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1672 - acc: 0.9461 - val_loss: 0.5677 - val_acc: 0.8721\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9468\n",
      "Epoch 00084: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1612 - acc: 0.9469 - val_loss: 0.5804 - val_acc: 0.8728\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9486\n",
      "Epoch 00085: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1592 - acc: 0.9486 - val_loss: 0.5684 - val_acc: 0.8670\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9478\n",
      "Epoch 00086: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1590 - acc: 0.9478 - val_loss: 0.5726 - val_acc: 0.8672\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9467\n",
      "Epoch 00087: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1628 - acc: 0.9467 - val_loss: 0.5648 - val_acc: 0.8691\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9465\n",
      "Epoch 00088: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1633 - acc: 0.9465 - val_loss: 0.5794 - val_acc: 0.8705\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9489\n",
      "Epoch 00089: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1574 - acc: 0.9489 - val_loss: 0.5715 - val_acc: 0.8756\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9469\n",
      "Epoch 00090: val_loss did not improve from 0.54927\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1594 - acc: 0.9469 - val_loss: 0.5874 - val_acc: 0.8700\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 596us/sample - loss: 0.6525 - acc: 0.8204\n",
      "Loss: 0.6524698791474197 Accuracy: 0.8203531\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3381 - acc: 0.2304\n",
      "Epoch 00001: val_loss improved from inf to 1.67086, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/001-1.6709.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 2.3380 - acc: 0.2305 - val_loss: 1.6709 - val_acc: 0.4836\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6529 - acc: 0.4554\n",
      "Epoch 00002: val_loss improved from 1.67086 to 1.36520, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/002-1.3652.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.6528 - acc: 0.4554 - val_loss: 1.3652 - val_acc: 0.5716\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4465 - acc: 0.5255\n",
      "Epoch 00003: val_loss improved from 1.36520 to 1.21335, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/003-1.2134.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4465 - acc: 0.5256 - val_loss: 1.2134 - val_acc: 0.6308\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3109 - acc: 0.5766\n",
      "Epoch 00004: val_loss improved from 1.21335 to 1.10876, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/004-1.1088.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3110 - acc: 0.5766 - val_loss: 1.1088 - val_acc: 0.6711\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1809 - acc: 0.6274\n",
      "Epoch 00005: val_loss improved from 1.10876 to 0.97924, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/005-0.9792.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1808 - acc: 0.6275 - val_loss: 0.9792 - val_acc: 0.7046\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0843 - acc: 0.6620\n",
      "Epoch 00006: val_loss improved from 0.97924 to 0.90008, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/006-0.9001.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0843 - acc: 0.6620 - val_loss: 0.9001 - val_acc: 0.7319\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9880 - acc: 0.6911\n",
      "Epoch 00007: val_loss improved from 0.90008 to 0.80290, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/007-0.8029.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9879 - acc: 0.6911 - val_loss: 0.8029 - val_acc: 0.7633\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9233 - acc: 0.7190\n",
      "Epoch 00008: val_loss improved from 0.80290 to 0.77980, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/008-0.7798.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9233 - acc: 0.7190 - val_loss: 0.7798 - val_acc: 0.7813\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8620 - acc: 0.7383\n",
      "Epoch 00009: val_loss improved from 0.77980 to 0.72662, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/009-0.7266.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8621 - acc: 0.7383 - val_loss: 0.7266 - val_acc: 0.7911\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8118 - acc: 0.7531\n",
      "Epoch 00010: val_loss improved from 0.72662 to 0.72478, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/010-0.7248.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8117 - acc: 0.7531 - val_loss: 0.7248 - val_acc: 0.7915\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7709\n",
      "Epoch 00011: val_loss improved from 0.72478 to 0.68663, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/011-0.6866.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7584 - acc: 0.7709 - val_loss: 0.6866 - val_acc: 0.8022\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7135 - acc: 0.7853\n",
      "Epoch 00012: val_loss improved from 0.68663 to 0.67481, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/012-0.6748.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7134 - acc: 0.7853 - val_loss: 0.6748 - val_acc: 0.8050\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.7908\n",
      "Epoch 00013: val_loss improved from 0.67481 to 0.56207, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/013-0.5621.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6916 - acc: 0.7907 - val_loss: 0.5621 - val_acc: 0.8456\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.8026\n",
      "Epoch 00014: val_loss improved from 0.56207 to 0.56196, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/014-0.5620.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6542 - acc: 0.8026 - val_loss: 0.5620 - val_acc: 0.8437\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6239 - acc: 0.8119\n",
      "Epoch 00015: val_loss did not improve from 0.56196\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6241 - acc: 0.8118 - val_loss: 0.5638 - val_acc: 0.8484\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6124 - acc: 0.8160\n",
      "Epoch 00016: val_loss improved from 0.56196 to 0.52982, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/016-0.5298.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6124 - acc: 0.8161 - val_loss: 0.5298 - val_acc: 0.8474\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5775 - acc: 0.8250\n",
      "Epoch 00017: val_loss improved from 0.52982 to 0.50329, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/017-0.5033.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5775 - acc: 0.8250 - val_loss: 0.5033 - val_acc: 0.8612\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8322\n",
      "Epoch 00018: val_loss did not improve from 0.50329\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5519 - acc: 0.8322 - val_loss: 0.5155 - val_acc: 0.8542\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5410 - acc: 0.8382\n",
      "Epoch 00019: val_loss improved from 0.50329 to 0.45623, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/019-0.4562.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5410 - acc: 0.8381 - val_loss: 0.4562 - val_acc: 0.8751\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8405\n",
      "Epoch 00020: val_loss did not improve from 0.45623\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5222 - acc: 0.8406 - val_loss: 0.4583 - val_acc: 0.8721\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8499\n",
      "Epoch 00021: val_loss did not improve from 0.45623\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5009 - acc: 0.8499 - val_loss: 0.4611 - val_acc: 0.8628\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.8493\n",
      "Epoch 00022: val_loss improved from 0.45623 to 0.44933, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/022-0.4493.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4884 - acc: 0.8493 - val_loss: 0.4493 - val_acc: 0.8714\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8565\n",
      "Epoch 00023: val_loss did not improve from 0.44933\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4727 - acc: 0.8565 - val_loss: 0.4867 - val_acc: 0.8705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8607\n",
      "Epoch 00024: val_loss improved from 0.44933 to 0.44622, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/024-0.4462.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4673 - acc: 0.8607 - val_loss: 0.4462 - val_acc: 0.8793\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.8658\n",
      "Epoch 00025: val_loss improved from 0.44622 to 0.40789, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/025-0.4079.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4431 - acc: 0.8658 - val_loss: 0.4079 - val_acc: 0.8891\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8658\n",
      "Epoch 00026: val_loss did not improve from 0.40789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4402 - acc: 0.8659 - val_loss: 0.4344 - val_acc: 0.8856\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8720\n",
      "Epoch 00027: val_loss improved from 0.40789 to 0.39837, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/027-0.3984.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4221 - acc: 0.8720 - val_loss: 0.3984 - val_acc: 0.8894\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8744\n",
      "Epoch 00028: val_loss did not improve from 0.39837\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4181 - acc: 0.8744 - val_loss: 0.4216 - val_acc: 0.8845\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8779\n",
      "Epoch 00029: val_loss improved from 0.39837 to 0.37872, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/029-0.3787.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4050 - acc: 0.8779 - val_loss: 0.3787 - val_acc: 0.9022\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8800\n",
      "Epoch 00030: val_loss did not improve from 0.37872\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3969 - acc: 0.8800 - val_loss: 0.3960 - val_acc: 0.8917\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8797\n",
      "Epoch 00031: val_loss did not improve from 0.37872\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3907 - acc: 0.8797 - val_loss: 0.3834 - val_acc: 0.8968\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8817\n",
      "Epoch 00032: val_loss improved from 0.37872 to 0.37539, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/032-0.3754.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3792 - acc: 0.8818 - val_loss: 0.3754 - val_acc: 0.8977\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8894\n",
      "Epoch 00033: val_loss did not improve from 0.37539\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3682 - acc: 0.8894 - val_loss: 0.3847 - val_acc: 0.8994\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3618 - acc: 0.8879\n",
      "Epoch 00034: val_loss improved from 0.37539 to 0.35834, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/034-0.3583.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3619 - acc: 0.8878 - val_loss: 0.3583 - val_acc: 0.9043\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8902\n",
      "Epoch 00035: val_loss improved from 0.35834 to 0.35782, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/035-0.3578.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3561 - acc: 0.8902 - val_loss: 0.3578 - val_acc: 0.9022\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8961\n",
      "Epoch 00036: val_loss improved from 0.35782 to 0.35013, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/036-0.3501.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3415 - acc: 0.8960 - val_loss: 0.3501 - val_acc: 0.9050\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8949\n",
      "Epoch 00037: val_loss did not improve from 0.35013\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3380 - acc: 0.8949 - val_loss: 0.3988 - val_acc: 0.8994\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8947\n",
      "Epoch 00038: val_loss did not improve from 0.35013\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3344 - acc: 0.8947 - val_loss: 0.3511 - val_acc: 0.9022\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.9020\n",
      "Epoch 00039: val_loss did not improve from 0.35013\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3227 - acc: 0.9020 - val_loss: 0.3522 - val_acc: 0.9043\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9010\n",
      "Epoch 00040: val_loss improved from 0.35013 to 0.34707, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/040-0.3471.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3191 - acc: 0.9010 - val_loss: 0.3471 - val_acc: 0.9073\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9027\n",
      "Epoch 00041: val_loss improved from 0.34707 to 0.33670, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/041-0.3367.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3148 - acc: 0.9027 - val_loss: 0.3367 - val_acc: 0.9133\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.9029\n",
      "Epoch 00042: val_loss improved from 0.33670 to 0.33615, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/042-0.3361.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3139 - acc: 0.9029 - val_loss: 0.3361 - val_acc: 0.9110\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9069\n",
      "Epoch 00043: val_loss improved from 0.33615 to 0.32963, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/043-0.3296.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3013 - acc: 0.9069 - val_loss: 0.3296 - val_acc: 0.9138\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9082\n",
      "Epoch 00044: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2925 - acc: 0.9082 - val_loss: 0.3528 - val_acc: 0.9054\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9082\n",
      "Epoch 00045: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2940 - acc: 0.9082 - val_loss: 0.3470 - val_acc: 0.9087\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9122\n",
      "Epoch 00046: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2825 - acc: 0.9122 - val_loss: 0.3432 - val_acc: 0.9061\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9111\n",
      "Epoch 00047: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2818 - acc: 0.9111 - val_loss: 0.3369 - val_acc: 0.9164\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9139\n",
      "Epoch 00048: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2753 - acc: 0.9139 - val_loss: 0.3297 - val_acc: 0.9159\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9136\n",
      "Epoch 00049: val_loss did not improve from 0.32963\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2715 - acc: 0.9136 - val_loss: 0.3357 - val_acc: 0.9108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9141\n",
      "Epoch 00050: val_loss improved from 0.32963 to 0.32008, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/050-0.3201.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2704 - acc: 0.9141 - val_loss: 0.3201 - val_acc: 0.9164\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9161\n",
      "Epoch 00051: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2681 - acc: 0.9161 - val_loss: 0.3248 - val_acc: 0.9157\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9170\n",
      "Epoch 00052: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2609 - acc: 0.9170 - val_loss: 0.3267 - val_acc: 0.9201\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9181\n",
      "Epoch 00053: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2599 - acc: 0.9181 - val_loss: 0.3300 - val_acc: 0.9185\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9203\n",
      "Epoch 00054: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2495 - acc: 0.9203 - val_loss: 0.3256 - val_acc: 0.9178\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9214\n",
      "Epoch 00055: val_loss did not improve from 0.32008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2465 - acc: 0.9214 - val_loss: 0.3225 - val_acc: 0.9143\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9240\n",
      "Epoch 00056: val_loss improved from 0.32008 to 0.31958, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/056-0.3196.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2371 - acc: 0.9240 - val_loss: 0.3196 - val_acc: 0.9236\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9235\n",
      "Epoch 00057: val_loss improved from 0.31958 to 0.31407, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/057-0.3141.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2438 - acc: 0.9235 - val_loss: 0.3141 - val_acc: 0.9201\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9236\n",
      "Epoch 00058: val_loss did not improve from 0.31407\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2426 - acc: 0.9236 - val_loss: 0.3284 - val_acc: 0.9164\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9252\n",
      "Epoch 00059: val_loss did not improve from 0.31407\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2351 - acc: 0.9253 - val_loss: 0.3195 - val_acc: 0.9208\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9243\n",
      "Epoch 00060: val_loss improved from 0.31407 to 0.30563, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/060-0.3056.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2327 - acc: 0.9243 - val_loss: 0.3056 - val_acc: 0.9231\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9268\n",
      "Epoch 00061: val_loss did not improve from 0.30563\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2294 - acc: 0.9269 - val_loss: 0.3173 - val_acc: 0.9161\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9275\n",
      "Epoch 00062: val_loss did not improve from 0.30563\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2238 - acc: 0.9275 - val_loss: 0.3155 - val_acc: 0.9194\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9283\n",
      "Epoch 00063: val_loss did not improve from 0.30563\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2246 - acc: 0.9284 - val_loss: 0.3336 - val_acc: 0.9164\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9317\n",
      "Epoch 00064: val_loss improved from 0.30563 to 0.29594, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/064-0.2959.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2175 - acc: 0.9317 - val_loss: 0.2959 - val_acc: 0.9203\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9312\n",
      "Epoch 00065: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2121 - acc: 0.9312 - val_loss: 0.3162 - val_acc: 0.9213\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9324\n",
      "Epoch 00066: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2127 - acc: 0.9324 - val_loss: 0.3176 - val_acc: 0.9206\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9327\n",
      "Epoch 00067: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2089 - acc: 0.9328 - val_loss: 0.3152 - val_acc: 0.9196\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9328\n",
      "Epoch 00068: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2094 - acc: 0.9328 - val_loss: 0.3118 - val_acc: 0.9224\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9340\n",
      "Epoch 00069: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2036 - acc: 0.9340 - val_loss: 0.3292 - val_acc: 0.9217\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9351\n",
      "Epoch 00070: val_loss did not improve from 0.29594\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2023 - acc: 0.9351 - val_loss: 0.3015 - val_acc: 0.9194\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9342\n",
      "Epoch 00071: val_loss improved from 0.29594 to 0.28977, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/071-0.2898.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1982 - acc: 0.9342 - val_loss: 0.2898 - val_acc: 0.9227\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9335\n",
      "Epoch 00072: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2074 - acc: 0.9335 - val_loss: 0.3074 - val_acc: 0.9187\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9378\n",
      "Epoch 00073: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1930 - acc: 0.9378 - val_loss: 0.3409 - val_acc: 0.9215\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9377\n",
      "Epoch 00074: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1933 - acc: 0.9377 - val_loss: 0.3236 - val_acc: 0.9227\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9401\n",
      "Epoch 00075: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1861 - acc: 0.9401 - val_loss: 0.3001 - val_acc: 0.9234\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9370\n",
      "Epoch 00076: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1904 - acc: 0.9370 - val_loss: 0.3053 - val_acc: 0.9234\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9399\n",
      "Epoch 00077: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1830 - acc: 0.9399 - val_loss: 0.3107 - val_acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9397\n",
      "Epoch 00078: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1857 - acc: 0.9397 - val_loss: 0.2924 - val_acc: 0.9192\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9420\n",
      "Epoch 00079: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1812 - acc: 0.9420 - val_loss: 0.3036 - val_acc: 0.9238\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9396\n",
      "Epoch 00080: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1819 - acc: 0.9396 - val_loss: 0.2906 - val_acc: 0.9241\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9407\n",
      "Epoch 00081: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1795 - acc: 0.9406 - val_loss: 0.3395 - val_acc: 0.9217\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9430\n",
      "Epoch 00082: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1752 - acc: 0.9430 - val_loss: 0.3223 - val_acc: 0.9252\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9436\n",
      "Epoch 00083: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1751 - acc: 0.9436 - val_loss: 0.3156 - val_acc: 0.9231\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9448\n",
      "Epoch 00084: val_loss did not improve from 0.28977\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1726 - acc: 0.9448 - val_loss: 0.3038 - val_acc: 0.9231\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9448\n",
      "Epoch 00085: val_loss improved from 0.28977 to 0.28724, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_6_conv_checkpoint/085-0.2872.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1689 - acc: 0.9448 - val_loss: 0.2872 - val_acc: 0.9234\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9426\n",
      "Epoch 00086: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1718 - acc: 0.9426 - val_loss: 0.3008 - val_acc: 0.9264\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9457\n",
      "Epoch 00087: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1691 - acc: 0.9457 - val_loss: 0.2997 - val_acc: 0.9259\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9448\n",
      "Epoch 00088: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1660 - acc: 0.9448 - val_loss: 0.2999 - val_acc: 0.9306\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9485\n",
      "Epoch 00089: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1605 - acc: 0.9485 - val_loss: 0.3098 - val_acc: 0.9278\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9467\n",
      "Epoch 00090: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1633 - acc: 0.9467 - val_loss: 0.3523 - val_acc: 0.9201\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9473\n",
      "Epoch 00091: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1605 - acc: 0.9473 - val_loss: 0.3182 - val_acc: 0.9259\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9457\n",
      "Epoch 00092: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1652 - acc: 0.9457 - val_loss: 0.3157 - val_acc: 0.9264\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9483\n",
      "Epoch 00093: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1561 - acc: 0.9483 - val_loss: 0.2935 - val_acc: 0.9273\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9493\n",
      "Epoch 00094: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1540 - acc: 0.9493 - val_loss: 0.3005 - val_acc: 0.9285\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9498\n",
      "Epoch 00095: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1507 - acc: 0.9498 - val_loss: 0.3017 - val_acc: 0.9301\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9482\n",
      "Epoch 00096: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1564 - acc: 0.9482 - val_loss: 0.3205 - val_acc: 0.9213\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9511\n",
      "Epoch 00097: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1451 - acc: 0.9511 - val_loss: 0.3039 - val_acc: 0.9283\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9498\n",
      "Epoch 00098: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1497 - acc: 0.9498 - val_loss: 0.3066 - val_acc: 0.9276\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9518\n",
      "Epoch 00099: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1475 - acc: 0.9517 - val_loss: 0.2998 - val_acc: 0.9264\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9515\n",
      "Epoch 00100: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1471 - acc: 0.9515 - val_loss: 0.3040 - val_acc: 0.9278\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9523\n",
      "Epoch 00101: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1440 - acc: 0.9523 - val_loss: 0.3225 - val_acc: 0.9241\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9531\n",
      "Epoch 00102: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1446 - acc: 0.9530 - val_loss: 0.2937 - val_acc: 0.9308\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9533\n",
      "Epoch 00103: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1429 - acc: 0.9533 - val_loss: 0.3167 - val_acc: 0.9278\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9531\n",
      "Epoch 00104: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1438 - acc: 0.9531 - val_loss: 0.3007 - val_acc: 0.9283\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9537\n",
      "Epoch 00105: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1396 - acc: 0.9537 - val_loss: 0.3257 - val_acc: 0.9255\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9555\n",
      "Epoch 00106: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1368 - acc: 0.9555 - val_loss: 0.3260 - val_acc: 0.9231\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9552\n",
      "Epoch 00107: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1376 - acc: 0.9552 - val_loss: 0.3122 - val_acc: 0.9301\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9539\n",
      "Epoch 00108: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1393 - acc: 0.9539 - val_loss: 0.3088 - val_acc: 0.9231\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9549\n",
      "Epoch 00109: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1345 - acc: 0.9550 - val_loss: 0.3020 - val_acc: 0.9313\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9566\n",
      "Epoch 00110: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1338 - acc: 0.9566 - val_loss: 0.3216 - val_acc: 0.9301\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9557\n",
      "Epoch 00111: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1368 - acc: 0.9557 - val_loss: 0.2938 - val_acc: 0.9257\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9572\n",
      "Epoch 00112: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1288 - acc: 0.9572 - val_loss: 0.3322 - val_acc: 0.9262\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9560\n",
      "Epoch 00113: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1337 - acc: 0.9560 - val_loss: 0.3021 - val_acc: 0.9294\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9582\n",
      "Epoch 00114: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1276 - acc: 0.9582 - val_loss: 0.3255 - val_acc: 0.9292\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9566\n",
      "Epoch 00115: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1317 - acc: 0.9566 - val_loss: 0.2963 - val_acc: 0.9308\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9582\n",
      "Epoch 00116: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1264 - acc: 0.9582 - val_loss: 0.3063 - val_acc: 0.9278\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9595\n",
      "Epoch 00117: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1226 - acc: 0.9595 - val_loss: 0.3067 - val_acc: 0.9299\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9582\n",
      "Epoch 00118: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1250 - acc: 0.9582 - val_loss: 0.3073 - val_acc: 0.9313\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9576\n",
      "Epoch 00119: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1236 - acc: 0.9576 - val_loss: 0.3287 - val_acc: 0.9266\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9596\n",
      "Epoch 00120: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1218 - acc: 0.9596 - val_loss: 0.2907 - val_acc: 0.9317\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9573\n",
      "Epoch 00121: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1271 - acc: 0.9573 - val_loss: 0.2936 - val_acc: 0.9327\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9602\n",
      "Epoch 00122: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1229 - acc: 0.9602 - val_loss: 0.3049 - val_acc: 0.9306\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9604\n",
      "Epoch 00123: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1184 - acc: 0.9604 - val_loss: 0.2977 - val_acc: 0.9292\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9610\n",
      "Epoch 00124: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1195 - acc: 0.9610 - val_loss: 0.3228 - val_acc: 0.9317\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9597\n",
      "Epoch 00125: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1185 - acc: 0.9597 - val_loss: 0.3145 - val_acc: 0.9315\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9619\n",
      "Epoch 00126: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1170 - acc: 0.9619 - val_loss: 0.3231 - val_acc: 0.9301\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9621\n",
      "Epoch 00127: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1179 - acc: 0.9622 - val_loss: 0.3355 - val_acc: 0.9278\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9619\n",
      "Epoch 00128: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1139 - acc: 0.9619 - val_loss: 0.3144 - val_acc: 0.9299\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9605\n",
      "Epoch 00129: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1173 - acc: 0.9605 - val_loss: 0.3162 - val_acc: 0.9317\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9649\n",
      "Epoch 00130: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1092 - acc: 0.9649 - val_loss: 0.3056 - val_acc: 0.9322\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9636\n",
      "Epoch 00131: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1097 - acc: 0.9636 - val_loss: 0.3228 - val_acc: 0.9290\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9626\n",
      "Epoch 00132: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1113 - acc: 0.9626 - val_loss: 0.3237 - val_acc: 0.9280\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9624\n",
      "Epoch 00133: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1174 - acc: 0.9625 - val_loss: 0.3003 - val_acc: 0.9350\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9630\n",
      "Epoch 00134: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1105 - acc: 0.9630 - val_loss: 0.2897 - val_acc: 0.9338\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9641\n",
      "Epoch 00135: val_loss did not improve from 0.28724\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1117 - acc: 0.9641 - val_loss: 0.3163 - val_acc: 0.9315\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXZwPHfmSUzmewrhARIEEQIS1hFENCiqFjRat1era97bV2q9rWlWltt3Wpt3ZXi0rrVpaCtW8UNRCtYFlGCgOxkhezJJLPPef84WUBCCCFDCPN8P5/5JHPnLs+dwHnuWe65SmuNEEIIsS+Wng5ACCHE4U0ShRBCiA5JohBCCNEhSRRCCCE6JIlCCCFEhyRRCCGE6JAkCiGEEB2SRCGEEKJDkiiEEEJ0yNbTARyo9PR0nZub29NhCCFEr7Jy5cpKrXVGV7btdYkiNzeXFStW9HQYQgjRqyiltnd1W2l6EkII0SFJFEIIITokiUIIIUSHel0fRXsCgQDFxcV4vd6eDqXXcjqd5OTkYLfbezoUIcRh5ohIFMXFxSQkJJCbm4tSqqfD6XW01lRVVVFcXExeXl5PhyOEOMwcEU1PXq+XtLQ0SRJdpJQiLS1NamRCiHYdEYkCkCRxkOT7E0LsyxGTKPYnFPLg85UQDgd6OhQhhOhVoiZRhMNe/P4ytO7+RFFbW8sTTzzRpW1nzZpFbW1tp9e/4447eOCBB7p0LCGE6IqoSRRKtZxquNv33VGiCAaDHW777rvvkpyc3O0xCSFEd4maRNFyqlp3f6KYM2cOmzdvpqCggFtuuYXFixczdepUZs+ezfDhwwE466yzGDduHPn5+cybN69129zcXCorK9m2bRvDhg3jqquuIj8/n5kzZ+LxeDo87urVq5k0aRKjRo3iBz/4ATU1NQA88sgjDB8+nFGjRnHBBRcA8Mknn1BQUEBBQQFjxoyhoaGh278HIcSR6YgYHru7jRtvxO1e3c4nIUKhJiyWWJQ6sNOOjy9gyJCH9vn5fffdR2FhIatXm+MuXryYVatWUVhY2Drc9NlnnyU1NRWPx8OECRM455xzSEtL+07sG3n55Zd56qmnOO+881iwYAEXX3zxPo97ySWX8OijjzJ9+nR+85vfcOedd/LQQw9x3333sXXrVhwOR2uz1gMPPMDjjz/OlClTcLvdOJ3OA/oOhBDRK4pqFId2VM/EiRP3uCfhkUceYfTo0UyaNImioiI2bty41zZ5eXkUFBQAMG7cOLZt27bP/dfV1VFbW8v06dMB+N///V+WLFkCwKhRo7jooot48cUXsdlMUpwyZQo333wzjzzyCLW1ta3LhRBif4640mJfV/6hkJempkKczjzs9rR21+lOcXFxrb8vXryYDz/8kKVLl+JyuTjhhBPavWfB4XC0/m61Wvfb9LQv77zzDkuWLOGtt97i7rvvZs2aNcyZM4fTTz+dd999lylTprBw4UKOOeaYLu1fCBFdoqZG0dKZHYk+ioSEhA7b/Ovq6khJScHlcrF+/XqWLVt20MdMSkoiJSWFTz/9FIAXXniB6dOnEw6HKSoq4sQTT+QPf/gDdXV1uN1uNm/ezMiRI/nlL3/JhAkTWL9+/UHHIISIDkdcjWLfIjfqKS0tjSlTpjBixAhOO+00Tj/99D0+P/XUU5k7dy7Dhg1j6NChTJo0qVuO+9xzz3HNNdfQ1NTEoEGD+Otf/0ooFOLiiy+mrq4OrTU33HADycnJ3H777SxatAiLxUJ+fj6nnXZat8QghDjyKa11T8dwQMaPH6+/++CidevWMWzYsA630zqM272KmJhsHI6sSIbYa3XmexRC9E5KqZVa6/Fd2TZqmp7aOrO7v0YhhBBHsqhJFGYuI0tE+iiEEOJIFjWJAlo6tCVRCCHEgYiqRCE1CiGEOHBRlSikRiGEEAcuqhKF1CiEEOLARV2iOFxqFPHx8Qe0XAghekpUJQqlpEYhhBAHKqoSRaRqFHPmzOHxxx9vfd/ycCG3282MGTMYO3YsI0eO5F//+len96m15pZbbmHEiBGMHDmSV199FYCysjKmTZtGQUEBI0aM4NNPPyUUCnHppZe2rvvggw92+zkKIaLXkTeFx403wur2phkHR9gLOgTWuHY/36eCAnho39OMn3/++dx4441ce+21ALz22mssXLgQp9PJG2+8QWJiIpWVlUyaNInZs2d36vnUr7/+OqtXr+arr76isrKSCRMmMG3aNP7+979zyimncNtttxEKhWhqamL16tWUlJRQWFgIcEBPzBNCiP058hJFDxgzZgy7du2itLSUiooKUlJS6N+/P4FAgFtvvZUlS5ZgsVgoKSlh586d9O3bd7/7/Oyzz7jwwguxWq306dOH6dOns3z5ciZMmMDll19OIBDgrLPOoqCggEGDBrFlyxauv/56Tj/9dGbOnHkIzloIES2OvETRwZV/wLuDQKCahISCbj/sueeey/z58ykvL+f8888H4KWXXqKiooKVK1dit9vJzc1td3rxAzFt2jSWLFnCO++8w6WXXsrNN9/MJZdcwldffcXChQuZO3cur732Gs8++2x3nJYQQkgfRXc5//zzeeWVV5g/fz7nnnsuYKYXz8zMxG63s2jRIrZv397p/U2dOpVXX32VUChERUUFS5YsYeLEiWzfvp0+ffpw1VVXceWVV7Jq1SoqKysJh8Occ8453HXXXaxatSoi5yiEiE5HXo2iA6ZvIIzWulP9BAciPz+fhoYGsrOzycoys9NedNFFnHHGGYwcOZLx48cf0IOCfvCDH7B06VJGjx6NUor777+fvn378txzz/HHP/4Ru91OfHw8zz//PCUlJVx22WWEwyYJ3nvvvd16bkKI6BY104wD+Hxl+P0lxMePQSlrpELstWSacSGOXDLNeCe1PeWudyVHIYToSVGVKCL5lDshhDhSRVWiiORzs4UQ4kgVsUShlOqvlFqklPpGKbVWKfWzdtZRSqlHlFKblFJfK6XGRioeQ2oUQghxoCI56ikI/FxrvUoplQCsVEp9oLX+Zrd1TgOGNL+OBZ5s/hkRUqMQQogDF7Eahda6TGu9qvn3BmAdkP2d1c4EntfGMiBZKZUVqZikRiGEEAfukPRRKKVygTHAF9/5KBso2u19MXsnE5RSVyulViilVlRUVBxEHJGpUdTW1vLEE090adtZs2bJ3ExCiMNaxBOFUioeWADcqLWu78o+tNbztNbjtdbjMzIyDiKayNQoOkoUwWCww23fffddkpOTuzUeIYToThFNFEopOyZJvKS1fr2dVUqA/ru9z2leFqF4IlOjmDNnDps3b6agoIBbbrmFxYsXM3XqVGbPns3w4cMBOOussxg3bhz5+fnMmzevddvc3FwqKyvZtm0bw4YN46qrriI/P5+ZM2fi8Xj2OtZbb73Fsccey5gxYzjppJPYuXMnAG63m8suu4yRI0cyatQoFixYAMB7773H2LFjGT16NDNmzOjW8xZCRIeIdWYrM0fGM8A6rfWf97Ham8B1SqlXMJ3YdVrrsoM5bgezjAN2QqGhWCwODmQGj/3MMs59991HYWEhq5sPvHjxYlatWkVhYSF5eXkAPPvss6SmpuLxeJgwYQLnnHMOaWlpe+xn48aNvPzyyzz11FOcd955LFiwgIsvvniPdY4//niWLVuGUoqnn36a+++/nz/96U/8/ve/JykpiTVr1gBQU1NDRUUFV111FUuWLCEvL4/q6urOn7QQQjSL5KinKcCPgDVKqZai+1ZgAIDWei7wLjAL2AQ0AZdFMB6ge+d36sjEiRNbkwTAI488whtvvAFAUVERGzdu3CtR5OXlUVBgZrYdN24c27Zt22u/xcXFnH/++ZSVleH3+1uP8eGHH/LKK6+0rpeSksJbb73FtGnTWtdJTU3t1nMUQkSHiCUKrfVn7Kdk1mYujWu787gdXflrrXG7NxAT0w+Ho193HnYvcXFtD0davHgxH374IUuXLsXlcnHCCSe0O924w+Fo/d1qtbbb9HT99ddz8803M3v2bBYvXswdd9wRkfiFEKJFFN6ZbWaQ7U4JCQk0NDTs8/O6ujpSUlJwuVysX7+eZcuWdflYdXV1ZGebgWHPPfdc6/KTTz55j8ex1tTUMGnSJJYsWcLWrVsBpOlJCNElUZUoDEu3TwqYlpbGlClTGDFiBLfccsten5966qkEg0GGDRvGnDlzmDRpUpePdccdd3Duuecybtw40tPTW5f/+te/pqamhhEjRjB69GgWLVpERkYG8+bN4+yzz2b06NGtD1QSQogDEVXTjAO43V9hsyXjdA6MRHi9mkwzLsSRS6YZPyAWtA71dBBCCNFrRF2iMKN2e1ctSgghelLUJQpTo5C5noQQorOiLlGYkU+SKIQQorOiLlFIjUIIIQ5M1CUKqVEIIcSBibpEcbjUKOLj43s6BCGE6JSoSxRSoxBCiAMTdYkiEjWKOXPm7DF9xh133MEDDzyA2+1mxowZjB07lpEjR/Kvf/1rv/va13Tk7U0Xvq+pxYUQojtFcvbYHnHjezeyunyf84wTDvvQ2o/VmtDpfRb0LeChU/c92+D555/PjTfeyLXXmvkNX3vtNRYuXIjT6eSNN94gMTGRyspKJk2axOzZs5vv5Whfe9ORh8PhdqcLb29qcSGE6G5HXKLYv+6fanzMmDHs2rWL0tJSKioqSElJoX///gQCAW699VaWLFmCxWKhpKSEnTt30rdv333uq73pyCsqKtqdLry9qcWFEKK7HXGJoqMrfwCfrxy/v5j4+DEoZe2245577rnMnz+f8vLy1sn3XnrpJSoqKli5ciV2u53c3Nx2pxdv0dnpyIUQ4lCKuj6KSD0O9fzzz+eVV15h/vz5nHvuuYCZEjwzMxO73c6iRYvYvn17h/vY13Tk+5ouvL2pxYUQortFXaJoO+XuTRT5+fk0NDSQnZ1NVlYWABdddBErVqxg5MiRPP/88xxzzDEd7mNf05Hva7rw9qYWF0KI7hZ104wHAtV4vVtwufKxWmMjEWKvJdOMC3HkkmnGD0hkahRCCHGkirpEEak+CiGEOFIdMYmi801oLafcu5rcIq23NUEKIQ6dIyJROJ1OqqqqOlXYtdzsJjWKNlprqqqqcDqdPR2KEOIwdETcR5GTk0NxcTEVFRX7XikYBJ+PsNOOP1CJ3Q5Wa9yhC/Iw53Q6ycnJ6ekwhBCHoSMiUdjt9ta7lvdp/nw491x8y99jqfs0hg59hqysyw9NgEII0YsdEU1PnZKRAYCl0g1AKNTUk9EIIUSvET2JIjMTAGu1B7AQCHTQTCWEEKJV9CSKlhpFVQ0ORzZe77aejUcIIXqJ6EkUqalgscCuXTiduZIohBCik6InUVgskJ4OFRU4nQPxejueoE8IIYQRPYkCTPNTc43C5ysmHA72dERCCHHYi65EkZnZXKPIBUL4fMU9HZEQQhz2oitR7FajAKSfQgghOiG6EkVzjcLhGAhIohBCiM6IrkSRkQE1NTitfQGFzycd2kIIsT/RlSiab7qzVDcQE9NPahRCCNEJEUsUSqlnlVK7lFKF+/j8BKVUnVJqdfPrN5GKpVXzTXdyL4UQQnReJGsUfwNO3c86n2qtC5pfv4tgLEZzjaLtXoptET+kEEL0dhFLFFrrJUB1pPbfJS01iuYhsl5vkdxLIYQQ+9HTfRTHKaW+Ukr9WymVH/GjfafpCUL4/aURP6wQQvRmPZkoVgEDtdajgUeBf+5rRaXU1UqpFUqpFR0+nGh/UlLAat3tpjsZIiuEEPvTY4lCa12vtXY3//4uYFdKpe9j3Xla6/Fa6/EZLbWCrmiZ72nXLpxOuZdCCCE6o8cShVKqr2p+gLVSamJzLFURP3DrTXcDAEkUQgixPxF7FKpS6mXgBCBdKVUM/BawA2it5wI/BH6ilAoCHuACrbWOVDytmqfxsFqdxMRkSaIQQoj9iFii0FpfuJ/PHwMei9Tx9ykzE1auBMDpHERT07eHPAQhhOhNenrU06HXXKMAiI8voLHxK7QO93BQQghx+Iq+RJGZCXV14PeTkDCGUMiNx7Opp6MSQojDVvQlit1uuouPHwuA2/1lDwYkhBCHt6hOFHFx+Shlp6FBEoUQQuxL9CWK3eZ7slhiiIsbgdu9qmdjEkKIw1j0JYrdpvEAiI8fg9v9JYdiZK4QQvRG0Zco+vQxP8vKAJMoAoFKfL6SHgxKCCEOX9GXKJKTTfPTunUAJCRIh7YQQnQk+hIFwIgRsHYtAHFxowAl/RRCCLEP0Zko8vNNotAamy2e2NijZeSTEELsQ/QmCrcbduwATPOTND0JIUT7ojdRQGvzU3z8GHy+HQQCkZ+8VggheptOJQql1M+UUonKeEYptUopNTPSwUVMO4kCkOYnIYRoR2drFJdrreuBmUAK8CPgvohFFWkpKZCV1ZooEhJMopDmJyGE2FtnE4Vq/jkLeEFrvXa3Zb3TiBFQWAiA3Z6GwzFARj4JIUQ7OpsoViql3sckioVKqQSgd8/NnZ9v7qUIm9NISBgrTU9CCNGOziaKK4A5wAStdRPmSXWXRSyqQyE/H5qaYNs2wPRTeDzfEgy6ezYuIYQ4zHQ2URwHbNBa1yqlLgZ+DdRFLqxDoN0ObU1j41c9F5MQQhyGOpsongSalFKjgZ8Dm4HnIxbVoTB8uPnZ2qFtpvKQ5ichhNhTZxNFUJvpVc8EHtNaPw4kRC6sQyApCXJyWju0Y2L6YbdnyMgnIYT4Dlsn12tQSv0KMyx2qlLKgumn6N3GjYMvvgBAKdU85biMfBJCiN11tkZxPuDD3E9RDuQAf4xYVIfKtGmwaROUlgKm+amxcS3hsL+HAxNCiMNHpxJFc3J4CUhSSn0f8Gqte3cfBcD06ebnJ58ApkNb6wCNjYU9GJQQQhxeOjuFx3nAf4FzgfOAL5RSP4xkYIdEQQEkJLQmisTEyQDU1n7Sk1EJIcRhpbN9FLdh7qHYBaCUygA+BOZHKrBDwmqF44+HJUsAcDpziI0dSk3Nh/Tvf1MPByeEEIeHzvZRWFqSRLOqA9j28DZ9urlDu/kZ2ikpJ1Fb+4n0UwghRLPOFvbvKaUWKqUuVUpdCrwDvBu5sA6hln6K5lpFSspJhMON1Nd/0YNBCSHE4aOzndm3APOAUc2veVrrX0YysENm3DhwuVoTRXLyCYCFmpoPezQsIYQ4XHS2jwKt9QJgQQRj6Rl2O0ye3Nqhbbcnk5AwgZqaD8nLu7OHgxNCiJ7XYY1CKdWglKpv59WglKo/VEFG3NSpsGYNNDQApvmpvv4LgsEj5xSFEKKrOkwUWusErXViO68ErXXioQoy4saPB63hSzN9R0rKSUBIhskKIQRHysilgzXWTAjIKjN9R1LScVgssdJPIYQQSKIw+vaFfv1g5UoALBYHSUnTJFEIIQSSKNqMG9daowDT/NTU9A0+X2kPBiWEED1PEkWLsWNh/XpobARa+imgpuajnoxKCCF6XMQShVLqWaXULqVUuzPsKeMRpdQmpdTXSqmxkYqlU8aNM8/PXr0agPj4Udjt6dL8JISIepGsUfwNOLWDz08DhjS/rsY8Ra/nfKdDWykLyckzqKn5EPPMJiGEiE4RSxRa6yVAdQernAk8r41lQLJSKitS8exXv37Qp09rhzaY5ie/v5SmpvU9FpYQQvS0nuyjyAaKdntf3LysZyjVboc2IM1PQoio1is6s5VSVyulViilVlRUVETuQGPHwjffgMcDQGxsLk7nUdTUvB+5YwohxGGuJxNFCdB/t/c5zcv2orWep7Uer7Uen5GREbmIxo2DUGiPWkV6+plUVy/E798ZueMKIcRhrNOTAkbAm8B1SqlXgGOBOq11WQ/GY6Yct9ngzTdhyhQA+vW7muLiP1NW9iwDB/6qR8MTQnSO1qZhwOkEi8Vc/1VVgd8PSUkQGwtuN9TXt720NstdLvPTZoOaGqiuhvh4c19uMAg7dkBlpVl/93Eu7f2+r8+rqqCwELZuBYfD7D8uzvy02cwAzJZXMGjOpakJvv99uPDCyH537YlYolBKvQycAKQrpYqB3wJ2AK31XMzzLGYBm4Am4LJIxdJpKSnwve/BggVw332gFC7XUJKTv0dZ2TwGDPgFSll7OkohDlooZApQpcDrhS1bzLO7kpIgOdkUUH6/KTCTk00hV1Ji1vF4wOcz23m9pqBLTjb7Ki426zidpuBrbDSFYmOjOebur0DAFIKBgHmFQnsXkHV1pqD2+028330pZQr82lqzfmyseXBlXZ3ZB5iC3+PZs6A+HCQmwlFHmXNvbDQvt7vtb7P7y+Uy3+eYMT0Ta8QShda6w7ynzZjTayN1/C475xz48Y/NbLKjRgHQr981fPPNeVRXv09a2mk9HKDoDVr+84dCptB0Ok0BprUpSNeuNYUbmMJ2505TIPbpAwMGmG2LisyExlar2d+uXWad5GSzntVq1qutNdtXVpoC3O/f+2WxQEaGKZzKykyhHw6bgtXrjWwharOZQs5mMzG3vOx2s8xuN6/dP2spILOyID/fJKPdk8jur/h4c41ntZqEEAya7yg+3pxbQ4M5fmam2U9dnbk6T0gw30diovldqbYr95b9pKRAaqopwMvLzTEGDDD7sjQ33CvVdq7t/d7esoQEyMnZ87Pdaa1p8DcQHxOPRfV8V3JPNj0dns46C665xtQqmhNFevqZ2O19KC19UhJFL+LzQVmFD6fNscfVWVOTaVLw+cyVmrKEqdhlobzcFLxeb9sVc22taR4oae49s9lMAbL7Oi2F8+4/Q6G942kpFJvHSuxJhcDmA22BsBXCNkChlCnEWwr65L611DV6qKgKE2pKJM4eT1ISpA8sJy67nEx7CvEqHWeMDXuMxmbTxMRovOFGiup3UOUrY/AYxXHpFuqtWygNfQ02DwNTchiScRTDbKdga+qPtvgpDX1FhWcntY1e/GEP8cleHHEetNVLyOLBo+toCtWSFZvH1JQLyIwZyK6YZWz2/pdwyIolFEuSK5aUhFhAU++rJ6RDDEoZRFZ8FstLl7N422IABiQNIMWZQjAcRClFn7g+9EvoR7+EfvSN78vmms0s3LSQyqZKTsw7kVF9RvHRlo94Z+M71IZ81MbEkxWfxYjMEQxKGURYh/EGveyo24G7dhvVQS91yoo/5Kc2tRZv0Et2QjY5iTkUBb3UeGqo9lRT46nBoiyMzB3JwOSBfF7xDauLV1PjraEp0IQtZCO1JJWUqhRSY1OJj4mnsqmSMncZWmucNmfrKy4mjkxXJumudBoDjVQ2VVLZVEmVp4o6bx2BcACFIicxh9zkXJKdycTaYtlQtYEPtnxAubsci7KQ7EwmxZlCSmwKlxVcxk8n/DRC/2P2TfW2m8nGjx+vV6xYEdmDTJ/e1ojYbMuW29ix4z4mTdqK0zkgssfv5XxBH6UNpaS70omPiafeV09pQyk5iTkkOBL2Wt/rhTABPtj2LtuqS2ios2H1p3J04hj6OAay1f0N6+tW0uDxEfI52OnexTZPITX+cqyBFKy+dPCkEXanEaxPx1+XRo1lPY2Dn4es1eDOhOrB5lVzFMQ0QGYhpG6G+HKweaD4ONj6PbAEIXkbaAV1A7CEXbj6f4slfTNhq5uQpQmFFZt2YiMWG04sFo3HXkqTpQylFDYcxKoUkqx9sCgrVaGtNOpKksJ5JIUGY41txGsvoUnX4A014Q15CLTzjHaFIiU2hUEpg0hyJLG2Yi3l7vI91om1xaKUoinQ1KW/VWpsKgkxCZQ2lBIIBwAYkjqEovoivEFvh9vGx8ST6EiktMHMh+a0Ofe7zXdlxmXitDkpqS8hpNvJrt8Ra4vFE2zLtHnJeaS70mnwN1BUV0RjoHGvbZw2Jy67i7AOY7PYSHGmEGONoaShhFqvqdYlOZJIjU0lJTYFX9DH+sr1hHQIl93F6D6j6RPfh1hbLIFwoC2peGto8DWQ5kojKz4Lm8WGN+htfTX4G9jVuAtv0ItCkeZKI92VTrornURHInaLnZAOUVRXxLbabdT76tFo0l3pnDzoZAr6FuD2u1uPVe2p5pxh53Dl2CsP6DtuoZRaqbUe36VtJVG045FH4Gc/M3M/DR0KgMezjS++GMTAgb8mL+93kT1+BPhDfgKhAHExcR2u5wv6KNxVyNbarSQ7k0mNTaXaU01JfQmpsakcm3MsGa4MGvwNlNSXUNpQyjfFJRTXlVIVKGFNxWpW71qOP+wDwKpshHTQ/K4d9Gs6lQRPPg3W7TTqSrxVmTTVxsGwNyC+nZFlYQtYwnsvr8uB+hwsrlpwVRJ2VO+1XjYTGOU6hXpdzq7gJipCm6gNF2PDQY5zGNnOo0lUfUHb2BT4hE2Nq7AoC1lxOSgFpe5iQjpEdkI2Q9KGkOhIJNYW23q12vLSaPol9CMr3twv6g16qfHWUO4uJxQOkZucS1psGltrt7KpehMJjgRyEnNIdabisruItcfisrtwWB1oNMFwkFA4RCAcoKqpii21W6jx1DA8YzgjMke0NkfUeevY2biTsA63XqXX+eqobKokFA6hlEKhUEoRa4tlQNIA+iX0AyAYDra+V0oR1mE2VG7g7W/f5tMdnzIkdQjH9T+OgUkDibXH4rQ5ibU1/2x+39IkUlxfzGtrX2N77XZOzDuRqQOmYrVY8QQ8eIIemgJNWJSFJEcSGs2m6k0U1xdT0LeA/Ix8lFIEw0Ea/Y3YrXbCOsxO905KG0opbSilzF1G3/i+zMibQZIziWXFy/h659dMGziNkZkjUc3tN2EdZkfdDnbU7cBmseGwOshOzKZPXJ/Wdb6r0d+I0+bEatmz77HlYmdA0oC9PjsQWmsaA43E2mL3ux+tNf6QH7vVHpHmJkkU3a24GPr3h3vugV+1jXT6+uvv43avYtKk7Vgs9sjGcJDCOtz6j+3fG//NFW9eQa23lgtGXMBpg09jXeU6vt75detVUYO/AbffTbm7nGA42OG+bTqWoGqn/cSbBJVDYcdUqBgGsdXgqoLGDGy+PsTkLcc3aAGh2DLsnv7EBDPAVUHAXkke32Ok/8cMjp1An6wAxJez2bOSnf4t5LpGckz8RFIT4nHG+chOS6Z/RjIJCabNuOV8a721VDVVUdlUSbqoyxgvAAAgAElEQVQrnSFpQ/YK0RPwYLfasVn2bnVt8DUQa49t/SwUDuEL+XDZXQf47Qtx+JFEEQkFBaYX6+OPWxdVVr5NYeEZ5OfPJyPjnMjHsB9rdq7hha9f4OXCl6n31TM4dTDxMfFsqt5EubucwamDyUnM4eOtH5Ofkc+x2ZN5pfDvNAUbUSgGxA8hwZKBNZRAQ2UC5TsSaNrZF2dtAY7GIdT56sBZDZ5U7N5sEvvtxJO2jICzhFBtPyyN/RiVl82Mif3IS+tHyBuH1Wo6AFsKcbsdBg0yHYCmM1cT0qF2C2ohRORIooiEn/8cHnvM9Hq6zBWl1iGWLcvD5RrK6NEfdPsh538zn6dXPU1mXCa5ybl8/+jvM6HfBILhIIu2LWJD5QZi7bFUe6p5ufBlVpevxmaxcergUxmYNJDNNZup87iJ8x2FrzKLHZ71VLKOuOLZhD/6HZXlTnDUQ/p6c8Xvb+sviIuDmTNN/31Dg+nwzcw0Y8cLCsxN6w5Ht5+yEOIQkUQRCf/+N8yaBe+/Dyef3Lp427a72LbtdiZO3IDLdXSXd+/2u3n+q+dJciTxvbzv8fjyx7n707vJTc4lrMOtnXtDUodQ7ammylO1x/ZHxU5gsPtHsPYCNqzKoLp5+kW3u238eGYmZGeb+Q6zs9t+T09vu1koJcW8HzLEDOEUQhyZDiZRSP1/X6ZONeMZP/poj0SRlXUl27ffxY4df+CYY57Z5+Z13jq8QS+psalUear4qvwrShpKiLXFUtpQyv2f38+uxl17bHPFmCt4fNbjOGwO6rx1PP35fJ5e+g/6+sdzsuU8VPFkPl7iY2eZlc0N/dhqMX3tkyaZcfVKmfHjU6aYZfHxEft2hBBRRGoUHZk2zbTBfOd4GzfeSEnJY0ycuB6Xa/BemxXuKuS4Z47D7Xfvc9fTB07n3hn3EmON4YMtH5BmG0DmzgtZuVJRVgabNsGSJaZ20DKWPjnZ5KyTTjLTUg0fbm6YEkKI/ZEaRaTMmAF33mluh01NbV08YMAcysrmsX377xk27Lk9NgnrMFe9dRUOq4N7T7uXak81iY5ERvcZTW5yLr6QD4ViUNLRfPKJ4p8fwMcfj2PVKpMULBbTZNSvH9x6K1x8sak1hEImYVh6/iZNIUSUkUTRkZNOgjvugEWLzNQezRyOvvTr91OKix9k4MBbcbmGtn42d8VclhUv47mznuOS0Ze0Lvf74csvzSzm//2vufG7osKMCjruOLj9djPN1LHHtt9pbJUppoQQPUQSRUcmTjQN/R9+uEeiABgw4BeUls5l27bfk5B9L5/t+Izi+mLu+vQuZuTN4EejfgSYVqtnn4VXX6W1wzk2tm0WyFNOaR1UJYQQhyVJFB2x201J/te/ms6Bs8/GF/SxZPsS6nx1bPNO418fv8Tn1S8T1mao0YCkAfxx+lzeekvx4IOweLFJDGedBWefDaNHQ16e6ScXQojeQIqr/fnLX+CMMyi57Bz+uPlEXtRf7zFUNckOVxwzkmumPM8n/8rl6ccTGXez6XzOzoY//QmuvNLMUCmEEL2RJIr9SUvjtcd+wo/fWE5T/SJ+MPAULpl+AwOSBmCz2AhWPMiHC7dw8cnDWLfOzqRJ8LvfmVarE06AmJiePgEhhDg4kig6oLXmpoU38fAXD3Ns9jheuGc9Q45LgitnAWbW01/87n4efTSJ/v0ref31dM46a99zzAshRG8kgy07MHfFXB7+4mGum3Adn129jCEX3wD/+Ads2MDChWZqi0cfTeKiiz7kqaeGcNpp2yVJCCGOOJIo9uGTbZ9ww3s3cPqQ03no1IfMJHY33USZI5fZJzVx6qnm3oaFC+Hpp48hNtbH5s2/7OmwhRCi20miaMe6inWc89o5DE4dzEtnv9Q6j/zHazIYY1nNR8VHc//0d1h72QPMHLodpzOH/v1/QUXFq9TV/aeHoxdCiO4lieI7ttZs5aQXTsJutfPWhW+R5EwC4JlnzP13qdmxLB/6I25ZejYxt91ihs02NTFgwC3ExPRj06abCO/neQ5CCNGbSKLYza7GXZz0wkl4Ah7ev/h9BqeaeZz+9S+4+mpzS8XyL+0MX/+66cl+/33YuBF+/Wus1jiOOuqPNDQsp7BwNsFgfQ+fjRBCdA9JFM1C4RAXLriQ0oZS/n3RvxnZZyQA//kPXHABjB8P8+eb5zYAZmjTySfDT38KDz0En35Knz7/w9FHz6O6+n2+/PJ4fL7SnjshIYToJpIomt35yZ18vPVjHp/1OMfmHAuYeZnOOMM8ne2dd3ZLErv7wx8gNxfOPBPuvJN+jrMZNeo9PJ4trFt3MVq387xnIYToRSRRAB9s/oC7ltzFpQWXcvmYywHz2OxTTzUT9L33nnm4T7vi4+Htt+H4480EgkOGkBocxeDBD1Fbu4iSkscO2XkIIUQkRH2icPvdXPnWlRyTfgyPz3ocgMZG83C72lrzoLu8vP3sZPhwePNNM1a2pgYWLSIr6wpSU09ny5Zf0ti4PvInIoQQERL1ieL2j2+nqK6Ip2c/jctupnG9/nooLDR9EgUFB7CzE080U8F+/jlKKYYOfQqLxcU335xPMNgQmRMQQogIi+pEsbxkOY/89xF+Mv4nTO4/GYAXXjCTxf761zBz5gHu0G43D5T4/HMAHI4shg9/mcbGtXzzzYVoHermMxBCiMiL2kShteb6f19P3/i+3DPjHgC2boWf/MQ8AfU3v+nijidPNk8oamwEIDV1JkOGPEZ19Tt8++01MmxWCNHrRG2iWFq8lC9KvuC2qbe13lT3+9+baTlefPEgnhcxebLZyfLlrYuys6+hf/9fUlb2NEuX9mfLll8RDvu64SyEECLyojZRPPzFwyQ5klofV7ppEzz/PPz4x9C//0HseNIk87O5+anFUUfdx9ixy0lNPZUdO+5j8+b/O4iDCCHEoROViaKorogF3yzgyrFXEh8TD8Bdd5kuhl8e7Lx+qakwbNheiQIgMXE8+fmvkpNzEyUlj1FR8c+DPJgQQkReVCaKJ1c8iUZz3cTrAFObePFFuOYayMrqhgNMngxLl0K4/ZvtBg26l/j4cWzYcDle745uOKAQQkRO1CUKX9DHvJXzmD10NrnJuYDpm4iJ6YbaRIvJk6G6Gr791rz3eKCkBBrMEFmLxcHw4a+gdZCvvpqJz1fWTQcWQojuF3WJYsn2JVR5qri8wNyBvXGjqU385CfQt283HWSyGWrLsGFmTiiXC3JyYOBAU30BXK7BjBz5Lj5fMatXnyjJQghx2Iq6R6G+u/FdHFYH38v7HmBqEw4H/OIX3XiQoUNh3jwzD4jWJlEkJpqbM8491zRLOZ0kJx/PqFHv8fXXp7J8eT5ZWVfSr99PiY3N7cZghBDi4ERdonhn4zucmHcicTFxbNgAL70EN90Effp040GUgquu2nv5gAFmlsGbboInnwQgOfl4xoz5jB077qao6M8UFz/MUUfdT3b2DSh5rqoQ4jAQ0aYnpdSpSqkNSqlNSqk57Xx+qVKqQim1uvl1ZSTj2Vi1kY3VG5k1eBYA99xjahO33BLJo+7m+983B5s7Fx55pHVxQkIB+fn/YNKkraSmnsKmTTdSWHgmgUDNIQpMCCH2LWI1CqWUFXgcOBkoBpYrpd7UWn/znVVf1VpfF6k4dvfuxncBOP3o06mthVdfhSuu6ObaxP7cc4/pp/jZz8DphJEj4f77YfBgnH/8IyNG/IuSkkfZvPn/WLXqWEaOfAuXa+ghDFAIIfYUyRrFRGCT1nqL1toPvAKcGcHj7de7m95laNpQBqUMYv588Png0ksPcRA2G7z8Mpx2mrm7b/JkM/Psn/8MW7eilCIn5wYKChYRDNayatUkdu78u8wTJYToMZFMFNlA0W7vi5uXfdc5SqmvlVLzlVLt3hOtlLpaKbVCKbWioqKiS8E0+htZvG0xpw85HTB3YQ8dap5cd8g5HLBgAVx7rUkQ69aZfo3HH29dJSlpCmPH/henM4916y5i+fIR7Nz5CjoQgK+/7oGghRDRqqeHx74F5GqtRwEfAM+1t5LWep7WerzWenxGRkaXDvTR1o/wh/zMGjKLrVvh00/hkktM+dwjYmPhscdMx/bRR8MPfwhPPw1u926r5DJu3HKGD38NpeysW3chlZfkwejRZg50IYQ4BCKZKEqA3WsIOc3LWmmtq7TWLbPjPQ2Mi1Qww9KHcfu025k6cCovvmiWXXRRpI7WBT/7GdTVmXnOd6NCmsyMHzJ+/JeM2HkjGa+UELaD/8cXsP7zH9LYuK6HAhZCRAultY7MjpWyAd8CMzAJYjnwP1rrtbutk6W1Lmv+/QfAL7XWkzra7/jx4/WKFSu6HJfWpskpOxsWLerybrqf1jBxorn3Ij8fysuhrMzc4T1ypEkkd96JjnWw8w8z6XPuk+w6ycbG2xIZPXohCQkRy7FCiCOAUmql1rpLje0RG/WktQ4qpa4DFgJW4Fmt9Vql1O+AFVrrN4EblFKzgSBQDVwaqXhalJebu7GvOyTjrA6AUuaZ2zffDE1Npjlq+nQzyeAbb8CVV4LVivr8c/pOnAi/TKHP3XdTNdPKan0ieXn3EBs7mPj4kTgc7XUFCSFE10SsRhEpB1uj+OILMxP4W2+Z2xp6Ba3hvffMJIOnm854vF4YMwbd2MDK5+Jxqw2kLIekb6xY7/wDOf1vQqme7oISQhwuDssaxeGqqHkc1kE9c+JQU8oMp92d0wnPPIM6/njGLfgpgZPnYL/tKlQgyMaE/+Ory98mPWU2KQu24jzjGqxDhvdM7EKIXk8SRW82eTJcfz3qkUeIedoBI0aiMzMZPO8jvhq9Gsfzi4lbAg1Pz6XuvT+Slf1jrFZnT0cthOhloq5toqjIzNGXktLTkXSTu++Go46CIUPg/fdRzz+PSkpl9I8byFgCvtOPI2FdgMZHb2Tp0izWf3khVUsfwe/b1bXjbdlibhAUQkSNqEsUO3aY2sQRM99efDysXg2rVkF6OmRmwvPPo9LS4MUXcbz1H/TxxzPk2UQGrhnHwNmvkTb5ZzRN6MP6RwawY9t9bXNKFRfDtm37PlY4bGa/PfNM+PLLQ3J6QoieF3Wd2cceC0lJ8P773RjU4Ujrtmz41VcwdiyEw+jcXHwXzsD2t/nYyurwpUP1cTaSKvoQ+0UpKi7OfDnHHbf3Pv/xDzjvPLBa4aSTTAe7EKJXOJjO7KirURQVHSH9E/uze5Vp9Gh48EH4v/9DrVmD856nsW3dCS+8gHXyDPp8CGpHCdt/pPGmhAjPPJG6jx4nGGy7S5xgEG6/3dzjce+9sHAhfPTRoT8vIcQhF1U1Cr/fDBb6zW/MLQuiWSiEP1hBadlfqCt8jaOv+gZHJQTjAUcM4QmjceSMQ82da+7pOPVUc59Herp54pPHA7m5ptaSlNTxsdatM81WEyeavpUjpg3wCBEOw+LFMGWKmZNMHDFkeGwnlZaaFpmoqFEcCKuVGGtfcnN/C7m/JfjZGnwP3kmgZguBqi3Ef74cVb0c/9hBeE7IINZSh/33v0ddeilceOGe+5owAa6+Gi64wPSf7O7dd82cVh6PeZ+WBsccY26VnzwZZswwCScarFljmvCGH+Sw5YYGqKkxD8XqDs88Y/5+l19ufheCKKtRfPopTJtmWk1mzuzmwI5QWofZWfYcO9//Be7kSgLJZrnVEkdqWS7pmeeQnnM+1i1F8N//mod8rG2epaXleeETJpgmq7lzTTPYY49BYSEsXw7ffmvWr6w02wwaZBJGVpZ5ZOz27WbixKuuMgUrQChkCsedO03ne3m5STD5+ebhIh3VUrZuNVO97+9qQWt47jlYudI029kO4Jpq1SoTf1ZW+59v2GCmLfZ64bbb4NZbISam8/vf3SmnwMcfm8fs3nor2O1d2w9AVZWpKQaDUF9vzv+SS7q+v4MVCJjvKBw2jxI+kNpnfb35u1199Z5/h9377vZ37MJCKCjoeq3X7TZPsvzb3+DGG9t/6uUhdDA1CrTWveo1btw43VUvvaQ1aP3NN13eRdQKhfy6sXG9rqx8WxcVPay//fYGvXz5WL1oEfqzzzJ0YeF5esuW23V52Qva/d5TOvjbOTr0q1/o8LXXaj1mjNZKaX3yyVrX1e2983BY68JCrR9+WOvZs7VOTDTrjxql9bHHmj/auHFaf//7WufkmM/Mf/m9X3FxWg8bpvUpp2h99dVa33uv1h98oPWGDVpffnnbtlOmaP3001r7/W0xLFum9ZIlWq9apfXZZ7ft86672mItLtb6k0+0fustrd97T+u1a7Wurzefud1aX3ut2WbAAK2LivY+18ZGrUeM0DotTesLL2w7t7KyA/+jfPyx2X7kyLaf99yj9RdfaB0KHfj+rrpKa6tV69WrtZ4+XWuXS+u//918L+393Xa3eLHWDzygdVPT/o+zcqXW552n9ddfty2rrTXfbYsPPtA6KantbzBsmNb//Kf5O31XKKT1n/+s9V/+0rbsyivbvlu32/yNTj9d64kTzbE6Ul2t9YwZZvtnntn/+dTXa33RRVr/+Mdav/++1p9+qvUNN2idmtr2bwG0vu46rXfu1Hr9eq1LS9u2X7vW/NseNUrrIUO0/ulP2/5d7u4//zHn0kWYqZO6VO72eMF/oK+DSRT33mvOuKGhy7sQuwmHw7qmZrEuLDxXL116lF60yKIXLWKP1+LFMXrdust1U9Xazu84EGj7I4XDprA66iit8/PNf8jf/MYklZdf1nrpUq23bNH6o4/MshtvNIX8+PFaZ2bumURiYrT++c+1vvtusy/Q+uijzT+MlsK25WW3a33//aZAs9u1/vJLrZ96yuyjvQSVnGwKfzAJKTHRFG6VlW3nFQxq/aMfmWT13ntm2euvm+R21FFab96stc+n9YoVWj/5pNZXXKH1HXdo7fXu/qW3/TzuOJM4PR6tFyzQevTotniGD9f6r381+9sft1vruXNNXD//uVlWUqJ1Vlbb/tLTTUHV3t/qttvaEvDRR5tz+/xzc25r1rTF7PebZGK3m3X79dN6+3ZTcObmmu/2t781CcHhMAn1gQe0/sMftD7mGLPNMceYv+8vfqH1229rvXWruShoifOJJ8zxQeuZM01cZ5xh/j1YrVrbbCYJ+Hzm9f77Wr/6qtYvvmi+r7/8xZyD3a710KHm77hjx57n7PWauMNhk0AnTzb7jotri8Ph0Pqcc8y/z2BQ65tv3vPfi1ImOdx6q1k3Lc1cJJ1+uvl81ixzUdHio4+0djpNMuoiSRSd9NOfap2S0uXNxX6EQl7tdq/Vu3a9rouKHtLbtt2r16+/Si9e7NCLFln1mjVn6Z07X9FNTVt1Q8NX2u1eq8PtXSF2p+pqUxg89JDWmza1LQ+HtX7zTVOggkkczzxjrmRffVXrdevMepWVWvft23Z1e/LJZp3//tdcOf7976Ygu/ZarS+4wFzla22usJ1OU7D9/e9af/ut1tOmmX3ccceeMS5bZq4+ExJModFSmKSkmJ8FBVrPn28KHqvVFCb3328+mzdvz33t3Kn13/7WljSOPtrUkvx+U4iOH2/2N3681pMmaX388ea4LcdpqR1pbZL1ihVav/GG1oMHm9geftgkhuOPN+eWkWG2vewyU8vKy9s7ifbta5KmzWben3WW+X4SE018aWlmP+ec07bNmDF7JtlAwCTq004zx21JNi2F8hNPmIJXKfNdDhtmEuif/2zWcTrN3/tvfzPvjzuuLfbvvjIzzd9282ZT+J98sim0163T+s47te7Tx6yXm2tisdm0/sc/TG3qn//U+pVX9vweW7zzjtaPPmqaNn7967b9/OAHWpeXt633l7+Y8xgzxvzdFy0ytbsRI7SuqDjg/wItJFF00hlnmNqdOLS83lK9adP/6f/8p+9eNY4VKybq8vKXdUPDau12r9PBYOP+d9idAgFzRdtRwnrnHXO1+8tfmqvDznr/fXNV2lIAJSZq/dxz7R9r7VqTaH7+c5Ootmwx6731lrmab0kcl17a1qQxeHD7TRRam23fftsUZtBWOxg3zly5nnaaKQBPPNEU8kuWdPwdVFSY5jowyWrSJFPbuuwyU0i2cLtNTe+dd7RevlzrZ5/V+n/+R+szz9T6V7/as/no44/N9zpokNYbN5plCxeapFtd3fF329Sk9Ycfmtrhl1+2LZs6VWuLxSTflu/h6adNYm9x990m0Zx9tkkehYWmaXLLFtP85fG0rfvEE3snklmzTAI680yT6N54o+NY98XnMxcQ7X3vr7/e1mTVkvC70jy5m4NJFFHVmT1mjHkOxdtvd3NQolO0DlFb+ykezyZstiT8/jJKSh7F49nUuo5SdhISJpKUNAWX6xhiYwdjt6dhs6Vgs6X03FxVPl/XhouGw+Yf3GefmUffDhx44PvYudNMezxjBsTFmY7aZ581I8UmTux428ZG+O1vzU2XN94Is2Z1vXPW5zOd+/n5+x8G3VnffmtmE0hO7p79eb1m+oWjj+54vUCgcx3/WsOf/mT2O2CAuWN36NDuiXV/QiEzUnDhQpgzB3JyDmp3B9OZHVWJIj3dzEDx5JPdHJToMq1D1NV9RiBQRTjsxe3+mtraRbjdX6J1YK/1lXLgcg0lNfVUkpKmYrMlYLUmER8/SqZVF6IDch9FJzQ1mdF/cg/F4UUpK8nJ01vf9+nzPwCEw0G83m14vVsIBKoJBmsJBmsIBqtpaFhJcfGDFBXd37qd0zmIfv2uJjn5ezidA7DbM1FyM58Q3SJqEkVxsfnZXfcliciyWGy4XINxuQa3+3kw2EBj41rC4SZ8viLKyv7Kli1zWj+PielHnz4XkZw8nfr6ZdTVfQ6A1RpPXNxw0tK+T2LiJJSyHpLzEaI3i5pEcUQ9h0JgsyWQlNT2ePW+ff8Xj2czjY1r8Xq3U1PzQXOt44+AlYSEMVgsTrzerVRXv8uOHfehlAOHIxuHoz8ORw5OZ3/i4kaTmDgJhyObcNiLUnZ5hoeIelGTKNxuM2OEJIojV2zsUcTGHgVATs71+P0VNDZ+TULCeGy2ts7XQKCWmpr3aWhYic9XhM9XTH39f6ioKNmrX0QpG4mJk0lNPYXExONISBiHzZZIS9+eNG+JaBBVndlCdCQcDtDYuIb6+mUEAtVYLE4CgQpqat7H7V6925pWIITFEofLNQSHoz9ahwFNXFw+iYmTiY0dhNWagN2egc1m5rxqbFxLVdW/SU4+gcTErs2kIERXyagnISIsEKiivn45DQ0rCIc9KGUnGKzF49mIz1eMUjYgTGPjN2jt32PbmJhsrNZ4PJ4Nrcv69LmYzMyLAI3NlkRCwgQsFjNcU+sQYJHaiuhWkiiEOEyEQl7c7i/x+UoIhRrw+8toalpPIFBBauosUlNPo7z8WYqK/ozWvtbtrNYEEhIm4veX0tT0LU5nLmlps3A6B+H3lxAKNeJyHYPLNRylrGjtx2qNJyYmi5iYftKPIvZLEoUQvYzPV4bXuxWw4PeXUl39Pg0N/8XhGIDLNZTGxrXU1n7cXHtxYLE4CYXq9rE3C7Gxg4iNHYJSdpSyEhPTB4djIE5nLk7nQKzWeHy+YgKBCuz2TJzO/jgc/bHZEg/laYseJPdRCNHLOBxZOBxt019nZJy91zqhkJdwuBGbLRUAv7+cpibTfGWxxBAKNbQmnKamb/B4tjQ3W4WorV1CMFi13zis1kRcrqEkJEzE5RqC31+O378Tuz2NmJhsAoFduN1fo5SVzMzzSEs7s7XPRUQPSRRCHKasVuceTUrfTS77Ewy68fm24/VuJxRqwOHIwW7PJBDYhddb1Dziq4jGxjWUl/+NcLgRpWzY7ZkEg9XNw4NtuFzDCAbrqKp6E6VsxMRk4XBkExOTjcPRD7+/nPr6pYTDPrKyriIr63L8/gqamr5p3l8GNlsiStmwWFw4nQP2GIUmDn/S9CSEQOsQgUAVdns6SlnQWhMMVmO1JmCxxKB1mLq6/1Bd/R4+XzE+Xwl+fwk+Xwk2WwpJSZMJhTxUVb0J7L9MsVrjAUU47MdqjcVmS0YpB+Gwefqhy3U0LtcwnM5cHI5slLITCrkJhRoIhdyEwwFcrmOIjy8gJiYDpUwNy+vdgdaB5sEBch28O2l6EkIcFNOvkbnbe4XdnrbbewvJyVNJTp7a4X48ns1UVb2D0zkQlysfgEBgF6GQG62DrYW5z1eCUhaUshMOewgGawiH/VgssUCIpqYNlJf/jVCooUvnY7OlkZJyIj5fCW73V8TEZJKUNA27PYOmpnUEAhXExY0kIWE8cXH5zf07Vvz+ckKhJmy25NaXxeLA691CQ8MKtA4TF5ePyzUUiyV6nikuNQohxGFJa00oVI/XWwSEsFoTsFrjsVoTAEVjYyGNjV8TDNYRDvuwWl04nQMJh/1UVb1Jbe0SnM5c4uNH4/OVUle3hGCwHpdrKHZ7Om73aoLB6k5EYu6b+e4yl2sIsbFHA2FCoUaUisFqjW+eqDIBi8VBOOwlHPbjcPTH5ToGrYN4vZsBC+nps5tHsbUNgw6HgzQ1rWueKHMXSUnTSEqa3FyjakQpC1ZrXJe+Txn1JIQQ+2HKOt06y7DWGp9vB01NG2hq+haAmJg+WK0ugsH65okoawmF6nE680hImIBSNhob19LUtJbGxkI8nk0oZcdicaF14DvNYx4sFidK2QgEKtuNKSYmG6UshEKNhEKNewyZbmMBwgAMGPArBg26p0vnL01PQgixH+bKXe3x3ukciNM5kNTUmZ3eT3z8yAM+djDoxuP5FovFgdOZRzBYR2XlP6mr+wyLJQaLJQ6r1byczoEkJU3Fbk+jtnYJ9fVfoJQNqzWexMRJ+z9YBEiNQgghosDB1CjkSS9CCCE6JIlCCCFEhyRRCCGE6JAkCiGEEB2KaKJQSp2qlNqglNqklJrTzucOpdSrzZ9/oZTKjWQ8QgghDlzEEoUyDyN+HDgNGA5cqItCphQAAAdfSURBVJQa/p3VrgBqtNaDgQeBP0QqHiGEEF0TyRrFRGCT1nqLNk9yeQU48zvrnAk81/z7fGCGkqe1CCHEYSWSiSIbKNrtfXHzsnbX0VoHgTogDSGEEIeNXnFntlLqauDq5rdupdSGjtbvQDrQ/r30h7feGHdvjBl6Z9y9MWbonXH35pgHdnUHkUwUJUD/3d7nNC9rb51iZR46nATs9bQVrfU8YN7BBqSUWtHVOxN7Um+MuzfGDL0z7t4YM/TOuKM15kg2PS0Hhiil8pRSMcAFwJvfWedN4H+bf/8h8LHubXOKCCHEES5iNQqtdVApdR2wEDNP77Na67VKqd8BK7TWbwLPAC/8f3t3GyNXWYZx/H9ptVJqXBBBbYktSHyBSKnGVFFDwCggoXyQ2FjxNfELiWBI1FrF6DejETVRIEFt0QYMWIQQMMBKavhQKtS+2YIsQnBJsSQCigZEuPzwPKun053pOtiZM5nrl0x2zstO7r13ztwzz5xzP5KmgL9QiklERLTIIf2OwvYtwC0d6y5t3H8GOP9QxtDhRQ9fDckoxj2KMcNoxj2KMcNoxj2WMY9c99iIiBistPCIiIiexqZQHKydSBtIOlbSnZJ2S/q9pIvq+iMl3S7pgfrziGHH2knSSyX9TtLNdXlpbcsyVdu0vHzYMXaSNCHpekn3Sdoj6V0jkuvP1+fHLknXSHpF2/It6ceS9kna1Vg3a25VfL/GvkPS8pbF/a36HNkh6QZJE41ta2rc90v6YFtibmy7RJIlHVWX+8r1WBSKObYTaYN/AZfYfiuwAriwxvklYNL2CcBkXW6bi4A9jeVvApfV9ixPUNq1tM33gF/ZfjNwMiX+Vuda0iLgc8A7bJ9EOVFkFe3L9zrgzI513XJ7FnBCvX0WuHxAMc5mHQfGfTtwku23AX8A1gDUY3MVcGL9nR/W15pBW8eBMSPpWOADwCON1X3leiwKBXNrJzJ0tvfa3lrv/43ywrWI/VudrAfOG06Es5O0GPgQcFVdFnA6pS0LtDPmVwHvo5x5h+1/2n6Slue6mgccVq89WgDspWX5tv0bypmMTd1yuxK42sVmYELS6wYT6f5mi9v2bbVzBMBmyjVhUOK+1vazth8CpiivNQPVJddQ+ud9AWh+Ed1XrselUMylnUir1E66pwB3A8fY3ls3PQYcM6Swuvku5Qn5Ql1+NfBk4+BqY76XAo8DP6lDZldJOpyW59r2o8C3Ke8S91La3txL+/MN3XM7Ssfnp4Fb6/3Wxi1pJfCo7e0dm/qKeVwKxUiRtBD4BXCx7b82t9ULEltzqpqkc4B9tu8ddiz/o3nAcuBy26cAf6djmKltuQao4/orKYXu9cDhzDLs0HZtzO3BSFpLGR7eMOxYepG0APgycOnB9p2rcSkUc2kn0gqSXkYpEhtsb6yr/zzz8bD+3Des+GZxKnCupIcpQ3qnU8b+J+rQCLQz39PAtO276/L1lMLR5lwDvB94yPbjtp8DNlL+B23PN3TPbeuPT0mfBM4BVje6R7Q17uMpbyS21+NyMbBV0mvpM+ZxKRRzaScydHVs/0fAHtvfaWxqtjr5BHDjoGPrxvYa24ttL6Hk9de2VwN3UtqyQMtiBrD9GPAnSW+qq84AdtPiXFePACskLajPl5m4W53vqltubwI+Xs/IWQE81RiiGjpJZ1KGVs+1/Y/GppuAVSoTsC2lfEG8ZRgxNtneafto20vqcTkNLK/P+f5ybXssbsDZlDMWHgTWDjueLjG+h/JxfAewrd7Opoz5TwIPAHcARw471i7xnwbcXO8fRzlopoDrgPnDjm+WeJcB99R8/xI4YhRyDXwduA/YBfwUmN+2fAPXUL5Dea6+UH2mW24BUc5KfBDYSTmjq01xT1HG9WeOySsa+6+tcd8PnNWWmDu2Pwwc9WJynSuzIyKip3EZeoqIiD6lUERERE8pFBER0VMKRURE9JRCERERPaVQRAyQpNNUO+xGjIoUioiI6CmFImIWkj4maYukbZKuVJlv42lJl9W5ICYlvabuu0zS5sZ8BTPzLLxR0h2StkvaKun4+vAL9d95MDbUK6wjWiuFIqKDpLcAHwFOtb0MeB5YTWnAd4/tE4FNwNfqr1wNfNFlvoKdjfUbgB/YPhl4N+XqWShdgS+mzI1yHKVXU0RrzTv4LhFj5wzg7cBv65v9wygN7F4Afl73+Rmwsc5rMWF7U12/HrhO0iuBRbZvALD9DEB9vC22p+vyNmAJcNeh/7Mi+pNCEXEgAettr9lvpfTVjv367X/zbOP+8+Q4jJbL0FPEgSaBD0s6Gv4z1/MbKMfLTIfWjwJ32X4KeELSe+v6C4BNLjMUTks6rz7G/DpPQMTIyTuZiA62d0v6CnCbpJdQunJeSJnc6J112z7K9xhQWmZfUQvBH4FP1fUXAFdK+kZ9jPMH+GdE/N+ke2zEHEl62vbCYccRMWgZeoqIiJ7yiSIiInrKJ4qIiOgphSIiInpKoYiIiJ5SKCIioqcUioiI6CmFIiIievo35R08My1tlbYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 606us/sample - loss: 0.3323 - acc: 0.9084\n",
      "Loss: 0.3323279743749157 Accuracy: 0.9084112\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5116 - acc: 0.1635\n",
      "Epoch 00001: val_loss improved from inf to 1.85380, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/001-1.8538.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.5115 - acc: 0.1636 - val_loss: 1.8538 - val_acc: 0.4372\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8019 - acc: 0.4051\n",
      "Epoch 00002: val_loss improved from 1.85380 to 1.45698, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/002-1.4570.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.8019 - acc: 0.4051 - val_loss: 1.4570 - val_acc: 0.5425\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5439 - acc: 0.4873\n",
      "Epoch 00003: val_loss improved from 1.45698 to 1.27374, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/003-1.2737.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5440 - acc: 0.4873 - val_loss: 1.2737 - val_acc: 0.6257\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3842 - acc: 0.5455\n",
      "Epoch 00004: val_loss improved from 1.27374 to 1.09240, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/004-1.0924.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3842 - acc: 0.5455 - val_loss: 1.0924 - val_acc: 0.6709\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2556 - acc: 0.5914\n",
      "Epoch 00005: val_loss improved from 1.09240 to 1.02219, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/005-1.0222.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2557 - acc: 0.5913 - val_loss: 1.0222 - val_acc: 0.7014\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1371 - acc: 0.6337\n",
      "Epoch 00006: val_loss improved from 1.02219 to 0.88721, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/006-0.8872.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1370 - acc: 0.6337 - val_loss: 0.8872 - val_acc: 0.7393\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0304 - acc: 0.6727\n",
      "Epoch 00007: val_loss improved from 0.88721 to 0.76571, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/007-0.7657.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0304 - acc: 0.6726 - val_loss: 0.7657 - val_acc: 0.7838\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9215 - acc: 0.7125\n",
      "Epoch 00008: val_loss improved from 0.76571 to 0.66559, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/008-0.6656.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9216 - acc: 0.7125 - val_loss: 0.6656 - val_acc: 0.8220\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8370 - acc: 0.7404\n",
      "Epoch 00009: val_loss improved from 0.66559 to 0.60225, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/009-0.6022.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8369 - acc: 0.7405 - val_loss: 0.6022 - val_acc: 0.8297\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7720 - acc: 0.7619\n",
      "Epoch 00010: val_loss improved from 0.60225 to 0.54562, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/010-0.5456.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7720 - acc: 0.7619 - val_loss: 0.5456 - val_acc: 0.8488\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7136 - acc: 0.7802\n",
      "Epoch 00011: val_loss improved from 0.54562 to 0.48211, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/011-0.4821.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7135 - acc: 0.7802 - val_loss: 0.4821 - val_acc: 0.8693\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.7969\n",
      "Epoch 00012: val_loss improved from 0.48211 to 0.45724, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/012-0.4572.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6592 - acc: 0.7969 - val_loss: 0.4572 - val_acc: 0.8693\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.8079\n",
      "Epoch 00013: val_loss did not improve from 0.45724\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6246 - acc: 0.8079 - val_loss: 0.4834 - val_acc: 0.8626\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8195\n",
      "Epoch 00014: val_loss improved from 0.45724 to 0.39528, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/014-0.3953.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5907 - acc: 0.8195 - val_loss: 0.3953 - val_acc: 0.8915\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8275\n",
      "Epoch 00015: val_loss improved from 0.39528 to 0.39432, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/015-0.3943.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5589 - acc: 0.8274 - val_loss: 0.3943 - val_acc: 0.8924\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8350\n",
      "Epoch 00016: val_loss improved from 0.39432 to 0.35083, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/016-0.3508.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5314 - acc: 0.8350 - val_loss: 0.3508 - val_acc: 0.9010\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8435\n",
      "Epoch 00017: val_loss did not improve from 0.35083\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5086 - acc: 0.8435 - val_loss: 0.3604 - val_acc: 0.9047\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.8479\n",
      "Epoch 00018: val_loss improved from 0.35083 to 0.33721, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/018-0.3372.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4887 - acc: 0.8478 - val_loss: 0.3372 - val_acc: 0.9031\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8533\n",
      "Epoch 00019: val_loss did not improve from 0.33721\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4745 - acc: 0.8533 - val_loss: 0.3529 - val_acc: 0.8996\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.8608\n",
      "Epoch 00020: val_loss improved from 0.33721 to 0.33162, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/020-0.3316.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4485 - acc: 0.8608 - val_loss: 0.3316 - val_acc: 0.9082\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8638\n",
      "Epoch 00021: val_loss improved from 0.33162 to 0.28651, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/021-0.2865.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4418 - acc: 0.8638 - val_loss: 0.2865 - val_acc: 0.9206\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8708\n",
      "Epoch 00022: val_loss did not improve from 0.28651\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4244 - acc: 0.8708 - val_loss: 0.2972 - val_acc: 0.9168\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8711\n",
      "Epoch 00023: val_loss did not improve from 0.28651\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4122 - acc: 0.8711 - val_loss: 0.2975 - val_acc: 0.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8784\n",
      "Epoch 00024: val_loss improved from 0.28651 to 0.26659, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/024-0.2666.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3937 - acc: 0.8784 - val_loss: 0.2666 - val_acc: 0.9304\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8809\n",
      "Epoch 00025: val_loss did not improve from 0.26659\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3872 - acc: 0.8809 - val_loss: 0.2706 - val_acc: 0.9273\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8796\n",
      "Epoch 00026: val_loss improved from 0.26659 to 0.25356, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/026-0.2536.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3791 - acc: 0.8797 - val_loss: 0.2536 - val_acc: 0.9320\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8860\n",
      "Epoch 00027: val_loss did not improve from 0.25356\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3650 - acc: 0.8859 - val_loss: 0.2780 - val_acc: 0.9250\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8877\n",
      "Epoch 00028: val_loss improved from 0.25356 to 0.24595, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/028-0.2460.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3582 - acc: 0.8877 - val_loss: 0.2460 - val_acc: 0.9343\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8915\n",
      "Epoch 00029: val_loss did not improve from 0.24595\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3442 - acc: 0.8914 - val_loss: 0.2471 - val_acc: 0.9287\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8922\n",
      "Epoch 00030: val_loss improved from 0.24595 to 0.24446, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/030-0.2445.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3423 - acc: 0.8922 - val_loss: 0.2445 - val_acc: 0.9331\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8969\n",
      "Epoch 00031: val_loss did not improve from 0.24446\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3299 - acc: 0.8969 - val_loss: 0.2576 - val_acc: 0.9217\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8954\n",
      "Epoch 00032: val_loss improved from 0.24446 to 0.24165, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/032-0.2416.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3342 - acc: 0.8954 - val_loss: 0.2416 - val_acc: 0.9324\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9040\n",
      "Epoch 00033: val_loss improved from 0.24165 to 0.21474, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/033-0.2147.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3132 - acc: 0.9040 - val_loss: 0.2147 - val_acc: 0.9415\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9042\n",
      "Epoch 00034: val_loss did not improve from 0.21474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3078 - acc: 0.9042 - val_loss: 0.2581 - val_acc: 0.9278\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9058\n",
      "Epoch 00035: val_loss did not improve from 0.21474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2998 - acc: 0.9058 - val_loss: 0.2162 - val_acc: 0.9429\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9079\n",
      "Epoch 00036: val_loss did not improve from 0.21474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2967 - acc: 0.9079 - val_loss: 0.2170 - val_acc: 0.9404\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9093\n",
      "Epoch 00037: val_loss did not improve from 0.21474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2910 - acc: 0.9093 - val_loss: 0.2276 - val_acc: 0.9387\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9114\n",
      "Epoch 00038: val_loss did not improve from 0.21474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2821 - acc: 0.9114 - val_loss: 0.2191 - val_acc: 0.9432\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9114\n",
      "Epoch 00039: val_loss improved from 0.21474 to 0.21243, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/039-0.2124.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2805 - acc: 0.9114 - val_loss: 0.2124 - val_acc: 0.9429\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9150\n",
      "Epoch 00040: val_loss improved from 0.21243 to 0.21169, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/040-0.2117.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2755 - acc: 0.9150 - val_loss: 0.2117 - val_acc: 0.9422\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9155\n",
      "Epoch 00041: val_loss did not improve from 0.21169\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2671 - acc: 0.9155 - val_loss: 0.2133 - val_acc: 0.9415\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9156\n",
      "Epoch 00042: val_loss did not improve from 0.21169\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2646 - acc: 0.9156 - val_loss: 0.2118 - val_acc: 0.9427\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9189\n",
      "Epoch 00043: val_loss improved from 0.21169 to 0.19717, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/043-0.1972.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2565 - acc: 0.9189 - val_loss: 0.1972 - val_acc: 0.9439\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9203\n",
      "Epoch 00044: val_loss did not improve from 0.19717\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2527 - acc: 0.9203 - val_loss: 0.2089 - val_acc: 0.9471\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9204\n",
      "Epoch 00045: val_loss did not improve from 0.19717\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2509 - acc: 0.9204 - val_loss: 0.1985 - val_acc: 0.9453\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9221\n",
      "Epoch 00046: val_loss did not improve from 0.19717\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2452 - acc: 0.9220 - val_loss: 0.2014 - val_acc: 0.9432\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9223\n",
      "Epoch 00047: val_loss did not improve from 0.19717\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2436 - acc: 0.9223 - val_loss: 0.2012 - val_acc: 0.9471\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9233\n",
      "Epoch 00048: val_loss improved from 0.19717 to 0.18789, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/048-0.1879.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2389 - acc: 0.9234 - val_loss: 0.1879 - val_acc: 0.9504\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9246\n",
      "Epoch 00049: val_loss did not improve from 0.18789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2339 - acc: 0.9246 - val_loss: 0.1974 - val_acc: 0.9469\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9268\n",
      "Epoch 00050: val_loss did not improve from 0.18789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2304 - acc: 0.9268 - val_loss: 0.1984 - val_acc: 0.9469\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9277\n",
      "Epoch 00051: val_loss did not improve from 0.18789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2260 - acc: 0.9278 - val_loss: 0.1893 - val_acc: 0.9490\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9281\n",
      "Epoch 00052: val_loss did not improve from 0.18789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2251 - acc: 0.9281 - val_loss: 0.1926 - val_acc: 0.9504\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9286\n",
      "Epoch 00053: val_loss did not improve from 0.18789\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2167 - acc: 0.9286 - val_loss: 0.2127 - val_acc: 0.9448\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9281\n",
      "Epoch 00054: val_loss improved from 0.18789 to 0.18369, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/054-0.1837.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2176 - acc: 0.9281 - val_loss: 0.1837 - val_acc: 0.9511\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9316\n",
      "Epoch 00055: val_loss did not improve from 0.18369\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2113 - acc: 0.9316 - val_loss: 0.1914 - val_acc: 0.9488\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9322\n",
      "Epoch 00056: val_loss improved from 0.18369 to 0.17471, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/056-0.1747.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2110 - acc: 0.9322 - val_loss: 0.1747 - val_acc: 0.9539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9330\n",
      "Epoch 00057: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2090 - acc: 0.9330 - val_loss: 0.1866 - val_acc: 0.9511\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9338\n",
      "Epoch 00058: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2033 - acc: 0.9338 - val_loss: 0.1982 - val_acc: 0.9457\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9348\n",
      "Epoch 00059: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2027 - acc: 0.9348 - val_loss: 0.2027 - val_acc: 0.9462\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9358\n",
      "Epoch 00060: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1984 - acc: 0.9358 - val_loss: 0.1777 - val_acc: 0.9509\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9365\n",
      "Epoch 00061: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1954 - acc: 0.9366 - val_loss: 0.1784 - val_acc: 0.9532\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9366\n",
      "Epoch 00062: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1943 - acc: 0.9366 - val_loss: 0.1775 - val_acc: 0.9543\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9374\n",
      "Epoch 00063: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1902 - acc: 0.9374 - val_loss: 0.1819 - val_acc: 0.9492\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9404\n",
      "Epoch 00064: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1862 - acc: 0.9404 - val_loss: 0.2001 - val_acc: 0.9464\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9376\n",
      "Epoch 00065: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1930 - acc: 0.9376 - val_loss: 0.1970 - val_acc: 0.9499\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9373\n",
      "Epoch 00066: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1899 - acc: 0.9373 - val_loss: 0.1928 - val_acc: 0.9467\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9431\n",
      "Epoch 00067: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1786 - acc: 0.9431 - val_loss: 0.1946 - val_acc: 0.9483\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9424\n",
      "Epoch 00068: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1796 - acc: 0.9424 - val_loss: 0.1754 - val_acc: 0.9534\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9391\n",
      "Epoch 00069: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1810 - acc: 0.9391 - val_loss: 0.1850 - val_acc: 0.9518\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9429\n",
      "Epoch 00070: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1736 - acc: 0.9428 - val_loss: 0.1819 - val_acc: 0.9527\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9424\n",
      "Epoch 00071: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1722 - acc: 0.9424 - val_loss: 0.1803 - val_acc: 0.9536\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9453\n",
      "Epoch 00072: val_loss did not improve from 0.17471\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1690 - acc: 0.9453 - val_loss: 0.1913 - val_acc: 0.9548\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9449\n",
      "Epoch 00073: val_loss improved from 0.17471 to 0.16051, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/073-0.1605.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1684 - acc: 0.9449 - val_loss: 0.1605 - val_acc: 0.9534\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9463\n",
      "Epoch 00074: val_loss did not improve from 0.16051\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1659 - acc: 0.9463 - val_loss: 0.1854 - val_acc: 0.9474\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9468\n",
      "Epoch 00075: val_loss did not improve from 0.16051\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1601 - acc: 0.9468 - val_loss: 0.1722 - val_acc: 0.9536\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9475\n",
      "Epoch 00076: val_loss improved from 0.16051 to 0.15617, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/076-0.1562.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1633 - acc: 0.9475 - val_loss: 0.1562 - val_acc: 0.9576\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9468\n",
      "Epoch 00077: val_loss improved from 0.15617 to 0.15158, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_7_conv_checkpoint/077-0.1516.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1658 - acc: 0.9468 - val_loss: 0.1516 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9476\n",
      "Epoch 00078: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1572 - acc: 0.9476 - val_loss: 0.1814 - val_acc: 0.9536\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9484\n",
      "Epoch 00079: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1567 - acc: 0.9484 - val_loss: 0.1601 - val_acc: 0.9578\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9483\n",
      "Epoch 00080: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1550 - acc: 0.9483 - val_loss: 0.1573 - val_acc: 0.9567\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9490\n",
      "Epoch 00081: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1530 - acc: 0.9490 - val_loss: 0.1717 - val_acc: 0.9534\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9498\n",
      "Epoch 00082: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1517 - acc: 0.9498 - val_loss: 0.1767 - val_acc: 0.9550\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9521\n",
      "Epoch 00083: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1469 - acc: 0.9521 - val_loss: 0.1595 - val_acc: 0.9555\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9530\n",
      "Epoch 00084: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1427 - acc: 0.9530 - val_loss: 0.1595 - val_acc: 0.9592\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9528\n",
      "Epoch 00085: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1401 - acc: 0.9528 - val_loss: 0.1594 - val_acc: 0.9555\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9536\n",
      "Epoch 00086: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1439 - acc: 0.9536 - val_loss: 0.1717 - val_acc: 0.9557\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9512\n",
      "Epoch 00087: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1461 - acc: 0.9512 - val_loss: 0.1632 - val_acc: 0.9567\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9538\n",
      "Epoch 00088: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1405 - acc: 0.9538 - val_loss: 0.1526 - val_acc: 0.9630\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9542\n",
      "Epoch 00089: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1371 - acc: 0.9542 - val_loss: 0.1859 - val_acc: 0.9504\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9543\n",
      "Epoch 00090: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1369 - acc: 0.9544 - val_loss: 0.1609 - val_acc: 0.9581\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9554\n",
      "Epoch 00091: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1346 - acc: 0.9554 - val_loss: 0.1757 - val_acc: 0.9553\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9555\n",
      "Epoch 00092: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1334 - acc: 0.9555 - val_loss: 0.1622 - val_acc: 0.9583\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9565\n",
      "Epoch 00093: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1309 - acc: 0.9565 - val_loss: 0.1701 - val_acc: 0.9576\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9565\n",
      "Epoch 00094: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1329 - acc: 0.9565 - val_loss: 0.1676 - val_acc: 0.9502\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9574\n",
      "Epoch 00095: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1264 - acc: 0.9575 - val_loss: 0.1771 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9577\n",
      "Epoch 00096: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1284 - acc: 0.9577 - val_loss: 0.1604 - val_acc: 0.9581\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9561\n",
      "Epoch 00097: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1295 - acc: 0.9561 - val_loss: 0.1579 - val_acc: 0.9588\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9581\n",
      "Epoch 00098: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1259 - acc: 0.9581 - val_loss: 0.1558 - val_acc: 0.9604\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9583\n",
      "Epoch 00099: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1239 - acc: 0.9583 - val_loss: 0.1783 - val_acc: 0.9567\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9581\n",
      "Epoch 00100: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1232 - acc: 0.9581 - val_loss: 0.1676 - val_acc: 0.9585\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9609\n",
      "Epoch 00101: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1157 - acc: 0.9609 - val_loss: 0.1691 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9587\n",
      "Epoch 00102: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1226 - acc: 0.9587 - val_loss: 0.1817 - val_acc: 0.9557\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9611\n",
      "Epoch 00103: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1154 - acc: 0.9611 - val_loss: 0.1756 - val_acc: 0.9553\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9600\n",
      "Epoch 00104: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1185 - acc: 0.9600 - val_loss: 0.1672 - val_acc: 0.9583\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9608\n",
      "Epoch 00105: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1149 - acc: 0.9608 - val_loss: 0.1837 - val_acc: 0.9525\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9613\n",
      "Epoch 00106: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1139 - acc: 0.9613 - val_loss: 0.1642 - val_acc: 0.9585\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9615\n",
      "Epoch 00107: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1139 - acc: 0.9615 - val_loss: 0.1716 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9618\n",
      "Epoch 00108: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1107 - acc: 0.9617 - val_loss: 0.1610 - val_acc: 0.9602\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9638\n",
      "Epoch 00109: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1079 - acc: 0.9638 - val_loss: 0.1704 - val_acc: 0.9576\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9631\n",
      "Epoch 00110: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1099 - acc: 0.9631 - val_loss: 0.1703 - val_acc: 0.9555\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9643\n",
      "Epoch 00111: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1083 - acc: 0.9643 - val_loss: 0.1772 - val_acc: 0.9602\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9648\n",
      "Epoch 00112: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1054 - acc: 0.9648 - val_loss: 0.1782 - val_acc: 0.9590\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9628\n",
      "Epoch 00113: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1098 - acc: 0.9628 - val_loss: 0.1763 - val_acc: 0.9555\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9636\n",
      "Epoch 00114: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1063 - acc: 0.9636 - val_loss: 0.1719 - val_acc: 0.9588\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9643\n",
      "Epoch 00115: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1057 - acc: 0.9642 - val_loss: 0.1893 - val_acc: 0.9548\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9638\n",
      "Epoch 00116: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1099 - acc: 0.9638 - val_loss: 0.1647 - val_acc: 0.9588\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9661\n",
      "Epoch 00117: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0997 - acc: 0.9661 - val_loss: 0.1711 - val_acc: 0.9611\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9674\n",
      "Epoch 00118: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0972 - acc: 0.9674 - val_loss: 0.1756 - val_acc: 0.9574\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9658\n",
      "Epoch 00119: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1013 - acc: 0.9658 - val_loss: 0.1678 - val_acc: 0.9606\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9668\n",
      "Epoch 00120: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0998 - acc: 0.9668 - val_loss: 0.1918 - val_acc: 0.9567\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9665\n",
      "Epoch 00121: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0985 - acc: 0.9665 - val_loss: 0.1759 - val_acc: 0.9592\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9682\n",
      "Epoch 00122: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0925 - acc: 0.9682 - val_loss: 0.1711 - val_acc: 0.9604\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9689\n",
      "Epoch 00123: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0905 - acc: 0.9689 - val_loss: 0.1806 - val_acc: 0.9555\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9659\n",
      "Epoch 00124: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0985 - acc: 0.9659 - val_loss: 0.1786 - val_acc: 0.9560\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9685\n",
      "Epoch 00125: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0946 - acc: 0.9685 - val_loss: 0.1806 - val_acc: 0.9585\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9694\n",
      "Epoch 00126: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0933 - acc: 0.9694 - val_loss: 0.1721 - val_acc: 0.9583\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9682\n",
      "Epoch 00127: val_loss did not improve from 0.15158\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0940 - acc: 0.9682 - val_loss: 0.1672 - val_acc: 0.9595\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 647us/sample - loss: 0.1840 - acc: 0.9448\n",
      "Loss: 0.18403320234976825 Accuracy: 0.944756\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5105 - acc: 0.1764\n",
      "Epoch 00001: val_loss improved from inf to 1.82729, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/001-1.8273.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 2.5104 - acc: 0.1764 - val_loss: 1.8273 - val_acc: 0.4500\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7312 - acc: 0.4327\n",
      "Epoch 00002: val_loss improved from 1.82729 to 1.21631, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/002-1.2163.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.7311 - acc: 0.4328 - val_loss: 1.2163 - val_acc: 0.6143\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3886 - acc: 0.5427\n",
      "Epoch 00003: val_loss improved from 1.21631 to 0.94063, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/003-0.9406.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.3885 - acc: 0.5427 - val_loss: 0.9406 - val_acc: 0.7000\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1949 - acc: 0.6060\n",
      "Epoch 00004: val_loss improved from 0.94063 to 0.88068, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/004-0.8807.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1950 - acc: 0.6059 - val_loss: 0.8807 - val_acc: 0.7268\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0485 - acc: 0.6561\n",
      "Epoch 00005: val_loss improved from 0.88068 to 0.68478, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/005-0.6848.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0485 - acc: 0.6561 - val_loss: 0.6848 - val_acc: 0.7955\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.6980\n",
      "Epoch 00006: val_loss improved from 0.68478 to 0.57485, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/006-0.5749.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9229 - acc: 0.6979 - val_loss: 0.5749 - val_acc: 0.8355\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7319\n",
      "Epoch 00007: val_loss improved from 0.57485 to 0.49884, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/007-0.4988.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8278 - acc: 0.7319 - val_loss: 0.4988 - val_acc: 0.8600\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7379 - acc: 0.7646\n",
      "Epoch 00008: val_loss improved from 0.49884 to 0.42381, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/008-0.4238.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7379 - acc: 0.7646 - val_loss: 0.4238 - val_acc: 0.8721\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6654 - acc: 0.7903\n",
      "Epoch 00009: val_loss improved from 0.42381 to 0.37591, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/009-0.3759.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6654 - acc: 0.7903 - val_loss: 0.3759 - val_acc: 0.8898\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6139 - acc: 0.8046\n",
      "Epoch 00010: val_loss improved from 0.37591 to 0.35518, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/010-0.3552.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6138 - acc: 0.8047 - val_loss: 0.3552 - val_acc: 0.8956\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8240\n",
      "Epoch 00011: val_loss improved from 0.35518 to 0.29946, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/011-0.2995.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5596 - acc: 0.8239 - val_loss: 0.2995 - val_acc: 0.9147\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8355\n",
      "Epoch 00012: val_loss improved from 0.29946 to 0.29153, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/012-0.2915.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5190 - acc: 0.8355 - val_loss: 0.2915 - val_acc: 0.9189\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8512\n",
      "Epoch 00013: val_loss improved from 0.29153 to 0.25540, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/013-0.2554.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4802 - acc: 0.8511 - val_loss: 0.2554 - val_acc: 0.9311\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8578\n",
      "Epoch 00014: val_loss improved from 0.25540 to 0.23770, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/014-0.2377.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4587 - acc: 0.8578 - val_loss: 0.2377 - val_acc: 0.9320\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8683\n",
      "Epoch 00015: val_loss did not improve from 0.23770\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4211 - acc: 0.8683 - val_loss: 0.2468 - val_acc: 0.9313\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8750\n",
      "Epoch 00016: val_loss improved from 0.23770 to 0.21293, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/016-0.2129.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3987 - acc: 0.8750 - val_loss: 0.2129 - val_acc: 0.9429\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8802\n",
      "Epoch 00017: val_loss did not improve from 0.21293\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3830 - acc: 0.8802 - val_loss: 0.2175 - val_acc: 0.9376\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8856\n",
      "Epoch 00018: val_loss improved from 0.21293 to 0.20167, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/018-0.2017.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3688 - acc: 0.8856 - val_loss: 0.2017 - val_acc: 0.9441\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8912\n",
      "Epoch 00019: val_loss improved from 0.20167 to 0.18583, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/019-0.1858.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3519 - acc: 0.8912 - val_loss: 0.1858 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8957\n",
      "Epoch 00020: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3344 - acc: 0.8957 - val_loss: 0.1948 - val_acc: 0.9450\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8988\n",
      "Epoch 00021: val_loss improved from 0.18583 to 0.17604, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/021-0.1760.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3229 - acc: 0.8988 - val_loss: 0.1760 - val_acc: 0.9495\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9008\n",
      "Epoch 00022: val_loss improved from 0.17604 to 0.16908, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/022-0.1691.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3152 - acc: 0.9008 - val_loss: 0.1691 - val_acc: 0.9520\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9074\n",
      "Epoch 00023: val_loss improved from 0.16908 to 0.16712, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/023-0.1671.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3022 - acc: 0.9075 - val_loss: 0.1671 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9077\n",
      "Epoch 00024: val_loss improved from 0.16712 to 0.15775, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/024-0.1577.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2892 - acc: 0.9077 - val_loss: 0.1577 - val_acc: 0.9562\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9126\n",
      "Epoch 00025: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2778 - acc: 0.9125 - val_loss: 0.1627 - val_acc: 0.9515\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9146\n",
      "Epoch 00026: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2748 - acc: 0.9146 - val_loss: 0.1685 - val_acc: 0.9520\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9165\n",
      "Epoch 00027: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2653 - acc: 0.9166 - val_loss: 0.1582 - val_acc: 0.9555\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9192\n",
      "Epoch 00028: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2541 - acc: 0.9192 - val_loss: 0.1604 - val_acc: 0.9557\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9204\n",
      "Epoch 00029: val_loss improved from 0.15775 to 0.14653, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/029-0.1465.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2500 - acc: 0.9204 - val_loss: 0.1465 - val_acc: 0.9557\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9241\n",
      "Epoch 00030: val_loss did not improve from 0.14653\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2422 - acc: 0.9241 - val_loss: 0.1638 - val_acc: 0.9513\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9252\n",
      "Epoch 00031: val_loss improved from 0.14653 to 0.14070, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/031-0.1407.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2354 - acc: 0.9252 - val_loss: 0.1407 - val_acc: 0.9569\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9297\n",
      "Epoch 00032: val_loss did not improve from 0.14070\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2257 - acc: 0.9297 - val_loss: 0.1493 - val_acc: 0.9574\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9301\n",
      "Epoch 00033: val_loss did not improve from 0.14070\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2169 - acc: 0.9301 - val_loss: 0.1451 - val_acc: 0.9616\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9323\n",
      "Epoch 00034: val_loss did not improve from 0.14070\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2125 - acc: 0.9323 - val_loss: 0.1419 - val_acc: 0.9581\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9330\n",
      "Epoch 00035: val_loss did not improve from 0.14070\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2128 - acc: 0.9330 - val_loss: 0.1457 - val_acc: 0.9550\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9350\n",
      "Epoch 00036: val_loss improved from 0.14070 to 0.12807, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/036-0.1281.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2043 - acc: 0.9350 - val_loss: 0.1281 - val_acc: 0.9611\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9354\n",
      "Epoch 00037: val_loss did not improve from 0.12807\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1989 - acc: 0.9354 - val_loss: 0.1309 - val_acc: 0.9644\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9408\n",
      "Epoch 00038: val_loss did not improve from 0.12807\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1889 - acc: 0.9408 - val_loss: 0.1321 - val_acc: 0.9592\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9402\n",
      "Epoch 00039: val_loss improved from 0.12807 to 0.12602, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/039-0.1260.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1859 - acc: 0.9403 - val_loss: 0.1260 - val_acc: 0.9646\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9408\n",
      "Epoch 00040: val_loss improved from 0.12602 to 0.12421, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/040-0.1242.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1868 - acc: 0.9408 - val_loss: 0.1242 - val_acc: 0.9644\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9403\n",
      "Epoch 00041: val_loss improved from 0.12421 to 0.11706, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/041-0.1171.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1862 - acc: 0.9403 - val_loss: 0.1171 - val_acc: 0.9651\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9436\n",
      "Epoch 00042: val_loss did not improve from 0.11706\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1777 - acc: 0.9436 - val_loss: 0.1230 - val_acc: 0.9655\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9439\n",
      "Epoch 00043: val_loss did not improve from 0.11706\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1774 - acc: 0.9439 - val_loss: 0.1187 - val_acc: 0.9655\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9463\n",
      "Epoch 00044: val_loss did not improve from 0.11706\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1661 - acc: 0.9463 - val_loss: 0.1319 - val_acc: 0.9641\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9453\n",
      "Epoch 00045: val_loss did not improve from 0.11706\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1701 - acc: 0.9453 - val_loss: 0.1180 - val_acc: 0.9669\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9472\n",
      "Epoch 00046: val_loss did not improve from 0.11706\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1660 - acc: 0.9472 - val_loss: 0.1180 - val_acc: 0.9672\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9488\n",
      "Epoch 00047: val_loss improved from 0.11706 to 0.11488, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/047-0.1149.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1597 - acc: 0.9488 - val_loss: 0.1149 - val_acc: 0.9690\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9479\n",
      "Epoch 00048: val_loss did not improve from 0.11488\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1628 - acc: 0.9479 - val_loss: 0.1565 - val_acc: 0.9522\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9501\n",
      "Epoch 00049: val_loss did not improve from 0.11488\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1575 - acc: 0.9501 - val_loss: 0.1197 - val_acc: 0.9667\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9520\n",
      "Epoch 00050: val_loss improved from 0.11488 to 0.11001, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/050-0.1100.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1492 - acc: 0.9520 - val_loss: 0.1100 - val_acc: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9517\n",
      "Epoch 00051: val_loss did not improve from 0.11001\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1468 - acc: 0.9517 - val_loss: 0.1121 - val_acc: 0.9665\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9536\n",
      "Epoch 00052: val_loss did not improve from 0.11001\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1460 - acc: 0.9536 - val_loss: 0.1164 - val_acc: 0.9660\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9550\n",
      "Epoch 00053: val_loss did not improve from 0.11001\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1393 - acc: 0.9550 - val_loss: 0.1149 - val_acc: 0.9653\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9545\n",
      "Epoch 00054: val_loss did not improve from 0.11001\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1365 - acc: 0.9545 - val_loss: 0.1184 - val_acc: 0.9660\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9555\n",
      "Epoch 00055: val_loss improved from 0.11001 to 0.10512, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/055-0.1051.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1370 - acc: 0.9555 - val_loss: 0.1051 - val_acc: 0.9695\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9568\n",
      "Epoch 00056: val_loss did not improve from 0.10512\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1324 - acc: 0.9568 - val_loss: 0.1191 - val_acc: 0.9660\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9570\n",
      "Epoch 00057: val_loss did not improve from 0.10512\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1312 - acc: 0.9570 - val_loss: 0.1118 - val_acc: 0.9676\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9563\n",
      "Epoch 00058: val_loss did not improve from 0.10512\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1316 - acc: 0.9563 - val_loss: 0.1066 - val_acc: 0.9713\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9585\n",
      "Epoch 00059: val_loss did not improve from 0.10512\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1246 - acc: 0.9585 - val_loss: 0.1194 - val_acc: 0.9672\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9574\n",
      "Epoch 00060: val_loss did not improve from 0.10512\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1279 - acc: 0.9575 - val_loss: 0.1132 - val_acc: 0.9667\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9601\n",
      "Epoch 00061: val_loss improved from 0.10512 to 0.10404, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/061-0.1040.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1213 - acc: 0.9601 - val_loss: 0.1040 - val_acc: 0.9711\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9612\n",
      "Epoch 00062: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1178 - acc: 0.9612 - val_loss: 0.1061 - val_acc: 0.9704\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9624\n",
      "Epoch 00063: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1151 - acc: 0.9624 - val_loss: 0.1182 - val_acc: 0.9693\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9635\n",
      "Epoch 00064: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1108 - acc: 0.9635 - val_loss: 0.1132 - val_acc: 0.9683\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9607\n",
      "Epoch 00065: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1152 - acc: 0.9607 - val_loss: 0.1054 - val_acc: 0.9706\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9645\n",
      "Epoch 00066: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1088 - acc: 0.9645 - val_loss: 0.1352 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9626\n",
      "Epoch 00067: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1102 - acc: 0.9626 - val_loss: 0.1097 - val_acc: 0.9713\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9632\n",
      "Epoch 00068: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1104 - acc: 0.9632 - val_loss: 0.1099 - val_acc: 0.9720\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9647\n",
      "Epoch 00069: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1070 - acc: 0.9647 - val_loss: 0.1116 - val_acc: 0.9718\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9665\n",
      "Epoch 00070: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1008 - acc: 0.9665 - val_loss: 0.1167 - val_acc: 0.9674\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9655\n",
      "Epoch 00071: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1030 - acc: 0.9654 - val_loss: 0.1069 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9634\n",
      "Epoch 00072: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1083 - acc: 0.9633 - val_loss: 0.1109 - val_acc: 0.9700\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9671\n",
      "Epoch 00073: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0992 - acc: 0.9671 - val_loss: 0.1111 - val_acc: 0.9674\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9673\n",
      "Epoch 00074: val_loss did not improve from 0.10404\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0953 - acc: 0.9673 - val_loss: 0.1057 - val_acc: 0.9709\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9673\n",
      "Epoch 00075: val_loss improved from 0.10404 to 0.10123, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/075-0.1012.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0976 - acc: 0.9673 - val_loss: 0.1012 - val_acc: 0.9711\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9683\n",
      "Epoch 00076: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0937 - acc: 0.9683 - val_loss: 0.1061 - val_acc: 0.9693\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9678\n",
      "Epoch 00077: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0964 - acc: 0.9678 - val_loss: 0.1033 - val_acc: 0.9706\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9692\n",
      "Epoch 00078: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0890 - acc: 0.9692 - val_loss: 0.1171 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9692\n",
      "Epoch 00079: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0924 - acc: 0.9692 - val_loss: 0.1091 - val_acc: 0.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9698\n",
      "Epoch 00080: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0918 - acc: 0.9698 - val_loss: 0.1096 - val_acc: 0.9704\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9705\n",
      "Epoch 00081: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0863 - acc: 0.9705 - val_loss: 0.1097 - val_acc: 0.9688\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9699\n",
      "Epoch 00082: val_loss did not improve from 0.10123\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0879 - acc: 0.9699 - val_loss: 0.1042 - val_acc: 0.9706\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9703\n",
      "Epoch 00083: val_loss improved from 0.10123 to 0.09683, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_8_conv_checkpoint/083-0.0968.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0878 - acc: 0.9703 - val_loss: 0.0968 - val_acc: 0.9718\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9717\n",
      "Epoch 00084: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0842 - acc: 0.9717 - val_loss: 0.1089 - val_acc: 0.9732\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9718\n",
      "Epoch 00085: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0819 - acc: 0.9718 - val_loss: 0.1119 - val_acc: 0.9725\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9728\n",
      "Epoch 00086: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0810 - acc: 0.9728 - val_loss: 0.1255 - val_acc: 0.9695\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9717\n",
      "Epoch 00087: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0842 - acc: 0.9717 - val_loss: 0.1068 - val_acc: 0.9713\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9735\n",
      "Epoch 00088: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0797 - acc: 0.9734 - val_loss: 0.1144 - val_acc: 0.9697\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9734\n",
      "Epoch 00089: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0791 - acc: 0.9734 - val_loss: 0.1133 - val_acc: 0.9713\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9740\n",
      "Epoch 00090: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0769 - acc: 0.9741 - val_loss: 0.1208 - val_acc: 0.9688\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9737\n",
      "Epoch 00091: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0777 - acc: 0.9737 - val_loss: 0.1041 - val_acc: 0.9739\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9749\n",
      "Epoch 00092: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0764 - acc: 0.9749 - val_loss: 0.1324 - val_acc: 0.9672\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9738\n",
      "Epoch 00093: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0739 - acc: 0.9738 - val_loss: 0.1187 - val_acc: 0.9681\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9751\n",
      "Epoch 00094: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0727 - acc: 0.9751 - val_loss: 0.1205 - val_acc: 0.9702\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9762\n",
      "Epoch 00095: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0720 - acc: 0.9763 - val_loss: 0.1146 - val_acc: 0.9679\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9748\n",
      "Epoch 00096: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0741 - acc: 0.9748 - val_loss: 0.1126 - val_acc: 0.9690\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9765\n",
      "Epoch 00097: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0676 - acc: 0.9765 - val_loss: 0.1216 - val_acc: 0.9702\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9762\n",
      "Epoch 00098: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0710 - acc: 0.9762 - val_loss: 0.1244 - val_acc: 0.9662\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9783\n",
      "Epoch 00099: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0673 - acc: 0.9783 - val_loss: 0.1186 - val_acc: 0.9718\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9764\n",
      "Epoch 00100: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0703 - acc: 0.9764 - val_loss: 0.1117 - val_acc: 0.9711\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9774\n",
      "Epoch 00101: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0659 - acc: 0.9774 - val_loss: 0.1240 - val_acc: 0.9711\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9772\n",
      "Epoch 00102: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0660 - acc: 0.9772 - val_loss: 0.1173 - val_acc: 0.9713\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9776\n",
      "Epoch 00103: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0676 - acc: 0.9776 - val_loss: 0.1242 - val_acc: 0.9674\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9776\n",
      "Epoch 00104: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0688 - acc: 0.9776 - val_loss: 0.1172 - val_acc: 0.9704\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9787\n",
      "Epoch 00105: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0637 - acc: 0.9788 - val_loss: 0.1245 - val_acc: 0.9693\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9787\n",
      "Epoch 00106: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0630 - acc: 0.9787 - val_loss: 0.1201 - val_acc: 0.9704\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9795\n",
      "Epoch 00107: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0620 - acc: 0.9795 - val_loss: 0.1310 - val_acc: 0.9711\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9785\n",
      "Epoch 00108: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0642 - acc: 0.9785 - val_loss: 0.1321 - val_acc: 0.9679\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9787\n",
      "Epoch 00109: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0634 - acc: 0.9787 - val_loss: 0.1255 - val_acc: 0.9702\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9789\n",
      "Epoch 00110: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0619 - acc: 0.9789 - val_loss: 0.1179 - val_acc: 0.9711\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9812\n",
      "Epoch 00111: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0553 - acc: 0.9812 - val_loss: 0.1175 - val_acc: 0.9709\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9813\n",
      "Epoch 00112: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0576 - acc: 0.9813 - val_loss: 0.1256 - val_acc: 0.9706\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9805\n",
      "Epoch 00113: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0581 - acc: 0.9805 - val_loss: 0.1172 - val_acc: 0.9700\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9801\n",
      "Epoch 00114: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0589 - acc: 0.9801 - val_loss: 0.1120 - val_acc: 0.9704\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9820\n",
      "Epoch 00115: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0528 - acc: 0.9820 - val_loss: 0.1282 - val_acc: 0.9700\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9807\n",
      "Epoch 00116: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0581 - acc: 0.9807 - val_loss: 0.1360 - val_acc: 0.9669\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9798\n",
      "Epoch 00117: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0586 - acc: 0.9798 - val_loss: 0.1161 - val_acc: 0.9690\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9813\n",
      "Epoch 00118: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0554 - acc: 0.9813 - val_loss: 0.1200 - val_acc: 0.9679\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9820\n",
      "Epoch 00119: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0538 - acc: 0.9820 - val_loss: 0.1210 - val_acc: 0.9681\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9816\n",
      "Epoch 00120: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0554 - acc: 0.9816 - val_loss: 0.1239 - val_acc: 0.9713\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9828\n",
      "Epoch 00121: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0504 - acc: 0.9828 - val_loss: 0.1350 - val_acc: 0.9725\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9816\n",
      "Epoch 00122: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0507 - acc: 0.9816 - val_loss: 0.1361 - val_acc: 0.9683\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9825\n",
      "Epoch 00123: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0506 - acc: 0.9825 - val_loss: 0.1301 - val_acc: 0.9693\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9824\n",
      "Epoch 00124: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0529 - acc: 0.9824 - val_loss: 0.1185 - val_acc: 0.9720\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9837\n",
      "Epoch 00125: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0475 - acc: 0.9837 - val_loss: 0.1495 - val_acc: 0.9665\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9817\n",
      "Epoch 00126: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0553 - acc: 0.9817 - val_loss: 0.1164 - val_acc: 0.9697\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9844\n",
      "Epoch 00127: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0466 - acc: 0.9844 - val_loss: 0.1338 - val_acc: 0.9686\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9828\n",
      "Epoch 00128: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0491 - acc: 0.9828 - val_loss: 0.1427 - val_acc: 0.9693\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9830\n",
      "Epoch 00129: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0508 - acc: 0.9830 - val_loss: 0.1388 - val_acc: 0.9688\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9835\n",
      "Epoch 00130: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0478 - acc: 0.9835 - val_loss: 0.1442 - val_acc: 0.9672\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9831\n",
      "Epoch 00131: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0498 - acc: 0.9831 - val_loss: 0.1204 - val_acc: 0.9695\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9848\n",
      "Epoch 00132: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0449 - acc: 0.9848 - val_loss: 0.1216 - val_acc: 0.9720\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9836\n",
      "Epoch 00133: val_loss did not improve from 0.09683\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0486 - acc: 0.9836 - val_loss: 0.1241 - val_acc: 0.9711\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 650us/sample - loss: 0.1450 - acc: 0.9574\n",
      "Loss: 0.14504509495642823 Accuracy: 0.9574247\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2380 - acc: 0.2691\n",
      "Epoch 00001: val_loss improved from inf to 1.35140, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/001-1.3514.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.2379 - acc: 0.2691 - val_loss: 1.3514 - val_acc: 0.5772\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3399 - acc: 0.5584\n",
      "Epoch 00002: val_loss improved from 1.35140 to 0.85749, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/002-0.8575.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.3399 - acc: 0.5583 - val_loss: 0.8575 - val_acc: 0.7314\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0366 - acc: 0.6608\n",
      "Epoch 00003: val_loss improved from 0.85749 to 0.70039, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/003-0.7004.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0366 - acc: 0.6608 - val_loss: 0.7004 - val_acc: 0.7794\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.7170\n",
      "Epoch 00004: val_loss improved from 0.70039 to 0.62280, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/004-0.6228.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8781 - acc: 0.7170 - val_loss: 0.6228 - val_acc: 0.8027\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7522\n",
      "Epoch 00005: val_loss improved from 0.62280 to 0.52214, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/005-0.5221.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7647 - acc: 0.7522 - val_loss: 0.5221 - val_acc: 0.8423\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6804 - acc: 0.7811\n",
      "Epoch 00006: val_loss improved from 0.52214 to 0.44459, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/006-0.4446.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6805 - acc: 0.7811 - val_loss: 0.4446 - val_acc: 0.8670\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6062 - acc: 0.8059\n",
      "Epoch 00007: val_loss improved from 0.44459 to 0.40638, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/007-0.4064.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6061 - acc: 0.8059 - val_loss: 0.4064 - val_acc: 0.8807\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8271\n",
      "Epoch 00008: val_loss improved from 0.40638 to 0.32413, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/008-0.3241.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5397 - acc: 0.8271 - val_loss: 0.3241 - val_acc: 0.9008\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4890 - acc: 0.8442\n",
      "Epoch 00009: val_loss improved from 0.32413 to 0.29475, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/009-0.2948.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4889 - acc: 0.8443 - val_loss: 0.2948 - val_acc: 0.9073\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8619\n",
      "Epoch 00010: val_loss improved from 0.29475 to 0.28191, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/010-0.2819.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4378 - acc: 0.8619 - val_loss: 0.2819 - val_acc: 0.9131\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8730\n",
      "Epoch 00011: val_loss improved from 0.28191 to 0.22962, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/011-0.2296.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4001 - acc: 0.8730 - val_loss: 0.2296 - val_acc: 0.9306\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8832\n",
      "Epoch 00012: val_loss improved from 0.22962 to 0.21495, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/012-0.2150.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3693 - acc: 0.8832 - val_loss: 0.2150 - val_acc: 0.9362\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8958\n",
      "Epoch 00013: val_loss did not improve from 0.21495\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3378 - acc: 0.8958 - val_loss: 0.2191 - val_acc: 0.9352\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9011\n",
      "Epoch 00014: val_loss improved from 0.21495 to 0.19444, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/014-0.1944.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3152 - acc: 0.9011 - val_loss: 0.1944 - val_acc: 0.9427\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9079\n",
      "Epoch 00015: val_loss improved from 0.19444 to 0.17900, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/015-0.1790.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2902 - acc: 0.9079 - val_loss: 0.1790 - val_acc: 0.9462\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9130\n",
      "Epoch 00016: val_loss improved from 0.17900 to 0.16509, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/016-0.1651.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2746 - acc: 0.9130 - val_loss: 0.1651 - val_acc: 0.9527\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9194\n",
      "Epoch 00017: val_loss did not improve from 0.16509\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2543 - acc: 0.9194 - val_loss: 0.1757 - val_acc: 0.9471\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9235\n",
      "Epoch 00018: val_loss improved from 0.16509 to 0.16325, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/018-0.1633.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2423 - acc: 0.9235 - val_loss: 0.1633 - val_acc: 0.9525\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9268\n",
      "Epoch 00019: val_loss improved from 0.16325 to 0.14801, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/019-0.1480.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2311 - acc: 0.9268 - val_loss: 0.1480 - val_acc: 0.9581\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9316\n",
      "Epoch 00020: val_loss did not improve from 0.14801\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2201 - acc: 0.9316 - val_loss: 0.1537 - val_acc: 0.9513\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9335\n",
      "Epoch 00021: val_loss improved from 0.14801 to 0.13723, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/021-0.1372.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2113 - acc: 0.9335 - val_loss: 0.1372 - val_acc: 0.9590\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9382\n",
      "Epoch 00022: val_loss did not improve from 0.13723\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1965 - acc: 0.9382 - val_loss: 0.1427 - val_acc: 0.9569\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9395\n",
      "Epoch 00023: val_loss improved from 0.13723 to 0.13549, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/023-0.1355.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1928 - acc: 0.9395 - val_loss: 0.1355 - val_acc: 0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9418\n",
      "Epoch 00024: val_loss improved from 0.13549 to 0.13007, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/024-0.1301.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1806 - acc: 0.9419 - val_loss: 0.1301 - val_acc: 0.9618\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9445\n",
      "Epoch 00025: val_loss did not improve from 0.13007\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1749 - acc: 0.9445 - val_loss: 0.1400 - val_acc: 0.9567\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9459\n",
      "Epoch 00026: val_loss did not improve from 0.13007\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1671 - acc: 0.9459 - val_loss: 0.1339 - val_acc: 0.9616\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9468\n",
      "Epoch 00027: val_loss did not improve from 0.13007\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1646 - acc: 0.9468 - val_loss: 0.1328 - val_acc: 0.9583\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9523\n",
      "Epoch 00028: val_loss did not improve from 0.13007\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1518 - acc: 0.9523 - val_loss: 0.1338 - val_acc: 0.9595\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9518\n",
      "Epoch 00029: val_loss did not improve from 0.13007\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1486 - acc: 0.9518 - val_loss: 0.1420 - val_acc: 0.9564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9540\n",
      "Epoch 00030: val_loss improved from 0.13007 to 0.12712, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/030-0.1271.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1428 - acc: 0.9540 - val_loss: 0.1271 - val_acc: 0.9616\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9551\n",
      "Epoch 00031: val_loss improved from 0.12712 to 0.11333, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/031-0.1133.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1347 - acc: 0.9551 - val_loss: 0.1133 - val_acc: 0.9651\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9562\n",
      "Epoch 00032: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1342 - acc: 0.9562 - val_loss: 0.1423 - val_acc: 0.9599\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9589\n",
      "Epoch 00033: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1270 - acc: 0.9589 - val_loss: 0.1153 - val_acc: 0.9665\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9595\n",
      "Epoch 00034: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1250 - acc: 0.9595 - val_loss: 0.1271 - val_acc: 0.9613\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9611\n",
      "Epoch 00035: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1182 - acc: 0.9611 - val_loss: 0.1309 - val_acc: 0.9606\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9624\n",
      "Epoch 00036: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1181 - acc: 0.9624 - val_loss: 0.1275 - val_acc: 0.9613\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9653\n",
      "Epoch 00037: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1085 - acc: 0.9653 - val_loss: 0.1248 - val_acc: 0.9639\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9642\n",
      "Epoch 00038: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1083 - acc: 0.9642 - val_loss: 0.1517 - val_acc: 0.9571\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9670\n",
      "Epoch 00039: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1001 - acc: 0.9670 - val_loss: 0.1157 - val_acc: 0.9648\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9674\n",
      "Epoch 00040: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0997 - acc: 0.9674 - val_loss: 0.1189 - val_acc: 0.9613\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9681\n",
      "Epoch 00041: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0977 - acc: 0.9681 - val_loss: 0.1221 - val_acc: 0.9681\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9688\n",
      "Epoch 00042: val_loss did not improve from 0.11333\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0954 - acc: 0.9688 - val_loss: 0.1489 - val_acc: 0.9595\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9697\n",
      "Epoch 00043: val_loss improved from 0.11333 to 0.10869, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_9_conv_checkpoint/043-0.1087.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0923 - acc: 0.9697 - val_loss: 0.1087 - val_acc: 0.9683\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9705\n",
      "Epoch 00044: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0892 - acc: 0.9705 - val_loss: 0.1260 - val_acc: 0.9679\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9704\n",
      "Epoch 00045: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0890 - acc: 0.9704 - val_loss: 0.1216 - val_acc: 0.9660\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9721\n",
      "Epoch 00046: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0857 - acc: 0.9721 - val_loss: 0.1143 - val_acc: 0.9669\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9723\n",
      "Epoch 00047: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0843 - acc: 0.9723 - val_loss: 0.1109 - val_acc: 0.9681\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9729\n",
      "Epoch 00048: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0782 - acc: 0.9729 - val_loss: 0.1385 - val_acc: 0.9623\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9740\n",
      "Epoch 00049: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0778 - acc: 0.9740 - val_loss: 0.1194 - val_acc: 0.9658\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9753\n",
      "Epoch 00050: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0760 - acc: 0.9753 - val_loss: 0.1127 - val_acc: 0.9681\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9760\n",
      "Epoch 00051: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0727 - acc: 0.9760 - val_loss: 0.1168 - val_acc: 0.9683\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9780\n",
      "Epoch 00052: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0671 - acc: 0.9780 - val_loss: 0.1290 - val_acc: 0.9660\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9765\n",
      "Epoch 00053: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0694 - acc: 0.9766 - val_loss: 0.1313 - val_acc: 0.9662\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9774\n",
      "Epoch 00054: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0654 - acc: 0.9774 - val_loss: 0.1196 - val_acc: 0.9667\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9785\n",
      "Epoch 00055: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0663 - acc: 0.9785 - val_loss: 0.1174 - val_acc: 0.9658\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9783\n",
      "Epoch 00056: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0654 - acc: 0.9783 - val_loss: 0.1275 - val_acc: 0.9651\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9795\n",
      "Epoch 00057: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0601 - acc: 0.9795 - val_loss: 0.1233 - val_acc: 0.9695\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9784\n",
      "Epoch 00058: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0644 - acc: 0.9784 - val_loss: 0.1334 - val_acc: 0.9632\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9798\n",
      "Epoch 00059: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0593 - acc: 0.9798 - val_loss: 0.1335 - val_acc: 0.9674\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9800\n",
      "Epoch 00060: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0599 - acc: 0.9800 - val_loss: 0.1211 - val_acc: 0.9676\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9806\n",
      "Epoch 00061: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0570 - acc: 0.9806 - val_loss: 0.1424 - val_acc: 0.9639\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9821\n",
      "Epoch 00062: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0512 - acc: 0.9821 - val_loss: 0.1317 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9808\n",
      "Epoch 00063: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0558 - acc: 0.9808 - val_loss: 0.1305 - val_acc: 0.9665\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9820\n",
      "Epoch 00064: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0518 - acc: 0.9820 - val_loss: 0.1155 - val_acc: 0.9704\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9814\n",
      "Epoch 00065: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0543 - acc: 0.9814 - val_loss: 0.1290 - val_acc: 0.9648\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9832\n",
      "Epoch 00066: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0512 - acc: 0.9832 - val_loss: 0.1270 - val_acc: 0.9667\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9829\n",
      "Epoch 00067: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0500 - acc: 0.9829 - val_loss: 0.1249 - val_acc: 0.9688\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9853\n",
      "Epoch 00068: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0440 - acc: 0.9853 - val_loss: 0.1323 - val_acc: 0.9700\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9850\n",
      "Epoch 00069: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0439 - acc: 0.9850 - val_loss: 0.1430 - val_acc: 0.9679\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9841\n",
      "Epoch 00070: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0499 - acc: 0.9841 - val_loss: 0.1228 - val_acc: 0.9700\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9849\n",
      "Epoch 00071: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0452 - acc: 0.9849 - val_loss: 0.1371 - val_acc: 0.9679\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9857\n",
      "Epoch 00072: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0422 - acc: 0.9857 - val_loss: 0.1375 - val_acc: 0.9655\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9852\n",
      "Epoch 00073: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0430 - acc: 0.9852 - val_loss: 0.1414 - val_acc: 0.9665\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9846\n",
      "Epoch 00074: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0461 - acc: 0.9846 - val_loss: 0.1261 - val_acc: 0.9676\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9871\n",
      "Epoch 00075: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0369 - acc: 0.9871 - val_loss: 0.1446 - val_acc: 0.9695\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9870\n",
      "Epoch 00076: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0397 - acc: 0.9870 - val_loss: 0.1389 - val_acc: 0.9695\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9868\n",
      "Epoch 00077: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0398 - acc: 0.9868 - val_loss: 0.1289 - val_acc: 0.9697\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9862\n",
      "Epoch 00078: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0409 - acc: 0.9862 - val_loss: 0.1283 - val_acc: 0.9704\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9872\n",
      "Epoch 00079: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0342 - acc: 0.9872 - val_loss: 0.1290 - val_acc: 0.9686\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9870\n",
      "Epoch 00080: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0395 - acc: 0.9870 - val_loss: 0.1347 - val_acc: 0.9706\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9878\n",
      "Epoch 00081: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0371 - acc: 0.9878 - val_loss: 0.1372 - val_acc: 0.9720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9872\n",
      "Epoch 00082: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0367 - acc: 0.9872 - val_loss: 0.1487 - val_acc: 0.9704\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9879\n",
      "Epoch 00083: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0358 - acc: 0.9879 - val_loss: 0.1431 - val_acc: 0.9700\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9889\n",
      "Epoch 00084: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0328 - acc: 0.9889 - val_loss: 0.1339 - val_acc: 0.9697\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.1368 - val_acc: 0.9709\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9890\n",
      "Epoch 00086: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0325 - acc: 0.9891 - val_loss: 0.1489 - val_acc: 0.9667\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9894\n",
      "Epoch 00087: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0339 - acc: 0.9894 - val_loss: 0.1296 - val_acc: 0.9702\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9893\n",
      "Epoch 00088: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0316 - acc: 0.9893 - val_loss: 0.1635 - val_acc: 0.9655\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9895\n",
      "Epoch 00089: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0324 - acc: 0.9895 - val_loss: 0.1394 - val_acc: 0.9718\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9882\n",
      "Epoch 00090: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0365 - acc: 0.9882 - val_loss: 0.1317 - val_acc: 0.9704\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9895\n",
      "Epoch 00091: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0309 - acc: 0.9895 - val_loss: 0.1946 - val_acc: 0.9637\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9896\n",
      "Epoch 00092: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0302 - acc: 0.9896 - val_loss: 0.1437 - val_acc: 0.9700\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9915\n",
      "Epoch 00093: val_loss did not improve from 0.10869\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.1396 - val_acc: 0.9713\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 686us/sample - loss: 0.1667 - acc: 0.9529\n",
      "Loss: 0.16666763805371332 Accuracy: 0.95285565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_DO_075_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_075_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 401us/sample - loss: 2.1545 - acc: 0.3225\n",
      "Loss: 2.154451996168491 Accuracy: 0.32253376\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 565us/sample - loss: 1.7904 - acc: 0.4538\n",
      "Loss: 1.7904148946051037 Accuracy: 0.45379025\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 630us/sample - loss: 1.3719 - acc: 0.5796\n",
      "Loss: 1.3718986924934982 Accuracy: 0.57964694\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 671us/sample - loss: 0.9202 - acc: 0.7502\n",
      "Loss: 0.9201614695917408 Accuracy: 0.75015575\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 686us/sample - loss: 0.6525 - acc: 0.8204\n",
      "Loss: 0.6524698791474197 Accuracy: 0.8203531\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 704us/sample - loss: 0.3323 - acc: 0.9084\n",
      "Loss: 0.3323279743749157 Accuracy: 0.9084112\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 735us/sample - loss: 0.1840 - acc: 0.9448\n",
      "Loss: 0.18403320234976825 Accuracy: 0.944756\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 745us/sample - loss: 0.1450 - acc: 0.9574\n",
      "Loss: 0.14504509495642823 Accuracy: 0.9574247\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 773us/sample - loss: 0.1667 - acc: 0.9529\n",
      "Loss: 0.16666763805371332 Accuracy: 0.95285565\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
