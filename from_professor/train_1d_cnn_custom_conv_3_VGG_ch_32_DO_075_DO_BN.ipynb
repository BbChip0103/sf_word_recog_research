{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())        \n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,504\n",
      "Trainable params: 8,195,376\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,740,464\n",
      "Trainable params: 2,740,208\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 926,256\n",
      "Trainable params: 925,872\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0767 - acc: 0.2390\n",
      "Epoch 00001: val_loss improved from inf to 1.98848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/001-1.9885.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 3.0766 - acc: 0.2390 - val_loss: 1.9885 - val_acc: 0.3818\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0853 - acc: 0.3955\n",
      "Epoch 00002: val_loss improved from 1.98848 to 1.40208, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/002-1.4021.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 2.0853 - acc: 0.3955 - val_loss: 1.4021 - val_acc: 0.5590\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7527 - acc: 0.4686\n",
      "Epoch 00003: val_loss improved from 1.40208 to 1.29467, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/003-1.2947.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.7526 - acc: 0.4686 - val_loss: 1.2947 - val_acc: 0.5977\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5563 - acc: 0.5184\n",
      "Epoch 00004: val_loss improved from 1.29467 to 1.21697, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/004-1.2170.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.5564 - acc: 0.5184 - val_loss: 1.2170 - val_acc: 0.6191\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4230 - acc: 0.5583\n",
      "Epoch 00005: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.4231 - acc: 0.5583 - val_loss: 1.2946 - val_acc: 0.5914\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3332 - acc: 0.5839\n",
      "Epoch 00006: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.3333 - acc: 0.5839 - val_loss: 1.2177 - val_acc: 0.6089\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2465 - acc: 0.6065\n",
      "Epoch 00007: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.2468 - acc: 0.6064 - val_loss: 1.7307 - val_acc: 0.4941\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1750 - acc: 0.6302\n",
      "Epoch 00008: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.1750 - acc: 0.6302 - val_loss: 1.2855 - val_acc: 0.5931\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1229 - acc: 0.6476\n",
      "Epoch 00009: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.1229 - acc: 0.6475 - val_loss: 1.2769 - val_acc: 0.6014\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0773 - acc: 0.6615\n",
      "Epoch 00010: val_loss did not improve from 1.21697\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.0774 - acc: 0.6615 - val_loss: 1.7436 - val_acc: 0.4878\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0273 - acc: 0.6781\n",
      "Epoch 00011: val_loss improved from 1.21697 to 1.07804, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/011-1.0780.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.0273 - acc: 0.6781 - val_loss: 1.0780 - val_acc: 0.6622\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.6884\n",
      "Epoch 00012: val_loss did not improve from 1.07804\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9884 - acc: 0.6884 - val_loss: 1.1967 - val_acc: 0.6348\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.6954\n",
      "Epoch 00013: val_loss did not improve from 1.07804\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9529 - acc: 0.6953 - val_loss: 1.7264 - val_acc: 0.5264\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7060\n",
      "Epoch 00014: val_loss did not improve from 1.07804\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.9249 - acc: 0.7060 - val_loss: 1.1953 - val_acc: 0.6226\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8896 - acc: 0.7161\n",
      "Epoch 00015: val_loss improved from 1.07804 to 1.01790, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/015-1.0179.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.8896 - acc: 0.7161 - val_loss: 1.0179 - val_acc: 0.6851\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.7285\n",
      "Epoch 00016: val_loss did not improve from 1.01790\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.8584 - acc: 0.7285 - val_loss: 1.0829 - val_acc: 0.6641\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.7314\n",
      "Epoch 00017: val_loss did not improve from 1.01790\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.8425 - acc: 0.7314 - val_loss: 1.0934 - val_acc: 0.6730\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8169 - acc: 0.7384\n",
      "Epoch 00018: val_loss did not improve from 1.01790\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.8170 - acc: 0.7384 - val_loss: 1.1485 - val_acc: 0.6485\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7462\n",
      "Epoch 00019: val_loss did not improve from 1.01790\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7958 - acc: 0.7462 - val_loss: 1.1176 - val_acc: 0.6573\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.7511\n",
      "Epoch 00020: val_loss improved from 1.01790 to 0.94350, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/020-0.9435.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7787 - acc: 0.7511 - val_loss: 0.9435 - val_acc: 0.7167\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7565\n",
      "Epoch 00021: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7602 - acc: 0.7565 - val_loss: 0.9785 - val_acc: 0.7081\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7509 - acc: 0.7604\n",
      "Epoch 00022: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7508 - acc: 0.7604 - val_loss: 3.7880 - val_acc: 0.3960\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7409 - acc: 0.7600\n",
      "Epoch 00023: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.7409 - acc: 0.7599 - val_loss: 0.9622 - val_acc: 0.7086\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7164 - acc: 0.7701\n",
      "Epoch 00024: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7164 - acc: 0.7701 - val_loss: 1.0468 - val_acc: 0.6746\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7739\n",
      "Epoch 00025: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.7072 - acc: 0.7739 - val_loss: 1.1463 - val_acc: 0.6536\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6876 - acc: 0.7776\n",
      "Epoch 00026: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6876 - acc: 0.7776 - val_loss: 1.0591 - val_acc: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.7799\n",
      "Epoch 00027: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6834 - acc: 0.7799 - val_loss: 1.1051 - val_acc: 0.6620\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6674 - acc: 0.7851\n",
      "Epoch 00028: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.6674 - acc: 0.7851 - val_loss: 1.3706 - val_acc: 0.6159\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.7877\n",
      "Epoch 00029: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6603 - acc: 0.7877 - val_loss: 1.0638 - val_acc: 0.6886\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6469 - acc: 0.7909\n",
      "Epoch 00030: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6470 - acc: 0.7908 - val_loss: 0.9642 - val_acc: 0.7216\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.7964\n",
      "Epoch 00031: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6341 - acc: 0.7964 - val_loss: 1.1037 - val_acc: 0.6858\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6296 - acc: 0.7972\n",
      "Epoch 00032: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6296 - acc: 0.7972 - val_loss: 1.1090 - val_acc: 0.6809\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6203 - acc: 0.7985\n",
      "Epoch 00033: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.6203 - acc: 0.7985 - val_loss: 1.2592 - val_acc: 0.6422\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.8050\n",
      "Epoch 00034: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.6108 - acc: 0.8050 - val_loss: 1.0291 - val_acc: 0.7102\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8022\n",
      "Epoch 00035: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.6075 - acc: 0.8022 - val_loss: 0.9546 - val_acc: 0.7207\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8077\n",
      "Epoch 00036: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5904 - acc: 0.8077 - val_loss: 0.9623 - val_acc: 0.7310\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5887 - acc: 0.8089\n",
      "Epoch 00037: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5886 - acc: 0.8090 - val_loss: 1.3205 - val_acc: 0.6364\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8114\n",
      "Epoch 00038: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5767 - acc: 0.8114 - val_loss: 0.9667 - val_acc: 0.7219\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.8137\n",
      "Epoch 00039: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5738 - acc: 0.8137 - val_loss: 0.9950 - val_acc: 0.7137\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5759 - acc: 0.8118\n",
      "Epoch 00040: val_loss did not improve from 0.94350\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5761 - acc: 0.8117 - val_loss: 1.4183 - val_acc: 0.6049\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.8180\n",
      "Epoch 00041: val_loss improved from 0.94350 to 0.89473, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/041-0.8947.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5644 - acc: 0.8180 - val_loss: 0.8947 - val_acc: 0.7459\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.8170\n",
      "Epoch 00042: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5639 - acc: 0.8170 - val_loss: 1.0168 - val_acc: 0.7158\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8171\n",
      "Epoch 00043: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5557 - acc: 0.8171 - val_loss: 1.0138 - val_acc: 0.7163\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8191\n",
      "Epoch 00044: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5511 - acc: 0.8191 - val_loss: 0.8953 - val_acc: 0.7412\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.8227\n",
      "Epoch 00045: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5446 - acc: 0.8227 - val_loss: 1.0212 - val_acc: 0.7095\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8248\n",
      "Epoch 00046: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5320 - acc: 0.8248 - val_loss: 1.7387 - val_acc: 0.5807\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.8261\n",
      "Epoch 00047: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5326 - acc: 0.8261 - val_loss: 1.1013 - val_acc: 0.6876\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8259\n",
      "Epoch 00048: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5303 - acc: 0.8259 - val_loss: 1.5301 - val_acc: 0.6054\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5203 - acc: 0.8318\n",
      "Epoch 00049: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5204 - acc: 0.8318 - val_loss: 0.9320 - val_acc: 0.7403\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8276\n",
      "Epoch 00050: val_loss did not improve from 0.89473\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5191 - acc: 0.8276 - val_loss: 0.9349 - val_acc: 0.7363\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8338\n",
      "Epoch 00051: val_loss improved from 0.89473 to 0.87342, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/051-0.8734.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.5131 - acc: 0.8338 - val_loss: 0.8734 - val_acc: 0.7549\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8329\n",
      "Epoch 00052: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5028 - acc: 0.8329 - val_loss: 0.8973 - val_acc: 0.7508\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8360\n",
      "Epoch 00053: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.5079 - acc: 0.8360 - val_loss: 1.1519 - val_acc: 0.6825\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8370\n",
      "Epoch 00054: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4978 - acc: 0.8371 - val_loss: 1.0465 - val_acc: 0.7179\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8391\n",
      "Epoch 00055: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4979 - acc: 0.8391 - val_loss: 0.9542 - val_acc: 0.7403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8383\n",
      "Epoch 00056: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4937 - acc: 0.8383 - val_loss: 1.1775 - val_acc: 0.6820\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8376\n",
      "Epoch 00057: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4919 - acc: 0.8375 - val_loss: 0.8958 - val_acc: 0.7505\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8398\n",
      "Epoch 00058: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4893 - acc: 0.8398 - val_loss: 0.9591 - val_acc: 0.7365\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8399\n",
      "Epoch 00059: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4903 - acc: 0.8399 - val_loss: 0.9357 - val_acc: 0.7426\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.8439\n",
      "Epoch 00060: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4848 - acc: 0.8439 - val_loss: 0.9986 - val_acc: 0.7223\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8415\n",
      "Epoch 00061: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4801 - acc: 0.8415 - val_loss: 1.0687 - val_acc: 0.7116\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.8436\n",
      "Epoch 00062: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4743 - acc: 0.8436 - val_loss: 1.1050 - val_acc: 0.7014\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8460\n",
      "Epoch 00063: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4759 - acc: 0.8460 - val_loss: 0.9130 - val_acc: 0.7584\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8464\n",
      "Epoch 00064: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4730 - acc: 0.8464 - val_loss: 0.9346 - val_acc: 0.7424\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8464\n",
      "Epoch 00065: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4667 - acc: 0.8464 - val_loss: 0.9991 - val_acc: 0.7310\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8462\n",
      "Epoch 00066: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4711 - acc: 0.8462 - val_loss: 0.9303 - val_acc: 0.7435\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4588 - acc: 0.8469\n",
      "Epoch 00067: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4587 - acc: 0.8469 - val_loss: 1.4324 - val_acc: 0.6389\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8481\n",
      "Epoch 00068: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4617 - acc: 0.8481 - val_loss: 0.9111 - val_acc: 0.7468\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8474\n",
      "Epoch 00069: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4649 - acc: 0.8474 - val_loss: 1.5097 - val_acc: 0.6317\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8507\n",
      "Epoch 00070: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4498 - acc: 0.8507 - val_loss: 1.0367 - val_acc: 0.7212\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8526\n",
      "Epoch 00071: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4497 - acc: 0.8526 - val_loss: 0.8940 - val_acc: 0.7589\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8516\n",
      "Epoch 00072: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4468 - acc: 0.8516 - val_loss: 1.0081 - val_acc: 0.7242\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8507\n",
      "Epoch 00073: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.4493 - acc: 0.8507 - val_loss: 1.1479 - val_acc: 0.6925\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4469 - acc: 0.8515\n",
      "Epoch 00074: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4468 - acc: 0.8516 - val_loss: 1.1768 - val_acc: 0.6883\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8545\n",
      "Epoch 00075: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4356 - acc: 0.8545 - val_loss: 1.0605 - val_acc: 0.7345\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.8543\n",
      "Epoch 00076: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4399 - acc: 0.8543 - val_loss: 0.9763 - val_acc: 0.7352\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8557\n",
      "Epoch 00077: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4396 - acc: 0.8556 - val_loss: 0.9226 - val_acc: 0.7498\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8552\n",
      "Epoch 00078: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4369 - acc: 0.8552 - val_loss: 1.0093 - val_acc: 0.7319\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8576\n",
      "Epoch 00079: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4315 - acc: 0.8577 - val_loss: 0.9531 - val_acc: 0.7454\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8562\n",
      "Epoch 00080: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4284 - acc: 0.8562 - val_loss: 0.9186 - val_acc: 0.7552\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4395 - acc: 0.8546\n",
      "Epoch 00081: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4395 - acc: 0.8545 - val_loss: 0.9348 - val_acc: 0.7531\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8590\n",
      "Epoch 00082: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4289 - acc: 0.8590 - val_loss: 1.1729 - val_acc: 0.6928\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8622\n",
      "Epoch 00083: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4179 - acc: 0.8622 - val_loss: 0.9080 - val_acc: 0.7594\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8613\n",
      "Epoch 00084: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4214 - acc: 0.8613 - val_loss: 0.8837 - val_acc: 0.7601\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8595\n",
      "Epoch 00085: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4246 - acc: 0.8595 - val_loss: 0.9592 - val_acc: 0.7398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8603\n",
      "Epoch 00086: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4183 - acc: 0.8603 - val_loss: 1.0507 - val_acc: 0.7282\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8613\n",
      "Epoch 00087: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4195 - acc: 0.8612 - val_loss: 0.9005 - val_acc: 0.7619\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8610\n",
      "Epoch 00088: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4171 - acc: 0.8609 - val_loss: 0.9169 - val_acc: 0.7519\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8619\n",
      "Epoch 00089: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4154 - acc: 0.8619 - val_loss: 0.8895 - val_acc: 0.7605\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8640\n",
      "Epoch 00090: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4118 - acc: 0.8641 - val_loss: 0.9616 - val_acc: 0.7473\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8617\n",
      "Epoch 00091: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4162 - acc: 0.8616 - val_loss: 1.1782 - val_acc: 0.6930\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8664\n",
      "Epoch 00092: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4097 - acc: 0.8664 - val_loss: 0.9357 - val_acc: 0.7517\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8624\n",
      "Epoch 00093: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4117 - acc: 0.8624 - val_loss: 0.9213 - val_acc: 0.7517\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8660\n",
      "Epoch 00094: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4056 - acc: 0.8659 - val_loss: 0.8870 - val_acc: 0.7671\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8644\n",
      "Epoch 00095: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4124 - acc: 0.8644 - val_loss: 1.2675 - val_acc: 0.6895\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8684\n",
      "Epoch 00096: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3980 - acc: 0.8684 - val_loss: 1.0765 - val_acc: 0.7244\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8633\n",
      "Epoch 00097: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4099 - acc: 0.8632 - val_loss: 1.0099 - val_acc: 0.7305\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4063 - acc: 0.8663\n",
      "Epoch 00098: val_loss did not improve from 0.87342\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4063 - acc: 0.8663 - val_loss: 0.9513 - val_acc: 0.7498\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8689\n",
      "Epoch 00099: val_loss improved from 0.87342 to 0.86965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/099-0.8697.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3993 - acc: 0.8689 - val_loss: 0.8697 - val_acc: 0.7703\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8659\n",
      "Epoch 00100: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4047 - acc: 0.8659 - val_loss: 1.1831 - val_acc: 0.6911\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8683\n",
      "Epoch 00101: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3942 - acc: 0.8683 - val_loss: 1.0795 - val_acc: 0.7279\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8669\n",
      "Epoch 00102: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3986 - acc: 0.8669 - val_loss: 0.9178 - val_acc: 0.7605\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8662\n",
      "Epoch 00103: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.4001 - acc: 0.8662 - val_loss: 1.6053 - val_acc: 0.6473\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8669\n",
      "Epoch 00104: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3966 - acc: 0.8669 - val_loss: 0.9901 - val_acc: 0.7393\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8676\n",
      "Epoch 00105: val_loss did not improve from 0.86965\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3930 - acc: 0.8675 - val_loss: 0.9746 - val_acc: 0.7470\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8695\n",
      "Epoch 00106: val_loss improved from 0.86965 to 0.86439, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/106-0.8644.hdf5\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3899 - acc: 0.8695 - val_loss: 0.8644 - val_acc: 0.7727\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8711\n",
      "Epoch 00107: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3891 - acc: 0.8711 - val_loss: 1.2169 - val_acc: 0.6986\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8702\n",
      "Epoch 00108: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3898 - acc: 0.8702 - val_loss: 0.9002 - val_acc: 0.7699\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8726\n",
      "Epoch 00109: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3852 - acc: 0.8726 - val_loss: 0.9758 - val_acc: 0.7363\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8711\n",
      "Epoch 00110: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3915 - acc: 0.8711 - val_loss: 0.9196 - val_acc: 0.7687\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8712\n",
      "Epoch 00111: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3871 - acc: 0.8712 - val_loss: 0.8803 - val_acc: 0.7703\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8718\n",
      "Epoch 00112: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3898 - acc: 0.8718 - val_loss: 0.9029 - val_acc: 0.7727\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8741\n",
      "Epoch 00113: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3782 - acc: 0.8741 - val_loss: 0.9140 - val_acc: 0.7626\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8764\n",
      "Epoch 00114: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3768 - acc: 0.8764 - val_loss: 0.8975 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8723\n",
      "Epoch 00115: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3830 - acc: 0.8722 - val_loss: 1.1269 - val_acc: 0.7184\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8738\n",
      "Epoch 00116: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3820 - acc: 0.8738 - val_loss: 1.1721 - val_acc: 0.7077\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8759\n",
      "Epoch 00117: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3733 - acc: 0.8759 - val_loss: 1.0397 - val_acc: 0.7410\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8737\n",
      "Epoch 00118: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3851 - acc: 0.8737 - val_loss: 0.8926 - val_acc: 0.7694\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8769\n",
      "Epoch 00119: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3741 - acc: 0.8769 - val_loss: 0.9695 - val_acc: 0.7475\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8751\n",
      "Epoch 00120: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3761 - acc: 0.8750 - val_loss: 1.2358 - val_acc: 0.6951\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8768\n",
      "Epoch 00121: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3669 - acc: 0.8768 - val_loss: 1.1172 - val_acc: 0.7163\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8773\n",
      "Epoch 00122: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3631 - acc: 0.8772 - val_loss: 0.9766 - val_acc: 0.7494\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8745\n",
      "Epoch 00123: val_loss did not improve from 0.86439\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3769 - acc: 0.8745 - val_loss: 1.1397 - val_acc: 0.7200\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8800\n",
      "Epoch 00124: val_loss improved from 0.86439 to 0.85957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/124-0.8596.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3657 - acc: 0.8799 - val_loss: 0.8596 - val_acc: 0.7808\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8768\n",
      "Epoch 00125: val_loss did not improve from 0.85957\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3693 - acc: 0.8768 - val_loss: 0.8765 - val_acc: 0.7787\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8749\n",
      "Epoch 00126: val_loss improved from 0.85957 to 0.84696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv_checkpoint/126-0.8470.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3688 - acc: 0.8749 - val_loss: 0.8470 - val_acc: 0.7806\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8780\n",
      "Epoch 00127: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3701 - acc: 0.8781 - val_loss: 0.8629 - val_acc: 0.7806\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8799\n",
      "Epoch 00128: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3599 - acc: 0.8799 - val_loss: 0.9663 - val_acc: 0.7538\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8790\n",
      "Epoch 00129: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3689 - acc: 0.8790 - val_loss: 1.0937 - val_acc: 0.7324\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8765\n",
      "Epoch 00130: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3691 - acc: 0.8765 - val_loss: 1.0199 - val_acc: 0.7536\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8770\n",
      "Epoch 00131: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3708 - acc: 0.8770 - val_loss: 0.8824 - val_acc: 0.7741\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8800\n",
      "Epoch 00132: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3619 - acc: 0.8800 - val_loss: 1.5002 - val_acc: 0.6718\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8824\n",
      "Epoch 00133: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3521 - acc: 0.8824 - val_loss: 0.8922 - val_acc: 0.7789\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8819\n",
      "Epoch 00134: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3537 - acc: 0.8819 - val_loss: 0.8726 - val_acc: 0.7810\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8805\n",
      "Epoch 00135: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3565 - acc: 0.8805 - val_loss: 1.3956 - val_acc: 0.6769\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8792\n",
      "Epoch 00136: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3640 - acc: 0.8791 - val_loss: 1.0401 - val_acc: 0.7501\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8809\n",
      "Epoch 00137: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3583 - acc: 0.8808 - val_loss: 0.9455 - val_acc: 0.7682\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.8814\n",
      "Epoch 00138: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3587 - acc: 0.8814 - val_loss: 0.9353 - val_acc: 0.7626\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3550 - acc: 0.8821\n",
      "Epoch 00139: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3550 - acc: 0.8821 - val_loss: 0.8892 - val_acc: 0.7692\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8805\n",
      "Epoch 00140: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3567 - acc: 0.8805 - val_loss: 0.9141 - val_acc: 0.7745\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8817\n",
      "Epoch 00141: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3602 - acc: 0.8817 - val_loss: 0.9335 - val_acc: 0.7671\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8829\n",
      "Epoch 00142: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3562 - acc: 0.8829 - val_loss: 0.9492 - val_acc: 0.7734\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8817\n",
      "Epoch 00143: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3542 - acc: 0.8816 - val_loss: 1.0197 - val_acc: 0.7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8822\n",
      "Epoch 00144: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3586 - acc: 0.8822 - val_loss: 0.8620 - val_acc: 0.7831\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8858\n",
      "Epoch 00145: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3436 - acc: 0.8858 - val_loss: 0.9112 - val_acc: 0.7668\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8796\n",
      "Epoch 00146: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3579 - acc: 0.8796 - val_loss: 0.9026 - val_acc: 0.7782\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8840\n",
      "Epoch 00147: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3483 - acc: 0.8840 - val_loss: 0.9224 - val_acc: 0.7736\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8835\n",
      "Epoch 00148: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3550 - acc: 0.8834 - val_loss: 0.9940 - val_acc: 0.7556\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8830\n",
      "Epoch 00149: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3492 - acc: 0.8830 - val_loss: 0.8838 - val_acc: 0.7727\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8819\n",
      "Epoch 00150: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 0.3485 - acc: 0.8819 - val_loss: 1.0397 - val_acc: 0.7349\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8836\n",
      "Epoch 00151: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3463 - acc: 0.8835 - val_loss: 0.9113 - val_acc: 0.7731\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8856\n",
      "Epoch 00152: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3442 - acc: 0.8856 - val_loss: 0.9747 - val_acc: 0.7598\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8881\n",
      "Epoch 00153: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3406 - acc: 0.8881 - val_loss: 0.8707 - val_acc: 0.7824\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8827\n",
      "Epoch 00154: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3499 - acc: 0.8827 - val_loss: 0.8724 - val_acc: 0.7813\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8832\n",
      "Epoch 00155: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3513 - acc: 0.8831 - val_loss: 0.9474 - val_acc: 0.7680\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8837\n",
      "Epoch 00156: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3469 - acc: 0.8836 - val_loss: 0.9311 - val_acc: 0.7773\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8845\n",
      "Epoch 00157: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3451 - acc: 0.8844 - val_loss: 1.0870 - val_acc: 0.7440\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8868\n",
      "Epoch 00158: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3365 - acc: 0.8868 - val_loss: 0.9005 - val_acc: 0.7780\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8887\n",
      "Epoch 00159: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3379 - acc: 0.8887 - val_loss: 0.9306 - val_acc: 0.7731\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8871\n",
      "Epoch 00160: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3440 - acc: 0.8872 - val_loss: 0.8712 - val_acc: 0.7801\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8857\n",
      "Epoch 00161: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3416 - acc: 0.8856 - val_loss: 0.8994 - val_acc: 0.7731\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8846\n",
      "Epoch 00162: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3462 - acc: 0.8847 - val_loss: 0.8753 - val_acc: 0.7843\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8871\n",
      "Epoch 00163: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3404 - acc: 0.8871 - val_loss: 0.9600 - val_acc: 0.7638\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8875\n",
      "Epoch 00164: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3356 - acc: 0.8875 - val_loss: 0.8708 - val_acc: 0.7780\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.8854\n",
      "Epoch 00165: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3399 - acc: 0.8854 - val_loss: 0.8592 - val_acc: 0.7869\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8868\n",
      "Epoch 00166: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3370 - acc: 0.8868 - val_loss: 0.8894 - val_acc: 0.7775\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8878\n",
      "Epoch 00167: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3391 - acc: 0.8878 - val_loss: 0.8825 - val_acc: 0.7829\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8875\n",
      "Epoch 00168: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3400 - acc: 0.8875 - val_loss: 0.8987 - val_acc: 0.7789\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8872\n",
      "Epoch 00169: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3416 - acc: 0.8872 - val_loss: 1.1472 - val_acc: 0.7226\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8893\n",
      "Epoch 00170: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3306 - acc: 0.8893 - val_loss: 0.9585 - val_acc: 0.7689\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8883\n",
      "Epoch 00171: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3355 - acc: 0.8882 - val_loss: 0.9767 - val_acc: 0.7570\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8890\n",
      "Epoch 00172: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3346 - acc: 0.8890 - val_loss: 0.8537 - val_acc: 0.7899\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8866\n",
      "Epoch 00173: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3388 - acc: 0.8866 - val_loss: 0.8890 - val_acc: 0.7815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8884\n",
      "Epoch 00174: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3360 - acc: 0.8884 - val_loss: 0.8927 - val_acc: 0.7759\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8893\n",
      "Epoch 00175: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3311 - acc: 0.8893 - val_loss: 1.0065 - val_acc: 0.7512\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8904\n",
      "Epoch 00176: val_loss did not improve from 0.84696\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 0.3300 - acc: 0.8904 - val_loss: 0.9812 - val_acc: 0.7529\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXl4VEXWxt/qpDv7ToAQlgRkhxD2ILIoiggjo6OADiqIg5+4IKLMIDoaRRQFF1AEQUFARAUEZR/BhICsYd/XsCQEsofs6XSf74+Tm9uddCcdSNNNUr/n6afvUvfeulu9dU5VnSuICBKJRCKRAIDG0RmQSCQSifMgRUEikUgkZUhRkEgkEkkZUhQkEolEUoYUBYlEIpGUIUVBIpFIJGVIUZBIJBJJGVIUJBKJRFKGFAWJRCKRlOHq6AxUl3r16lFYWJijsyGRSCR3FPv3708jouCq0t1xohAWFob4+HhHZ0MikUjuKIQQl2xJJ91HEolEIilDioJEIpFIypCiIJFIJJIy7rg2BUvo9XokJiaisLDQ0Vm5Y3F3d0fjxo2h1WodnRWJROJAaoUoJCYmwsfHB2FhYRBCODo7dxxEhPT0dCQmJiI8PNzR2ZFIJA6kVriPCgsLERQUJAXhJhFCICgoSFpaEomkdogCACkIt4i8fhKJBKhFonBHkpsL5Oc7OhcSiURShhSFGiArKwtff/119Te8fBmDBw9GVlaWzZtER0dj5syZ1T+WRCKR2IAUhRqgMlEoKSmxviERNixYAH9/fzvlTCKRSKqHFIUaYPLkyTh//jwiIyMxadIkxMbGok+fPhg6dCjatWsHAHjkkUfQtWtXtG/fHvPnz+cNiRB2771IS0vDxYsX0bZtW4wdOxbt27fHwIEDUVBQUOlxDx06hKioKERERODRRx9FZmYmAGD27Nlo164dIiIi8MQTTwAAtm3bhsjISERGRqJz587Iycmx3wWRSCR3LLWiS6opZ89OQG7uoRrdp7d3JFq2/MLq+unTp+PYsWM4dIiPGxsbiwMHDuDYsWNlXTwXLlyIwMBAFBQUoHv37njssccQVCHvZ7F8+XIsWLAAw4cPx6pVq/DUU09ZPe4zzzyDL7/8Ev369cM777yD9957D1988QWmT5+OhIQEuLm5lbmmZs6ciTlz5qB3797Izc2Fu7v7rV0UiURSK5GWgp3o0aOHWZ//2bNno1OnToiKisKVK1dw9uxZgIh/pYSHhyMyMhIA0LVrV1y8eNHq/rOzs5GVlYV+/foBAEaNGoW4uDgAQEREBEaOHIkffvgBrq6s+71798bEiRMxe/ZsZGVllS2XSCQSU2pdyVBZjf524uXlVTYdGxuLLVu2YNeuXfD09ET//v15TICnp9k2bm5uZdMuLi5Vuo+ssX79esTFxWHt2rWYNm0ajh49ismTJ2PIkCHYsGEDevfujc2bN6NNmzY3d3ISiaTWIi2FGsDHx6dSH312djYCAgLg6emJU6dOYffu3bd8TD8/PwQEBGD79u0AgKVLl6Jfv34wGo24cuUK7r33Xnz88cfIzs5Gbm4uzp8/j44dO+I///kPunfvjlOnTt1yHiQSSe2j1lkKjiAoKAi9e/dGhw4d8NBDD2HIkCFm6wcNGoR58+ahbdu2aN26NaKioniFievoZli8eDFeeOEF5Ofno3nz5li0aBEMBgOeeuopZGdng4gwfvx4+Pv747///S9iYmKg0WjQvn17PPTQQ7d0bIlEUjsRdIsF0+2mW7duVP4jOydPnkTbtm0dlKNb4PBhQKcDnCTvd+x1lEgkVSKE2E9E3apKJ91HjqRcQ7NEIpE4GikKEolEIilDioIjkZaCRCJxMqQoOBIpCBKJxMmwmygIIdyFEHuFEIeFEMeFEO9ZSDNaCJEqhDhU+vuXvfLjtEhhkEgkToQ9u6QWAbiPiHKFEFoAO4QQG4mofCf9n4noZTvmw3mRgiCRSJwMu1kKxOSWzmpLf7IULMXb25snyglD2XKJRCJxAHZtUxBCuAghDgFIAfAHEe2xkOwxIcQRIcRKIUQTK/t5XggRL4SIT01NtWeWby+yoVkikTgZdhUFIjIQUSSAxgB6CCE6lEuyFkAYEUUA+APAYiv7mU9E3YioW3BwsD2zfFNMnjwZc+bMKZtXPoSTm5uLAQMGoEuXLujYsSN+++03m/dJRJg0aRI6dOiAjh074ueffwYAJCcno2/fvoiMjESHDh2wfft2GAwGjB49uizt559/XuPnKJFI6ga3JcwFEWUJIWIADAJwzGR5ukmybwF8cssHmzABOFSzobMRGQl8YT3Q3ogRIzBhwgS89NJLAIBffvkFmzdvhru7O1avXg1fX1+kpaUhKioKQ4cONf8eshVL4ddff8WhQ4dw+PBhpKWloXv37ujbty9+/PFHPPjgg3jrrbdgMBiQn5+PQ4cOISkpCceO8aWtzpfcJBKJxBS7iYIQIhiAvlQQPAA8AODjcmlCiCi5dHYogJP2yo896dy5M1JSUnD16lWkpqYiICAATZo0gV6vx5QpUxAXFweNRoOkpCRcv34dDRs2rHKfO3bswJNPPgkXFxc0aNAA/fr1w759+9C9e3eMGTMGer0ejzzyCCIjI9G8eXNcuHABr7zyCoYMGYKBAwfehrOWSCS1EXtaCiEAFgshXMBuql+IaJ0Q4n0A8UT0O4DxQoihAEoAZAAYfctHraRGb0+GDRuGlStX4tq1axgxYgQAYNmyZUhNTcX+/fuh1WoRFhbGIbNNqWabQt++fREXF4f169dj9OjRmDhxIp555hkcPnwYmzdvxrx58/DLL79g4cKFNXVqEomkDmE3USCiIwA6W1j+jsn0mwDetFcebicjRozA2LFjkZaWhm3btgHgkNn169eHVqtFTEwMLl26ZPP++vTpg2+++QajRo1CRkYG4uLiMGPGDFy6dAmNGzfG2LFjUVRUhAMHDmDw4MHQ6XR47LHH0Lp160q/1iaRSCSVIUNn1xDt27dHTk4OQkNDERISAgAYOXIkHn74YXTs2BHdunWz/FEbK5bCo48+il27dqFTp04QQuCTTz5Bw4YNsXjxYsyYMQNarRbe3t5YsmQJkpKS8Oyzz8JoNAIAPvroI7udp0Qiqd3I0NmOoqSEG8Q1GqBLF0fnBsAdeh0lEolNyNDZzs4dJsYSiaRuIEXB0UhxkEgkToQUBUchxUAikTghUhQcjRQHiUTiREhRcBSmYiCFQSKROAlSFByFFAKJROKESFGoAbKysvD111/f1LaDX30VWZmZNZwjiUQiuTmkKNQAlYlCSUmJ5Y1KLYUNs2bB39/fXlmTSCSSaiFFoQaYPHkyzp8/j8jISEyaNAmxsbHo06cPhg4dinbt2gEAHnnkEXTt2hXt27fH/Pnzy7YNGzoUaampuHjxItq2bYuxY8eiffv2GDhwIAoKCioca+3atejZsyc6d+6M+++/H9evXwcA5Obm4tlnn0XHjh0RERGBVatWAQA2bdqELl26oFOnThgwYMBtuBoSieROptaFuXBA5GxMnz4dx44dw6HSA8fGxuLAgQM4duwYwsPDAQALFy5EYGAgCgoK0L17dzz20EMIUnZQajWcPXsWy5cvx4IFCzB8+HCsWrWqQhyje+65B7t374YQAt9++y0++eQTfPrpp5g6dSr8/Pxw9OhRAEBmZiZSU1MxduxYxMXFITw8HBkZGTV6XSQSSe2j1omCs9CjR48yQQCA2bNnY/Xq1QCAK1eu4Oz58wgq9+nN8PBwREZGAgC6du2KixcvVthvYmIiRowYgeTkZBQXF5cdY8uWLfjpp5/K0gUEBGDt2rXo27dvWZrAwMAaPUeJRFL7qHWi4KDI2RXw8vIqm46NjcWWLVuwa9cueHp6on///igsKADKfafZzc2tbBsXFxeL7qNXXnkFEydOxNChQxEbG4vo6Gi7nodEIqlbyDaFGsDHxwc5OTlW12dnZyMgIACenp44deoUdu/efdPHys7ORmhoKABg8WL166UPPPCA2SdBMzMzERUVhbi4OCQkJACAdB9JJJIqkaJQAwQFBaF3797o0KEDJk2aVGH9oEGDUFJSgrZt22Ly5MmIiooyT1CNMQvR0dEYNmwYunbtinr16pUtf/vtt5GZmYkOHTqgU6dOiImJQXBwMObPn49//OMf6NSpU9nHfyQSicQaMnS2o8jJAU6f5umICECnc2x+cIdeR4lEYhMydHY59Pos5OYegcFQWHXi280dJswSiaT2YjdREEK4CyH2CiEOCyGOCyHes5DGTQjxsxDinBBijxAizF75AQhExQCM9jtEdZCxjyQSiRNiT0uhCMB9RNQJQCSAQUKIcs50PAcgk4juAvA5gI/tlRkh+FSJnEQUJBKJxAmxmygQk1s6qy39la8S/x2A0oVmJYABQghhnxwpp+oktXJpKUgkEifErm0KQggXIcQhACkA/iCiPeWShAK4AgBEVAIgG1AH+tZwbkr/ncRSkEIgkUicELuKAhEZiCgSQGMAPYQQHW5mP0KI54UQ8UKI+NTU1JvKi1O7j6RASCQSJ+G29D4ioiwAMQAGlVuVBKAJAAghXAH4AUi3sP18IupGRN2Cg4NvMheKpeAcBbB348aOzoJEIpFUwJ69j4KFEP6l0x4AHgBwqlyy3wGMKp1+HMCfZKeBE9JSkEgkkqqxp6UQAiBGCHEEwD5wm8I6IcT7QoihpWm+AxAkhDgHYCKAyfbLjnKqNS8KkydPNgsxER0djZkzZyI3NxcDBgxAly5d0LFjR/z222+Wd2AiCtZCbFsKgW0tXLZEIpHcLHYLiEdERwB0trD8HZPpQgDDavK4EzZNwKFrlmJnEwyGXGg0bhCieqOHIxtG4otB1iPtjRgxAhMmTMBLL70EAPjll1+wefNmuLu7Y/Xq1fD19UVaWhqioqIwdOhQVNbBqkKI7cceg9FotBgC21K4bIlEIrkVal2UVOtwQUwE1HSn186dOyMlJQVXr15FamoqAgIC0KRJE+j1ekyZMgVxcXHQaDRISkrC9evX0bBhQ/MdmFgKFUJsnz2L1NRUiyGwLYXLlkgkkluh1omCtRo9ESE3dz90ukZwc2tU48cdNmwYVq5ciWvXrpUFnlu2bBlSU1Oxf/9+aLVahIWFobCwUMlQhX1YDLFd6IRhOSQSSa2lzsQ+YpeNsFtD84gRI/DTTz9h5cqVGDaMPWLZ2dmoX78+tFotYmJicOnSJcsblwqEtRDb1kJgWwqXLZFIJLdCnREFRgN7DV5r3749cnJyEBoaipCQEADAyJEjER8fj44dO2LJkiVo06ZNpfuwFmLbWghsS+GyJRKJ5FaoU6Gzc3MPw9XVH+7uzeyVPdtJTQUUy6FlS8DPz7H5gQydLZHUZmTobIvYz31UbWTsI4lE4oTUKVHgAWxOKAoSiUTiJNQaUbDNDaaxMd1txgny5JTXRSKR3HZqhSi4u7sjPT3dhoJNQFoKFSEipKenw93d3dFZkUgkDqZWjFNo3LgxEhMTUVUE1eLi6wAIOp3h9mSsMrKzgawsdd7Ly3F5AQtrYxmkTyKp89QKUdBqtWWjfSvjyJGJ0Osz0KlT+c86OIDp04E33+TpH38EnnzSsfmRSCQS1BL3ka1oNO4wGp1khLDBYHlaIpFIHIgUBUdhKgRGJ2nnkEgkdR4pCo5CWgoSicQJkaLgKKQoSCQSJ0SKgqOQoiCRSJwQKQqOwrQdQYqCRCJxEuqcKBAVO0f8I9nQLJFInJA6JwoAnMNakO4jiUTihNhNFIQQTYQQMUKIE0KI40KIVy2k6S+EyBZCHCr9vWNpXzWF04mCi4s6LZFIJE6APUc0lwB4nYgOCCF8AOwXQvxBRCfKpdtORH+zYz7K0Gg8ADiRKGi1/C9FQSKROAl2sxSIKJmIDpRO5wA4CSDUXsezBaezFLRadVoikUicgNvSpiCECAPQGYCloEO9hBCHhRAbhRDt7ZkPpxIFoxHQ6XhaioJEInES7B4QTwjhDWAVgAlEdKPc6gMAmhFRrhBiMIA1AFpa2MfzAJ4HgKZNm950XpxKFAwGVRRk7yOJROIk2NVSEEJowYKwjIh+Lb+eiG4QUW7p9AYAWiFEPQvp5hNRNyLqFhwcfNP5cTpRkO4jiUTiZNiz95EA8B2Ak0T0mZU0DUvTQQjRozQ/6fbKk9OJgosLoNFIUZBIJE6DPd1HvQE8DeCoEOJQ6bIpAJoCABHNA/A4gHFCiBIABQCeIDt+F1KKgkQikVSO3USBiHaAv39ZWZqvAHxlrzyUxylFwcVFioJEInEa5IhmR2EwsJUgRUEikTgRUhQchdGoWgqy95FEInESpCg4Cuk+kkgkTogUBUchRUEikTghUhQchex9JJFInJA6JQo8lk7jXKIgLQWJROJE1DFREKVfXytwdFZk7yOJROKU1ClRAJzok5yy95FEInFCpCg4Cuk+kkgkTogUBUchG5olEokTIkXBUUhLQSKROCFSFByFFAWJROKESFFwFFIUJBKJEyJFwVEYjWqXVNn7SCKROAlSFByFtBQkEokTYpMoCCFeFUL4CuY7IcQBIcRAe2fOHjidKMjeRxKJxImw1VIYQ0Q3AAwEEAD+otp0u+XKjjidKEhLQSKROBG2ioLyBbXBAJYS0XFU8VU1Z0WKgkQikVjHVlHYL4T4H1gUNgshfADcka2jrq6+0OszHZ0NKQoSicQpsVUUngMwGUB3IsoHoAXwbGUbCCGaCCFihBAnhBDHhRCvWkgjhBCzhRDnhBBHhBBdqn0G1USnawijMQ8lJbn2PlTlmAbEk72PJBKJk2CrKPQCcJqIsoQQTwF4G0B2FduUAHidiNoBiALwkhCiXbk0DwFoWfp7HsBcm3N+k+h0DQEAev11ex+qcpSAeLKhWSKROBG2isJcAPlCiE4AXgdwHsCSyjYgomQiOlA6nQPgJIDQcsn+DmAJMbsB+AshQqpzAtVFEYXi4mv2PEzVSPeRRCJxQmwVhRIiInAh/hURzQHgY+tBhBBhADoD2FNuVSiAKybziagoHDXDtWvAxo3QFfsCkKIgkUgklrBVFHKEEG+Cu6KuF0JowO0KVSKE8AawCsCE0m6t1UYI8bwQIl4IEZ+amnozuwDi4oDBg+GWXARAioJEIpFYwlZRGAGgCDxe4RqAxgBmVLWR4O9frgKwjIh+tZAkCUATk/nGpcvMIKL5RNSNiLoFBwfbmOVy+LKF4FrgCkAjRUEikUgsYJMolArBMgB+Qoi/ASgkokrbFIQQAsB3AE4S0WdWkv0O4JnSXkhRALKJKNn27FeDUlEQOXnQ6eo7lyjI3kcSicRJcLUlkRBiONgyiAUPWvtSCDGJiFZWsllvsLvpqBDiUOmyKQCaAgARzQOwATz24RyAfFTRzfWW8CltArlxA7qgho4XBSUgnux9JJFInAibRAHAW+AxCikAIIQIBrAFgFVRIKIdqGLUc2nj9Us25uHWKLUUkJMDnS7E8aIg3UcSicQJsbVNQaMIQinp1djWOVBE4cYN6HROYClIUZBIJE6IrZbCJiHEZgDLS+dHgF0/dw6m7iNdQxQXXweREdyRygFIUZBIJE6ITaJARJOEEI+B2wkAYD4RrbZftuyAqyvg4VHqPmoCIj1KSjKh1Qbd/rwQ8U+KgkQicTJstRRARKvA3UvvXHx9yywFgMcqOEQUFBGQvY8kEomTUakoCCFyAJClVeB2Yl+75Mpe+PiUWgqqKHh5tb/9+TAVBdn7SCKROBGVigIR2RzK4o7AgqXgEBTLQImSKkVBIpE4CXdWD6JbxcfHOUShvPtIioJEInES6pYo+PoCOTlwcfGBRuMhRUEikUjKUfdE4cYNCCGg0zVEUVGFMEvWiYkBVlY2gLsa3GmisG8fkJ/v6FxIJJLbQN0ShVL3EQB4eLREfv4Zy+lGjwYWLjRfNnMm8N//mi87dQp4/33uXlodyjc0O3Pvo+xsoFcvYEmloa4kEkktoW6JQqn7CAA8PdsgP/8UiCwUyKtXA3/8Yb4sIwPILfcJz19+Ad59F8jKql4+7iRLISuL83ezIcslEskdRd0ThaIioLgYnp5tYTTmVXQhGY0sHJmZ5sszMyuKQna2+b+t3Em9j/Ly+L9UTCUSSe2mbomCEuoiJwdeXm0BAPn5J83T3LjB7qCMDPPliiiYuopuVhTuJEtBEYXygiiRSGoldUsUTILieXq2AWBBFBRXkKkoELEolJQAxcXq8poSBaOx+u0StwtpKUgkdYo6KwpabX24ugYgL6+cKCgFvKko5OcDej1Pm9aYa0oUAOdtbJaWgkRSp6hbomDiPhJCwNOzLfLzT5mnUSwFpYEVMG9fMC0clbS3Igqa0lvg7KIgLQWJpE5Qt0TBxFIAUCoKViwFInXamijUpKXgrO0K0lJwLoYPB374wdG5kNRi6rgotIFenwK93sRVZNq9VHEhmbqSakIUFKvgThAF5XylpeAcrF0LxMU5OheSWkzdEgUT9xEAkx5IJi4kS6JgL0tB6ZJquszZkO4j58FgAAoL5b2Q2BW7iYIQYqEQIkUIcczK+v5CiGwhxKHS3zv2yksZFSyFdgCAvLyjahrTAr4yUSgu5he0/Da2IN1HkptBCrTkNmBPS+F7AIOqSLOdiCJLf+/bMS+Mtzf/l75U7u5h0GqDkZ29U01jq6VgKgR1paGZCNiyBfjwQ8fmqa4iXXmS24DdRIGI4gBkVJnwdqLRsDCUWgpCCPj53YPs7B1qmuxsQKfj6dshCneKpaCM0Vi6FPjkE8fmyRQi4IsvgMRER+fE/khRkNwGHN2m0EsIcVgIsVEIcXs+gVYaKVXBz+8eFBZeQFHRVV6QlQWEhfG0qShotTxdV0UB4MIoPV21GpyB1FTgtdeAH390dE7sT21yHy1eDISHO6+FXIdxpCgcANCMiDoB+BLAGmsJhRDPCyHihRDxqbcamM0kKB4A+Pn1AQBkZ/+F0gkgKIjTmYpCaChPlxeFoKBbD4hnuszZMBWF3Fy+Jkaj84TSVgS+fKyq2khtshSOHwcuXnSe50hShsNEgYhuEFFu6fQGAFohRD0raecTUTci6hYcHHxrBzYJnw0A3t6R0Gg8VRdSVhbg7w8EBpp3Sa1XD/D0rCgKTZveekA8wHlFwbSBWbEUlGlnQMlHXRCF2mQpKOdQG86lluEwURBCNBRCiNLpHqV5Sbf7gcu5jzQaLXx9oyoXhcxMICCA2yNqQhTudEsBcOzLnJsLXLhgno+6IArKs5ef77zPi60o983kXZQ4B/bskrocwC4ArYUQiUKI54QQLwghXihN8jiAY0KIwwBmA3iC6DY4qn18KhRofn73IDf3EEpKsrmA9/OrWhQUl1HTprysOi+pM/Q+SkwEfv216nR5eWwlAfwCK9fEkS/zjBlAz57m+aiuC+9OxNRqu9O7CEtLwWmxZ++jJ4kohIi0RNSYiL4jonlENK90/VdE1J6IOhFRFBHtrGqfNUI5SwEAAgMHAjAiLfW3m7MUgOoVks5gKcydCzz+uBrozxp5eUCDBjydmKiKlyNf5sREIC2Ne0PVJUuhfKP/nYw1UTh//vbnRWKGo3sf3X6aNuVCxaRm6evbC25uzZB65QcuJE0tBSVstiVR8PLidMq8rTiDKKSm8rlVJWZ5eUDDhjx96ZK63JGWgmkgwrokCuXbd+5kLInCrl3AXXcBR444Jk8SAHVRFAYN4gL4f/8rWySEBg0a/BM5V7byAlNLITeX++hbEgU/P/4p87ZSXVE4epTzXZM9NZQG46rybWopmIqCIwslU1Goi72PgNopCpcv87+0FhxK3ROFqCgu8DdsMFtcv/4/4Zpb6hrx82MRMBjUgtCaKPj7q/O2Ut2AeJs38+/0aduPURW2iAKRuaVw8aK6zhlEIStLzUdWlvOMnbAXtd19pDyLKSm3Pz+SMuqeKLi4AA8+CGzcaNa46+3dAd6GFjyjWAqAWmuxh6Wg0agNzZWJglKDunrV9mNUhSIKlTXQFhXxNQoO5nw6s/vIYLjzC8qqqO2WgvIOXb9++/MjKaPuiQIADB7MtZH9+80W13PtBwAo8sirKAqBgfZ1H1XW++hWReGFF4AnnzRfZouloJyrlxf32jINJeEMloKp+wio/S6k3Fy1EnEniwKR+myZ3j/lvkpLwaHUTVEYNAgQAli/nucTE4H//hf+ho4AgDR93O2xFGx1H92qKMTH80+ByDZRUNwVXl587gYDXzdvb8cVSqYfPzJ1HwG1XxRM23fuZFHIy1NdfdJScDrqpijUqwf06QMsW8Y19GnTgA8+gO6HtQCAlOINIGXk9Hff8b8iCoWF3PCcnc1uptspCklJth/DlOvXzWtf+fncnROo3H1kKgrKtyj8/fnnKPdRXp56re5kS6GkpPo9znJzgZAQnr6TRcE077JNwemom6IAAGPHAufO8Zesli3jZX/+CQDI0ZxHbmgBL3/kEW6DCA1VQ2/n5amWgpsb/+wlCnl5aq2+Mkth0ybLn2kk4pfsxg31+w/pJgPHq2MpAGxBWRgAeNswFTGlTUGx6u6kAWwDBnAgv+qgDCTUaGqnKCj3T1oKDqXuisLjj3NhMmYMP5hDhwIAyNUV5OGK6ylLgX/+E/jpJy5wtVq1YMzI4AJWsRL8/OzX++jKFXW6MlGIjgbefLPi8hs3VKtAqYHdjCgoloISLNCZRKFZM56/kyyFY8eAHTuqTmdKbi7fh3Lxu+xOcTGPIagprDWYS0vBKai7ouDuDowaxQV8hw7A/PmAqyuEvz/qBT+G5ORFKCkpF0pAEQXFjXOzomApzIU1UVBcR23aWBeF4mLg0CHOl2INKJjWupTp6oqCt7d67kFBt79QMsVUFLKyOB/KqPI7RRRKSvi5O3myei6k3FxVoC2J8rlz5t1Wa4pffgHuvpvHy9QESt5dXCyLQmamWpGR3HbqrigAwP/9H+DqCrz8MjfgjRwJNG+Oxo1fhcGQjevXl5inVwpGpRfOrYqCaZRUa72PlG6gvXpxDcpSWIpjx7j7KJH5WALAvNZVXhRcXKrfpuCM7qNGjfha3imioFz/wkLzbr5VkZfHz6Cl6280At26ATNn1lw+Fc6d43+TAZ+3hJL3hg0tiwJiu8t0AAAgAElEQVTAI+4lDqFui0Lr1lwTf/55np8/H4iNha9vFHx8eiAxcRaITAprRRSUCJ0BAfzv76/GSbKF6rQpXL7MBV63blzoW/K37tunTpcfDWoqCuXdR1VFeLXUpqC4j6prKRiNNRPKQxGF+vVVUfD15XtQmSg4YmBbYaHlGq9pgXfihPXt161jsVeozFLIzOTrYY/RwIq1umVLzexPyXujRhW7pDZpwtPSheQw6rYoANybgyN482c4PTwghEDjxq+ioOAM0tJ+V9MqBaMyGrpHD/6/6y7g1CnbC57qikJoqOoiseRC2rsX8PDg6cpEobylEB5evXEKwM1bCq+8Ajz0UPW2sYQiCs2a8bkVF7MoBARYFgUi4K23uFaanFy9Y+3YwW1PNytmQ4eyNVoeW0Th9Gng4YeBefN43mBgkbFmKSj39ma7LR8+zMezZDkqohAXVzNuHSXvoaHqtNHIAtGqFc/LxmaHIUXBCsHBw+Dp2Rbnz0+EwVDACxVR2LkTiIjg2ioAdOzIhaul7wTv2lWxka4yUUhJ4fYBhcuXWRAaNeJ5Sy/9vn1Av35ceCtWjILycnl4mFsKPj48UvlmLAWlULImgl9+WdaTq4xdu4ADB6wfy1ZMRUFphPfxsSwKRMCLLwIffsjnvnlz9Y7122/AqlU31xWYCNizp8IASQDmonDypOXtlYL499JKiWn7jiVRUO5tdYUPAAoKeHDjunWWrYHLl9lFmp8P7N5d/f2Xx9RSUKZzc/maKaJQFyyFDRvMLUEnQYqCFTQaLVq2/AqFhQm4cqX0Q/VKwUjEXQoVOvKgtwoNcampXDvu2xdYvVpdXllD8/jxwD33qLX0qkQhL48/bdijB9CihWVLITCQtze1FAID+UWvqk1BCG6UL9/7qKSkYqO2cm3efBN45x3zZefO8XFvNahfVhZ/Ac9U0KyJwvHjXNN+5RVuM/rjj+odSxFYpYCuDqmpXPNNSKgonoootG9v3VJQ7nNcHJ9neavNmqVwM6IwZQqLk6srsH27+ToiPv9hw/hZ3bpVPYfmzVn4qotpm0JxMf+U57BlS/Pzqa2cOgUMGaJ2h3cipChUQkDAfQgOHoFLlz5CXt4pVRQA4P771ekOHfi/vChER/PL3KEDMHw4sG0bL7fWJbWwkEdZ5+VxLdVo5Npw06ZcCLq4mIuCXq/GcOre3boo1K/PhaLyomVkcOHu71+1peDlpY5iBlT3EWDZhZSRwdvt2sXfPAC4AFHS2lLrrsxdo3zvQmnkB1T3UXmB21n6iY7x4/l+bdlSvY8ZJSTwf3UagxXOnuX/3Fzz3l6AKgp9+rAoWLK4lMK9pIS7RCuiUJX7KDPTslhbIyMDmDWL3Vx9+lTsJpuayrXZTp2Arl1VS2LnTr4+O2/iMyg5OfxcKfcwJ0d9Dhs35kpIbbcUlMb7murRVYNIUaiCu+76HC4uXjh1ahSMHm680NWVa/8KAQH8MCs3mIhfnm++4bhD27ax+2bFCl5vrffR1q388ru6cg3i4EEu+Js143QNG6qicO0aC8WwYbyuRw+uuV24YF7wXb/OglC/vrn7KCiIX8qiIusmrNLbBTC3FCoTBaX3k9HIhRlgLlSm4y4ssWIF799aTH1lJLkSnVbJmyVLYdcuHuzVogWLQkpK9V7CW7EUFFEAVHFRSE3l/EZE8P225Ha8epXPq149HmBpq/sIqJ61cPYsP6+DB7OFeuiQ5XDWTZsC/fuzq7KwkJ9NwHLey3P8uLkbLSdHHW+hzCui4O9vXoGprSjvyfHjDs2GJaQoVIGbWwhatZqLnJy9uHztM26M7tnT3GoA+AU/coRvdqtWwAMP8AsdHc012YgItUCy1qawZg2nffVV7v739NNcmA8bxmkaNVJFITaWX6RPP+XaZv36XPgVFZkXCpYsBVNRAKxbC4qlALAb7J13gMhIziNguQeSUqvWaNhHDai1IqDyQmTXLj7nggLrg6UsWQqmomBa6961i7vyCsH3A7C9B43SmwewjygEBwPt2vG8JRfS1avcEDt4MPueFStIcR/p9eZiblqIVkcUFOFr0YItBaPR/NqbikKvXnzcAwdUUahK5AHguef4PJT8ViYKfn7mFRhnoLiYz3/Roprbp/JMVNb7zEFIUbCB+vWHl7qRpqLkwb7Av/5VMVHHjuwnfPddflEWLeJ55fvGimgQWRYFvZ5dRoMHA88+y2lOngQWLFD30aiRWqju2sW+9fHj1ca5FqWhv01r5qaikJbG+y0vCtbaFUxFISAAeO89tmJssRSGDmVLQa9nUVB6eFkThfx84B//YIvL09N6A2xl7iO9Xm2zSE/nHjy9evF8aCjQtq3t7QqmDfY3KwpK8DprohARwSPlf/+94vbJyXy/Bw5kgVK6HSuWAmB+/a9f530p29qK8qyEh/O3RjQacxdSeVEAuLHZVkshJ4eDMaakACtXqst8fMwrF8oz6OfnfJbCsWP8TsfE1Nw+lfckKal6Y5xuA3YTBSHEQiFEihDimJX1QggxWwhxTghxRAjRxV55qQlatpwFFxdPHH1PDxo1qmKCjh25UFqyhEVj9GhzF0dEBD/4iYmW3Ufbt3Nh8eij3AA5aBD3nCkNvwGA/bonT3K6nTvZZeTqqq4vLwrFxVyg1K/PP2WcQ1aW2qYAWH8olX7x5anKUvD25hp/djbw118sCs2a8TGt1SyXLGGX2KJFXHhbq0EpomDJfQSoLiSll4xSkAFs7fz5Z0UfvyWUgjwszLIozJvH7rqSEsvbnz0LdOnC52xNFAICgGeeARYurFgzvnqVu0t37crzcXH8b9o92FQUUlL4ugHVF4WQEBZiHx+gc+eKouDlxXlt2JCvx7p1fB+FqNpS2LWLn3c3N+Crr9R8V+Y+srelQMQdP6r6PrmCEmG4Jl09Fy+y1wFwOmvBnpbC9wAGVbL+IQAtS3/PA5hrx7zcMjpdAzRv/gmys7chOfnbigmUHkiursC//219/ZEj/JJoNPxSKb2PVqzgF1Ppy79xIzBnjvk+/v53Nu9XrGDfr2mBB3BtzsUFOHOG55UGzQYN1Frr6dP8UpR3H6Wn84to2h5haimYUpmlcOkSFxwDB3KD4cqVLAp33cVWgKWapdEIfP45N5bfcw+7VaoShfLuI8WaUgrEXbv4WnTvrqZ7+mkuCH7+2fK+TVEshf79+ZzKNwbPncuFvaV8ErEotGzJNXBrogAAkyaxW2XWLPPtFUuhZUt+Lv76i9dVZim0b8/nXF1RUCoTANC7N/coUsRO6f2mWHq9eqk15p49+VjWhBHg9jQXF3Y97t7NBWxV7qOwMN6vrcJgMJiHhq+KnTvZKv3xR9vSK+0hJ09Wr6NCZVy8yM8WUHdEgYjiAFQ2zPfvAJYQsxuAvxAixF75qQlCQp6Dv/99OHv2Fdy4Ue4hbNOGC9BRo9SBZqYoPZSOHOEHS7EQlP8bN/hBVV4US3TuzPueNo1fxLvvNl+v1fJL/fPP/KIoL5ViKQCqW0bpkgqwCRsezsLh769aGlevsniUpyr3UbNmXHgNGaKKQosWPFrVkiisX89CNnEiFz7t2nGeTC2R9HQ+J2ui0LkzTyuFw65dbFmZilpkJFtsixery44c4aCIBQXmebpwgc+9QwfzQgtgt6DSEL53b8XzuX6dBdWSKBiNfC6KKLRuzfd9zhzz2D9FRVyDd3HhfCvuFUuioFiAISHmnRFsobwo9OzJLjilVqyIgoJpReThh/meXLtmff/btrG18/LL3Nnihx843+XPIyuLa87u7mwdE3Ebmy189hmLv62fq1XumdIbsCoUUSgoqCjwN8ONG9zrq39/Pt+qLJC4OA7MeZtG5TuyTSEUgKntmVi6zGkRQoN27X6CTtcAx48/iqIik5dBp+MGuC+/tLyxUgNSLIXyogCwoFSeAQ7lrbz0UVEV07z4Ij+4mzaZi4JiKSiNv/XqqQXrunX8Yo4bx//r1nFhcOlSReEBqnYfhYXx9IgRXFhlZKiWQnl3AxEwYwave+wxXqa4QRQBu3qV9/nmmyyGpt/G9vTka9isGZ/j7t3cO2bnTrY6yjNqFBcKp07x/IwZ7LKaPds8XUICF+hKgWjqQvr5Z74XXl7mIUYUlEZmRRQuXVJrmJmZfP8VUQB4nEB2thq3SKnpK2NTFMEDzN1H8fHqd7QLCvj8Q0JstxQKCvjalhcFQC04rYlCkyYsukDl7UR79/LASl9f3vfOndYtBeWeRkTwtVN661WGXq9aWZYEWuHjj/ndANRC3hZRKCrid7ZPH56vCReS0p7QokXlrlKFF1/kwYXPPXdbBrvdEQ3NQojnhRDxQoj4VAcHytLpgtGhwxro9Rk4duxhGAwmUSlbtVLDTVgiIoLDCZw5ozYKKqIQGgrce2/VGXjkEf5v2VJ1mZjy6KNcMMyZozbWmbqPNm7ktoi+fdWXcONGzs/MmVx4b9mivjD9+lU8hlL7Lm8pZGdzjU8JZT14MBfagCoK6enmtfJt27g9ZdIk9ZqU75Xz6afcvqEU3KaWglKwCMEiuWsX+8QLC9mFVZ6RI/maL1jA+/z1V3bhffih+UjjCxe4zaC8KBBxra1fPy4gbRGF4mJVyJVjmIpCly48juWzz/ieKWmVD+p06aKeo4cHWy+tWgETJvAgSmXsR/36qiisWcOuyMrGLCi1XlNRaN6crcg9e3jb69fNRaFTJ85D585qnCJr7Qp79nChrTxDd9/NDdSZmZZFQbmnQnB4kZgYdayLNVasUM/f0uhxgI/3/vsc2ywri8VUCL7HiYl83adOtbztsWN8Dk8/zfM14epRRCEsjJ/1gwfZWhw7tmLapCQWoq5dufJS3W9w3ASOFIUkAE1M5huXLqsAEc0nom5E1C3Y9GVyED4+ndGu3U/IyTmA48dHwGi0Ub0jIrj2u2YNv9AAm48aDT90plaDNfr04QJe8UeWR6fjAH8bN/JIXiE4vdLVb9AgbmxVGhaF4JpmVBQvu/9+Lqi3bOHGRaUtxBSNxvInOZXuqIql4OXFLgaARUEpRLZt44Ltq6+4R1PDhuYvRHg4N0yeOMGF6Lx5XAgptSR/f963i4tqtQB8DmfPAsuXs8BYukYNGvB3MubM4cIgP5/3n5cH/Pe/nMZg4HOxZCkcPsxWxogRLK5HjlR0PR06xMdv2pT3AagFsCVRAIAPPuDzmzrVuqXg7c33y9eXuze/+y4XnErvpQYN1G7L777L1qI1yxVQ3YSmoiAEn9fevWrjtiLSAJ/XokXcRlCVKGzdyveod2+ev/tutvSKi/nZ02r5+S8vCgB3wzYYzCMBlKe4mO9h69Z8762Jwnff8X1W9nfmDLfPAfysvPkm8NFH5vexoIDvrbLP++7jSk1VlsLMmWyhVubqUUQhPJzbgVJSOF+LFlV8p5TecgsXcpq33678+DUBEdntByAMwDEr64YA2AhAAIgCsNeWfXbt2pWchaSkeRQTAzp48F7S67Oq3mDlSiKAaNQoIqNRXR4bS5SXZ/uBL18mys62vv7aNaL77yd6/nmidevU5Tk55sclIvL15Ty9+y7Pr1jB8zod0d//bv0YjRoRRUYSTZhAdOkSL/v9d952zx413e7dREOHEhUVEW3dyuvvuov/ld/nn1fcf8eORIMHE73yCpEQRCdOEPXqxek3b+Y0gYFEXbqo28TE8HpXV6J777We96QkIi8vTtusGZHBQDRxIs/PmcP5B4i++YbX6XRE//43b/vCC0Tu7kTp6USrV3O6nTvVfefmEvn7Ez3+OM+fPs1p3nqLSK8nWrWK5w8cqJiv557jfU+ezGlyc3l5URGRVkvUsKF5+sxMIhcXonbtOP3+/UTR0ep1DQwk8vMjSk21fB0+/5zTlV//zjtEGg3RAw8QBQcTFRZa3t5oJPL0JHrtNcvrIyKI+vZV59PS1LzNnMnLgoP5mvbqRTRggPm+W7Xie/ngg0QnT6rrDAaib78latKE97VwIdHLL/M9NRjM86DX8z3u3ZuvRevWvM3vv/Ozr9OpeTJ9V8aP52W+vnw/jUbOR+fOfO8PHap4viUl/F6UfybK89prfN2MRn5XP/iAaOlS3m7DBvO0TzzB9738e3sTAIgnW8ptWxLdzA/AcgDJAPTg9oLnALwA4IXS9QLAHADnARwF0M2W/TqTKBARJScvpdhYV4qP70Z6fU7liYuKiH75hai4+PZkzhaaNuXHIDaW59PSuBC2VlgrDBjAaYTgB5eIaPZsXnbtmuVtzpxRX8BXX+WC4aGHiPLzK6YdMUJN+9xzvGzdOi6sTp/m+fBwov791W1ycng9QPTRR5Wf94cfqoU1ERceDz/M56PREHl7q8dp0YLPMTubC57Ro3l5YiLvY8oULozj44nmzuVl27dzmuJiop49VTFUxOfKlYp52rdPLcx9fc3Xde7M25fnnnvU63TlCguZUpjt3cvn8sILanqjkeivv4h++41o3DhOV77AWb9e3ed//lP5dWzdWhVAUy5eNC/8Fdq0UQWXiKh5c6KRI1nYHnvMPO3583z8wECuJBQWmlcOoqK4gmA0Ei1axMtOniS6cIF/BQVEr7/Oy1ev5nwq53XtGlc6AD6+tzfR//0fH/fGDSIfH77mwcFqviZOZNFu0YLF6uxZ8/xu3qzu/+WXrV+zRx/l8zUlL4+Ff9IkdZnBQBQURPTMM9b3VQ0cLgr2+jmbKBARpab+RjExGjp8+CEyGPSOzk716NiRH3TT2mCXLmS1NqtQUsIiN2UKF6RHj3JB7+5uvVaTl8f7dXcnunq18nzNmcPpZs82r/1lZqrT995L9PTT5tt16kRltebKKCwkmjqV6Pp18/w9+SS//KbLH3iAqEEDopde4n3v3auuU2qGAJGHB1FoKF8/02tgMHChFBysprVU+zYa+X4AXNiaMncuX+vyTJum7rOoSLXWxo/n9a+9xvPLl7OoNm+upge44CtPSoq6/vz5yq/jgAFcOJfnyy95e0VYFcaM4eU//sjznTqxJRkayusssXYtb/Pww1x4BwURLV5sfo0PH+Y0M2awRQBwWoBFvKSE6LvveL5xY97miy/YUjh3jugf/+A8GI2qsO/cyZUFfek7/e23RAAZ/fzJ4O7JYpKfz1b2nj087+/P+axfX92OSH2Gb9xgC2jw4LLFOTn8OBjv6UPUrRuny8sjWreOjAAVLvqRMjK4DpKeXvntqAxbRUFw2juHbt26UXx1+iTfJq5eXYAzZ55H/foj0bbtYghhQ/uAM/CPf/D/r7+qy6ZN44bY8+erbufIyOA2hJAQTt+vnxpJ0xLt2rE/96OPKt+v8lEepfHZEpmZ5iOsAeCNN7gb7IUL6hiQW+XAAc5zYiJ/7Mi0cXnmTPYzjx3L4Uni47nL6zPPVNzP/v3cziGE9Y8UffEFNybee2/F8OPW8ta1K8g/AMXXMqBJS4HLqKeQNWMBcgKbQSf00A5/FNr9u1FUokFu667IfWocbhi9kf79Wnj06IjwqWNgMLBbPzubL3vAuCdQ0CAM11+bDp2Om3CKi9nVXlDAzU/+/kDCu9/jyrEsBLz8FIJ9i1CvUyiKioC0tz5HaooRuc+/XtbT1N0dcN+/Ay5Lv0fG6NeREtQWKcu3Iv96DjRGPUTLu6Dp0tlsCI/yX7J9J4znE+AV7AXd4PuRr/FGfr7aDKARRris/Q355I5ENEZgmB866k4jPaQDkowhIAJEcRHEnl0Q9YIgOnYEyIiSAj0yct2QnpiPzGwNGjQA2ubFw0VD0Pe8B3q9QEkJtzVnp+mReSUHmeSPYr0G7iiAl0shPA058EI+dKIYmV6NccPoDUN+Edz93OAfrEVJWhbys4qRr/FGsdEVLjBAo9NC6LTIyzNvfnBDIdw8XKAryEIR3JAPTxigDlCdPLnqV8caQoj9RNStynRSFGqOS5emIyHhTdSvPxKtW38LFxd3R2epavR6LoDd3NRlRiM3CCojLqvi3Xe5d8ewYcC335o3/pbHdOCePSguBuUXQPj7mS0mUj/pbDSqPyJu301PV9s/vb25AEtO5vbmS5eAG9fyodvxJ7Sd20PbMhxaLZ9GURF30iksBApz9Ci6kITCBs0gNAKBgXy88+dNBuqezkBOWhG8WoSUfYIjJIR1NSMDuJaoR8qRa9B56xDcvgHy8rjt0c1NLVgNBrVwLiggFF7PRh55Qg8b79dtxMXFetBbDw+gQZAenplJoLx8GIMbgAKCzO6N8u+iMUJTkIc84Y3iYgFPTxYmpbOfwQAYz1+Ae0EmGrf1wXXfVjh+nNvzmzThe0UE0IUEwN8P5B/I+3XhzlZBXgXw/2EOktAIZ9AKaNYM2pBguLpyvUSrVaOpBAYC7lSA/C/mI6/EDfm9BiDvwnUUJacjYMjd8G0aANfvvkGhzheZ/uHQJZ6HZ9NgeLoUQetKMPbsBUP9EBiNvE9vb34NC09dRNHSn1EIdxQHhsC9R0d4Ng6EV3iDsvPt0oXrJTeDFAUHcenSh0hIeAs6XSjCwt5FSMi/IOxVADoLJSVce46KAkEgJ4d7eyqh8ouK+J+IX678fC6gS0p4+uJFThMUxC/3jRtcECr/hYXmBUT5QsNgYKMhNZU7cpTVHjVqeCmDwfaoBpZwd+ftLRVwyicnTAtuZYyalxd37snO5rw1aMAFQV4e/4eGcmehy5f5/Bs2BOoXXkKxux9Siv3Lem4q4lNQwOfj4WHyO30QnqIA3g/cXXY7/P15O6Wzj17P+fP25jz5+vLx8vL4+ru6cucfPz/ef2Ymn4fyyYO8PJPavjvft/R0IOzsH2g6bghujB6P1L0JSDt+HW5UiHruuagXuxKePTrAYFCD8RYWcl4CA9Wo7CDibsmmwRZvhhkzuNfd5s2VW5jWWLWKb0RwMPDEE+YhZCyxdy9XnCIj+YYnJ/MNBbj33rRpHLhy3DjuaVeV5VpYyKoTHMzjbZTeZzWEFAUHkpkZg4SE/+LGjb9Qv/4/S62GSsYvOJDiYi58XVz4HRBCLZCzs/m//PTly9wzU6PhXnoXLnBPUK1WLUBuFTc3Lh98fHjaxUV1JVhyLyjvUnAwb0PEBbjBwO+rac/c8vvSarmA1OlYzHJyuPBVQv00bcoFIcD70utVA8vdnbev7bpvFUWZXVz4wr3wAps+//63Ooq+LlO+q21VxMZyV1VlrE8NIkXBwRARLl/+EAkJb8PLKwJt2y6Bt3cnux4zKUntVu3vz9Elzp3jWrkyriwjgwt2xWRVxuZUh6AgHsMkBAtEWBgPzDQauYAMCeH3QKfjn5ub6onS61V/tKsrF6rNmvF/RobaRGCr50oikdiGFAUnIT19A06dGoOSkgw0afIGmjZ9E66ulcQ3KofBwFbplSvsf05MZHfDqVM8Du7GDdW/bMm1obgFlEHAgYGq+wJgy7dRI7VWTaRGNfbzq/jv41O1VS2pWxDRLbtIjWSERtxaxwC9QY8iQxG8dd5VJ7ZAvj4f+fp8eOu84e5avfbAEmMJruVeQ2PfxmXLMgoysDtxN9rWa4vwgHDkFechT5+HYM9gq9fr15O/Irc4F/c3vx+NfBzjPpKvt50JChqM7t2P4fz513D58ke4dm0RwsM/RMOGoyBMXoJTp9gNefEiF/xXrvDv6tWKhb1OxxEUunVjt4niW27YkEfDa7XsEw4P56gFtgyUltwesguzkafPs/rCExH+TPgTLYNaoqmfhcCKFth4diM0QoMH73oQRjJi64Wt6BfWDzoXHS5kXsD2S9uRWZiJrMIsFBuKcV/4fWgZ2BLbLm1Dh/od0CWkYtT6A8kH4Ovmi7sC76r02MuPLscza56Bl9YLbq5uKCopQmFJIVw0LvhowEcY33O8Wfoz6Wfw/aHv8Z/e/4Gfux9e2/Qavjv4HXKLc7Hg4QV4rstzZWnPpp9Fan4q2tZri6s5V3Hw2kEcTD6IPH0e7m5yN0qMJTiXcQ6jI0ejRUAL/G3537Dj8g682vNVaIQGa8+sRaBHICIbROKNu9+Av7s/vjv4HXo36Y2ujbridNppLD+2HP/s+E/sS9qHcevHIac4BxqhwcOtHkb/sP7YmrAVrYNaY8YDM7A7cTcm/m8iAj0C4eHqgT1Je6DVaNGpYSfsvLITKXkpeKLDExjWbhi+3vc1Yi7GwEgc86qBVwOk5KWAQPBw9cC0+6bhtV6voUBfgO2Xt6NP0z6YtWcW3tz6Ztn5N/JphLub3I1Zg2bVuEBUhrQUbhMGA7Bnz2H88cdvOHPGFcnJ3XD1aicUFvrDYNAhMZFrDm5u3FuiSRP21yvTyi80lGv7NeXDrola3uFrh/Hz8Z/R0LshHmzxIFrXa2017YnUE/B18zWrUVli64WtWHBgASZETUBUYwuB/8A1QyEEXDVq3WbxocXYcXkHvhr8Fdxc3SxuB3DN9Mj1I2hbry3cXN1gJCPir8Zj3Zl1KNAXoIlfE4ztMhYeWg98vONjLD2yFCXGEjza5lG80+8deGg9UFhSiGErhuFC5gX8reXfkJiTiKPXj+Lvrf+Ocd3Hlb3IsRdjcS7jHM5lnMPc+LnQarS4/NpleGo9zfJ0OfsyXlz/ItafXY/7wu/D1me4ay8RYfKWydAb9fjswc/K0pcYS/BuzLv4cMeH8HPzQ9LEJKw8sRKjfxuNx9s9jjd6vYGBPwzEjSK1+6uLcIGB1FpGiHcITr98Gj5uqvW65PASjPltDLQuWnxy/yd4ucfLZs/Ilewr8Hf3h5urG1p92QoeWg8MbD4QRYYiuLu6w83FDYeuH8L/zv8PL3V/CZ8O/BSuGlfM3jMbU/6cgsKSQnz10Ff4V5d/IfCTQHRq0AlJOUkI9w9H7OhYZBdm482tb+Kb/d+UFaoKyv6zi9SotfW96mNIyyFYdGgR+jbri7hLcdAIDfo264uikiLsT94PV40rfHQ+uJ53HUEeQdg4ciOGrxyOi1kXy/bTu0lvjGg/ApezLwhTQxsAACAASURBVOP7w98jLT8NDb0b4lruNTwd8TTWnlkLL60XgjyDkFechx6hPaA36nEg+QA6N+yMcP9wzNozC3qjHo19G2N0p9HoH9YfR1OO4uC1g7gr4C74ufvh15O/Ynfibpx++TTeiX0HSw4vgY/OBznFOXiyw5N44+43EHsxFoevH8aqE6vg4+aDt/q8Bb1Bjy4hXdAvrJ/V57oypPvIgaSkcG3//HnulLNnD/v6FZeNq6sRoaGJaNToMLy9MyGED/r2bYKnnupmFrr+VskpykF6QTrC/MOsphn560gkZCbg9yd/Rz3PeiAizNozC3uT9uKbv31jVliY8kHcB9ifvB86Fx1WnljJA19ACHAPwKUJl8q2W3liJTac3YA5g+cgtzgXzb5oBr1RjyEthyC7KBsF+gJMv386+of1L9t3UUkR2s5pi4SsBADAvWH3YkT7EYi/Go8dV3Zg08hNaObfDPcuvhdNfJtgyaNLAACbz23G4B8Hw0hGPNHhCSz7xzIzl8TFrItYc2oNcopysOzoMpxOP40WAS3wVMRT+PHojzibcRYaoYHORYfCkkK83edtvNj9RYTNCkObem3QyKcRNp3bhJaBLTEhagK2XNiC1adWo1fjXtiTtAf1POuhdVBr7Li8A4EegTj18imcSD2Bft+rL3H/sP6IvRiLRX9fhNGRowGw9bD48GK89edbMJIRPUN7IuZiDM69cg7NA5pjwqYJmL13Nlw1rkj/dzo0QoMxv43BpnObkFOcgwdbPIjN5zdj3pB5mBs/FxezLiK7KBsaoUET3yZY88QaNPVrCj83PxQbirH5/GZczr6M+l718eSqJ/FGrzcwY+AMAMCXe77E+E3jMSB8ANxd3bH+7Hp8cv8nmNR7EgBg55WdeGDpAwj1CcXw9sMxbfs0bBy5EYPuMv90ipGM+M8f/8HMXTPRsX5H+Lj5YOeVnXi41cM4lnIMLYNa4vVer+PBHx7EuifXYeeVnfj4r4+ROikVE/83EUsPL8WL3V/E/c3vx+m00wjxCUHnhp3Rul5raIQGJ1JPQOeig8FowIAlA5Ccm4znOj+Hb4d+i4TMBLi7uiPEhwMKJmQm4O2Yt5FRkIHRnUZj3PpxyCrM4md3+EocuX4E3jpvvNj9xbIKRmFJIa7lXkMzv2Zl17+RTyP8NeavSt+n02mncSb9DAbdNQhaF8s9oK5kX0Grr1qhZWBLHE05imcjn4XeqIeHqwfmDJ5jtt2xlGN49OdHcS6DP2lreq+qi62i4PARytX9OeOIZiKi5GSOCtG9u/mAUZ2Ooxy88grRt98X0LT1i2jvlYNkMBqoqOg6Xb36HcXH96SYGNDp0y9RXt7ZSo9TYiihN7e8SSuOr7C4/kbhDVp2ZBk98tMj5P6BO7lNdaNrOdfIaDTShI0TaPyG8bT5HMcO0hv05DXNixAN6vB1B5q7by499vNjhGgQokH3LLyHcooqhu4oKiki9w/cKfDjQAr6OIheWv8SZeRnUGxCLCEa9NnOz6iopIjGrBlTtq9PdnxC7/z5DiEaNPb3sdTo00bUc0FPCv8inBANGr5iOP1+6ncq1BfS57s+J0SDVp1YRdO3T6fms5oTokGe0zwJ0aAZf82g5JxkQjTI9yNf0hv0lJCZQH4f+VHE3AiKjokmRINeXv8yGUtHveYU5ZTtB9GgzvM60+e7Pqd2c9oRokFR30bR9we/p/R8HjI6fMVw8v7Qm5777TnSvKehs+l8X/44/wdFzI0o28+s3bOIiCivOI8MRh61Gp8UTy7vudC4dePo7u/upkafNqILGRcoLS+NjEYjtfmqDUV9G0UJmQkUOS+ybF8PLn2QEjITKDE7kTTvaWjyH5Pp/dj3CdGgAYsHEKJBa06uoQX7FxCiQWPWjKHfT/1ORqORIudFUuDHgYRo0Lf7v6Wp26ZSh6870PmMykckP/fbc+T6viutOL6C5sfPJ0SDHvnpESrUF5LRaKShy4eS1zQvupJ9hfYl7SO/j/yo+azm5PeRHyEa1Pu73mXX2BLrTq+jBjMakP90f1pyaAkZjUZ6Y/MbpH1fS6PXjCb3D9wprziPdl7eSYgGfbHrC9JN1dGL616sNN+mnEs/R1O3TaX8YgvhUiyw5fwWCvo4iJYeXmpTeoPRQPPj59OZtDM256kqpmyZQogGRcyNoKKSokrTFpcU0+Wsy5RZkEn6W4iYABnmwv5cvcohVx54QA2507kz0fTpHNUgPp4jDygoLziiQd3m83D2An0B3TX7Lnrz974UEwOKiQHt29eVsrN3VziewWigZ9c8S4gGubznUla4KxToC8oKvkafNqJnVj9DiAbN3TeX9iXtI0SDNO9pCNGgzec208Hkg4Ro0P+t/T/y/tC7bL/Tt0+nn4/9TC7vudCYNRVDD+y6sosQDVp5fGWFdX0X9aUmnzWhp359ihANmrJlCg1cOpACpgdQ0MdBNHT5ULP0ecV59J8//lNWoPl+5EveH3rTA0seKEtjNBrp8LXDlFWQRZ3mdqK+i/rSwgMLy67lX5f/osl/TCaX91zoQsYFMhqN9Prm1wnRoNc2vUYlhhIat24ciWhBW85vobxiNfig3qCny1mXK5zHydSTZddqxIoRZuuMRiPtv7qf1p1eV2E7hVc2vFKWv/nx883WKaLX+LPGFDA9gD7Y9gFtvbDVrHB9+MeHywT76V+fpkJ9IXlN86Jx68bR/UvupxazWpil/3b/t4RoUL1P6tlcOBIRpeSmUNPPm5bldeDSgVSoV0NwXMi4QO4fuFOHrzuQ9n0tNf28KV3KukQHkw9Sn4V9aE/inkr2zuQU5VBmgRqeJO5iHCEaJKIFPfTDQ0TElZ3gT4LJ4wMPQjToRMoJm8/hZlAE3FFkF2bTuHXj7H6epkhRsBN793Jg0PBw1Rpo3pzorbeN9OIv79Ivx36xWHNKy0sj3498afCywWWFdYG+gE6nnS57Iefunk5Xrsyiv/4KpakrQAv+6ESnTo2lzEwOrjZ121RCNOjf//s3RcyNIJ8PfWjcunFlNTClprfsyDIyGA1kNBqp1ZetaMDiAfTKhlfIbaobJd1IIq9pXvTiuhfp671fE6JBFzIuUG5RLiXnJNONwhtleR65aiQ1mNGAjEYjnU47TQ/98BCl5aXRzL9mEqJByTnJFc5z7em1Zefzfuz7RER0KPkQiWhBiAZtv7Td4nUtLimmTWc30Zg1Y6jNV23oyLUjFtO9tfUtcnnPhe5bfB/V+6QeiWhB78a8S80+b0aDfhhUls5oNNL4DeMJ0SDt+1pCNGjipom232giGrV6FCEadOBqJTGgrJCen06BHwdSy9ktK9Tu0vPTyW2qG7lNdbN6PX4/9TshGtRnYZ+yQvpvP/6NQmaGkOY9DU3ZYh4HKb84n5p81oQ+jPuw2nktLimmDWc20NRtUym3KLfC+vdi3yNEg4b9MoxS86xEXK0GJYYSCvo4iBAN+mrPV2XLR68ZTYiGWYVAUnNIUahhDh7kOFcIiSddUCI9/DDRZ59xYEujkWj9mfVlheGAxQOow9cdqPms5hSbEEtERJP+N4lEtKBj14/Rdwe+KyuMYxJiCNGgZp83K3MZvLXl32X7emieltb/Adq5rw/5fuhBQ5b2J6PRSInZiXT/kvvJf7o/IRo0fft0avVlK+r6TVczUZqyZQpp3tNQwPQAGr5iOBERDV0+lMK/CKenf32a6s+ob9X8V0TmdNppmvzH/7d37+FRVveix7+/uU9mcr9BuCXcRCACsVK8UUWrgiJUFCvWU0+Pm+p2uzd2n+eItfZQq922fbprbW21WnfphgrUS8EqVIsVLz0qF5FwEzAgQkISciOTZDKXd50/3jfDJGQgIMlMzPo8T57MrHln8ps1k/nNurxrLY51Dc1dMVeN+sWobu8TNaLqmmXXqHvX3dvpce9+5W517fJrT9rV0BMdrRSWoO5YfYea+vTU2AfM0q1LOx1rGIZauX2lWvz6YrVo7aLT+gatlFKNbY1qfcX6M451z9E96lDToW5v+/OuP6u3DryV8L5RI6pWbV+l6lvrY2WPv/d47Ll/dOSjbu/zees3USw7anac1cfsSLgV9RWxstW7VyuWcNIWmHbmepoU9JTUU3j7bViyxFybLCMrgmvRV8jwpvG9W//C1CFTATOxLnlzCcVZxSwsW8hj7z9GaUEpB5sOcsUfruCcvHPYWbuT2867jQkFE/jsmLkpSVWgispmc5et5+c/z282/oZH330UgNsn306Rv4gfv/tjPL4yzmn8mGOhNq5Of5MtW6YxZMjdrFvwMjabm5ufv5nF6xcDsGLeik4zRW6acBM/eudHNAQbuO08c/eomaNnsubjNdS11XF58eUJZx9NHzEdgLc+fYtX9r4CwLNbn6U6UM3MMTO7vY9NbKy9de0J5b+a9avTqvdEpg6ZSn5aPrWttcwaM4tCfyEfHP4Aj8PD3HFzOx0rIsyfMJ/5E+af0d/K9GQyo2TGGcc6JndMwtvmjJtz0vvaxMZNE27qVHb16KsBGJc3jtKCEzc/+rzz/E8Wy/j88ac+8DQ8OP1BLh1+KSXZJbGy2WNnU35XORMLJp7Vv6WdngGfFJTqfkrmkSNw993m4qGDB8Ojj8L0eXu4aHkLx0JhLvv9ZWxauInx+eNZu28tGys38vTsp7mj7A7uv9Sca3ys/RiL1i3iYNNBbp90O3d+6U7AnAYIUNlcSVWzucvW6JzR/G7O77h54s1UNFTw7fO/jYjgsrtYsmEJ2Z5sJheWMrfsDiorf83u3d9k3757SU+/gO+VjmFXzUjCysG88fM6PY9JhZMYlT2KY+3HuHqU+aEyc/TMWHwXDr2QRMbmjqXAV8CybcsorylnfP54ttdsB+CSYd3sf9wHOuaQLytfxpUjryQ3LZdH3n6E68ZeR4b7c6yb0w+MyRnDdWOv4/qx1/f79bRG5YxiVM6oTmUiohNCKuhJcyKVfs5W95FhGOqZzc+o/J/kq4fefEgZhqG2VG5R7332nnrtNXM5dK/X3BSpY1O0FeUrFEtQr+x5Rdl+YFMPvvGgUkqpGUtnqBE/H6FCkZ5tnlMTqFEsQT3+3uPqO+u+o7wPexM2+0ORkCp7qkyxBPVfH/5XLPa6utfVzp23qY0by9SGDR712nrUq2/4VHn5PHXgwCPqyJFlsU1/3vn0HfW3T/7W6XE7Zt10dG8lcuOqG2NdFv84+A/ledijWMJZ7044HTWBmtgAZygSUrc8f0uPBjw1bSBDdx8lFjWiLHhxAat2rGJYxjC+/+b3eWn3S3x45EM8KpvgD2s591y44ZcP8dzRF/j1kw3s/OedbKvehsPm4IqSK7hw6IW8uvdV7pl6D28eeJMHLn0g4bzkrnLTcnHanGZLIVBFUXpRwm9+TruTlTeu5Pdbf88tE28BzG9UOTlXkpNzpfl8oq00NLxBXd3L1Ne/ytGjLwDgcg1i6NB7GeMZQVpa5+b/9WOvZ3/Dfr5UdPJpy9OHT+f5nc9TklXCtKHTmD9hPq998hrj8sb16Ln2hnxfPvk+c49jp93JH+f9MWmxaNoXzYBMCg/+/UFW7VjFIzMe4b6L7+MHG37Af/6/n5NTN5P63LVcf+cmvnnnUea9+BCTCidR2VzJ+v3r2VazjXF543A73MwaM4sH3niAJzc9iaEM5p0779R/2GITG4P8g6gMVFLZXBk7ySaR0TmjeXjGwwlvt9vTyMu7jry86wAzSTQ3b6Si4rtUVNwXO87vLyMjYxpOZy63Dg1wZd5tGO17wDUl4WN3jCtcO+ZaRIQnZj1BY7Cx1/qvNU1LrgF3RvPq3auZu3IuC8sW8tTspwBzJdErv6qoajxKaFEBP7z8hxxoPMCfdv6Jyu9UMvhng1lQuoB1+9Zx8fCLWX7DcrYe2cqUp6bgcXgYkj6EvffsPa1+3mnPTCPDncGnTZ8yedBkVt648oyfUyJKKUKhKsLhehob/0519TLa2vYSiTRgt6djGEGUCmOz+bDZXPh8pWRnz8Dvn4LPV4rHMwIF/OjtH7GgdAEjs0ee9Rg1TesbKbEgnohcA/wCsAPPKKUe7XL77cBPgcNW0a+UUs/0ZkxLP1rK8Mzh/HLWLwFz6Ym5cyHQLLz913zu2lrG2n1r2Vu3l1ljZuFz+ZhRMoPVH6/mSOAIdxaYg8WTCicx2D+YqkAV886dd9oDf4PTB7O3bi+VzZWxgd+zTURwu4twu4vw+ycydOg9AChlIGIjHK6npmYVbW17MYw2jh17nwMHfgCYXxTsdj9ZWTO4a8JC3M5WAoFyHI4sXK5CbDa9trWmfRH1WlIQc5PiJ4CvAoeAjSKyRim1s8uhK5VS/9JbcXRVXlPOBUUX4LK7UMrcWnfXLnOzpqlT4apjV8WmhX5t3NcAuGrUVaz+eDUA5xWeB5gfuDNHz+TZrc+eMOOnJ4r8Rfx1319pi7T16QqIQGx1VqczhyFD7ux0WyTSTEvLdlpaygkEtlJb+wJ1dWs6HWOzpZGTMxO/fzJ2u5+0tHNITz8fl0tvqqJp/V1vthSmAvuUUhUAIrICmAN0TQp9piXUwif1n/CN0m8AsHIlPPccPPwwXGmO2fLVUV/l0XcfxWV3xRb56pjKCceTAsC/X/TvDMscxgVFF5x2LEXpRbRF2mKXU4XDkU5m5oVkZppTVUePfoyGhvVEowFAiEQaCQS2cPTon2MD2h0yMi4kLW08bW17cLuHk5c3B6czDxEnHk8xbndRp+XCNU1LPb2ZFIYAn8VdPwR8uZvj5onIdGAPcK9S6rOuB4jIQmAhwPDhPVtjvjs7a3eiUJQWltLQAIsWmXsSLF58/JiLh11MmjONy4ovi817H5UzipHZI2loa2BI+pDYsePzx7PksiVnFEv84HLHeQupyGZzkZt7YvfW2LG/xjDCRCJNtLbuoLHxbWprn6eubg1e71jq69dRU7O866PhcGTjdObgcGShVBiw4/Odi883kbS0Cfh8E/F4hgMCGJgNTk3T+kqyZx+9DDynlGoXkW8DS4ETTiFVSv0W+C2YA81n+sfKa8oBKC0o5bvfhaNHzX2+4zehcTvcvLLglRM2OFl88WKqAlVn7aSh+NZBKrUUTofN5sTlysPl+gpZWV+huPh7sdsMI0IgsJlotA3DCBIM7icUqiQcricSqScSaUTEiWGEaGzcQHX1sth9RVwoFQEUTmceHs9I0tOnEI22EA4fJTd3Njk5VxMO11kD6UfJypqB11vc95WgaV8wvZkUDgPD4q4P5fiAMgBKqbq4q88AP+nFeCivLifNmYanbSTPPAN33QVTupmNGb+2f4d/Ov+fzmos8a2DU01J7Y9sNgcZGd01DLsXiTTR0rKDlpbttLXtw2ZzA0IoVE1r626qq5/D4cjEZvOwd+8/n3B/EQd5eTdYg+BunM5CwLBmWvlxu4eSk3MtLlceShkoFUXErruzNK2L3kwKG4ExIlKCmQy+DiyIP0BEBiulqqyr1wO7ejEeymvKmZA/gad/aycahXvv7c2/dnIdrQOvw0umOzN5gaQIhyOTzMyLyMy86KTHKaVobt5MIPAhLlchLlcRdruXysqnqal5DqXCGIbZOgEzWZitDvOyx1NMMHgQpULW383C5SrC5xuP1zsGp7OApqa3aWhYT0bGlykouIW8vLk4nVkopYhEGgiH6/B4irHZenayoqb1J716noKIzAIew5yS+qxS6hEReQjzdOs1IvIfmMkgAtQDdymldp/sMT/PeQoFPy1g1ujZrL3zd0ydCi+/fEYPc1YYysDzsIfhmcPZ96/7khfIF5BSimj0GGDHbvehVIiWlp3U1DxHMLgfj6cEuz0DpSJEInUEg5/R0rKdYPAAEMXpLCQn5yqamt4hGNyPiAu3exih0OG4ZOPG4ylGqXZEnDid+Tid+bhc5m+wEQxWYLf78PvLELFjGCF8vvHY7ek0N2/C4ym2usFqaWr6By5XAV7vmNgsLsMI6am/2lmTEucpKKVeBV7tUvb9uMv3A/d3vV9vqA5UU9taS+RwKTU1cM89ffFXE+s4q/mL2HWUbCKCw5EZd91NevoU0tMTn7kN5vkb4XAdDkc2NpvDapV8QE3NSkKhKlyuubjdQ3A4smht3Ulb237s9jQMI0Q4XEswWEFz8/uEw0dRysDjGU4k0kRVVeJTb9zuEYRCh2OtGQCPpwSlwrS3H8Lvn0JW1uWEQpWEQkeIRgPY7Rm43UPjfsyYzMH54y0iu91HIPAhhhHG6x2JUmGi0Rb8/kl6AF9LKNkDzX2mY5D54MZShg07PgU1mW4494aUnnk00IjYcLny464LGRlfPq2xEehYZDIaSyzt7Z9ZH8I2Wlq2E40ew+8vo6npHaqr/0BBwXzy8ubGxlWam9/HZvPgdg+joeENDh36BR7PCNzuITideUQix2hsfJNQqLJTMukpl6uI3NxrcTiyCYUqCQS2oVQEhyOLzMxLsdnc1NSswG73kZ8/H4cjHaWiZGVdjs83EREhGDxIbe0LpKefT2bmJRhGiEikAaUi1niNDbd7GCKCUlHC4XoMI2i1tBRe72g9npOiBkxScNgczCiZwd4VpUybBrYUeD8+ds1jyQ5B6wUigogjdtmcYmtyu49/CfB6Sxg06LZO9+1u+q9KsLy7UlFCoRra2z8jGm2OlRtGO8HgfiKRY/j9k7HZPASDFdhsHpQyqK1dRW3ti0SjAZzOHPz+KdhsXsLhag4d+lksAUSjAfbv79yQdzhycLuH0tKyA4gC5smMhtF6QnweTwk+33k0NW0gEmnsdJvTWUhu7rXk5l6H1zvaGqupJxQ6TFPTu4RCVfh8E60JA1Gra66QSKSRUKiGcLgWm82Lx1OM05mPzeYiGNyP3Z5OQcHNRKMBjh5dQ1rauaSnl9HauototAWfrxSHIz3BK3e8XqPRFuv5frGXY+/OgFr7qK4O8vLMvRHuu+/Ux2vaQBOJNGMYrbhchQC0tx9BxIZhhKivX0dz8yba2w/i801k0KBvEQhsoanpH7jdg3E4chFxWOMnrdTVvUJr626ysi7D7y/Dbvci4kapdurrX6e+fh3RaNMJMbhcRbjdw2ht3WGdNHkim81ntTqi3dx/EJHIsbhEZQOM2O12ewZ2ux+73Y/DkYHTWYjLVYjTmUtz8yaamt6OtcDMJGh2z/n9ZWRlTcecFVdJc/OHGEYbDkcWoVAVoVA1Xu8ofL4JpKVNAAyCwYPY7X6czmyi0Vai0Wai0QDRaDOG0U5m5nSys2fQ3n6YcLgOm81NOFxLe/shq5uwCK93LE5n1ud4VU09HVMYUEnh9dfhqqvM36nQfaRpA5lhhGlqepdwuNY6oTEHpzMft3uI1e2krK4osVoHNdbJj/nY7V4MI0IodDjWNeXxFNPSsoPDh3+B05lPUdG3aWvbRyBQjt8/CbvdT0tLOeFwrfXBHLBaHtWEQtWEw7WkpZ1DTs41uFyDUSpKW9snhEJHCIePEghsjk00AHA4cnE4MolEGqykUkgw+Ant7Yd6WAN2zKQmdKw3lojN5sNu9zJ06CJGjHjgjOo7JQaaU83mzebvsrLkxqFpmnnyY3b2ZQlvj++Gc7sHd+p6M+/vwOMZgcczIlbmdg+O7TMCkJHxZQoLj98nL292wr+XqJuuQzQapKVlGyIunM68WPLqKhxupLV1FyJ2PJ4RRKOtRCL12Gw+HI507PZ0a1ZchLq6tTQ3f4DHMxKXqwDDaMfpzMXtHko02kx7+yFaWz8mFDqCYbSdsC9KbxhQLYWbbjITQ0XFWQ5K0zQtxfW0pZACw619Z/Nmc60jTdM0rXsDJinU1cH+/XD++cmORNM0LXUNmKSwZYv5WycFTdO0xAZMUvB6YfZsPcisaZp2MgNm9tEll5g/mqZpWmIDpqWgaZqmnZpOCpqmaVqMTgqapmlajE4KmqZpWoxOCpqmaVqMTgqapmlajE4KmqZpWoxOCpqmaVpMv1slVURqgU/P8O55wNGzGE5v60/x9qdYoX/F259ihf4Vb3+KFT5fvCOUUvmnOqjfJYXPQ0Q29WTp2FTRn+LtT7FC/4q3P8UK/Sve/hQr9E28uvtI0zRNi9FJQdM0TYsZaEnht8kO4DT1p3j7U6zQv+LtT7FC/4q3P8UKfRDvgBpT0DRN005uoLUUNE3TtJMYMElBRK4RkY9FZJ+ILE52PPFEZJiI/F1EdorIDhH5N6t8iYgcFpGt1s+sZMfaQUQOiEi5FdcmqyxHRF4Xkb3W7+wUiPOcuPrbKiLHRGRRKtWtiDwrIjUisj2urNu6FNPj1vt4m4j06bZRCWL9qYjstuJ5SUSyrPJiEWmLq+Mn+zLWk8Sb8LUXkfutuv1YRK5OgVhXxsV5QES2WuW9V7dKqS/8D2AHPgFGAi7gI2B8suOKi28wUGZdTgf2AOOBJcD/TnZ8CWI+AOR1KfsJsNi6vBj4cbLj7OZ9cAQYkUp1C0wHyoDtp6pLYBawFhBgGvB+CsR6FeCwLv84Ltbi+ONSqG67fe2t/7mPADdQYn1m2JMZa5fbfwZ8v7frdqC0FKYC+5RSFUqpELACmJPkmGKUUlVKqS3W5WZgFzAkuVGdkTnAUuvyUmBuEmPpzhXAJ0qpMz35sVcopd4C6rsUJ6rLOcAflOk9IEtEBvdNpN3HqpR6TSkVsa6+Bwztq3hOJUHdJjIHWKGUaldK7Qf2YX529ImTxSoiAswHnuvtOAZKUhgCfBZ3/RAp+qErIsXAFOB9q+hfrGb5s6nQHRNHAa+JyGYRWWiVFSqlqqzLR4DC5ISW0Nfp/E+VqnULiesy1d/L38JsyXQoEZEPRWSDiFyarKC60d1rn8p1eylQrZTaG1fWK3U7UJJCvyAifuAFYJFS6hjwG2AUMBmowmw+popLlFJlwEzgbhGZHn+jMtu4KTO1TURcwPXAn6yiVK7bTlKtLhMRkQeACLDcKqoChiulpgDfsLyb9AAAA6NJREFUAf4oIhnJii9Ov3nt49xC5y80vVa3AyUpHAaGxV0fapWlDBFxYiaE5UqpFwGUUtVKqahSygCepg+bsqeilDps/a4BXsKMrbqjK8P6XZO8CE8wE9iilKqG1K5bS6K6TMn3sojcDlwH3GolMaxumDrr8mbMPvqxSQvScpLXPlXr1gHcAKzsKOvNuh0oSWEjMEZESqxvjF8H1iQ5phirv/B3wC6l1H/Glcf3FX8N2N71vskgIj4RSe+4jDnQuB2zTr9pHfZNYHVyIuxWp29aqVq3cRLV5Rrgf1izkKYBTXHdTEkhItcA/we4XinVGleeLyJ26/JIYAxQkZwojzvJa78G+LqIuEWkBDPeD/o6vm5cCexWSh3qKOjVuu2rkfVk/2DO2tiDmVEfSHY8XWK7BLN7YBuw1fqZBfw3UG6VrwEGJztWK96RmLM0PgJ2dNQnkAusB/YCfwNykh2rFZcPqAMy48pSpm4xk1UVEMbsx/5fieoSc9bRE9b7uBz4UgrEug+zL77jvfukdew86/2xFdgCzE6Ruk342gMPWHX7MTAz2bFa5b8H7uxybK/VrT6jWdM0TYsZKN1HmqZpWg/opKBpmqbF6KSgaZqmxeikoGmapsXopKBpmqbF6KSgaX1IRC4Tkb8kOw5NS0QnBU3TNC1GJwVN64aIfENEPrDWqn9KROwiEhCRn4u558V6Ecm3jp0sIu/F7SfQsffBaBH5m4h8JCJbRGSU9fB+EXne2oNguXVGu6alBJ0UNK0LETkXuBm4WCk1GYgCt2KeGb1JKTUB2AD8X+sufwDuU0qdh3mmbEf5cuAJpdQk4CLMs1XBXAV3Eeb6/SOBi3v9SWlaDzmSHYCmpaArgPOBjdaXeC/mgnQGxxclWwa8KCKZQJZSaoNVvhT4k7U21BCl1EsASqkggPV4HyhrHRtrJ61i4J3ef1qadmo6KWjaiQRYqpS6v1OhyINdjjvTNWLa4y5H0f+HWgrR3UeadqL1wI0iUgCx/ZJHYP6/3GgdswB4RynVBDTEbXJyG7BBmTvoHRKRudZjuEUkrU+fhaadAf0NRdO6UErtFJHvYe4sZ8NctfJuoAWYat1WgznuAObS1k9aH/oVwP+0ym8DnhKRh6zHuKkPn4amnRG9Sqqm9ZCIBJRS/mTHoWm9SXcfaZqmaTG6paBpmqbF6JaCpmmaFqOTgqZpmhajk4KmaZoWo5OCpmmaFqOTgqZpmhajk4KmaZoW8/8BUiTyMTn61W8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 767us/sample - loss: 0.9613 - acc: 0.7362\n",
      "Loss: 0.9613271430769195 Accuracy: 0.7362409\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1851 - acc: 0.2320\n",
      "Epoch 00001: val_loss improved from inf to 1.84292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/001-1.8429.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 3.1854 - acc: 0.2320 - val_loss: 1.8429 - val_acc: 0.4086\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2065 - acc: 0.3862\n",
      "Epoch 00002: val_loss improved from 1.84292 to 1.69928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/002-1.6993.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 2.2065 - acc: 0.3862 - val_loss: 1.6993 - val_acc: 0.4801\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8729 - acc: 0.4611\n",
      "Epoch 00003: val_loss improved from 1.69928 to 1.26497, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/003-1.2650.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.8733 - acc: 0.4611 - val_loss: 1.2650 - val_acc: 0.6164\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6489 - acc: 0.5219\n",
      "Epoch 00004: val_loss improved from 1.26497 to 1.24527, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/004-1.2453.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.6489 - acc: 0.5219 - val_loss: 1.2453 - val_acc: 0.6247\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5004 - acc: 0.5577\n",
      "Epoch 00005: val_loss improved from 1.24527 to 1.15410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/005-1.1541.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.5007 - acc: 0.5577 - val_loss: 1.1541 - val_acc: 0.6497\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3954 - acc: 0.5854\n",
      "Epoch 00006: val_loss improved from 1.15410 to 1.10762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/006-1.1076.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.3958 - acc: 0.5854 - val_loss: 1.1076 - val_acc: 0.6751\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3060 - acc: 0.6104\n",
      "Epoch 00007: val_loss did not improve from 1.10762\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.3060 - acc: 0.6104 - val_loss: 1.1494 - val_acc: 0.6597\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2249 - acc: 0.6288\n",
      "Epoch 00008: val_loss did not improve from 1.10762\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.2251 - acc: 0.6287 - val_loss: 1.1242 - val_acc: 0.6560\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.6469\n",
      "Epoch 00009: val_loss improved from 1.10762 to 1.02472, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/009-1.0247.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.1613 - acc: 0.6469 - val_loss: 1.0247 - val_acc: 0.6956\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1062 - acc: 0.6639\n",
      "Epoch 00010: val_loss did not improve from 1.02472\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.1065 - acc: 0.6639 - val_loss: 1.0476 - val_acc: 0.6876\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.6727\n",
      "Epoch 00011: val_loss improved from 1.02472 to 0.99081, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/011-0.9908.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.0663 - acc: 0.6726 - val_loss: 0.9908 - val_acc: 0.7128\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0213 - acc: 0.6886\n",
      "Epoch 00012: val_loss improved from 0.99081 to 0.99006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/012-0.9901.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 1.0214 - acc: 0.6886 - val_loss: 0.9901 - val_acc: 0.7056\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.6967\n",
      "Epoch 00013: val_loss improved from 0.99006 to 0.93626, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/013-0.9363.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.9854 - acc: 0.6967 - val_loss: 0.9363 - val_acc: 0.7165\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9543 - acc: 0.7076\n",
      "Epoch 00014: val_loss improved from 0.93626 to 0.92605, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/014-0.9260.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.9542 - acc: 0.7076 - val_loss: 0.9260 - val_acc: 0.7207\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9170 - acc: 0.7169\n",
      "Epoch 00015: val_loss did not improve from 0.92605\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.9169 - acc: 0.7169 - val_loss: 0.9367 - val_acc: 0.7200\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.7215\n",
      "Epoch 00016: val_loss did not improve from 0.92605\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.8963 - acc: 0.7215 - val_loss: 0.9366 - val_acc: 0.7314\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8704 - acc: 0.7291\n",
      "Epoch 00017: val_loss improved from 0.92605 to 0.89016, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/017-0.8902.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.8703 - acc: 0.7291 - val_loss: 0.8902 - val_acc: 0.7363\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8466 - acc: 0.7382\n",
      "Epoch 00018: val_loss improved from 0.89016 to 0.87971, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/018-0.8797.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.8466 - acc: 0.7382 - val_loss: 0.8797 - val_acc: 0.7340\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8285 - acc: 0.7435\n",
      "Epoch 00019: val_loss did not improve from 0.87971\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.8284 - acc: 0.7435 - val_loss: 0.9231 - val_acc: 0.7228\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7476\n",
      "Epoch 00020: val_loss improved from 0.87971 to 0.86955, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/020-0.8695.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.8109 - acc: 0.7476 - val_loss: 0.8695 - val_acc: 0.7456\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7521\n",
      "Epoch 00021: val_loss did not improve from 0.86955\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7937 - acc: 0.7520 - val_loss: 0.8853 - val_acc: 0.7365\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7603\n",
      "Epoch 00022: val_loss improved from 0.86955 to 0.80945, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/022-0.8095.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7713 - acc: 0.7603 - val_loss: 0.8095 - val_acc: 0.7619\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7614\n",
      "Epoch 00023: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7593 - acc: 0.7614 - val_loss: 0.8410 - val_acc: 0.7503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7681\n",
      "Epoch 00024: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7437 - acc: 0.7681 - val_loss: 1.1652 - val_acc: 0.6529\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7332 - acc: 0.7705\n",
      "Epoch 00025: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7333 - acc: 0.7705 - val_loss: 0.8137 - val_acc: 0.7633\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.7698\n",
      "Epoch 00026: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7317 - acc: 0.7698 - val_loss: 0.8832 - val_acc: 0.7393\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7010 - acc: 0.7812\n",
      "Epoch 00027: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7010 - acc: 0.7812 - val_loss: 0.9334 - val_acc: 0.7254\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6999 - acc: 0.7819\n",
      "Epoch 00028: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.7000 - acc: 0.7819 - val_loss: 0.8535 - val_acc: 0.7503\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6889 - acc: 0.7826\n",
      "Epoch 00029: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6889 - acc: 0.7826 - val_loss: 0.8368 - val_acc: 0.7549\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6737 - acc: 0.7877\n",
      "Epoch 00030: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6737 - acc: 0.7877 - val_loss: 0.8542 - val_acc: 0.7463\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.7896\n",
      "Epoch 00031: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6649 - acc: 0.7896 - val_loss: 0.9286 - val_acc: 0.7216\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6519 - acc: 0.7929\n",
      "Epoch 00032: val_loss did not improve from 0.80945\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6519 - acc: 0.7929 - val_loss: 0.8738 - val_acc: 0.7508\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6437 - acc: 0.7949\n",
      "Epoch 00033: val_loss improved from 0.80945 to 0.80265, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/033-0.8026.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6436 - acc: 0.7950 - val_loss: 0.8026 - val_acc: 0.7673\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.7966\n",
      "Epoch 00034: val_loss did not improve from 0.80265\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6372 - acc: 0.7966 - val_loss: 0.8836 - val_acc: 0.7552\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6346 - acc: 0.8011\n",
      "Epoch 00035: val_loss did not improve from 0.80265\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6348 - acc: 0.8011 - val_loss: 0.8467 - val_acc: 0.7543\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.8025\n",
      "Epoch 00036: val_loss did not improve from 0.80265\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6259 - acc: 0.8024 - val_loss: 0.9080 - val_acc: 0.7515\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.8034\n",
      "Epoch 00037: val_loss improved from 0.80265 to 0.79944, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/037-0.7994.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6205 - acc: 0.8034 - val_loss: 0.7994 - val_acc: 0.7680\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8061\n",
      "Epoch 00038: val_loss did not improve from 0.79944\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6059 - acc: 0.8061 - val_loss: 0.8270 - val_acc: 0.7710\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.8079\n",
      "Epoch 00039: val_loss did not improve from 0.79944\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6030 - acc: 0.8079 - val_loss: 0.9031 - val_acc: 0.7382\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6016 - acc: 0.8084\n",
      "Epoch 00040: val_loss improved from 0.79944 to 0.77447, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/040-0.7745.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.6016 - acc: 0.8084 - val_loss: 0.7745 - val_acc: 0.7803\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5823 - acc: 0.8125\n",
      "Epoch 00041: val_loss did not improve from 0.77447\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5823 - acc: 0.8125 - val_loss: 0.8086 - val_acc: 0.7794\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8149\n",
      "Epoch 00042: val_loss did not improve from 0.77447\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5806 - acc: 0.8149 - val_loss: 0.8726 - val_acc: 0.7554\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8148\n",
      "Epoch 00043: val_loss did not improve from 0.77447\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5797 - acc: 0.8148 - val_loss: 0.7908 - val_acc: 0.7773\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.8204\n",
      "Epoch 00044: val_loss did not improve from 0.77447\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5640 - acc: 0.8204 - val_loss: 0.8131 - val_acc: 0.7708\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.8161\n",
      "Epoch 00045: val_loss improved from 0.77447 to 0.77355, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/045-0.7736.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5693 - acc: 0.8161 - val_loss: 0.7736 - val_acc: 0.7803\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.8198\n",
      "Epoch 00046: val_loss did not improve from 0.77355\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5611 - acc: 0.8198 - val_loss: 0.7975 - val_acc: 0.7761\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8224\n",
      "Epoch 00047: val_loss improved from 0.77355 to 0.74635, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/047-0.7464.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5572 - acc: 0.8223 - val_loss: 0.7464 - val_acc: 0.7939\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.8238\n",
      "Epoch 00048: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5495 - acc: 0.8238 - val_loss: 0.7842 - val_acc: 0.7838\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8250\n",
      "Epoch 00049: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5454 - acc: 0.8250 - val_loss: 0.8363 - val_acc: 0.7654\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.8198\n",
      "Epoch 00050: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5540 - acc: 0.8198 - val_loss: 0.8384 - val_acc: 0.7601\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5310 - acc: 0.8295\n",
      "Epoch 00051: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5312 - acc: 0.8295 - val_loss: 0.7955 - val_acc: 0.7745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8288\n",
      "Epoch 00052: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5276 - acc: 0.8289 - val_loss: 0.7853 - val_acc: 0.7794\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.8327\n",
      "Epoch 00053: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5219 - acc: 0.8326 - val_loss: 0.7715 - val_acc: 0.7885\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.8329\n",
      "Epoch 00054: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5240 - acc: 0.8329 - val_loss: 0.7815 - val_acc: 0.7796\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8370\n",
      "Epoch 00055: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5041 - acc: 0.8370 - val_loss: 0.8118 - val_acc: 0.7789\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5193 - acc: 0.8342\n",
      "Epoch 00056: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5194 - acc: 0.8342 - val_loss: 0.8430 - val_acc: 0.7675\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8370\n",
      "Epoch 00057: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5031 - acc: 0.8370 - val_loss: 0.8093 - val_acc: 0.7720\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8318\n",
      "Epoch 00058: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.5156 - acc: 0.8318 - val_loss: 0.8614 - val_acc: 0.7615\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4976 - acc: 0.8403\n",
      "Epoch 00059: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4977 - acc: 0.8403 - val_loss: 0.7607 - val_acc: 0.7885\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8391\n",
      "Epoch 00060: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4954 - acc: 0.8391 - val_loss: 0.8086 - val_acc: 0.7689\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4935 - acc: 0.8389\n",
      "Epoch 00061: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4935 - acc: 0.8389 - val_loss: 0.7816 - val_acc: 0.7873\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8428\n",
      "Epoch 00062: val_loss did not improve from 0.74635\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4872 - acc: 0.8428 - val_loss: 0.8149 - val_acc: 0.7701\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8430\n",
      "Epoch 00063: val_loss improved from 0.74635 to 0.73006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/063-0.7301.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4858 - acc: 0.8430 - val_loss: 0.7301 - val_acc: 0.7990\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.8411\n",
      "Epoch 00064: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4841 - acc: 0.8411 - val_loss: 0.7893 - val_acc: 0.7836\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4790 - acc: 0.8438\n",
      "Epoch 00065: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4791 - acc: 0.8437 - val_loss: 0.7533 - val_acc: 0.7966\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8431\n",
      "Epoch 00066: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4817 - acc: 0.8431 - val_loss: 0.7502 - val_acc: 0.7873\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8488\n",
      "Epoch 00067: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4679 - acc: 0.8488 - val_loss: 0.7780 - val_acc: 0.7820\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.8494\n",
      "Epoch 00068: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4640 - acc: 0.8494 - val_loss: 0.8176 - val_acc: 0.7743\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8456\n",
      "Epoch 00069: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4724 - acc: 0.8456 - val_loss: 0.7764 - val_acc: 0.7892\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.8507\n",
      "Epoch 00070: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4574 - acc: 0.8507 - val_loss: 0.7479 - val_acc: 0.7915\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8474\n",
      "Epoch 00071: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4673 - acc: 0.8474 - val_loss: 0.8383 - val_acc: 0.7650\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4718 - acc: 0.8473\n",
      "Epoch 00072: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4717 - acc: 0.8473 - val_loss: 0.7366 - val_acc: 0.7918\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8523\n",
      "Epoch 00073: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4606 - acc: 0.8523 - val_loss: 0.7429 - val_acc: 0.8008\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8523\n",
      "Epoch 00074: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4521 - acc: 0.8523 - val_loss: 0.7460 - val_acc: 0.7920\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8560\n",
      "Epoch 00075: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4385 - acc: 0.8559 - val_loss: 0.7780 - val_acc: 0.7943\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.8541\n",
      "Epoch 00076: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4540 - acc: 0.8541 - val_loss: 0.7315 - val_acc: 0.7978\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8578\n",
      "Epoch 00077: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4389 - acc: 0.8578 - val_loss: 0.7882 - val_acc: 0.7855\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8526\n",
      "Epoch 00078: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4465 - acc: 0.8525 - val_loss: 0.7766 - val_acc: 0.7876\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8583\n",
      "Epoch 00079: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4419 - acc: 0.8583 - val_loss: 0.7600 - val_acc: 0.7936\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8557\n",
      "Epoch 00080: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4381 - acc: 0.8558 - val_loss: 0.7808 - val_acc: 0.7857\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4359 - acc: 0.8568\n",
      "Epoch 00081: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4359 - acc: 0.8568 - val_loss: 0.7389 - val_acc: 0.8013\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8592\n",
      "Epoch 00082: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4305 - acc: 0.8591 - val_loss: 0.7858 - val_acc: 0.7876\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8573\n",
      "Epoch 00083: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4303 - acc: 0.8574 - val_loss: 0.7979 - val_acc: 0.7827\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8606\n",
      "Epoch 00084: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4311 - acc: 0.8606 - val_loss: 0.7690 - val_acc: 0.7885\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8586\n",
      "Epoch 00085: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4328 - acc: 0.8586 - val_loss: 0.7670 - val_acc: 0.7929\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8630\n",
      "Epoch 00086: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4220 - acc: 0.8630 - val_loss: 0.7537 - val_acc: 0.7948\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8612\n",
      "Epoch 00087: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4244 - acc: 0.8612 - val_loss: 0.7559 - val_acc: 0.7929\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8619\n",
      "Epoch 00088: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4221 - acc: 0.8618 - val_loss: 0.7980 - val_acc: 0.7920\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8619\n",
      "Epoch 00089: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4213 - acc: 0.8619 - val_loss: 0.8098 - val_acc: 0.7894\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8618\n",
      "Epoch 00090: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4233 - acc: 0.8618 - val_loss: 0.8104 - val_acc: 0.7904\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8634\n",
      "Epoch 00091: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4128 - acc: 0.8634 - val_loss: 0.7546 - val_acc: 0.7939\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8652\n",
      "Epoch 00092: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4086 - acc: 0.8652 - val_loss: 0.7576 - val_acc: 0.8001\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8655\n",
      "Epoch 00093: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4103 - acc: 0.8656 - val_loss: 0.7564 - val_acc: 0.7978\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8667\n",
      "Epoch 00094: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4071 - acc: 0.8668 - val_loss: 0.8131 - val_acc: 0.7887\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8659\n",
      "Epoch 00095: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4064 - acc: 0.8659 - val_loss: 0.7658 - val_acc: 0.8032\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8679\n",
      "Epoch 00096: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4050 - acc: 0.8679 - val_loss: 0.7520 - val_acc: 0.7990\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8692\n",
      "Epoch 00097: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3943 - acc: 0.8692 - val_loss: 0.8394 - val_acc: 0.7706\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8680\n",
      "Epoch 00098: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4003 - acc: 0.8680 - val_loss: 0.7684 - val_acc: 0.7943\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8652\n",
      "Epoch 00099: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4025 - acc: 0.8652 - val_loss: 0.7398 - val_acc: 0.7983\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8663\n",
      "Epoch 00100: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.4015 - acc: 0.8663 - val_loss: 0.7389 - val_acc: 0.8069\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8680\n",
      "Epoch 00101: val_loss did not improve from 0.73006\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3951 - acc: 0.8680 - val_loss: 0.7319 - val_acc: 0.8034\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8707\n",
      "Epoch 00102: val_loss improved from 0.73006 to 0.71562, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv_checkpoint/102-0.7156.hdf5\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3958 - acc: 0.8707 - val_loss: 0.7156 - val_acc: 0.8076\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8671\n",
      "Epoch 00103: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3994 - acc: 0.8672 - val_loss: 0.7414 - val_acc: 0.8085\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8668\n",
      "Epoch 00104: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3954 - acc: 0.8668 - val_loss: 0.8126 - val_acc: 0.7859\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8717\n",
      "Epoch 00105: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3910 - acc: 0.8716 - val_loss: 0.8338 - val_acc: 0.7754\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8707\n",
      "Epoch 00106: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3906 - acc: 0.8706 - val_loss: 0.7515 - val_acc: 0.8036\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8696\n",
      "Epoch 00107: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3950 - acc: 0.8696 - val_loss: 0.7309 - val_acc: 0.8071\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8714\n",
      "Epoch 00108: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3852 - acc: 0.8713 - val_loss: 0.8205 - val_acc: 0.7841\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8707\n",
      "Epoch 00109: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3886 - acc: 0.8707 - val_loss: 0.7402 - val_acc: 0.8060\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3832 - acc: 0.8726\n",
      "Epoch 00110: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3835 - acc: 0.8725 - val_loss: 0.7390 - val_acc: 0.8043\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8729\n",
      "Epoch 00111: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3848 - acc: 0.8730 - val_loss: 0.7480 - val_acc: 0.8043\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8763\n",
      "Epoch 00112: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3689 - acc: 0.8763 - val_loss: 0.8011 - val_acc: 0.7883\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8745\n",
      "Epoch 00113: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3783 - acc: 0.8745 - val_loss: 0.7779 - val_acc: 0.7941\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8745\n",
      "Epoch 00114: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3787 - acc: 0.8745 - val_loss: 0.7645 - val_acc: 0.7999\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8755\n",
      "Epoch 00115: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3739 - acc: 0.8754 - val_loss: 0.7773 - val_acc: 0.8001\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8774\n",
      "Epoch 00116: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3724 - acc: 0.8774 - val_loss: 0.8064 - val_acc: 0.7873\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8761\n",
      "Epoch 00117: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3747 - acc: 0.8760 - val_loss: 0.8144 - val_acc: 0.7873\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8786\n",
      "Epoch 00118: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3716 - acc: 0.8785 - val_loss: 0.8580 - val_acc: 0.7810\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8742\n",
      "Epoch 00119: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3761 - acc: 0.8742 - val_loss: 0.7404 - val_acc: 0.8039\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8765\n",
      "Epoch 00120: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3744 - acc: 0.8765 - val_loss: 0.7706 - val_acc: 0.8029\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8776\n",
      "Epoch 00121: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3700 - acc: 0.8776 - val_loss: 0.7884 - val_acc: 0.7987\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8777\n",
      "Epoch 00122: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3708 - acc: 0.8777 - val_loss: 0.7345 - val_acc: 0.8099\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8787\n",
      "Epoch 00123: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3679 - acc: 0.8787 - val_loss: 0.7970 - val_acc: 0.7959\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8778\n",
      "Epoch 00124: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3678 - acc: 0.8778 - val_loss: 0.7405 - val_acc: 0.8092\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8781\n",
      "Epoch 00125: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3654 - acc: 0.8781 - val_loss: 0.8023 - val_acc: 0.8001\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3607 - acc: 0.8801\n",
      "Epoch 00126: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3607 - acc: 0.8801 - val_loss: 0.7572 - val_acc: 0.8057\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8793\n",
      "Epoch 00127: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3610 - acc: 0.8793 - val_loss: 0.8081 - val_acc: 0.7948\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8809\n",
      "Epoch 00128: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3576 - acc: 0.8809 - val_loss: 0.7559 - val_acc: 0.8069\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8805\n",
      "Epoch 00129: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3552 - acc: 0.8805 - val_loss: 0.8294 - val_acc: 0.7883\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8785\n",
      "Epoch 00130: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3632 - acc: 0.8784 - val_loss: 0.8052 - val_acc: 0.7913\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8780\n",
      "Epoch 00131: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3655 - acc: 0.8780 - val_loss: 0.7696 - val_acc: 0.7999\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8814\n",
      "Epoch 00132: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3542 - acc: 0.8813 - val_loss: 0.7488 - val_acc: 0.8099\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8814\n",
      "Epoch 00133: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3509 - acc: 0.8814 - val_loss: 0.7390 - val_acc: 0.8062\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8794\n",
      "Epoch 00134: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3576 - acc: 0.8794 - val_loss: 0.7271 - val_acc: 0.8104\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3485 - acc: 0.8843\n",
      "Epoch 00135: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3485 - acc: 0.8843 - val_loss: 0.7371 - val_acc: 0.8118\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8830\n",
      "Epoch 00136: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3512 - acc: 0.8830 - val_loss: 0.7502 - val_acc: 0.8074\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8829\n",
      "Epoch 00137: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3539 - acc: 0.8829 - val_loss: 0.7441 - val_acc: 0.8092\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8852\n",
      "Epoch 00138: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3480 - acc: 0.8853 - val_loss: 0.8768 - val_acc: 0.7738\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8820\n",
      "Epoch 00139: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3497 - acc: 0.8819 - val_loss: 0.7432 - val_acc: 0.8048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8849\n",
      "Epoch 00140: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3491 - acc: 0.8849 - val_loss: 0.7887 - val_acc: 0.7985\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8833\n",
      "Epoch 00141: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3501 - acc: 0.8833 - val_loss: 0.8124 - val_acc: 0.7934\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8846\n",
      "Epoch 00142: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3492 - acc: 0.8846 - val_loss: 0.7681 - val_acc: 0.8046\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8865\n",
      "Epoch 00143: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3448 - acc: 0.8865 - val_loss: 0.7619 - val_acc: 0.8053\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8855\n",
      "Epoch 00144: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3409 - acc: 0.8855 - val_loss: 0.7771 - val_acc: 0.8053\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8845\n",
      "Epoch 00145: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3463 - acc: 0.8845 - val_loss: 0.7404 - val_acc: 0.8055\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8875\n",
      "Epoch 00146: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3403 - acc: 0.8874 - val_loss: 0.7582 - val_acc: 0.8171\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8865\n",
      "Epoch 00147: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3444 - acc: 0.8865 - val_loss: 0.8382 - val_acc: 0.7841\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8845\n",
      "Epoch 00148: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3433 - acc: 0.8844 - val_loss: 0.7936 - val_acc: 0.8004\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8870\n",
      "Epoch 00149: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3413 - acc: 0.8870 - val_loss: 0.7658 - val_acc: 0.8053\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8855\n",
      "Epoch 00150: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3397 - acc: 0.8855 - val_loss: 0.7508 - val_acc: 0.8123\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8877\n",
      "Epoch 00151: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3361 - acc: 0.8877 - val_loss: 0.7617 - val_acc: 0.8043\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8878\n",
      "Epoch 00152: val_loss did not improve from 0.71562\n",
      "36805/36805 [==============================] - 78s 2ms/sample - loss: 0.3363 - acc: 0.8878 - val_loss: 0.8004 - val_acc: 0.8004\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 827us/sample - loss: 0.7974 - acc: 0.7861\n",
      "Loss: 0.7974302536973329 Accuracy: 0.7860851\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3064 - acc: 0.1971\n",
      "Epoch 00001: val_loss improved from inf to 2.19655, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/001-2.1965.hdf5\n",
      "36805/36805 [==============================] - 96s 3ms/sample - loss: 3.3063 - acc: 0.1971 - val_loss: 2.1965 - val_acc: 0.3019\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3237 - acc: 0.3376\n",
      "Epoch 00002: val_loss improved from 2.19655 to 1.53590, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/002-1.5359.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 2.3237 - acc: 0.3376 - val_loss: 1.5359 - val_acc: 0.5302\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9775 - acc: 0.4225\n",
      "Epoch 00003: val_loss improved from 1.53590 to 1.31435, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/003-1.3144.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.9774 - acc: 0.4225 - val_loss: 1.3144 - val_acc: 0.6101\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7482 - acc: 0.4822\n",
      "Epoch 00004: val_loss improved from 1.31435 to 1.22438, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/004-1.2244.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.7486 - acc: 0.4822 - val_loss: 1.2244 - val_acc: 0.6145\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5893 - acc: 0.5245\n",
      "Epoch 00005: val_loss improved from 1.22438 to 1.17470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/005-1.1747.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.5892 - acc: 0.5245 - val_loss: 1.1747 - val_acc: 0.6422\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4597 - acc: 0.5621\n",
      "Epoch 00006: val_loss did not improve from 1.17470\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.4596 - acc: 0.5621 - val_loss: 1.1764 - val_acc: 0.6445\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3622 - acc: 0.5873\n",
      "Epoch 00007: val_loss improved from 1.17470 to 1.12689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/007-1.1269.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.3622 - acc: 0.5873 - val_loss: 1.1269 - val_acc: 0.6497\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2883 - acc: 0.6099\n",
      "Epoch 00008: val_loss improved from 1.12689 to 1.11582, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/008-1.1158.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.2888 - acc: 0.6099 - val_loss: 1.1158 - val_acc: 0.6590\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2013 - acc: 0.6333\n",
      "Epoch 00009: val_loss improved from 1.11582 to 1.02522, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/009-1.0252.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.2013 - acc: 0.6333 - val_loss: 1.0252 - val_acc: 0.6867\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1413 - acc: 0.6489\n",
      "Epoch 00010: val_loss did not improve from 1.02522\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.1421 - acc: 0.6488 - val_loss: 1.2463 - val_acc: 0.6157\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0826 - acc: 0.6627\n",
      "Epoch 00011: val_loss did not improve from 1.02522\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.0828 - acc: 0.6626 - val_loss: 1.1224 - val_acc: 0.6473\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0504 - acc: 0.6791\n",
      "Epoch 00012: val_loss did not improve from 1.02522\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.0505 - acc: 0.6791 - val_loss: 1.1055 - val_acc: 0.6590\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0086 - acc: 0.6938\n",
      "Epoch 00013: val_loss improved from 1.02522 to 0.97933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/013-0.9793.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 1.0086 - acc: 0.6938 - val_loss: 0.9793 - val_acc: 0.6949\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9716 - acc: 0.7022\n",
      "Epoch 00014: val_loss improved from 0.97933 to 0.88286, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/014-0.8829.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.9717 - acc: 0.7022 - val_loss: 0.8829 - val_acc: 0.7403\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9422 - acc: 0.7124\n",
      "Epoch 00015: val_loss improved from 0.88286 to 0.82810, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/015-0.8281.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.9421 - acc: 0.7124 - val_loss: 0.8281 - val_acc: 0.7540\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9178 - acc: 0.7196\n",
      "Epoch 00016: val_loss improved from 0.82810 to 0.81119, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/016-0.8112.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.9178 - acc: 0.7195 - val_loss: 0.8112 - val_acc: 0.7547\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8948 - acc: 0.7221\n",
      "Epoch 00017: val_loss improved from 0.81119 to 0.80312, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/017-0.8031.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8949 - acc: 0.7221 - val_loss: 0.8031 - val_acc: 0.7689\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8706 - acc: 0.7333\n",
      "Epoch 00018: val_loss improved from 0.80312 to 0.76617, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/018-0.7662.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8706 - acc: 0.7334 - val_loss: 0.7662 - val_acc: 0.7803\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8631 - acc: 0.7355\n",
      "Epoch 00019: val_loss did not improve from 0.76617\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8631 - acc: 0.7355 - val_loss: 0.9087 - val_acc: 0.7230\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.7449\n",
      "Epoch 00020: val_loss did not improve from 0.76617\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8312 - acc: 0.7449 - val_loss: 0.8289 - val_acc: 0.7536\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8167 - acc: 0.7491\n",
      "Epoch 00021: val_loss did not improve from 0.76617\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8167 - acc: 0.7491 - val_loss: 0.7836 - val_acc: 0.7710\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8092 - acc: 0.7513\n",
      "Epoch 00022: val_loss did not improve from 0.76617\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.8093 - acc: 0.7513 - val_loss: 0.8233 - val_acc: 0.7575\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.7574\n",
      "Epoch 00023: val_loss improved from 0.76617 to 0.75078, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/023-0.7508.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7925 - acc: 0.7574 - val_loss: 0.7508 - val_acc: 0.7773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7618\n",
      "Epoch 00024: val_loss improved from 0.75078 to 0.73736, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/024-0.7374.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7823 - acc: 0.7617 - val_loss: 0.7374 - val_acc: 0.7815\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.7633\n",
      "Epoch 00025: val_loss did not improve from 0.73736\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7682 - acc: 0.7632 - val_loss: 0.7880 - val_acc: 0.7610\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7568 - acc: 0.7671\n",
      "Epoch 00026: val_loss improved from 0.73736 to 0.72961, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/026-0.7296.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7568 - acc: 0.7671 - val_loss: 0.7296 - val_acc: 0.7871\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7699\n",
      "Epoch 00027: val_loss did not improve from 0.72961\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7487 - acc: 0.7698 - val_loss: 0.7652 - val_acc: 0.7796\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7764\n",
      "Epoch 00028: val_loss did not improve from 0.72961\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7273 - acc: 0.7764 - val_loss: 0.7827 - val_acc: 0.7827\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7270 - acc: 0.7770\n",
      "Epoch 00029: val_loss did not improve from 0.72961\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7271 - acc: 0.7770 - val_loss: 0.8207 - val_acc: 0.7498\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7149 - acc: 0.7803\n",
      "Epoch 00030: val_loss did not improve from 0.72961\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7149 - acc: 0.7803 - val_loss: 0.8074 - val_acc: 0.7643\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.7827\n",
      "Epoch 00031: val_loss improved from 0.72961 to 0.70226, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/031-0.7023.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7075 - acc: 0.7827 - val_loss: 0.7023 - val_acc: 0.7862\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7021 - acc: 0.7848\n",
      "Epoch 00032: val_loss did not improve from 0.70226\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.7021 - acc: 0.7848 - val_loss: 0.7802 - val_acc: 0.7706\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6902 - acc: 0.7879\n",
      "Epoch 00033: val_loss did not improve from 0.70226\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6903 - acc: 0.7879 - val_loss: 0.7124 - val_acc: 0.7901\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6796 - acc: 0.7926\n",
      "Epoch 00034: val_loss did not improve from 0.70226\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6796 - acc: 0.7926 - val_loss: 0.8447 - val_acc: 0.7454\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.7929\n",
      "Epoch 00035: val_loss improved from 0.70226 to 0.66580, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/035-0.6658.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6766 - acc: 0.7928 - val_loss: 0.6658 - val_acc: 0.8148\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.7928\n",
      "Epoch 00036: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6737 - acc: 0.7928 - val_loss: 0.7643 - val_acc: 0.7741\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6677 - acc: 0.7937\n",
      "Epoch 00037: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6676 - acc: 0.7938 - val_loss: 0.6846 - val_acc: 0.8060\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.7989\n",
      "Epoch 00038: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6535 - acc: 0.7990 - val_loss: 1.0357 - val_acc: 0.6890\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.8024\n",
      "Epoch 00039: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6460 - acc: 0.8024 - val_loss: 0.7021 - val_acc: 0.8025\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6449 - acc: 0.8020\n",
      "Epoch 00040: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6449 - acc: 0.8020 - val_loss: 0.7762 - val_acc: 0.7724\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.8042\n",
      "Epoch 00041: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6375 - acc: 0.8042 - val_loss: 0.7178 - val_acc: 0.7897\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.8041\n",
      "Epoch 00042: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6340 - acc: 0.8041 - val_loss: 0.8036 - val_acc: 0.7713\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.8075\n",
      "Epoch 00043: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6277 - acc: 0.8075 - val_loss: 0.7268 - val_acc: 0.7920\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6228 - acc: 0.8094\n",
      "Epoch 00044: val_loss did not improve from 0.66580\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6229 - acc: 0.8094 - val_loss: 0.7160 - val_acc: 0.7890\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8105\n",
      "Epoch 00045: val_loss improved from 0.66580 to 0.64211, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/045-0.6421.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6172 - acc: 0.8105 - val_loss: 0.6421 - val_acc: 0.8132\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.8113\n",
      "Epoch 00046: val_loss did not improve from 0.64211\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6173 - acc: 0.8113 - val_loss: 0.7085 - val_acc: 0.7969\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.8129\n",
      "Epoch 00047: val_loss improved from 0.64211 to 0.63102, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/047-0.6310.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6094 - acc: 0.8129 - val_loss: 0.6310 - val_acc: 0.8232\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6047 - acc: 0.8139\n",
      "Epoch 00048: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.6048 - acc: 0.8139 - val_loss: 0.8241 - val_acc: 0.7561\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5974 - acc: 0.8148\n",
      "Epoch 00049: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5977 - acc: 0.8147 - val_loss: 0.7117 - val_acc: 0.7834\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.8164\n",
      "Epoch 00050: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5902 - acc: 0.8164 - val_loss: 0.7302 - val_acc: 0.7876\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8144\n",
      "Epoch 00051: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5921 - acc: 0.8144 - val_loss: 0.6881 - val_acc: 0.7943\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8175\n",
      "Epoch 00052: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5795 - acc: 0.8174 - val_loss: 0.6609 - val_acc: 0.8099\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8218\n",
      "Epoch 00053: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5789 - acc: 0.8218 - val_loss: 0.6985 - val_acc: 0.7990\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5721 - acc: 0.8219\n",
      "Epoch 00054: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5723 - acc: 0.8218 - val_loss: 0.6700 - val_acc: 0.8067\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8230\n",
      "Epoch 00055: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5644 - acc: 0.8230 - val_loss: 0.6657 - val_acc: 0.8104\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5684 - acc: 0.8222\n",
      "Epoch 00056: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5687 - acc: 0.8222 - val_loss: 0.6873 - val_acc: 0.7992\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5657 - acc: 0.8249\n",
      "Epoch 00057: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5659 - acc: 0.8248 - val_loss: 0.8042 - val_acc: 0.7624\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.8249\n",
      "Epoch 00058: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5654 - acc: 0.8249 - val_loss: 0.6560 - val_acc: 0.8123\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5625 - acc: 0.8264\n",
      "Epoch 00059: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5626 - acc: 0.8263 - val_loss: 0.7220 - val_acc: 0.7964\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5535 - acc: 0.8286\n",
      "Epoch 00060: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5535 - acc: 0.8286 - val_loss: 0.6856 - val_acc: 0.8055\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8297\n",
      "Epoch 00061: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5493 - acc: 0.8296 - val_loss: 0.6318 - val_acc: 0.8171\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5414 - acc: 0.8327\n",
      "Epoch 00062: val_loss did not improve from 0.63102\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5415 - acc: 0.8327 - val_loss: 0.6468 - val_acc: 0.8220\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.8287\n",
      "Epoch 00063: val_loss improved from 0.63102 to 0.62681, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/063-0.6268.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5427 - acc: 0.8286 - val_loss: 0.6268 - val_acc: 0.8218\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.8292\n",
      "Epoch 00064: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5439 - acc: 0.8292 - val_loss: 0.6659 - val_acc: 0.8064\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8298\n",
      "Epoch 00065: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5454 - acc: 0.8298 - val_loss: 0.6450 - val_acc: 0.8164\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8306\n",
      "Epoch 00066: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5375 - acc: 0.8306 - val_loss: 0.7045 - val_acc: 0.8039\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8325\n",
      "Epoch 00067: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5327 - acc: 0.8324 - val_loss: 0.7834 - val_acc: 0.7685\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.8316\n",
      "Epoch 00068: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5330 - acc: 0.8316 - val_loss: 0.6470 - val_acc: 0.8227\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8324\n",
      "Epoch 00069: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5263 - acc: 0.8325 - val_loss: 0.6970 - val_acc: 0.8050\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5212 - acc: 0.8369\n",
      "Epoch 00070: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5212 - acc: 0.8370 - val_loss: 0.6364 - val_acc: 0.8190\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8371\n",
      "Epoch 00071: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5202 - acc: 0.8370 - val_loss: 0.7215 - val_acc: 0.8027\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5160 - acc: 0.8352\n",
      "Epoch 00072: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5161 - acc: 0.8351 - val_loss: 0.7368 - val_acc: 0.7850\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8386\n",
      "Epoch 00073: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5182 - acc: 0.8385 - val_loss: 0.6296 - val_acc: 0.8188\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8400\n",
      "Epoch 00074: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5168 - acc: 0.8400 - val_loss: 0.7169 - val_acc: 0.8013\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8409\n",
      "Epoch 00075: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5078 - acc: 0.8409 - val_loss: 0.7181 - val_acc: 0.7964\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8378\n",
      "Epoch 00076: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5104 - acc: 0.8378 - val_loss: 0.6479 - val_acc: 0.8150\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8408\n",
      "Epoch 00077: val_loss did not improve from 0.62681\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5066 - acc: 0.8408 - val_loss: 0.6574 - val_acc: 0.8076\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8410\n",
      "Epoch 00078: val_loss improved from 0.62681 to 0.60850, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv_checkpoint/078-0.6085.hdf5\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.5010 - acc: 0.8409 - val_loss: 0.6085 - val_acc: 0.8239\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8441\n",
      "Epoch 00079: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4984 - acc: 0.8441 - val_loss: 0.6092 - val_acc: 0.8307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8432\n",
      "Epoch 00080: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4984 - acc: 0.8432 - val_loss: 0.6998 - val_acc: 0.8032\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4948 - acc: 0.8432\n",
      "Epoch 00081: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4947 - acc: 0.8433 - val_loss: 0.6641 - val_acc: 0.8164\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8421\n",
      "Epoch 00082: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4961 - acc: 0.8421 - val_loss: 0.6840 - val_acc: 0.8062\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8428\n",
      "Epoch 00083: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4909 - acc: 0.8428 - val_loss: 0.6273 - val_acc: 0.8244\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.8449\n",
      "Epoch 00084: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4864 - acc: 0.8448 - val_loss: 0.6526 - val_acc: 0.8169\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8474\n",
      "Epoch 00085: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4849 - acc: 0.8474 - val_loss: 0.7101 - val_acc: 0.7987\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8496\n",
      "Epoch 00086: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4750 - acc: 0.8496 - val_loss: 0.6333 - val_acc: 0.8192\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.8451\n",
      "Epoch 00087: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4881 - acc: 0.8450 - val_loss: 0.6963 - val_acc: 0.8067\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4759 - acc: 0.8495\n",
      "Epoch 00088: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4758 - acc: 0.8496 - val_loss: 0.6956 - val_acc: 0.7978\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4719 - acc: 0.8495\n",
      "Epoch 00089: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4718 - acc: 0.8495 - val_loss: 0.7326 - val_acc: 0.8034\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4716 - acc: 0.8494\n",
      "Epoch 00090: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4716 - acc: 0.8494 - val_loss: 0.6728 - val_acc: 0.8123\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8507\n",
      "Epoch 00091: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4670 - acc: 0.8506 - val_loss: 0.6817 - val_acc: 0.8169\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8495\n",
      "Epoch 00092: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4738 - acc: 0.8494 - val_loss: 0.6500 - val_acc: 0.8197\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4682 - acc: 0.8524\n",
      "Epoch 00093: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4682 - acc: 0.8524 - val_loss: 0.6420 - val_acc: 0.8162\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8535\n",
      "Epoch 00094: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4614 - acc: 0.8536 - val_loss: 0.6590 - val_acc: 0.8120\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4629 - acc: 0.8512\n",
      "Epoch 00095: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4631 - acc: 0.8512 - val_loss: 0.6413 - val_acc: 0.8239\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8512\n",
      "Epoch 00096: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4616 - acc: 0.8512 - val_loss: 0.6951 - val_acc: 0.8123\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.8547\n",
      "Epoch 00097: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4539 - acc: 0.8547 - val_loss: 0.7190 - val_acc: 0.8032\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8540\n",
      "Epoch 00098: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4562 - acc: 0.8540 - val_loss: 0.7642 - val_acc: 0.7862\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8530\n",
      "Epoch 00099: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4590 - acc: 0.8530 - val_loss: 0.6435 - val_acc: 0.8195\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4576 - acc: 0.8543\n",
      "Epoch 00100: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4576 - acc: 0.8543 - val_loss: 0.6715 - val_acc: 0.8090\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.8562\n",
      "Epoch 00101: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4559 - acc: 0.8562 - val_loss: 0.7030 - val_acc: 0.8060\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8537\n",
      "Epoch 00102: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4556 - acc: 0.8537 - val_loss: 0.6709 - val_acc: 0.8120\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8578\n",
      "Epoch 00103: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4495 - acc: 0.8578 - val_loss: 0.6589 - val_acc: 0.8185\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8603\n",
      "Epoch 00104: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4410 - acc: 0.8603 - val_loss: 0.6341 - val_acc: 0.8283\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8563\n",
      "Epoch 00105: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4416 - acc: 0.8564 - val_loss: 0.8097 - val_acc: 0.7761\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4345 - acc: 0.8611\n",
      "Epoch 00106: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4344 - acc: 0.8611 - val_loss: 0.6926 - val_acc: 0.8039\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8580\n",
      "Epoch 00107: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4371 - acc: 0.8580 - val_loss: 0.6604 - val_acc: 0.8237\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8580\n",
      "Epoch 00108: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4427 - acc: 0.8580 - val_loss: 0.7010 - val_acc: 0.8132\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8607\n",
      "Epoch 00109: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4363 - acc: 0.8606 - val_loss: 0.7088 - val_acc: 0.8102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.8610\n",
      "Epoch 00110: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4378 - acc: 0.8610 - val_loss: 0.6386 - val_acc: 0.8237\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8617\n",
      "Epoch 00111: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4339 - acc: 0.8617 - val_loss: 0.7629 - val_acc: 0.7894\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.8585\n",
      "Epoch 00112: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4408 - acc: 0.8584 - val_loss: 0.6401 - val_acc: 0.8279\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4329 - acc: 0.8608\n",
      "Epoch 00113: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4332 - acc: 0.8607 - val_loss: 0.7934 - val_acc: 0.7792\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8593\n",
      "Epoch 00114: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4401 - acc: 0.8593 - val_loss: 0.6989 - val_acc: 0.8109\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8643\n",
      "Epoch 00115: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4270 - acc: 0.8644 - val_loss: 0.6698 - val_acc: 0.8164\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8647\n",
      "Epoch 00116: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4219 - acc: 0.8646 - val_loss: 0.7505 - val_acc: 0.8043\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8640\n",
      "Epoch 00117: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4219 - acc: 0.8640 - val_loss: 0.6638 - val_acc: 0.8190\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8666\n",
      "Epoch 00118: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4187 - acc: 0.8666 - val_loss: 0.6454 - val_acc: 0.8220\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8647\n",
      "Epoch 00119: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4188 - acc: 0.8647 - val_loss: 0.6914 - val_acc: 0.8074\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8635\n",
      "Epoch 00120: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4222 - acc: 0.8635 - val_loss: 0.7778 - val_acc: 0.7843\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4196 - acc: 0.8635\n",
      "Epoch 00121: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4196 - acc: 0.8635 - val_loss: 0.7086 - val_acc: 0.8018\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8642\n",
      "Epoch 00122: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4219 - acc: 0.8641 - val_loss: 0.7346 - val_acc: 0.8006\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4142 - acc: 0.8666\n",
      "Epoch 00123: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4141 - acc: 0.8666 - val_loss: 0.6738 - val_acc: 0.8248\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8701\n",
      "Epoch 00124: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4090 - acc: 0.8700 - val_loss: 0.6313 - val_acc: 0.8223\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8676\n",
      "Epoch 00125: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4141 - acc: 0.8676 - val_loss: 0.7389 - val_acc: 0.7929\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8662\n",
      "Epoch 00126: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4108 - acc: 0.8662 - val_loss: 0.6568 - val_acc: 0.8241\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8696\n",
      "Epoch 00127: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4051 - acc: 0.8696 - val_loss: 0.6605 - val_acc: 0.8202\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8678\n",
      "Epoch 00128: val_loss did not improve from 0.60850\n",
      "36805/36805 [==============================] - 80s 2ms/sample - loss: 0.4078 - acc: 0.8678 - val_loss: 0.6867 - val_acc: 0.8137\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 905us/sample - loss: 0.7005 - acc: 0.7952\n",
      "Loss: 0.7004882454376974 Accuracy: 0.79522324\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4200 - acc: 0.1475\n",
      "Epoch 00001: val_loss improved from inf to 2.30190, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/001-2.3019.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 3.4201 - acc: 0.1475 - val_loss: 2.3019 - val_acc: 0.2972\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5483 - acc: 0.2719\n",
      "Epoch 00002: val_loss improved from 2.30190 to 1.74669, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/002-1.7467.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 2.5483 - acc: 0.2719 - val_loss: 1.7467 - val_acc: 0.4645\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2027 - acc: 0.3530\n",
      "Epoch 00003: val_loss improved from 1.74669 to 1.59269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/003-1.5927.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.2026 - acc: 0.3530 - val_loss: 1.5927 - val_acc: 0.5092\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9328 - acc: 0.4227\n",
      "Epoch 00004: val_loss improved from 1.59269 to 1.33765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/004-1.3376.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.9327 - acc: 0.4227 - val_loss: 1.3376 - val_acc: 0.6045\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7539 - acc: 0.4679\n",
      "Epoch 00005: val_loss improved from 1.33765 to 1.22169, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/005-1.2217.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.7541 - acc: 0.4678 - val_loss: 1.2217 - val_acc: 0.6245\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5966 - acc: 0.5146\n",
      "Epoch 00006: val_loss improved from 1.22169 to 1.19049, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/006-1.1905.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.5969 - acc: 0.5146 - val_loss: 1.1905 - val_acc: 0.6362\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4720 - acc: 0.5468\n",
      "Epoch 00007: val_loss improved from 1.19049 to 1.06385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/007-1.0639.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.4720 - acc: 0.5469 - val_loss: 1.0639 - val_acc: 0.6797\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3699 - acc: 0.5776\n",
      "Epoch 00008: val_loss improved from 1.06385 to 1.02895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/008-1.0290.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.3700 - acc: 0.5776 - val_loss: 1.0290 - val_acc: 0.6883\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2922 - acc: 0.6012\n",
      "Epoch 00009: val_loss improved from 1.02895 to 1.02230, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/009-1.0223.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.2921 - acc: 0.6011 - val_loss: 1.0223 - val_acc: 0.6860\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2194 - acc: 0.6249\n",
      "Epoch 00010: val_loss improved from 1.02230 to 0.92776, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/010-0.9278.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.2194 - acc: 0.6249 - val_loss: 0.9278 - val_acc: 0.7177\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1647 - acc: 0.6398\n",
      "Epoch 00011: val_loss did not improve from 0.92776\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1647 - acc: 0.6398 - val_loss: 0.9641 - val_acc: 0.7072\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1054 - acc: 0.6625\n",
      "Epoch 00012: val_loss improved from 0.92776 to 0.87490, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/012-0.8749.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1056 - acc: 0.6625 - val_loss: 0.8749 - val_acc: 0.7396\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0649 - acc: 0.6747\n",
      "Epoch 00013: val_loss did not improve from 0.87490\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0649 - acc: 0.6747 - val_loss: 0.9509 - val_acc: 0.7249\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0283 - acc: 0.6873\n",
      "Epoch 00014: val_loss did not improve from 0.87490\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.0284 - acc: 0.6872 - val_loss: 0.9227 - val_acc: 0.7212\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9917 - acc: 0.6968\n",
      "Epoch 00015: val_loss did not improve from 0.87490\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9916 - acc: 0.6969 - val_loss: 0.8831 - val_acc: 0.7496\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9609 - acc: 0.7053\n",
      "Epoch 00016: val_loss improved from 0.87490 to 0.80239, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/016-0.8024.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9609 - acc: 0.7053 - val_loss: 0.8024 - val_acc: 0.7596\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9341 - acc: 0.7145\n",
      "Epoch 00017: val_loss did not improve from 0.80239\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9341 - acc: 0.7145 - val_loss: 0.8520 - val_acc: 0.7405\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9073 - acc: 0.7240\n",
      "Epoch 00018: val_loss improved from 0.80239 to 0.78479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/018-0.7848.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9073 - acc: 0.7241 - val_loss: 0.7848 - val_acc: 0.7745\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8825 - acc: 0.7308\n",
      "Epoch 00019: val_loss did not improve from 0.78479\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8827 - acc: 0.7308 - val_loss: 0.8222 - val_acc: 0.7561\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8708 - acc: 0.7348\n",
      "Epoch 00020: val_loss improved from 0.78479 to 0.73558, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/020-0.7356.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8708 - acc: 0.7348 - val_loss: 0.7356 - val_acc: 0.7911\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7412\n",
      "Epoch 00021: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8454 - acc: 0.7412 - val_loss: 0.8386 - val_acc: 0.7468\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8278 - acc: 0.7484\n",
      "Epoch 00022: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8278 - acc: 0.7484 - val_loss: 0.7477 - val_acc: 0.7713\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8106 - acc: 0.7533\n",
      "Epoch 00023: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8106 - acc: 0.7532 - val_loss: 0.7417 - val_acc: 0.7778\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7992 - acc: 0.7537\n",
      "Epoch 00024: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7993 - acc: 0.7536 - val_loss: 0.7617 - val_acc: 0.7778\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7868 - acc: 0.7602\n",
      "Epoch 00025: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7867 - acc: 0.7603 - val_loss: 0.7854 - val_acc: 0.7724\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7654\n",
      "Epoch 00026: val_loss did not improve from 0.73558\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7713 - acc: 0.7654 - val_loss: 0.7831 - val_acc: 0.7647\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7593 - acc: 0.7688\n",
      "Epoch 00027: val_loss improved from 0.73558 to 0.73227, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/027-0.7323.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7593 - acc: 0.7688 - val_loss: 0.7323 - val_acc: 0.7848\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7755\n",
      "Epoch 00028: val_loss improved from 0.73227 to 0.68545, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/028-0.6854.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7487 - acc: 0.7754 - val_loss: 0.6854 - val_acc: 0.7913\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7347 - acc: 0.7773\n",
      "Epoch 00029: val_loss did not improve from 0.68545\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7347 - acc: 0.7773 - val_loss: 0.7361 - val_acc: 0.7866\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7806\n",
      "Epoch 00030: val_loss did not improve from 0.68545\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7239 - acc: 0.7806 - val_loss: 0.7394 - val_acc: 0.7722\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7206 - acc: 0.7823\n",
      "Epoch 00031: val_loss improved from 0.68545 to 0.67846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/031-0.6785.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7206 - acc: 0.7822 - val_loss: 0.6785 - val_acc: 0.8078\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7039 - acc: 0.7855\n",
      "Epoch 00032: val_loss improved from 0.67846 to 0.67782, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/032-0.6778.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7041 - acc: 0.7855 - val_loss: 0.6778 - val_acc: 0.8015\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7029 - acc: 0.7865\n",
      "Epoch 00033: val_loss did not improve from 0.67782\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.7032 - acc: 0.7865 - val_loss: 0.7312 - val_acc: 0.7952\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6953 - acc: 0.7890\n",
      "Epoch 00034: val_loss improved from 0.67782 to 0.62292, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/034-0.6229.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6954 - acc: 0.7889 - val_loss: 0.6229 - val_acc: 0.8204\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.7923\n",
      "Epoch 00035: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6807 - acc: 0.7923 - val_loss: 0.6927 - val_acc: 0.7906\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.7941\n",
      "Epoch 00036: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6761 - acc: 0.7940 - val_loss: 0.7747 - val_acc: 0.7824\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.7950\n",
      "Epoch 00037: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6682 - acc: 0.7951 - val_loss: 0.6886 - val_acc: 0.7997\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6565 - acc: 0.8008\n",
      "Epoch 00038: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6564 - acc: 0.8008 - val_loss: 0.7036 - val_acc: 0.7908\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8017\n",
      "Epoch 00039: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6512 - acc: 0.8016 - val_loss: 0.6410 - val_acc: 0.8164\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6393 - acc: 0.8030\n",
      "Epoch 00040: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6395 - acc: 0.8029 - val_loss: 0.6630 - val_acc: 0.8104\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.8060\n",
      "Epoch 00041: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6393 - acc: 0.8060 - val_loss: 0.6412 - val_acc: 0.8125\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6273 - acc: 0.8081\n",
      "Epoch 00042: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6273 - acc: 0.8081 - val_loss: 0.6755 - val_acc: 0.8127\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6150 - acc: 0.8130\n",
      "Epoch 00043: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6151 - acc: 0.8130 - val_loss: 0.6568 - val_acc: 0.8160\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6195 - acc: 0.8094\n",
      "Epoch 00044: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6194 - acc: 0.8094 - val_loss: 0.6892 - val_acc: 0.7866\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6164 - acc: 0.8103\n",
      "Epoch 00045: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6165 - acc: 0.8103 - val_loss: 0.6428 - val_acc: 0.8157\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6109 - acc: 0.8110\n",
      "Epoch 00046: val_loss did not improve from 0.62292\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6110 - acc: 0.8109 - val_loss: 0.6505 - val_acc: 0.8088\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.8172\n",
      "Epoch 00047: val_loss improved from 0.62292 to 0.61016, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/047-0.6102.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5982 - acc: 0.8171 - val_loss: 0.6102 - val_acc: 0.8232\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.8173\n",
      "Epoch 00048: val_loss improved from 0.61016 to 0.59449, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/048-0.5945.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5965 - acc: 0.8173 - val_loss: 0.5945 - val_acc: 0.8281\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8189\n",
      "Epoch 00049: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5906 - acc: 0.8189 - val_loss: 0.6261 - val_acc: 0.8183\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.8202\n",
      "Epoch 00050: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5874 - acc: 0.8202 - val_loss: 0.6261 - val_acc: 0.8171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8233\n",
      "Epoch 00051: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5738 - acc: 0.8233 - val_loss: 0.6186 - val_acc: 0.8162\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8245\n",
      "Epoch 00052: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5703 - acc: 0.8244 - val_loss: 0.6509 - val_acc: 0.7999\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.8246\n",
      "Epoch 00053: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5713 - acc: 0.8246 - val_loss: 0.5968 - val_acc: 0.8307\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.8273\n",
      "Epoch 00054: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5610 - acc: 0.8273 - val_loss: 0.6675 - val_acc: 0.8060\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.8259\n",
      "Epoch 00055: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5555 - acc: 0.8259 - val_loss: 0.6648 - val_acc: 0.8188\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.8290\n",
      "Epoch 00056: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5489 - acc: 0.8290 - val_loss: 0.6372 - val_acc: 0.8181\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.8294\n",
      "Epoch 00057: val_loss did not improve from 0.59449\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5516 - acc: 0.8293 - val_loss: 0.6461 - val_acc: 0.8146\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5493 - acc: 0.8323\n",
      "Epoch 00058: val_loss improved from 0.59449 to 0.58066, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/058-0.5807.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5495 - acc: 0.8323 - val_loss: 0.5807 - val_acc: 0.8393\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8316\n",
      "Epoch 00059: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5452 - acc: 0.8316 - val_loss: 0.6027 - val_acc: 0.8239\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8352\n",
      "Epoch 00060: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5303 - acc: 0.8352 - val_loss: 0.6363 - val_acc: 0.8178\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8373\n",
      "Epoch 00061: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5302 - acc: 0.8372 - val_loss: 0.6246 - val_acc: 0.8176\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8369\n",
      "Epoch 00062: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5285 - acc: 0.8368 - val_loss: 0.6125 - val_acc: 0.8290\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.8381\n",
      "Epoch 00063: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5217 - acc: 0.8381 - val_loss: 0.5899 - val_acc: 0.8383\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8393\n",
      "Epoch 00064: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5190 - acc: 0.8393 - val_loss: 0.6361 - val_acc: 0.8197\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8403\n",
      "Epoch 00065: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5088 - acc: 0.8402 - val_loss: 0.6181 - val_acc: 0.8220\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8420\n",
      "Epoch 00066: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5101 - acc: 0.8420 - val_loss: 0.6445 - val_acc: 0.8232\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8433\n",
      "Epoch 00067: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5058 - acc: 0.8433 - val_loss: 0.6006 - val_acc: 0.8309\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8437\n",
      "Epoch 00068: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5019 - acc: 0.8437 - val_loss: 0.6048 - val_acc: 0.8192\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8439\n",
      "Epoch 00069: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4977 - acc: 0.8439 - val_loss: 0.5858 - val_acc: 0.8274\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8459\n",
      "Epoch 00070: val_loss did not improve from 0.58066\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4953 - acc: 0.8459 - val_loss: 0.6860 - val_acc: 0.7983\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8485\n",
      "Epoch 00071: val_loss improved from 0.58066 to 0.56861, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/071-0.5686.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4907 - acc: 0.8484 - val_loss: 0.5686 - val_acc: 0.8393\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8492\n",
      "Epoch 00072: val_loss did not improve from 0.56861\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4824 - acc: 0.8492 - val_loss: 0.5879 - val_acc: 0.8404\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8509\n",
      "Epoch 00073: val_loss did not improve from 0.56861\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4803 - acc: 0.8508 - val_loss: 0.6594 - val_acc: 0.8157\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.8486\n",
      "Epoch 00074: val_loss did not improve from 0.56861\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4829 - acc: 0.8486 - val_loss: 0.5876 - val_acc: 0.8397\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8517\n",
      "Epoch 00075: val_loss did not improve from 0.56861\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4826 - acc: 0.8517 - val_loss: 0.5940 - val_acc: 0.8314\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8535\n",
      "Epoch 00076: val_loss did not improve from 0.56861\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4743 - acc: 0.8535 - val_loss: 0.6962 - val_acc: 0.8095\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8546\n",
      "Epoch 00077: val_loss improved from 0.56861 to 0.55738, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/077-0.5574.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4674 - acc: 0.8546 - val_loss: 0.5574 - val_acc: 0.8414\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4602 - acc: 0.8554\n",
      "Epoch 00078: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4603 - acc: 0.8553 - val_loss: 0.6142 - val_acc: 0.8262\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8563\n",
      "Epoch 00079: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4625 - acc: 0.8563 - val_loss: 0.6491 - val_acc: 0.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8583\n",
      "Epoch 00080: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4552 - acc: 0.8583 - val_loss: 0.6265 - val_acc: 0.8251\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.8562\n",
      "Epoch 00081: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4612 - acc: 0.8562 - val_loss: 0.6330 - val_acc: 0.8274\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8559\n",
      "Epoch 00082: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4596 - acc: 0.8559 - val_loss: 0.5977 - val_acc: 0.8286\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8556\n",
      "Epoch 00083: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4498 - acc: 0.8555 - val_loss: 0.5926 - val_acc: 0.8314\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4520 - acc: 0.8575\n",
      "Epoch 00084: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4520 - acc: 0.8575 - val_loss: 0.6463 - val_acc: 0.8209\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8612\n",
      "Epoch 00085: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4411 - acc: 0.8612 - val_loss: 0.6284 - val_acc: 0.8234\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8631\n",
      "Epoch 00086: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4404 - acc: 0.8631 - val_loss: 0.5710 - val_acc: 0.8442\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4327 - acc: 0.8635\n",
      "Epoch 00087: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4327 - acc: 0.8634 - val_loss: 0.5647 - val_acc: 0.8439\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8637\n",
      "Epoch 00088: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4336 - acc: 0.8636 - val_loss: 0.6288 - val_acc: 0.8230\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8619\n",
      "Epoch 00089: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4399 - acc: 0.8619 - val_loss: 0.6252 - val_acc: 0.8279\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8641\n",
      "Epoch 00090: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4281 - acc: 0.8641 - val_loss: 0.6183 - val_acc: 0.8260\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8651\n",
      "Epoch 00091: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4258 - acc: 0.8651 - val_loss: 0.7071 - val_acc: 0.8015\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8680\n",
      "Epoch 00092: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4220 - acc: 0.8680 - val_loss: 0.7253 - val_acc: 0.8004\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8674\n",
      "Epoch 00093: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4173 - acc: 0.8675 - val_loss: 0.6431 - val_acc: 0.8213\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8657\n",
      "Epoch 00094: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4203 - acc: 0.8656 - val_loss: 0.6514 - val_acc: 0.8209\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8667\n",
      "Epoch 00095: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4185 - acc: 0.8668 - val_loss: 0.5841 - val_acc: 0.8393\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8701\n",
      "Epoch 00096: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4116 - acc: 0.8700 - val_loss: 0.6600 - val_acc: 0.8185\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8706\n",
      "Epoch 00097: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4050 - acc: 0.8706 - val_loss: 0.6183 - val_acc: 0.8290\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8712\n",
      "Epoch 00098: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4029 - acc: 0.8712 - val_loss: 0.5775 - val_acc: 0.8425\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8713\n",
      "Epoch 00099: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4069 - acc: 0.8713 - val_loss: 0.5700 - val_acc: 0.8458\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8719\n",
      "Epoch 00100: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4035 - acc: 0.8719 - val_loss: 0.6273 - val_acc: 0.8346\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8739\n",
      "Epoch 00101: val_loss did not improve from 0.55738\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3997 - acc: 0.8739 - val_loss: 0.6356 - val_acc: 0.8321\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8731\n",
      "Epoch 00102: val_loss improved from 0.55738 to 0.55227, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv_checkpoint/102-0.5523.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3993 - acc: 0.8731 - val_loss: 0.5523 - val_acc: 0.8439\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8781\n",
      "Epoch 00103: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3929 - acc: 0.8780 - val_loss: 0.6257 - val_acc: 0.8258\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8701\n",
      "Epoch 00104: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4035 - acc: 0.8700 - val_loss: 0.7387 - val_acc: 0.8067\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8759\n",
      "Epoch 00105: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3904 - acc: 0.8759 - val_loss: 0.6328 - val_acc: 0.8302\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8780\n",
      "Epoch 00106: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3819 - acc: 0.8780 - val_loss: 0.6155 - val_acc: 0.8334\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8751\n",
      "Epoch 00107: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3892 - acc: 0.8750 - val_loss: 0.6549 - val_acc: 0.8251\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8769\n",
      "Epoch 00108: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3860 - acc: 0.8769 - val_loss: 0.5897 - val_acc: 0.8390\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8753\n",
      "Epoch 00109: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3814 - acc: 0.8753 - val_loss: 0.6300 - val_acc: 0.8288\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8792\n",
      "Epoch 00110: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3774 - acc: 0.8792 - val_loss: 0.5854 - val_acc: 0.8369\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8792\n",
      "Epoch 00111: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3765 - acc: 0.8792 - val_loss: 0.5852 - val_acc: 0.8355\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8810\n",
      "Epoch 00112: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3720 - acc: 0.8810 - val_loss: 0.6799 - val_acc: 0.8258\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8803\n",
      "Epoch 00113: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3761 - acc: 0.8803 - val_loss: 0.6093 - val_acc: 0.8379\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8826\n",
      "Epoch 00114: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3706 - acc: 0.8825 - val_loss: 0.6182 - val_acc: 0.8328\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8818\n",
      "Epoch 00115: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3643 - acc: 0.8818 - val_loss: 0.6147 - val_acc: 0.8341\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8825\n",
      "Epoch 00116: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3664 - acc: 0.8825 - val_loss: 0.6204 - val_acc: 0.8348\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8817\n",
      "Epoch 00117: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3694 - acc: 0.8816 - val_loss: 0.6925 - val_acc: 0.8199\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8820\n",
      "Epoch 00118: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3655 - acc: 0.8819 - val_loss: 0.5979 - val_acc: 0.8360\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8831\n",
      "Epoch 00119: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3609 - acc: 0.8831 - val_loss: 0.6020 - val_acc: 0.8360\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8841\n",
      "Epoch 00120: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3543 - acc: 0.8841 - val_loss: 0.6120 - val_acc: 0.8365\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8853\n",
      "Epoch 00121: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3517 - acc: 0.8853 - val_loss: 0.6188 - val_acc: 0.8337\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8881\n",
      "Epoch 00122: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3572 - acc: 0.8881 - val_loss: 0.6294 - val_acc: 0.8360\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8845\n",
      "Epoch 00123: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3529 - acc: 0.8845 - val_loss: 0.6155 - val_acc: 0.8397\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8863\n",
      "Epoch 00124: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3559 - acc: 0.8863 - val_loss: 0.6550 - val_acc: 0.8290\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8876\n",
      "Epoch 00125: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3567 - acc: 0.8875 - val_loss: 0.5927 - val_acc: 0.8367\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8879\n",
      "Epoch 00126: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3487 - acc: 0.8878 - val_loss: 0.6374 - val_acc: 0.8351\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8908\n",
      "Epoch 00127: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3406 - acc: 0.8908 - val_loss: 0.6112 - val_acc: 0.8416\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8902\n",
      "Epoch 00128: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3441 - acc: 0.8902 - val_loss: 0.6150 - val_acc: 0.8337\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8896\n",
      "Epoch 00129: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3406 - acc: 0.8896 - val_loss: 0.6310 - val_acc: 0.8297\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8889\n",
      "Epoch 00130: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3417 - acc: 0.8889 - val_loss: 0.6551 - val_acc: 0.8267\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8908\n",
      "Epoch 00131: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3343 - acc: 0.8909 - val_loss: 0.5992 - val_acc: 0.8442\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8898\n",
      "Epoch 00132: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3361 - acc: 0.8897 - val_loss: 0.6180 - val_acc: 0.8390\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8904\n",
      "Epoch 00133: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3366 - acc: 0.8904 - val_loss: 0.6084 - val_acc: 0.8418\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8924\n",
      "Epoch 00134: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3314 - acc: 0.8924 - val_loss: 0.6098 - val_acc: 0.8367\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8913\n",
      "Epoch 00135: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3363 - acc: 0.8912 - val_loss: 0.6059 - val_acc: 0.8423\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8954\n",
      "Epoch 00136: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3289 - acc: 0.8954 - val_loss: 0.6235 - val_acc: 0.8407\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8949\n",
      "Epoch 00137: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3213 - acc: 0.8950 - val_loss: 0.6630 - val_acc: 0.8218\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8957\n",
      "Epoch 00138: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3177 - acc: 0.8957 - val_loss: 0.5974 - val_acc: 0.8388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8949\n",
      "Epoch 00139: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3218 - acc: 0.8949 - val_loss: 0.6041 - val_acc: 0.8437\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8952\n",
      "Epoch 00140: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3232 - acc: 0.8952 - val_loss: 0.6543 - val_acc: 0.8255\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8970\n",
      "Epoch 00141: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3177 - acc: 0.8969 - val_loss: 0.6395 - val_acc: 0.8328\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8945\n",
      "Epoch 00142: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3241 - acc: 0.8945 - val_loss: 0.6247 - val_acc: 0.8348\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.8971\n",
      "Epoch 00143: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3136 - acc: 0.8971 - val_loss: 0.6316 - val_acc: 0.8390\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.8961\n",
      "Epoch 00144: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3201 - acc: 0.8960 - val_loss: 0.6587 - val_acc: 0.8311\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8971\n",
      "Epoch 00145: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3198 - acc: 0.8971 - val_loss: 0.6119 - val_acc: 0.8374\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8972\n",
      "Epoch 00146: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3091 - acc: 0.8972 - val_loss: 0.6994 - val_acc: 0.8155\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8998\n",
      "Epoch 00147: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3067 - acc: 0.8997 - val_loss: 0.7411 - val_acc: 0.8104\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.8967\n",
      "Epoch 00148: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3168 - acc: 0.8967 - val_loss: 0.6558 - val_acc: 0.8279\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9004\n",
      "Epoch 00149: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3092 - acc: 0.9004 - val_loss: 0.6270 - val_acc: 0.8365\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8980\n",
      "Epoch 00150: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3155 - acc: 0.8980 - val_loss: 0.6362 - val_acc: 0.8362\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8983\n",
      "Epoch 00151: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3090 - acc: 0.8983 - val_loss: 0.6299 - val_acc: 0.8355\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8991\n",
      "Epoch 00152: val_loss did not improve from 0.55227\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3111 - acc: 0.8991 - val_loss: 0.7111 - val_acc: 0.8260\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 954us/sample - loss: 0.6379 - acc: 0.8222\n",
      "Loss: 0.6378942030674448 Accuracy: 0.82222223\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5043 - acc: 0.1391\n",
      "Epoch 00001: val_loss improved from inf to 2.31339, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/001-2.3134.hdf5\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 3.5042 - acc: 0.1391 - val_loss: 2.3134 - val_acc: 0.2828\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6239 - acc: 0.2370\n",
      "Epoch 00002: val_loss improved from 2.31339 to 1.80423, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/002-1.8042.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.6238 - acc: 0.2370 - val_loss: 1.8042 - val_acc: 0.4689\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2762 - acc: 0.3158\n",
      "Epoch 00003: val_loss improved from 1.80423 to 1.52519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/003-1.5252.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.2762 - acc: 0.3158 - val_loss: 1.5252 - val_acc: 0.5607\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9910 - acc: 0.3849\n",
      "Epoch 00004: val_loss improved from 1.52519 to 1.34895, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/004-1.3489.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.9910 - acc: 0.3849 - val_loss: 1.3489 - val_acc: 0.6187\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7455 - acc: 0.4531\n",
      "Epoch 00005: val_loss did not improve from 1.34895\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.7455 - acc: 0.4531 - val_loss: 1.4954 - val_acc: 0.5234\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5767 - acc: 0.4989\n",
      "Epoch 00006: val_loss improved from 1.34895 to 1.15761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/006-1.1576.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.5766 - acc: 0.4989 - val_loss: 1.1576 - val_acc: 0.6448\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4291 - acc: 0.5457\n",
      "Epoch 00007: val_loss improved from 1.15761 to 0.95431, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/007-0.9543.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.4290 - acc: 0.5458 - val_loss: 0.9543 - val_acc: 0.7172\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3259 - acc: 0.5796\n",
      "Epoch 00008: val_loss did not improve from 0.95431\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.3259 - acc: 0.5796 - val_loss: 0.9879 - val_acc: 0.6918\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2217 - acc: 0.6101\n",
      "Epoch 00009: val_loss improved from 0.95431 to 0.86225, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/009-0.8623.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.2220 - acc: 0.6101 - val_loss: 0.8623 - val_acc: 0.7384\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1420 - acc: 0.6378\n",
      "Epoch 00010: val_loss improved from 0.86225 to 0.81429, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/010-0.8143.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.1420 - acc: 0.6378 - val_loss: 0.8143 - val_acc: 0.7540\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0799 - acc: 0.6587\n",
      "Epoch 00011: val_loss did not improve from 0.81429\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0799 - acc: 0.6587 - val_loss: 0.8310 - val_acc: 0.7549\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0189 - acc: 0.6811\n",
      "Epoch 00012: val_loss improved from 0.81429 to 0.79561, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/012-0.7956.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0189 - acc: 0.6811 - val_loss: 0.7956 - val_acc: 0.7608\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9500 - acc: 0.7042\n",
      "Epoch 00013: val_loss improved from 0.79561 to 0.68244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/013-0.6824.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9500 - acc: 0.7042 - val_loss: 0.6824 - val_acc: 0.8020\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9052 - acc: 0.7175\n",
      "Epoch 00014: val_loss improved from 0.68244 to 0.67169, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/014-0.6717.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9052 - acc: 0.7175 - val_loss: 0.6717 - val_acc: 0.8060\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8628 - acc: 0.7299\n",
      "Epoch 00015: val_loss did not improve from 0.67169\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8627 - acc: 0.7299 - val_loss: 0.7003 - val_acc: 0.7880\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7466\n",
      "Epoch 00016: val_loss improved from 0.67169 to 0.59919, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/016-0.5992.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8175 - acc: 0.7466 - val_loss: 0.5992 - val_acc: 0.8258\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7863 - acc: 0.7586\n",
      "Epoch 00017: val_loss improved from 0.59919 to 0.56193, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/017-0.5619.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7862 - acc: 0.7586 - val_loss: 0.5619 - val_acc: 0.8381\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7522 - acc: 0.7664\n",
      "Epoch 00018: val_loss did not improve from 0.56193\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7521 - acc: 0.7664 - val_loss: 0.5968 - val_acc: 0.8323\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7786\n",
      "Epoch 00019: val_loss improved from 0.56193 to 0.50300, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/019-0.5030.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7173 - acc: 0.7786 - val_loss: 0.5030 - val_acc: 0.8549\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6878 - acc: 0.7899\n",
      "Epoch 00020: val_loss did not improve from 0.50300\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6880 - acc: 0.7898 - val_loss: 0.5304 - val_acc: 0.8446\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6727 - acc: 0.7922\n",
      "Epoch 00021: val_loss improved from 0.50300 to 0.48714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/021-0.4871.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6728 - acc: 0.7922 - val_loss: 0.4871 - val_acc: 0.8651\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.8064\n",
      "Epoch 00022: val_loss improved from 0.48714 to 0.48380, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/022-0.4838.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6365 - acc: 0.8063 - val_loss: 0.4838 - val_acc: 0.8644\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6139 - acc: 0.8109\n",
      "Epoch 00023: val_loss did not improve from 0.48380\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6139 - acc: 0.8109 - val_loss: 0.5081 - val_acc: 0.8523\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5918 - acc: 0.8177\n",
      "Epoch 00024: val_loss did not improve from 0.48380\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5919 - acc: 0.8176 - val_loss: 0.4852 - val_acc: 0.8670\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5829 - acc: 0.8224\n",
      "Epoch 00025: val_loss improved from 0.48380 to 0.43470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/025-0.4347.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5830 - acc: 0.8224 - val_loss: 0.4347 - val_acc: 0.8765\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8299\n",
      "Epoch 00026: val_loss improved from 0.43470 to 0.41552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/026-0.4155.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5540 - acc: 0.8299 - val_loss: 0.4155 - val_acc: 0.8849\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.8340\n",
      "Epoch 00027: val_loss improved from 0.41552 to 0.39308, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/027-0.3931.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5427 - acc: 0.8340 - val_loss: 0.3931 - val_acc: 0.8924\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8407\n",
      "Epoch 00028: val_loss did not improve from 0.39308\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5228 - acc: 0.8406 - val_loss: 0.4492 - val_acc: 0.8717\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8426\n",
      "Epoch 00029: val_loss did not improve from 0.39308\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5098 - acc: 0.8426 - val_loss: 0.3956 - val_acc: 0.8919\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4939 - acc: 0.8489\n",
      "Epoch 00030: val_loss improved from 0.39308 to 0.38764, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/030-0.3876.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4939 - acc: 0.8489 - val_loss: 0.3876 - val_acc: 0.8901\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8521\n",
      "Epoch 00031: val_loss improved from 0.38764 to 0.36043, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/031-0.3604.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4818 - acc: 0.8521 - val_loss: 0.3604 - val_acc: 0.9019\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.8553\n",
      "Epoch 00032: val_loss did not improve from 0.36043\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4720 - acc: 0.8553 - val_loss: 0.3738 - val_acc: 0.8928\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8581\n",
      "Epoch 00033: val_loss improved from 0.36043 to 0.35259, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/033-0.3526.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4627 - acc: 0.8581 - val_loss: 0.3526 - val_acc: 0.9015\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8616\n",
      "Epoch 00034: val_loss did not improve from 0.35259\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4500 - acc: 0.8615 - val_loss: 0.3729 - val_acc: 0.8982\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.8659\n",
      "Epoch 00035: val_loss did not improve from 0.35259\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4446 - acc: 0.8659 - val_loss: 0.3715 - val_acc: 0.8998\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8682\n",
      "Epoch 00036: val_loss did not improve from 0.35259\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4251 - acc: 0.8681 - val_loss: 0.3614 - val_acc: 0.9005\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8721\n",
      "Epoch 00037: val_loss improved from 0.35259 to 0.33139, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/037-0.3314.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4189 - acc: 0.8720 - val_loss: 0.3314 - val_acc: 0.9099\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8740\n",
      "Epoch 00038: val_loss did not improve from 0.33139\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4069 - acc: 0.8740 - val_loss: 0.3596 - val_acc: 0.9008\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8756\n",
      "Epoch 00039: val_loss improved from 0.33139 to 0.32485, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/039-0.3248.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4053 - acc: 0.8756 - val_loss: 0.3248 - val_acc: 0.9078\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8768\n",
      "Epoch 00040: val_loss improved from 0.32485 to 0.32192, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/040-0.3219.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3981 - acc: 0.8768 - val_loss: 0.3219 - val_acc: 0.9094\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8786\n",
      "Epoch 00041: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3982 - acc: 0.8785 - val_loss: 0.3606 - val_acc: 0.9057\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8821\n",
      "Epoch 00042: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3851 - acc: 0.8821 - val_loss: 0.3698 - val_acc: 0.9026\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8815\n",
      "Epoch 00043: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3802 - acc: 0.8815 - val_loss: 0.3339 - val_acc: 0.9085\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8850\n",
      "Epoch 00044: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3727 - acc: 0.8850 - val_loss: 0.3285 - val_acc: 0.9108\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8884\n",
      "Epoch 00045: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3556 - acc: 0.8884 - val_loss: 0.3515 - val_acc: 0.9075\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8868\n",
      "Epoch 00046: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3565 - acc: 0.8868 - val_loss: 0.3949 - val_acc: 0.8889\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8879\n",
      "Epoch 00047: val_loss did not improve from 0.32192\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3566 - acc: 0.8879 - val_loss: 0.3350 - val_acc: 0.9092\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8913\n",
      "Epoch 00048: val_loss improved from 0.32192 to 0.31464, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/048-0.3146.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3513 - acc: 0.8912 - val_loss: 0.3146 - val_acc: 0.9101\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8965\n",
      "Epoch 00049: val_loss did not improve from 0.31464\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3328 - acc: 0.8965 - val_loss: 0.3187 - val_acc: 0.9182\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8946\n",
      "Epoch 00050: val_loss improved from 0.31464 to 0.29905, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/050-0.2991.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3357 - acc: 0.8946 - val_loss: 0.2991 - val_acc: 0.9208\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8967\n",
      "Epoch 00051: val_loss did not improve from 0.29905\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3325 - acc: 0.8967 - val_loss: 0.3102 - val_acc: 0.9150\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8991\n",
      "Epoch 00052: val_loss did not improve from 0.29905\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3234 - acc: 0.8991 - val_loss: 0.3117 - val_acc: 0.9133\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8978\n",
      "Epoch 00053: val_loss did not improve from 0.29905\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3205 - acc: 0.8978 - val_loss: 0.3175 - val_acc: 0.9126\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8990\n",
      "Epoch 00054: val_loss did not improve from 0.29905\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3249 - acc: 0.8991 - val_loss: 0.3330 - val_acc: 0.9129\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9007\n",
      "Epoch 00055: val_loss improved from 0.29905 to 0.29545, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/055-0.2955.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3136 - acc: 0.9007 - val_loss: 0.2955 - val_acc: 0.9222\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9035\n",
      "Epoch 00056: val_loss did not improve from 0.29545\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3067 - acc: 0.9035 - val_loss: 0.3447 - val_acc: 0.9075\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9028\n",
      "Epoch 00057: val_loss did not improve from 0.29545\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3062 - acc: 0.9028 - val_loss: 0.3759 - val_acc: 0.9068\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9031\n",
      "Epoch 00058: val_loss did not improve from 0.29545\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3153 - acc: 0.9031 - val_loss: 0.3073 - val_acc: 0.9164\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9100\n",
      "Epoch 00059: val_loss improved from 0.29545 to 0.28560, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/059-0.2856.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2886 - acc: 0.9100 - val_loss: 0.2856 - val_acc: 0.9220\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9092\n",
      "Epoch 00060: val_loss did not improve from 0.28560\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2857 - acc: 0.9091 - val_loss: 0.3042 - val_acc: 0.9164\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9082\n",
      "Epoch 00061: val_loss did not improve from 0.28560\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2882 - acc: 0.9082 - val_loss: 0.2997 - val_acc: 0.9189\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9108\n",
      "Epoch 00062: val_loss improved from 0.28560 to 0.28185, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/062-0.2818.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2824 - acc: 0.9107 - val_loss: 0.2818 - val_acc: 0.9171\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9120\n",
      "Epoch 00063: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2823 - acc: 0.9119 - val_loss: 0.2836 - val_acc: 0.9231\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9135\n",
      "Epoch 00064: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2726 - acc: 0.9135 - val_loss: 0.2997 - val_acc: 0.9126\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9127\n",
      "Epoch 00065: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2750 - acc: 0.9127 - val_loss: 0.3019 - val_acc: 0.9168\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9146\n",
      "Epoch 00066: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2672 - acc: 0.9146 - val_loss: 0.2979 - val_acc: 0.9161\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9164\n",
      "Epoch 00067: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2590 - acc: 0.9164 - val_loss: 0.2987 - val_acc: 0.9196\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9165\n",
      "Epoch 00068: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2641 - acc: 0.9165 - val_loss: 0.2959 - val_acc: 0.9145\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9171\n",
      "Epoch 00069: val_loss did not improve from 0.28185\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2605 - acc: 0.9171 - val_loss: 0.3118 - val_acc: 0.9157\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9193\n",
      "Epoch 00070: val_loss improved from 0.28185 to 0.27957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv_checkpoint/070-0.2796.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2540 - acc: 0.9193 - val_loss: 0.2796 - val_acc: 0.9222\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9177\n",
      "Epoch 00071: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2547 - acc: 0.9176 - val_loss: 0.3332 - val_acc: 0.9164\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9210\n",
      "Epoch 00072: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2488 - acc: 0.9210 - val_loss: 0.2885 - val_acc: 0.9236\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9218\n",
      "Epoch 00073: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2454 - acc: 0.9217 - val_loss: 0.2972 - val_acc: 0.9178\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9214\n",
      "Epoch 00074: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2436 - acc: 0.9215 - val_loss: 0.2963 - val_acc: 0.9180\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9221\n",
      "Epoch 00075: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2378 - acc: 0.9221 - val_loss: 0.2910 - val_acc: 0.9175\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9246\n",
      "Epoch 00076: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2345 - acc: 0.9246 - val_loss: 0.2879 - val_acc: 0.9201\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9236\n",
      "Epoch 00077: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2409 - acc: 0.9236 - val_loss: 0.2990 - val_acc: 0.9201\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9221\n",
      "Epoch 00078: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2383 - acc: 0.9220 - val_loss: 0.2937 - val_acc: 0.9208\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9257\n",
      "Epoch 00079: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2286 - acc: 0.9257 - val_loss: 0.3264 - val_acc: 0.9166\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9261\n",
      "Epoch 00080: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2276 - acc: 0.9262 - val_loss: 0.2859 - val_acc: 0.9192\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9281\n",
      "Epoch 00081: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2197 - acc: 0.9281 - val_loss: 0.2886 - val_acc: 0.9229\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9256\n",
      "Epoch 00082: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2263 - acc: 0.9256 - val_loss: 0.3385 - val_acc: 0.9164\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9306\n",
      "Epoch 00083: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2175 - acc: 0.9306 - val_loss: 0.2916 - val_acc: 0.9229\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9300\n",
      "Epoch 00084: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2145 - acc: 0.9300 - val_loss: 0.2845 - val_acc: 0.9224\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9286\n",
      "Epoch 00085: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2184 - acc: 0.9286 - val_loss: 0.3441 - val_acc: 0.9087\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9269\n",
      "Epoch 00086: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2208 - acc: 0.9269 - val_loss: 0.2893 - val_acc: 0.9262\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9309\n",
      "Epoch 00087: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2119 - acc: 0.9309 - val_loss: 0.2889 - val_acc: 0.9238\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9304\n",
      "Epoch 00088: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2133 - acc: 0.9304 - val_loss: 0.2964 - val_acc: 0.9222\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9328\n",
      "Epoch 00089: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2091 - acc: 0.9327 - val_loss: 0.3175 - val_acc: 0.9194\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9304\n",
      "Epoch 00090: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2120 - acc: 0.9304 - val_loss: 0.2995 - val_acc: 0.9173\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9373\n",
      "Epoch 00091: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1971 - acc: 0.9372 - val_loss: 0.2983 - val_acc: 0.9194\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9377\n",
      "Epoch 00092: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1901 - acc: 0.9378 - val_loss: 0.2951 - val_acc: 0.9217\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9360\n",
      "Epoch 00093: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1965 - acc: 0.9360 - val_loss: 0.3305 - val_acc: 0.9101\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9362\n",
      "Epoch 00094: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1963 - acc: 0.9361 - val_loss: 0.3299 - val_acc: 0.9122\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9315\n",
      "Epoch 00095: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2077 - acc: 0.9315 - val_loss: 0.3052 - val_acc: 0.9173\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9376\n",
      "Epoch 00096: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1890 - acc: 0.9376 - val_loss: 0.3046 - val_acc: 0.9159\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9374\n",
      "Epoch 00097: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1905 - acc: 0.9373 - val_loss: 0.3394 - val_acc: 0.9110\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9331\n",
      "Epoch 00098: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2072 - acc: 0.9331 - val_loss: 0.3851 - val_acc: 0.9089\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9383\n",
      "Epoch 00099: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1891 - acc: 0.9383 - val_loss: 0.3041 - val_acc: 0.9222\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9398\n",
      "Epoch 00100: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1824 - acc: 0.9397 - val_loss: 0.2854 - val_acc: 0.9236\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9397\n",
      "Epoch 00101: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1826 - acc: 0.9397 - val_loss: 0.3644 - val_acc: 0.9143\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9396\n",
      "Epoch 00102: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1819 - acc: 0.9396 - val_loss: 0.3271 - val_acc: 0.9164\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9400\n",
      "Epoch 00103: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1812 - acc: 0.9400 - val_loss: 0.2978 - val_acc: 0.9217\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9422\n",
      "Epoch 00104: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1771 - acc: 0.9422 - val_loss: 0.3124 - val_acc: 0.9182\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9416\n",
      "Epoch 00105: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1761 - acc: 0.9416 - val_loss: 0.3080 - val_acc: 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9414\n",
      "Epoch 00106: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1758 - acc: 0.9414 - val_loss: 0.3162 - val_acc: 0.9252\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9401\n",
      "Epoch 00107: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1810 - acc: 0.9400 - val_loss: 0.3150 - val_acc: 0.9187\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9446\n",
      "Epoch 00108: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1697 - acc: 0.9446 - val_loss: 0.3105 - val_acc: 0.9147\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9434\n",
      "Epoch 00109: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1702 - acc: 0.9434 - val_loss: 0.3027 - val_acc: 0.9201\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9421\n",
      "Epoch 00110: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1721 - acc: 0.9421 - val_loss: 0.3035 - val_acc: 0.9203\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9470\n",
      "Epoch 00111: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1642 - acc: 0.9470 - val_loss: 0.3302 - val_acc: 0.9192\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9440\n",
      "Epoch 00112: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1748 - acc: 0.9440 - val_loss: 0.3260 - val_acc: 0.9196\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9431\n",
      "Epoch 00113: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1687 - acc: 0.9430 - val_loss: 0.3110 - val_acc: 0.9206\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9429\n",
      "Epoch 00114: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1733 - acc: 0.9429 - val_loss: 0.3080 - val_acc: 0.9255\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9475\n",
      "Epoch 00115: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1578 - acc: 0.9475 - val_loss: 0.3314 - val_acc: 0.9145\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9472\n",
      "Epoch 00116: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1559 - acc: 0.9472 - val_loss: 0.3053 - val_acc: 0.9159\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9477\n",
      "Epoch 00117: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1580 - acc: 0.9477 - val_loss: 0.3485 - val_acc: 0.9133\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9478\n",
      "Epoch 00118: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1577 - acc: 0.9478 - val_loss: 0.3195 - val_acc: 0.9203\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9467\n",
      "Epoch 00119: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1582 - acc: 0.9467 - val_loss: 0.3112 - val_acc: 0.9196\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9497\n",
      "Epoch 00120: val_loss did not improve from 0.27957\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1547 - acc: 0.9497 - val_loss: 0.3263 - val_acc: 0.9201\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3485 - acc: 0.9040\n",
      "Loss: 0.3485432178555743 Accuracy: 0.9040499\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2364 - acc: 0.1989\n",
      "Epoch 00001: val_loss improved from inf to 2.22806, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/001-2.2281.hdf5\n",
      "36805/36805 [==============================] - 133s 4ms/sample - loss: 3.2367 - acc: 0.1989 - val_loss: 2.2281 - val_acc: 0.2991\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1924 - acc: 0.3592\n",
      "Epoch 00002: val_loss improved from 2.22806 to 1.31975, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/002-1.3197.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 2.1926 - acc: 0.3591 - val_loss: 1.3197 - val_acc: 0.6196\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7494 - acc: 0.4712\n",
      "Epoch 00003: val_loss improved from 1.31975 to 1.00468, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/003-1.0047.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.7495 - acc: 0.4712 - val_loss: 1.0047 - val_acc: 0.7100\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4314 - acc: 0.5646\n",
      "Epoch 00004: val_loss improved from 1.00468 to 0.82709, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/004-0.8271.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.4313 - acc: 0.5646 - val_loss: 0.8271 - val_acc: 0.7519\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1779 - acc: 0.6422\n",
      "Epoch 00005: val_loss improved from 0.82709 to 0.64883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/005-0.6488.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.1781 - acc: 0.6422 - val_loss: 0.6488 - val_acc: 0.8106\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9923 - acc: 0.6950\n",
      "Epoch 00006: val_loss improved from 0.64883 to 0.53396, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/006-0.5340.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.9923 - acc: 0.6950 - val_loss: 0.5340 - val_acc: 0.8481\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8429 - acc: 0.7404\n",
      "Epoch 00007: val_loss improved from 0.53396 to 0.47723, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/007-0.4772.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8430 - acc: 0.7404 - val_loss: 0.4772 - val_acc: 0.8642\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7289 - acc: 0.7747\n",
      "Epoch 00008: val_loss improved from 0.47723 to 0.41393, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/008-0.4139.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7290 - acc: 0.7747 - val_loss: 0.4139 - val_acc: 0.8782\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.7987\n",
      "Epoch 00009: val_loss improved from 0.41393 to 0.39747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/009-0.3975.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6532 - acc: 0.7987 - val_loss: 0.3975 - val_acc: 0.8880\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.8213\n",
      "Epoch 00010: val_loss improved from 0.39747 to 0.33862, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/010-0.3386.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5892 - acc: 0.8214 - val_loss: 0.3386 - val_acc: 0.9022\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8358\n",
      "Epoch 00011: val_loss improved from 0.33862 to 0.31496, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/011-0.3150.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5389 - acc: 0.8358 - val_loss: 0.3150 - val_acc: 0.9122\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4888 - acc: 0.8500\n",
      "Epoch 00012: val_loss improved from 0.31496 to 0.30272, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/012-0.3027.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4887 - acc: 0.8500 - val_loss: 0.3027 - val_acc: 0.9173\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4584 - acc: 0.8598\n",
      "Epoch 00013: val_loss improved from 0.30272 to 0.28263, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/013-0.2826.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4583 - acc: 0.8599 - val_loss: 0.2826 - val_acc: 0.9208\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8695\n",
      "Epoch 00014: val_loss did not improve from 0.28263\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4262 - acc: 0.8695 - val_loss: 0.2942 - val_acc: 0.9203\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8769\n",
      "Epoch 00015: val_loss improved from 0.28263 to 0.25273, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/015-0.2527.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4033 - acc: 0.8769 - val_loss: 0.2527 - val_acc: 0.9276\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8846\n",
      "Epoch 00016: val_loss did not improve from 0.25273\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3755 - acc: 0.8846 - val_loss: 0.2627 - val_acc: 0.9222\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8906\n",
      "Epoch 00017: val_loss improved from 0.25273 to 0.22807, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/017-0.2281.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3601 - acc: 0.8905 - val_loss: 0.2281 - val_acc: 0.9364\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8983\n",
      "Epoch 00018: val_loss improved from 0.22807 to 0.22542, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/018-0.2254.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3367 - acc: 0.8983 - val_loss: 0.2254 - val_acc: 0.9327\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.9033\n",
      "Epoch 00019: val_loss did not improve from 0.22542\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3202 - acc: 0.9033 - val_loss: 0.2563 - val_acc: 0.9292\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9039\n",
      "Epoch 00020: val_loss did not improve from 0.22542\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3102 - acc: 0.9038 - val_loss: 0.2732 - val_acc: 0.9241\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9095\n",
      "Epoch 00021: val_loss did not improve from 0.22542\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3011 - acc: 0.9095 - val_loss: 0.2357 - val_acc: 0.9348\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9140\n",
      "Epoch 00022: val_loss improved from 0.22542 to 0.21142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/022-0.2114.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2872 - acc: 0.9140 - val_loss: 0.2114 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9156\n",
      "Epoch 00023: val_loss improved from 0.21142 to 0.20563, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/023-0.2056.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2738 - acc: 0.9156 - val_loss: 0.2056 - val_acc: 0.9415\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9204\n",
      "Epoch 00024: val_loss did not improve from 0.20563\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2643 - acc: 0.9204 - val_loss: 0.2077 - val_acc: 0.9394\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9230\n",
      "Epoch 00025: val_loss did not improve from 0.20563\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2476 - acc: 0.9229 - val_loss: 0.2116 - val_acc: 0.9380\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9238\n",
      "Epoch 00026: val_loss improved from 0.20563 to 0.20470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/026-0.2047.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2470 - acc: 0.9237 - val_loss: 0.2047 - val_acc: 0.9420\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9278\n",
      "Epoch 00027: val_loss did not improve from 0.20470\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2347 - acc: 0.9278 - val_loss: 0.2322 - val_acc: 0.9336\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9289\n",
      "Epoch 00028: val_loss improved from 0.20470 to 0.20328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/028-0.2033.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2272 - acc: 0.9289 - val_loss: 0.2033 - val_acc: 0.9418\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9331\n",
      "Epoch 00029: val_loss improved from 0.20328 to 0.19257, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/029-0.1926.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2164 - acc: 0.9331 - val_loss: 0.1926 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9343\n",
      "Epoch 00030: val_loss improved from 0.19257 to 0.18858, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/030-0.1886.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2082 - acc: 0.9343 - val_loss: 0.1886 - val_acc: 0.9450\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9333\n",
      "Epoch 00031: val_loss did not improve from 0.18858\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2134 - acc: 0.9333 - val_loss: 0.2020 - val_acc: 0.9406\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9370\n",
      "Epoch 00032: val_loss improved from 0.18858 to 0.18670, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/032-0.1867.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2011 - acc: 0.9370 - val_loss: 0.1867 - val_acc: 0.9448\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9399\n",
      "Epoch 00033: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1893 - acc: 0.9399 - val_loss: 0.1985 - val_acc: 0.9429\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9410\n",
      "Epoch 00034: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1879 - acc: 0.9410 - val_loss: 0.1895 - val_acc: 0.9439\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9433\n",
      "Epoch 00035: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1802 - acc: 0.9433 - val_loss: 0.1953 - val_acc: 0.9455\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9440\n",
      "Epoch 00036: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1756 - acc: 0.9440 - val_loss: 0.1987 - val_acc: 0.9406\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9454\n",
      "Epoch 00037: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1703 - acc: 0.9453 - val_loss: 0.1890 - val_acc: 0.9485\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9441\n",
      "Epoch 00038: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1755 - acc: 0.9441 - val_loss: 0.1905 - val_acc: 0.9420\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9484\n",
      "Epoch 00039: val_loss did not improve from 0.18670\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1604 - acc: 0.9483 - val_loss: 0.2276 - val_acc: 0.9324\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9484\n",
      "Epoch 00040: val_loss improved from 0.18670 to 0.18250, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv_checkpoint/040-0.1825.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1632 - acc: 0.9484 - val_loss: 0.1825 - val_acc: 0.9450\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9517\n",
      "Epoch 00041: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1565 - acc: 0.9517 - val_loss: 0.2073 - val_acc: 0.9392\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9528\n",
      "Epoch 00042: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1471 - acc: 0.9528 - val_loss: 0.1984 - val_acc: 0.9446\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9540\n",
      "Epoch 00043: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1442 - acc: 0.9540 - val_loss: 0.2300 - val_acc: 0.9345\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9545\n",
      "Epoch 00044: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1421 - acc: 0.9544 - val_loss: 0.2236 - val_acc: 0.9366\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9569\n",
      "Epoch 00045: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1354 - acc: 0.9569 - val_loss: 0.1994 - val_acc: 0.9476\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9563\n",
      "Epoch 00046: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1359 - acc: 0.9562 - val_loss: 0.2128 - val_acc: 0.9415\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9596\n",
      "Epoch 00047: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1282 - acc: 0.9596 - val_loss: 0.2101 - val_acc: 0.9411\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9590\n",
      "Epoch 00048: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1280 - acc: 0.9590 - val_loss: 0.2023 - val_acc: 0.9443\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9605\n",
      "Epoch 00049: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1237 - acc: 0.9605 - val_loss: 0.2068 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9572\n",
      "Epoch 00050: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1329 - acc: 0.9572 - val_loss: 0.2125 - val_acc: 0.9443\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9615\n",
      "Epoch 00051: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1229 - acc: 0.9615 - val_loss: 0.2220 - val_acc: 0.9376\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9623\n",
      "Epoch 00052: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1176 - acc: 0.9623 - val_loss: 0.2029 - val_acc: 0.9469\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9642\n",
      "Epoch 00053: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1132 - acc: 0.9642 - val_loss: 0.2117 - val_acc: 0.9434\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9641\n",
      "Epoch 00054: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1126 - acc: 0.9641 - val_loss: 0.1995 - val_acc: 0.9488\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9615\n",
      "Epoch 00055: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1189 - acc: 0.9615 - val_loss: 0.1970 - val_acc: 0.9462\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9648\n",
      "Epoch 00056: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1103 - acc: 0.9648 - val_loss: 0.2283 - val_acc: 0.9401\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9684\n",
      "Epoch 00057: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0980 - acc: 0.9684 - val_loss: 0.2234 - val_acc: 0.9434\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9682\n",
      "Epoch 00058: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0997 - acc: 0.9682 - val_loss: 0.2109 - val_acc: 0.9436\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9676\n",
      "Epoch 00059: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1004 - acc: 0.9676 - val_loss: 0.2029 - val_acc: 0.9446\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9693\n",
      "Epoch 00060: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0963 - acc: 0.9693 - val_loss: 0.2223 - val_acc: 0.9413\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9676\n",
      "Epoch 00061: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1001 - acc: 0.9676 - val_loss: 0.2046 - val_acc: 0.9453\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9684\n",
      "Epoch 00062: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0990 - acc: 0.9683 - val_loss: 0.2169 - val_acc: 0.9455\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9685\n",
      "Epoch 00063: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0981 - acc: 0.9685 - val_loss: 0.2197 - val_acc: 0.9427\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9720\n",
      "Epoch 00064: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0891 - acc: 0.9720 - val_loss: 0.2284 - val_acc: 0.9390\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9710\n",
      "Epoch 00065: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0911 - acc: 0.9710 - val_loss: 0.2317 - val_acc: 0.9439\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9715\n",
      "Epoch 00066: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0892 - acc: 0.9716 - val_loss: 0.2302 - val_acc: 0.9425\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9740\n",
      "Epoch 00067: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0807 - acc: 0.9740 - val_loss: 0.2285 - val_acc: 0.9373\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9736\n",
      "Epoch 00068: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0805 - acc: 0.9736 - val_loss: 0.2259 - val_acc: 0.9474\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9733\n",
      "Epoch 00069: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0835 - acc: 0.9732 - val_loss: 0.2645 - val_acc: 0.9376\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9727\n",
      "Epoch 00070: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0854 - acc: 0.9727 - val_loss: 0.2146 - val_acc: 0.9432\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9732\n",
      "Epoch 00071: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0797 - acc: 0.9731 - val_loss: 0.2214 - val_acc: 0.9411\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9703\n",
      "Epoch 00072: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0901 - acc: 0.9703 - val_loss: 0.2372 - val_acc: 0.9406\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9723\n",
      "Epoch 00073: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0849 - acc: 0.9723 - val_loss: 0.2280 - val_acc: 0.9378\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9755\n",
      "Epoch 00074: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0758 - acc: 0.9755 - val_loss: 0.2337 - val_acc: 0.9399\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9770\n",
      "Epoch 00075: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0704 - acc: 0.9770 - val_loss: 0.2400 - val_acc: 0.9411\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9731\n",
      "Epoch 00076: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0805 - acc: 0.9731 - val_loss: 0.2218 - val_acc: 0.9446\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9761\n",
      "Epoch 00077: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0741 - acc: 0.9761 - val_loss: 0.2502 - val_acc: 0.9411\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9766\n",
      "Epoch 00078: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0736 - acc: 0.9766 - val_loss: 0.2525 - val_acc: 0.9385\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9755\n",
      "Epoch 00079: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0733 - acc: 0.9754 - val_loss: 0.2874 - val_acc: 0.9322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9774\n",
      "Epoch 00080: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0684 - acc: 0.9774 - val_loss: 0.2531 - val_acc: 0.9357\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9782\n",
      "Epoch 00081: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0671 - acc: 0.9782 - val_loss: 0.2539 - val_acc: 0.9394\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9769\n",
      "Epoch 00082: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0713 - acc: 0.9769 - val_loss: 0.2421 - val_acc: 0.9418\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9785\n",
      "Epoch 00083: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0638 - acc: 0.9784 - val_loss: 0.2476 - val_acc: 0.9413\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9780\n",
      "Epoch 00084: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0702 - acc: 0.9780 - val_loss: 0.2452 - val_acc: 0.9434\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9805\n",
      "Epoch 00085: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0611 - acc: 0.9805 - val_loss: 0.2489 - val_acc: 0.9425\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9814\n",
      "Epoch 00086: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0590 - acc: 0.9814 - val_loss: 0.2453 - val_acc: 0.9397\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9765\n",
      "Epoch 00087: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0708 - acc: 0.9766 - val_loss: 0.2531 - val_acc: 0.9373\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9822\n",
      "Epoch 00088: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0565 - acc: 0.9822 - val_loss: 0.2591 - val_acc: 0.9371\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9810\n",
      "Epoch 00089: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0567 - acc: 0.9810 - val_loss: 0.2598 - val_acc: 0.9376\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9775\n",
      "Epoch 00090: val_loss did not improve from 0.18250\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.0670 - acc: 0.9775 - val_loss: 0.2448 - val_acc: 0.9443\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2482 - acc: 0.9277\n",
      "Loss: 0.2481907156676147 Accuracy: 0.92772585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN'\n",
    "\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9613 - acc: 0.7362\n",
      "Loss: 0.9613271430769195 Accuracy: 0.7362409\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.7974 - acc: 0.7861\n",
      "Loss: 0.7974302536973329 Accuracy: 0.7860851\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.7005 - acc: 0.7952\n",
      "Loss: 0.7004882454376974 Accuracy: 0.79522324\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6379 - acc: 0.8222\n",
      "Loss: 0.6378942030674448 Accuracy: 0.82222223\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3485 - acc: 0.9040\n",
      "Loss: 0.3485432178555743 Accuracy: 0.9040499\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2482 - acc: 0.9277\n",
      "Loss: 0.2481907156676147 Accuracy: 0.92772585\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 326,000\n",
      "Trainable params: 325,488\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.0836 - acc: 0.7279\n",
      "Loss: 1.083635373996921 Accuracy: 0.7279335\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,696\n",
      "Trainable params: 242,928\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8934 - acc: 0.7672\n",
      "Loss: 0.8933985202359509 Accuracy: 0.76718587\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 133,744\n",
      "Trainable params: 132,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8034 - acc: 0.7734\n",
      "Loss: 0.8034019653299516 Accuracy: 0.7734164\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 113,904\n",
      "Trainable params: 112,624\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.7984 - acc: 0.7877\n",
      "Loss: 0.7983788866491704 Accuracy: 0.7877466\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 124,784\n",
      "Trainable params: 123,248\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.4257 - acc: 0.8918\n",
      "Loss: 0.4257411071970703 Accuracy: 0.89179647\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 196,720\n",
      "Trainable params: 194,672\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2999 - acc: 0.9273\n",
      "Loss: 0.29992394235201747 Accuracy: 0.92731047\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
