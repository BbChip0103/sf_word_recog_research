{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_BN(conv_num=1):\n",
    "    init_channel = 128\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048000)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 2048000)           8192000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                32768016  \n",
      "=================================================================\n",
      "Total params: 40,961,296\n",
      "Trainable params: 36,865,040\n",
      "Non-trainable params: 4,096,256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 682624)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 682624)            2730496   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                10922000  \n",
      "=================================================================\n",
      "Total params: 13,736,336\n",
      "Trainable params: 12,370,576\n",
      "Non-trainable params: 1,365,760\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 227456)            909824    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 4,715,536\n",
      "Trainable params: 4,259,856\n",
      "Non-trainable params: 455,680\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 75776)             303104    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,764,496\n",
      "Trainable params: 1,611,920\n",
      "Non-trainable params: 152,576\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 12608)             50432     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 542,416\n",
      "Trainable params: 516,048\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 394,256\n",
      "Trainable params: 384,656\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 358,736\n",
      "Trainable params: 354,640\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 361,616\n",
      "Trainable params: 359,184\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,336\n",
      "Trainable params: 362,608\n",
      "Non-trainable params: 1,728\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7658 - acc: 0.4550\n",
      "Epoch 00001: val_loss improved from inf to 1.47387, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/001-1.4739.hdf5\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 1.7658 - acc: 0.4550 - val_loss: 1.4739 - val_acc: 0.5539\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0918 - acc: 0.6686\n",
      "Epoch 00002: val_loss improved from 1.47387 to 1.00929, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/002-1.0093.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 1.0918 - acc: 0.6686 - val_loss: 1.0093 - val_acc: 0.6956\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9041 - acc: 0.7330\n",
      "Epoch 00003: val_loss improved from 1.00929 to 0.83211, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/003-0.8321.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.9040 - acc: 0.7330 - val_loss: 0.8321 - val_acc: 0.7619\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7703\n",
      "Epoch 00004: val_loss did not improve from 0.83211\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.7821 - acc: 0.7703 - val_loss: 0.8670 - val_acc: 0.7452\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7100 - acc: 0.7946\n",
      "Epoch 00005: val_loss improved from 0.83211 to 0.71023, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/005-0.7102.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.7104 - acc: 0.7945 - val_loss: 0.7102 - val_acc: 0.7957\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.8133\n",
      "Epoch 00006: val_loss did not improve from 0.71023\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.6480 - acc: 0.8133 - val_loss: 0.8037 - val_acc: 0.7577\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5869 - acc: 0.8305\n",
      "Epoch 00007: val_loss improved from 0.71023 to 0.66131, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/007-0.6613.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.5870 - acc: 0.8305 - val_loss: 0.6613 - val_acc: 0.8132\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8434\n",
      "Epoch 00008: val_loss improved from 0.66131 to 0.64604, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/008-0.6460.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.5377 - acc: 0.8434 - val_loss: 0.6460 - val_acc: 0.8167\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.8585\n",
      "Epoch 00009: val_loss did not improve from 0.64604\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4923 - acc: 0.8585 - val_loss: 0.6533 - val_acc: 0.8141\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.8697\n",
      "Epoch 00010: val_loss did not improve from 0.64604\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4552 - acc: 0.8696 - val_loss: 0.6578 - val_acc: 0.8053\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4250 - acc: 0.8797\n",
      "Epoch 00011: val_loss improved from 0.64604 to 0.57490, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/011-0.5749.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.4250 - acc: 0.8797 - val_loss: 0.5749 - val_acc: 0.8379\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.8885\n",
      "Epoch 00012: val_loss did not improve from 0.57490\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3914 - acc: 0.8885 - val_loss: 0.5759 - val_acc: 0.8383\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8967\n",
      "Epoch 00013: val_loss improved from 0.57490 to 0.56767, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/013-0.5677.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3580 - acc: 0.8967 - val_loss: 0.5677 - val_acc: 0.8390\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.9071\n",
      "Epoch 00014: val_loss improved from 0.56767 to 0.56168, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/014-0.5617.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3295 - acc: 0.9071 - val_loss: 0.5617 - val_acc: 0.8372\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9123\n",
      "Epoch 00015: val_loss did not improve from 0.56168\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.3085 - acc: 0.9122 - val_loss: 0.5885 - val_acc: 0.8439\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9156\n",
      "Epoch 00016: val_loss improved from 0.56168 to 0.54766, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/016-0.5477.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2964 - acc: 0.9156 - val_loss: 0.5477 - val_acc: 0.8502\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9278\n",
      "Epoch 00017: val_loss improved from 0.54766 to 0.54065, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/017-0.5407.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2579 - acc: 0.9278 - val_loss: 0.5407 - val_acc: 0.8514\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9323\n",
      "Epoch 00018: val_loss did not improve from 0.54065\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2407 - acc: 0.9322 - val_loss: 0.5456 - val_acc: 0.8544\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9356\n",
      "Epoch 00019: val_loss did not improve from 0.54065\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2302 - acc: 0.9356 - val_loss: 0.5632 - val_acc: 0.8528\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9430\n",
      "Epoch 00020: val_loss did not improve from 0.54065\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.2080 - acc: 0.9429 - val_loss: 0.6089 - val_acc: 0.8302\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9468\n",
      "Epoch 00021: val_loss did not improve from 0.54065\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1933 - acc: 0.9467 - val_loss: 0.6755 - val_acc: 0.8199\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9489\n",
      "Epoch 00022: val_loss improved from 0.54065 to 0.51580, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/022-0.5158.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1882 - acc: 0.9489 - val_loss: 0.5158 - val_acc: 0.8574\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9584\n",
      "Epoch 00023: val_loss did not improve from 0.51580\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1607 - acc: 0.9583 - val_loss: 0.5654 - val_acc: 0.8521\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9543\n",
      "Epoch 00024: val_loss did not improve from 0.51580\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1686 - acc: 0.9543 - val_loss: 0.5693 - val_acc: 0.8463\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9625\n",
      "Epoch 00025: val_loss did not improve from 0.51580\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1445 - acc: 0.9625 - val_loss: 0.5301 - val_acc: 0.8553\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9654\n",
      "Epoch 00026: val_loss improved from 0.51580 to 0.49584, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/026-0.4958.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1361 - acc: 0.9654 - val_loss: 0.4958 - val_acc: 0.8668\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9718\n",
      "Epoch 00027: val_loss did not improve from 0.49584\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1169 - acc: 0.9718 - val_loss: 0.5337 - val_acc: 0.8602\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9710\n",
      "Epoch 00028: val_loss did not improve from 0.49584\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1157 - acc: 0.9709 - val_loss: 0.6421 - val_acc: 0.8360\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9715\n",
      "Epoch 00029: val_loss did not improve from 0.49584\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.1126 - acc: 0.9716 - val_loss: 0.5077 - val_acc: 0.8684\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9780\n",
      "Epoch 00030: val_loss improved from 0.49584 to 0.49326, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_6_conv_checkpoint/030-0.4933.hdf5\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0963 - acc: 0.9780 - val_loss: 0.4933 - val_acc: 0.8737\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9790\n",
      "Epoch 00031: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0915 - acc: 0.9791 - val_loss: 0.5717 - val_acc: 0.8532\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9819\n",
      "Epoch 00032: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0868 - acc: 0.9819 - val_loss: 0.5538 - val_acc: 0.8656\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9809\n",
      "Epoch 00033: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0853 - acc: 0.9809 - val_loss: 0.5358 - val_acc: 0.8637\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9811\n",
      "Epoch 00034: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0815 - acc: 0.9811 - val_loss: 0.5512 - val_acc: 0.8546\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9851\n",
      "Epoch 00035: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0692 - acc: 0.9851 - val_loss: 0.5977 - val_acc: 0.8572\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9817\n",
      "Epoch 00036: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0803 - acc: 0.9817 - val_loss: 0.5257 - val_acc: 0.8733\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9880\n",
      "Epoch 00037: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0601 - acc: 0.9880 - val_loss: 0.5554 - val_acc: 0.8558\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9871\n",
      "Epoch 00038: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0623 - acc: 0.9871 - val_loss: 0.5530 - val_acc: 0.8614\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9835\n",
      "Epoch 00039: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0722 - acc: 0.9835 - val_loss: 0.5370 - val_acc: 0.8747\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9869\n",
      "Epoch 00040: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0607 - acc: 0.9869 - val_loss: 0.5525 - val_acc: 0.8658\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9888\n",
      "Epoch 00041: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0532 - acc: 0.9888 - val_loss: 0.5591 - val_acc: 0.8656\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9910\n",
      "Epoch 00042: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0497 - acc: 0.9909 - val_loss: 0.5634 - val_acc: 0.8682\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9825\n",
      "Epoch 00043: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0703 - acc: 0.9825 - val_loss: 0.6820 - val_acc: 0.8472\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9919\n",
      "Epoch 00044: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0439 - acc: 0.9918 - val_loss: 0.7476 - val_acc: 0.8355\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9877\n",
      "Epoch 00045: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0576 - acc: 0.9877 - val_loss: 0.5790 - val_acc: 0.8623\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9854\n",
      "Epoch 00046: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0639 - acc: 0.9854 - val_loss: 0.5232 - val_acc: 0.8800\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9924\n",
      "Epoch 00047: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0416 - acc: 0.9924 - val_loss: 0.5541 - val_acc: 0.8730\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9941\n",
      "Epoch 00048: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0366 - acc: 0.9941 - val_loss: 0.6136 - val_acc: 0.8602\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9914\n",
      "Epoch 00049: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0451 - acc: 0.9914 - val_loss: 0.5300 - val_acc: 0.8775\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0343 - acc: 0.9945 - val_loss: 0.5682 - val_acc: 0.8675\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9927\n",
      "Epoch 00051: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0385 - acc: 0.9927 - val_loss: 0.6642 - val_acc: 0.8519\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9889\n",
      "Epoch 00052: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0496 - acc: 0.9889 - val_loss: 0.6444 - val_acc: 0.8614\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9922\n",
      "Epoch 00053: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0398 - acc: 0.9922 - val_loss: 0.6013 - val_acc: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9943\n",
      "Epoch 00054: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0336 - acc: 0.9943 - val_loss: 0.5710 - val_acc: 0.8710\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9949\n",
      "Epoch 00055: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0310 - acc: 0.9948 - val_loss: 0.5689 - val_acc: 0.8684\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9874\n",
      "Epoch 00056: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0527 - acc: 0.9874 - val_loss: 0.6151 - val_acc: 0.8607\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9946\n",
      "Epoch 00057: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0309 - acc: 0.9946 - val_loss: 0.5371 - val_acc: 0.8733\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9938\n",
      "Epoch 00058: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0339 - acc: 0.9938 - val_loss: 0.6611 - val_acc: 0.8491\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9906\n",
      "Epoch 00059: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0426 - acc: 0.9906 - val_loss: 0.5846 - val_acc: 0.8705\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9961\n",
      "Epoch 00060: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0252 - acc: 0.9961 - val_loss: 0.5842 - val_acc: 0.8724\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9959\n",
      "Epoch 00061: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0252 - acc: 0.9959 - val_loss: 0.7570 - val_acc: 0.8502\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9908\n",
      "Epoch 00062: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0395 - acc: 0.9908 - val_loss: 0.6219 - val_acc: 0.8670\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9901\n",
      "Epoch 00063: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0442 - acc: 0.9901 - val_loss: 0.7189 - val_acc: 0.8549\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9889\n",
      "Epoch 00064: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0455 - acc: 0.9888 - val_loss: 0.5897 - val_acc: 0.8775\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9932\n",
      "Epoch 00065: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0347 - acc: 0.9931 - val_loss: 0.7368 - val_acc: 0.8474\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9900\n",
      "Epoch 00066: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0433 - acc: 0.9900 - val_loss: 0.5575 - val_acc: 0.8800\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9977\n",
      "Epoch 00067: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0183 - acc: 0.9977 - val_loss: 0.5960 - val_acc: 0.8737\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9973\n",
      "Epoch 00068: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0200 - acc: 0.9972 - val_loss: 0.5672 - val_acc: 0.8791\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9930\n",
      "Epoch 00069: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0341 - acc: 0.9930 - val_loss: 0.6948 - val_acc: 0.8647\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9956\n",
      "Epoch 00070: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0267 - acc: 0.9956 - val_loss: 0.6325 - val_acc: 0.8670\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9892\n",
      "Epoch 00071: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0428 - acc: 0.9892 - val_loss: 0.5899 - val_acc: 0.8749\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9974\n",
      "Epoch 00072: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0203 - acc: 0.9974 - val_loss: 0.6326 - val_acc: 0.8742\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9924\n",
      "Epoch 00073: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0353 - acc: 0.9924 - val_loss: 0.5645 - val_acc: 0.8789\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9963\n",
      "Epoch 00074: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0219 - acc: 0.9963 - val_loss: 0.5610 - val_acc: 0.8852\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9933\n",
      "Epoch 00075: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0318 - acc: 0.9932 - val_loss: 0.7892 - val_acc: 0.8274\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9905\n",
      "Epoch 00076: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0408 - acc: 0.9905 - val_loss: 0.6015 - val_acc: 0.8726\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9948\n",
      "Epoch 00077: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0262 - acc: 0.9948 - val_loss: 0.5388 - val_acc: 0.8833\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9985\n",
      "Epoch 00078: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0149 - acc: 0.9984 - val_loss: 0.6158 - val_acc: 0.8747\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9926\n",
      "Epoch 00079: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0319 - acc: 0.9926 - val_loss: 0.6411 - val_acc: 0.8644\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9946\n",
      "Epoch 00080: val_loss did not improve from 0.49326\n",
      "36805/36805 [==============================] - 184s 5ms/sample - loss: 0.0280 - acc: 0.9946 - val_loss: 0.6197 - val_acc: 0.8700\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.5478 - acc: 0.8474\n",
      "Loss: 0.5478097885816144 Accuracy: 0.847352\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9388 - acc: 0.4016\n",
      "Epoch 00001: val_loss improved from inf to 1.56341, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/001-1.5634.hdf5\n",
      "36805/36805 [==============================] - 191s 5ms/sample - loss: 1.9388 - acc: 0.4016 - val_loss: 1.5634 - val_acc: 0.5003\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1778 - acc: 0.6426\n",
      "Epoch 00002: val_loss improved from 1.56341 to 0.97998, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/002-0.9800.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 1.1777 - acc: 0.6426 - val_loss: 0.9800 - val_acc: 0.7051\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7367\n",
      "Epoch 00003: val_loss improved from 0.97998 to 0.89023, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/003-0.8902.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.8967 - acc: 0.7367 - val_loss: 0.8902 - val_acc: 0.7307\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7340 - acc: 0.7908\n",
      "Epoch 00004: val_loss improved from 0.89023 to 0.65747, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/004-0.6575.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.7342 - acc: 0.7907 - val_loss: 0.6575 - val_acc: 0.8081\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.8213\n",
      "Epoch 00005: val_loss improved from 0.65747 to 0.61944, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/005-0.6194.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.6340 - acc: 0.8213 - val_loss: 0.6194 - val_acc: 0.8127\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8468\n",
      "Epoch 00006: val_loss improved from 0.61944 to 0.55020, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/006-0.5502.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.5444 - acc: 0.8468 - val_loss: 0.5502 - val_acc: 0.8442\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8655\n",
      "Epoch 00007: val_loss improved from 0.55020 to 0.49529, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/007-0.4953.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4859 - acc: 0.8655 - val_loss: 0.4953 - val_acc: 0.8621\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8779\n",
      "Epoch 00008: val_loss improved from 0.49529 to 0.44126, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/008-0.4413.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.4325 - acc: 0.8779 - val_loss: 0.4413 - val_acc: 0.8803\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8893\n",
      "Epoch 00009: val_loss improved from 0.44126 to 0.42801, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/009-0.4280.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3922 - acc: 0.8893 - val_loss: 0.4280 - val_acc: 0.8826\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.9001\n",
      "Epoch 00010: val_loss did not improve from 0.42801\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3517 - acc: 0.9001 - val_loss: 0.4784 - val_acc: 0.8637\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.9087\n",
      "Epoch 00011: val_loss improved from 0.42801 to 0.41175, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/011-0.4117.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.3220 - acc: 0.9087 - val_loss: 0.4117 - val_acc: 0.8812\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.9154\n",
      "Epoch 00012: val_loss improved from 0.41175 to 0.36151, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/012-0.3615.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2985 - acc: 0.9153 - val_loss: 0.3615 - val_acc: 0.8973\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9220\n",
      "Epoch 00013: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2762 - acc: 0.9220 - val_loss: 0.4405 - val_acc: 0.8770\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9293\n",
      "Epoch 00014: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2546 - acc: 0.9292 - val_loss: 0.3649 - val_acc: 0.8989\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9326\n",
      "Epoch 00015: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2389 - acc: 0.9326 - val_loss: 0.3892 - val_acc: 0.8945\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9386\n",
      "Epoch 00016: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2194 - acc: 0.9386 - val_loss: 0.3733 - val_acc: 0.9001\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9431\n",
      "Epoch 00017: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2048 - acc: 0.9431 - val_loss: 0.3733 - val_acc: 0.8963\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9444\n",
      "Epoch 00018: val_loss did not improve from 0.36151\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.2003 - acc: 0.9444 - val_loss: 0.3871 - val_acc: 0.8952\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9471\n",
      "Epoch 00019: val_loss improved from 0.36151 to 0.33370, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/019-0.3337.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1868 - acc: 0.9470 - val_loss: 0.3337 - val_acc: 0.9075\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9545\n",
      "Epoch 00020: val_loss improved from 0.33370 to 0.32609, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/020-0.3261.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1668 - acc: 0.9545 - val_loss: 0.3261 - val_acc: 0.9126\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9545\n",
      "Epoch 00021: val_loss improved from 0.32609 to 0.31328, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/021-0.3133.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1635 - acc: 0.9545 - val_loss: 0.3133 - val_acc: 0.9147\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9597\n",
      "Epoch 00022: val_loss did not improve from 0.31328\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1436 - acc: 0.9597 - val_loss: 0.3419 - val_acc: 0.9050\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9630\n",
      "Epoch 00023: val_loss improved from 0.31328 to 0.29861, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/023-0.2986.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1372 - acc: 0.9630 - val_loss: 0.2986 - val_acc: 0.9185\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9666\n",
      "Epoch 00024: val_loss did not improve from 0.29861\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1255 - acc: 0.9666 - val_loss: 0.3561 - val_acc: 0.9094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9650\n",
      "Epoch 00025: val_loss did not improve from 0.29861\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1270 - acc: 0.9650 - val_loss: 0.3207 - val_acc: 0.9210\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9717\n",
      "Epoch 00026: val_loss improved from 0.29861 to 0.28922, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/026-0.2892.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1091 - acc: 0.9717 - val_loss: 0.2892 - val_acc: 0.9206\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9730\n",
      "Epoch 00027: val_loss did not improve from 0.28922\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1045 - acc: 0.9729 - val_loss: 0.3183 - val_acc: 0.9129\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9733\n",
      "Epoch 00028: val_loss improved from 0.28922 to 0.28505, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_7_conv_checkpoint/028-0.2850.hdf5\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1024 - acc: 0.9733 - val_loss: 0.2850 - val_acc: 0.9210\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9712\n",
      "Epoch 00029: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.1086 - acc: 0.9713 - val_loss: 0.3132 - val_acc: 0.9129\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9812\n",
      "Epoch 00030: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0784 - acc: 0.9812 - val_loss: 0.2999 - val_acc: 0.9192\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9818\n",
      "Epoch 00031: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0770 - acc: 0.9818 - val_loss: 0.3889 - val_acc: 0.8980\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9783\n",
      "Epoch 00032: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0835 - acc: 0.9782 - val_loss: 0.3338 - val_acc: 0.9178\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9797\n",
      "Epoch 00033: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0800 - acc: 0.9796 - val_loss: 0.3088 - val_acc: 0.9220\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9842\n",
      "Epoch 00034: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0687 - acc: 0.9842 - val_loss: 0.3226 - val_acc: 0.9227\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9879\n",
      "Epoch 00035: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0579 - acc: 0.9879 - val_loss: 0.3138 - val_acc: 0.9213\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9862\n",
      "Epoch 00036: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0593 - acc: 0.9862 - val_loss: 0.3319 - val_acc: 0.9147\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9854\n",
      "Epoch 00037: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0588 - acc: 0.9854 - val_loss: 0.3034 - val_acc: 0.9224\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9886\n",
      "Epoch 00038: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0508 - acc: 0.9886 - val_loss: 0.3615 - val_acc: 0.9099\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9834\n",
      "Epoch 00039: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0663 - acc: 0.9834 - val_loss: 0.3273 - val_acc: 0.9178\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9924\n",
      "Epoch 00040: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0416 - acc: 0.9924 - val_loss: 0.3285 - val_acc: 0.9192\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9873\n",
      "Epoch 00041: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0547 - acc: 0.9873 - val_loss: 0.3136 - val_acc: 0.9199\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9905\n",
      "Epoch 00042: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0449 - acc: 0.9904 - val_loss: 0.3298 - val_acc: 0.9124\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9905\n",
      "Epoch 00043: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0460 - acc: 0.9905 - val_loss: 0.3615 - val_acc: 0.9059\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9906\n",
      "Epoch 00044: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0427 - acc: 0.9905 - val_loss: 0.3125 - val_acc: 0.9227\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9843\n",
      "Epoch 00045: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0619 - acc: 0.9843 - val_loss: 0.3162 - val_acc: 0.9159\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9957\n",
      "Epoch 00046: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0283 - acc: 0.9957 - val_loss: 0.2977 - val_acc: 0.9227\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9945\n",
      "Epoch 00047: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0303 - acc: 0.9945 - val_loss: 0.3717 - val_acc: 0.9087\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9903\n",
      "Epoch 00048: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0424 - acc: 0.9903 - val_loss: 0.3434 - val_acc: 0.9168\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9940\n",
      "Epoch 00049: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0314 - acc: 0.9939 - val_loss: 0.3544 - val_acc: 0.9080\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9908\n",
      "Epoch 00050: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0412 - acc: 0.9908 - val_loss: 0.3009 - val_acc: 0.9245\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9966\n",
      "Epoch 00051: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0233 - acc: 0.9966 - val_loss: 0.3121 - val_acc: 0.9213\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9895\n",
      "Epoch 00052: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0413 - acc: 0.9895 - val_loss: 0.3257 - val_acc: 0.9208\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9902\n",
      "Epoch 00053: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0394 - acc: 0.9901 - val_loss: 0.3291 - val_acc: 0.9189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9916\n",
      "Epoch 00054: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0370 - acc: 0.9916 - val_loss: 0.3394 - val_acc: 0.9159\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9936\n",
      "Epoch 00055: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0310 - acc: 0.9936 - val_loss: 0.3114 - val_acc: 0.9257\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9971\n",
      "Epoch 00056: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0211 - acc: 0.9970 - val_loss: 0.4546 - val_acc: 0.8970\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9890\n",
      "Epoch 00057: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0430 - acc: 0.9890 - val_loss: 0.2994 - val_acc: 0.9276\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9976\n",
      "Epoch 00058: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0178 - acc: 0.9976 - val_loss: 0.3299 - val_acc: 0.9215\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9926\n",
      "Epoch 00059: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0313 - acc: 0.9926 - val_loss: 0.3364 - val_acc: 0.9175\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9912\n",
      "Epoch 00060: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0367 - acc: 0.9912 - val_loss: 0.3305 - val_acc: 0.9266\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9942\n",
      "Epoch 00061: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0281 - acc: 0.9942 - val_loss: 0.3238 - val_acc: 0.9238\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9915\n",
      "Epoch 00062: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0367 - acc: 0.9915 - val_loss: 0.2914 - val_acc: 0.9301\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9986\n",
      "Epoch 00063: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0139 - acc: 0.9986 - val_loss: 0.3289 - val_acc: 0.9248\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9927\n",
      "Epoch 00064: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0319 - acc: 0.9927 - val_loss: 0.3636 - val_acc: 0.9159\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9980\n",
      "Epoch 00065: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0154 - acc: 0.9980 - val_loss: 0.3427 - val_acc: 0.9236\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9936\n",
      "Epoch 00066: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0289 - acc: 0.9936 - val_loss: 0.3051 - val_acc: 0.9278\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9964\n",
      "Epoch 00067: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0206 - acc: 0.9964 - val_loss: 0.4402 - val_acc: 0.9068\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0163 - acc: 0.9973 - val_loss: 0.3400 - val_acc: 0.9210\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9943\n",
      "Epoch 00069: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0242 - acc: 0.9943 - val_loss: 0.3907 - val_acc: 0.9161\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9923\n",
      "Epoch 00070: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0301 - acc: 0.9923 - val_loss: 0.3345 - val_acc: 0.9238\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9965\n",
      "Epoch 00071: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0193 - acc: 0.9965 - val_loss: 0.3829 - val_acc: 0.9161\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9932\n",
      "Epoch 00072: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0275 - acc: 0.9932 - val_loss: 0.3144 - val_acc: 0.9248\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9982\n",
      "Epoch 00073: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0125 - acc: 0.9981 - val_loss: 0.3124 - val_acc: 0.9252\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9942\n",
      "Epoch 00074: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0251 - acc: 0.9941 - val_loss: 0.3218 - val_acc: 0.9250\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9934\n",
      "Epoch 00075: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0276 - acc: 0.9934 - val_loss: 0.3525 - val_acc: 0.9208\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9935\n",
      "Epoch 00076: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0277 - acc: 0.9935 - val_loss: 0.3093 - val_acc: 0.9280\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9931\n",
      "Epoch 00077: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0278 - acc: 0.9931 - val_loss: 0.3333 - val_acc: 0.9243\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9986\n",
      "Epoch 00078: val_loss did not improve from 0.28505\n",
      "36805/36805 [==============================] - 185s 5ms/sample - loss: 0.0107 - acc: 0.9986 - val_loss: 0.3024 - val_acc: 0.9278\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.3399 - acc: 0.9043\n",
      "Loss: 0.3399248241511708 Accuracy: 0.90425754\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9011 - acc: 0.4158\n",
      "Epoch 00001: val_loss improved from inf to 1.59456, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/001-1.5946.hdf5\n",
      "36805/36805 [==============================] - 196s 5ms/sample - loss: 1.9009 - acc: 0.4159 - val_loss: 1.5946 - val_acc: 0.5155\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.7122\n",
      "Epoch 00002: val_loss improved from 1.59456 to 0.83291, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/002-0.8329.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.9960 - acc: 0.7122 - val_loss: 0.8329 - val_acc: 0.7554\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.8065\n",
      "Epoch 00003: val_loss improved from 0.83291 to 0.59932, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/003-0.5993.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.6924 - acc: 0.8065 - val_loss: 0.5993 - val_acc: 0.8304\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.8516\n",
      "Epoch 00004: val_loss improved from 0.59932 to 0.50692, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/004-0.5069.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.5365 - acc: 0.8516 - val_loss: 0.5069 - val_acc: 0.8621\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8801\n",
      "Epoch 00005: val_loss improved from 0.50692 to 0.45338, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/005-0.4534.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.4376 - acc: 0.8801 - val_loss: 0.4534 - val_acc: 0.8770\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8972\n",
      "Epoch 00006: val_loss improved from 0.45338 to 0.32612, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/006-0.3261.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.3702 - acc: 0.8971 - val_loss: 0.3261 - val_acc: 0.9126\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9101\n",
      "Epoch 00007: val_loss improved from 0.32612 to 0.30767, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/007-0.3077.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.3225 - acc: 0.9101 - val_loss: 0.3077 - val_acc: 0.9166\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9207\n",
      "Epoch 00008: val_loss did not improve from 0.30767\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.2841 - acc: 0.9207 - val_loss: 0.3742 - val_acc: 0.8873\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9297\n",
      "Epoch 00009: val_loss improved from 0.30767 to 0.27707, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/009-0.2771.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.2523 - acc: 0.9297 - val_loss: 0.2771 - val_acc: 0.9222\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9345\n",
      "Epoch 00010: val_loss improved from 0.27707 to 0.24146, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/010-0.2415.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.2294 - acc: 0.9344 - val_loss: 0.2415 - val_acc: 0.9285\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9403\n",
      "Epoch 00011: val_loss did not improve from 0.24146\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.2092 - acc: 0.9402 - val_loss: 0.3039 - val_acc: 0.9047\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9450\n",
      "Epoch 00012: val_loss improved from 0.24146 to 0.23511, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/012-0.2351.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1951 - acc: 0.9450 - val_loss: 0.2351 - val_acc: 0.9294\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9499\n",
      "Epoch 00013: val_loss improved from 0.23511 to 0.22534, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/013-0.2253.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1764 - acc: 0.9499 - val_loss: 0.2253 - val_acc: 0.9362\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9527\n",
      "Epoch 00014: val_loss did not improve from 0.22534\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1652 - acc: 0.9527 - val_loss: 0.2389 - val_acc: 0.9257\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9584\n",
      "Epoch 00015: val_loss did not improve from 0.22534\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1476 - acc: 0.9584 - val_loss: 0.2261 - val_acc: 0.9334\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9614\n",
      "Epoch 00016: val_loss did not improve from 0.22534\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1409 - acc: 0.9614 - val_loss: 0.2402 - val_acc: 0.9301\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9661\n",
      "Epoch 00017: val_loss improved from 0.22534 to 0.19448, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/017-0.1945.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1233 - acc: 0.9661 - val_loss: 0.1945 - val_acc: 0.9420\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9676\n",
      "Epoch 00018: val_loss did not improve from 0.19448\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1172 - acc: 0.9675 - val_loss: 0.2006 - val_acc: 0.9392\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9710\n",
      "Epoch 00019: val_loss improved from 0.19448 to 0.18852, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/019-0.1885.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1075 - acc: 0.9710 - val_loss: 0.1885 - val_acc: 0.9448\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9723\n",
      "Epoch 00020: val_loss did not improve from 0.18852\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1007 - acc: 0.9722 - val_loss: 0.2444 - val_acc: 0.9259\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9707\n",
      "Epoch 00021: val_loss improved from 0.18852 to 0.18285, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/021-0.1829.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.1054 - acc: 0.9707 - val_loss: 0.1829 - val_acc: 0.9443\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9800\n",
      "Epoch 00022: val_loss improved from 0.18285 to 0.17735, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/022-0.1774.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0790 - acc: 0.9800 - val_loss: 0.1774 - val_acc: 0.9464\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9762\n",
      "Epoch 00023: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0850 - acc: 0.9762 - val_loss: 0.2173 - val_acc: 0.9327\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9832\n",
      "Epoch 00024: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0690 - acc: 0.9831 - val_loss: 0.1988 - val_acc: 0.9411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9820\n",
      "Epoch 00025: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0689 - acc: 0.9820 - val_loss: 0.2007 - val_acc: 0.9392\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9854\n",
      "Epoch 00026: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0602 - acc: 0.9853 - val_loss: 0.2570 - val_acc: 0.9278\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9803\n",
      "Epoch 00027: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0747 - acc: 0.9803 - val_loss: 0.1953 - val_acc: 0.9448\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9878\n",
      "Epoch 00028: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0513 - acc: 0.9878 - val_loss: 0.1872 - val_acc: 0.9478\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9871\n",
      "Epoch 00029: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0522 - acc: 0.9871 - val_loss: 0.2012 - val_acc: 0.9429\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9887\n",
      "Epoch 00030: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0483 - acc: 0.9886 - val_loss: 0.2098 - val_acc: 0.9390\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9892\n",
      "Epoch 00031: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0455 - acc: 0.9891 - val_loss: 0.2584 - val_acc: 0.9278\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9870\n",
      "Epoch 00032: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0501 - acc: 0.9870 - val_loss: 0.2376 - val_acc: 0.9341\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9926\n",
      "Epoch 00033: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0356 - acc: 0.9926 - val_loss: 0.1941 - val_acc: 0.9453\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9936\n",
      "Epoch 00034: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0316 - acc: 0.9936 - val_loss: 0.2233 - val_acc: 0.9413\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9926\n",
      "Epoch 00035: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0346 - acc: 0.9926 - val_loss: 0.2185 - val_acc: 0.9415\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9936\n",
      "Epoch 00036: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0312 - acc: 0.9936 - val_loss: 0.2188 - val_acc: 0.9425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9897\n",
      "Epoch 00037: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0412 - acc: 0.9897 - val_loss: 0.2732 - val_acc: 0.9315\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9898\n",
      "Epoch 00038: val_loss did not improve from 0.17735\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0402 - acc: 0.9898 - val_loss: 0.1880 - val_acc: 0.9497\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9915\n",
      "Epoch 00039: val_loss improved from 0.17735 to 0.17530, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_8_conv_checkpoint/039-0.1753.hdf5\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0357 - acc: 0.9915 - val_loss: 0.1753 - val_acc: 0.9520\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9964\n",
      "Epoch 00040: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0211 - acc: 0.9964 - val_loss: 0.2086 - val_acc: 0.9455\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9965\n",
      "Epoch 00041: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0205 - acc: 0.9965 - val_loss: 0.3518 - val_acc: 0.9092\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9890\n",
      "Epoch 00042: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0402 - acc: 0.9890 - val_loss: 0.2272 - val_acc: 0.9406\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9913\n",
      "Epoch 00043: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0367 - acc: 0.9913 - val_loss: 0.1901 - val_acc: 0.9515\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9977\n",
      "Epoch 00044: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0170 - acc: 0.9977 - val_loss: 0.2065 - val_acc: 0.9450\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9946\n",
      "Epoch 00045: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0254 - acc: 0.9946 - val_loss: 0.2028 - val_acc: 0.9467\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9932\n",
      "Epoch 00046: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0298 - acc: 0.9932 - val_loss: 0.2051 - val_acc: 0.9457\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9941\n",
      "Epoch 00047: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0247 - acc: 0.9941 - val_loss: 0.1976 - val_acc: 0.9490\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9977\n",
      "Epoch 00048: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0153 - acc: 0.9976 - val_loss: 0.2097 - val_acc: 0.9453\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0292 - acc: 0.9932 - val_loss: 0.1812 - val_acc: 0.9525\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9980\n",
      "Epoch 00050: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0138 - acc: 0.9980 - val_loss: 0.2115 - val_acc: 0.9471\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9938\n",
      "Epoch 00051: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0252 - acc: 0.9938 - val_loss: 0.1773 - val_acc: 0.9548\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9952\n",
      "Epoch 00052: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0232 - acc: 0.9951 - val_loss: 0.2152 - val_acc: 0.9478\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9895\n",
      "Epoch 00053: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0389 - acc: 0.9895 - val_loss: 0.2040 - val_acc: 0.9485\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9947\n",
      "Epoch 00054: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0223 - acc: 0.9946 - val_loss: 0.1828 - val_acc: 0.9529\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9970\n",
      "Epoch 00055: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0163 - acc: 0.9970 - val_loss: 0.1904 - val_acc: 0.9515\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9989\n",
      "Epoch 00056: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0087 - acc: 0.9989 - val_loss: 0.1956 - val_acc: 0.9506\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9987\n",
      "Epoch 00057: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0098 - acc: 0.9987 - val_loss: 0.2180 - val_acc: 0.9453\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0179 - acc: 0.9958 - val_loss: 0.2309 - val_acc: 0.9474\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9971\n",
      "Epoch 00059: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0138 - acc: 0.9970 - val_loss: 0.2037 - val_acc: 0.9490\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9927\n",
      "Epoch 00060: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0276 - acc: 0.9927 - val_loss: 0.1897 - val_acc: 0.9536\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9972\n",
      "Epoch 00061: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0135 - acc: 0.9971 - val_loss: 0.1980 - val_acc: 0.9539\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9919\n",
      "Epoch 00062: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0292 - acc: 0.9918 - val_loss: 0.1966 - val_acc: 0.9520\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9935\n",
      "Epoch 00063: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0262 - acc: 0.9935 - val_loss: 0.2152 - val_acc: 0.9497\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9963\n",
      "Epoch 00064: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0162 - acc: 0.9963 - val_loss: 0.1946 - val_acc: 0.9506\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9990\n",
      "Epoch 00065: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0080 - acc: 0.9990 - val_loss: 0.1916 - val_acc: 0.9513\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9989\n",
      "Epoch 00066: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0076 - acc: 0.9989 - val_loss: 0.2159 - val_acc: 0.9483\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9989\n",
      "Epoch 00067: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0081 - acc: 0.9989 - val_loss: 0.2050 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9952\n",
      "Epoch 00068: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0199 - acc: 0.9952 - val_loss: 0.2143 - val_acc: 0.9511\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9985\n",
      "Epoch 00069: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0081 - acc: 0.9985 - val_loss: 0.2472 - val_acc: 0.9420\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0136 - acc: 0.9970 - val_loss: 0.2980 - val_acc: 0.9378\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9979\n",
      "Epoch 00071: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0107 - acc: 0.9979 - val_loss: 0.2103 - val_acc: 0.9522\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9971\n",
      "Epoch 00072: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0123 - acc: 0.9971 - val_loss: 0.2610 - val_acc: 0.9390\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9932\n",
      "Epoch 00073: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0254 - acc: 0.9931 - val_loss: 0.2012 - val_acc: 0.9534\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 00074: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0122 - acc: 0.9973 - val_loss: 0.1966 - val_acc: 0.9509\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9969\n",
      "Epoch 00075: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0131 - acc: 0.9969 - val_loss: 0.2118 - val_acc: 0.9534\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9993\n",
      "Epoch 00076: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0052 - acc: 0.9993 - val_loss: 0.1951 - val_acc: 0.9534\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9990\n",
      "Epoch 00077: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0067 - acc: 0.9989 - val_loss: 0.2534 - val_acc: 0.9387\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9893\n",
      "Epoch 00078: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0342 - acc: 0.9893 - val_loss: 0.2110 - val_acc: 0.9481\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9958\n",
      "Epoch 00079: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0148 - acc: 0.9958 - val_loss: 0.1755 - val_acc: 0.9564\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9991\n",
      "Epoch 00080: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0063 - acc: 0.9991 - val_loss: 0.2138 - val_acc: 0.9481\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9996\n",
      "Epoch 00081: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0047 - acc: 0.9996 - val_loss: 0.2113 - val_acc: 0.9504\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9987\n",
      "Epoch 00082: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.2116 - val_acc: 0.9478\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 00083: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0217 - acc: 0.9936 - val_loss: 0.2025 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9984\n",
      "Epoch 00084: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0084 - acc: 0.9984 - val_loss: 0.1850 - val_acc: 0.9555\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9992\n",
      "Epoch 00085: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0054 - acc: 0.9992 - val_loss: 0.1992 - val_acc: 0.9532\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 00086: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0133 - acc: 0.9966 - val_loss: 0.2151 - val_acc: 0.9509\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9971\n",
      "Epoch 00087: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0117 - acc: 0.9971 - val_loss: 0.1761 - val_acc: 0.9571\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9992\n",
      "Epoch 00088: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0057 - acc: 0.9992 - val_loss: 0.1770 - val_acc: 0.9564\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9956\n",
      "Epoch 00089: val_loss did not improve from 0.17530\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0167 - acc: 0.9955 - val_loss: 0.2225 - val_acc: 0.9504\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2236 - acc: 0.9360\n",
      "Loss: 0.22356352788760778 Accuracy: 0.93603325\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7370 - acc: 0.4733\n",
      "Epoch 00001: val_loss improved from inf to 1.46946, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/001-1.4695.hdf5\n",
      "36805/36805 [==============================] - 201s 5ms/sample - loss: 1.7369 - acc: 0.4733 - val_loss: 1.4695 - val_acc: 0.5833\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9273 - acc: 0.7398\n",
      "Epoch 00002: val_loss improved from 1.46946 to 0.73683, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/002-0.7368.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.9273 - acc: 0.7398 - val_loss: 0.7368 - val_acc: 0.8067\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.8254\n",
      "Epoch 00003: val_loss improved from 0.73683 to 0.57657, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/003-0.5766.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.6494 - acc: 0.8254 - val_loss: 0.5766 - val_acc: 0.8393\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.8651\n",
      "Epoch 00004: val_loss did not improve from 0.57657\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.5048 - acc: 0.8650 - val_loss: 0.6265 - val_acc: 0.8123\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8901\n",
      "Epoch 00005: val_loss improved from 0.57657 to 0.44372, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/005-0.4437.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.4118 - acc: 0.8901 - val_loss: 0.4437 - val_acc: 0.8696\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.9069\n",
      "Epoch 00006: val_loss improved from 0.44372 to 0.32711, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/006-0.3271.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.3463 - acc: 0.9069 - val_loss: 0.3271 - val_acc: 0.9147\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9193\n",
      "Epoch 00007: val_loss improved from 0.32711 to 0.27260, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/007-0.2726.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.3018 - acc: 0.9193 - val_loss: 0.2726 - val_acc: 0.9206\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9282\n",
      "Epoch 00008: val_loss improved from 0.27260 to 0.26630, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/008-0.2663.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.2658 - acc: 0.9282 - val_loss: 0.2663 - val_acc: 0.9192\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9326\n",
      "Epoch 00009: val_loss did not improve from 0.26630\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.2404 - acc: 0.9326 - val_loss: 0.3659 - val_acc: 0.8868\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9419\n",
      "Epoch 00010: val_loss improved from 0.26630 to 0.24028, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/010-0.2403.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.2131 - acc: 0.9419 - val_loss: 0.2403 - val_acc: 0.9304\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9504\n",
      "Epoch 00011: val_loss did not improve from 0.24028\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1885 - acc: 0.9503 - val_loss: 0.2795 - val_acc: 0.9189\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9501\n",
      "Epoch 00012: val_loss improved from 0.24028 to 0.20782, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/012-0.2078.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1782 - acc: 0.9501 - val_loss: 0.2078 - val_acc: 0.9345\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9573\n",
      "Epoch 00013: val_loss improved from 0.20782 to 0.20434, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/013-0.2043.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1567 - acc: 0.9572 - val_loss: 0.2043 - val_acc: 0.9387\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9593\n",
      "Epoch 00014: val_loss did not improve from 0.20434\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1478 - acc: 0.9593 - val_loss: 0.2258 - val_acc: 0.9324\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9642\n",
      "Epoch 00015: val_loss improved from 0.20434 to 0.20025, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/015-0.2003.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1302 - acc: 0.9642 - val_loss: 0.2003 - val_acc: 0.9378\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9676\n",
      "Epoch 00016: val_loss improved from 0.20025 to 0.19393, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/016-0.1939.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1219 - acc: 0.9676 - val_loss: 0.1939 - val_acc: 0.9439\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9678\n",
      "Epoch 00017: val_loss did not improve from 0.19393\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.1157 - acc: 0.9678 - val_loss: 0.2014 - val_acc: 0.9364\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9736\n",
      "Epoch 00018: val_loss did not improve from 0.19393\n",
      "36805/36805 [==============================] - 188s 5ms/sample - loss: 0.0990 - acc: 0.9736 - val_loss: 0.2074 - val_acc: 0.9392\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9730\n",
      "Epoch 00019: val_loss did not improve from 0.19393\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0976 - acc: 0.9729 - val_loss: 0.1960 - val_acc: 0.9401\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9735\n",
      "Epoch 00020: val_loss did not improve from 0.19393\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0946 - acc: 0.9735 - val_loss: 0.1976 - val_acc: 0.9439\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9782\n",
      "Epoch 00021: val_loss improved from 0.19393 to 0.18517, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/021-0.1852.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0818 - acc: 0.9781 - val_loss: 0.1852 - val_acc: 0.9460\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9795\n",
      "Epoch 00022: val_loss improved from 0.18517 to 0.18165, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/022-0.1817.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0795 - acc: 0.9795 - val_loss: 0.1817 - val_acc: 0.9415\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9827\n",
      "Epoch 00023: val_loss improved from 0.18165 to 0.17912, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/023-0.1791.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0718 - acc: 0.9827 - val_loss: 0.1791 - val_acc: 0.9467\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9859\n",
      "Epoch 00024: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0584 - acc: 0.9859 - val_loss: 0.1964 - val_acc: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9844\n",
      "Epoch 00025: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0605 - acc: 0.9844 - val_loss: 0.1894 - val_acc: 0.9455\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9877\n",
      "Epoch 00026: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0517 - acc: 0.9877 - val_loss: 0.1944 - val_acc: 0.9394\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9883\n",
      "Epoch 00027: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0489 - acc: 0.9883 - val_loss: 0.1930 - val_acc: 0.9460\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9901\n",
      "Epoch 00028: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0435 - acc: 0.9901 - val_loss: 0.2070 - val_acc: 0.9425\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9902\n",
      "Epoch 00029: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0410 - acc: 0.9902 - val_loss: 0.2032 - val_acc: 0.9469\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9899\n",
      "Epoch 00030: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0416 - acc: 0.9899 - val_loss: 0.1955 - val_acc: 0.9439\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9891\n",
      "Epoch 00031: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0444 - acc: 0.9891 - val_loss: 0.2007 - val_acc: 0.9436\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9896\n",
      "Epoch 00032: val_loss did not improve from 0.17912\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0408 - acc: 0.9895 - val_loss: 0.1965 - val_acc: 0.9441\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9886\n",
      "Epoch 00033: val_loss improved from 0.17912 to 0.17536, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/033-0.1754.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0442 - acc: 0.9885 - val_loss: 0.1754 - val_acc: 0.9474\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.17536\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0346 - acc: 0.9917 - val_loss: 0.1947 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9910\n",
      "Epoch 00035: val_loss did not improve from 0.17536\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0360 - acc: 0.9910 - val_loss: 0.1798 - val_acc: 0.9471\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9911\n",
      "Epoch 00036: val_loss improved from 0.17536 to 0.16979, saving model to model/checkpoint/1D_CNN_custom_2_ch_128_BN_9_conv_checkpoint/036-0.1698.hdf5\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0346 - acc: 0.9911 - val_loss: 0.1698 - val_acc: 0.9555\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9968\n",
      "Epoch 00037: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0194 - acc: 0.9968 - val_loss: 0.1748 - val_acc: 0.9518\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9961\n",
      "Epoch 00038: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0208 - acc: 0.9961 - val_loss: 0.2215 - val_acc: 0.9357\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9947\n",
      "Epoch 00039: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0245 - acc: 0.9947 - val_loss: 0.1968 - val_acc: 0.9471\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9960\n",
      "Epoch 00040: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0211 - acc: 0.9959 - val_loss: 0.1919 - val_acc: 0.9474\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9930\n",
      "Epoch 00041: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0283 - acc: 0.9929 - val_loss: 0.2217 - val_acc: 0.9485\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9872\n",
      "Epoch 00042: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0442 - acc: 0.9872 - val_loss: 0.1977 - val_acc: 0.9469\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9973\n",
      "Epoch 00043: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0152 - acc: 0.9973 - val_loss: 0.1926 - val_acc: 0.9532\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9915\n",
      "Epoch 00044: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0315 - acc: 0.9915 - val_loss: 0.1783 - val_acc: 0.9546\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9979\n",
      "Epoch 00045: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0127 - acc: 0.9979 - val_loss: 0.1976 - val_acc: 0.9504\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9979\n",
      "Epoch 00046: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0126 - acc: 0.9979 - val_loss: 0.2250 - val_acc: 0.9408\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9966\n",
      "Epoch 00047: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 186s 5ms/sample - loss: 0.0168 - acc: 0.9966 - val_loss: 0.2122 - val_acc: 0.9427\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9953\n",
      "Epoch 00048: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0195 - acc: 0.9953 - val_loss: 0.2163 - val_acc: 0.9490\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9973\n",
      "Epoch 00049: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0135 - acc: 0.9973 - val_loss: 0.2070 - val_acc: 0.9460\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9927\n",
      "Epoch 00050: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0265 - acc: 0.9927 - val_loss: 0.2278 - val_acc: 0.9436\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9985\n",
      "Epoch 00051: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0103 - acc: 0.9985 - val_loss: 0.1843 - val_acc: 0.9515\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9979\n",
      "Epoch 00052: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0113 - acc: 0.9979 - val_loss: 0.2410 - val_acc: 0.9467\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9914\n",
      "Epoch 00053: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0298 - acc: 0.9914 - val_loss: 0.2426 - val_acc: 0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9920\n",
      "Epoch 00054: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0284 - acc: 0.9920 - val_loss: 0.2092 - val_acc: 0.9481\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9987\n",
      "Epoch 00055: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0085 - acc: 0.9987 - val_loss: 0.2029 - val_acc: 0.9476\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9959\n",
      "Epoch 00056: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0176 - acc: 0.9959 - val_loss: 0.1904 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9989\n",
      "Epoch 00057: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0078 - acc: 0.9989 - val_loss: 0.1779 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 00058: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.1894 - val_acc: 0.9539\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9971\n",
      "Epoch 00059: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0126 - acc: 0.9971 - val_loss: 0.2243 - val_acc: 0.9446\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9962\n",
      "Epoch 00060: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0156 - acc: 0.9962 - val_loss: 0.2309 - val_acc: 0.9420\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9930\n",
      "Epoch 00061: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0246 - acc: 0.9930 - val_loss: 0.2267 - val_acc: 0.9527\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9935\n",
      "Epoch 00062: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0229 - acc: 0.9935 - val_loss: 0.1981 - val_acc: 0.9529\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9991\n",
      "Epoch 00063: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0069 - acc: 0.9991 - val_loss: 0.2135 - val_acc: 0.9499\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9960\n",
      "Epoch 00064: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0160 - acc: 0.9960 - val_loss: 0.2009 - val_acc: 0.9564\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9990\n",
      "Epoch 00065: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0070 - acc: 0.9990 - val_loss: 0.2025 - val_acc: 0.9529\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9989\n",
      "Epoch 00066: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0062 - acc: 0.9989 - val_loss: 0.2369 - val_acc: 0.9436\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966\n",
      "Epoch 00067: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0135 - acc: 0.9965 - val_loss: 0.2358 - val_acc: 0.9429\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9903\n",
      "Epoch 00068: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0338 - acc: 0.9903 - val_loss: 0.2092 - val_acc: 0.9499\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9988\n",
      "Epoch 00069: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0067 - acc: 0.9988 - val_loss: 0.1739 - val_acc: 0.9539\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 00070: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0063 - acc: 0.9989 - val_loss: 0.2139 - val_acc: 0.9511\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9940\n",
      "Epoch 00071: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0220 - acc: 0.9940 - val_loss: 0.1968 - val_acc: 0.9555\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9958\n",
      "Epoch 00072: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0165 - acc: 0.9958 - val_loss: 0.1800 - val_acc: 0.9576\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9994\n",
      "Epoch 00073: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0048 - acc: 0.9994 - val_loss: 0.1901 - val_acc: 0.9548\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9995\n",
      "Epoch 00074: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0046 - acc: 0.9995 - val_loss: 0.2115 - val_acc: 0.9525\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9937\n",
      "Epoch 00075: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0225 - acc: 0.9938 - val_loss: 0.2305 - val_acc: 0.9439\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 00076: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0060 - acc: 0.9989 - val_loss: 0.2115 - val_acc: 0.9504\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 00077: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.2039 - val_acc: 0.9522\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9964\n",
      "Epoch 00078: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0144 - acc: 0.9964 - val_loss: 0.2066 - val_acc: 0.9506\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9966\n",
      "Epoch 00079: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0128 - acc: 0.9966 - val_loss: 0.2238 - val_acc: 0.9481\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00080: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.1971 - val_acc: 0.9550\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9988\n",
      "Epoch 00081: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0063 - acc: 0.9988 - val_loss: 0.1978 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9970\n",
      "Epoch 00082: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0103 - acc: 0.9970 - val_loss: 0.2128 - val_acc: 0.9515\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9979\n",
      "Epoch 00083: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0090 - acc: 0.9979 - val_loss: 0.2562 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9940\n",
      "Epoch 00084: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.1868 - val_acc: 0.9532\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 00085: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0047 - acc: 0.9993 - val_loss: 0.1832 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9964\n",
      "Epoch 00086: val_loss did not improve from 0.16979\n",
      "36805/36805 [==============================] - 187s 5ms/sample - loss: 0.0134 - acc: 0.9964 - val_loss: 0.1996 - val_acc: 0.9534\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2347 - acc: 0.9379\n",
      "Loss: 0.23471872432395305 Accuracy: 0.9379024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_ch_128_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 394,256\n",
      "Trainable params: 384,656\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.5478 - acc: 0.8474\n",
      "Loss: 0.5478097885816144 Accuracy: 0.847352\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 358,736\n",
      "Trainable params: 354,640\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.3399 - acc: 0.9043\n",
      "Loss: 0.3399248241511708 Accuracy: 0.90425754\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 361,616\n",
      "Trainable params: 359,184\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2236 - acc: 0.9360\n",
      "Loss: 0.22356352788760778 Accuracy: 0.93603325\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,336\n",
      "Trainable params: 362,608\n",
      "Non-trainable params: 1,728\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2347 - acc: 0.9379\n",
      "Loss: 0.23471872432395305 Accuracy: 0.9379024\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_ch_128_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_128_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 4160)              16640     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 394,256\n",
      "Trainable params: 384,656\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.6891 - acc: 0.8426\n",
      "Loss: 0.6890897744169859 Accuracy: 0.8425753\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 1344)              5376      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 358,736\n",
      "Trainable params: 354,640\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.3664 - acc: 0.9105\n",
      "Loss: 0.36638891243860355 Accuracy: 0.91048807\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 448)               1792      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 361,616\n",
      "Trainable params: 359,184\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2944 - acc: 0.9254\n",
      "Loss: 0.29443591366892413 Accuracy: 0.9254413\n",
      "\n",
      "1D_CNN_custom_2_ch_128_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 64)           41024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 32)             10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 7, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                1040      \n",
      "=================================================================\n",
      "Total params: 364,336\n",
      "Trainable params: 362,608\n",
      "Non-trainable params: 1,728\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2877 - acc: 0.9340\n",
      "Loss: 0.28767999671863803 Accuracy: 0.9339564\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
