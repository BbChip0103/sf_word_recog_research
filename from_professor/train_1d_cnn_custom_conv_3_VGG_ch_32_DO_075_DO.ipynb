{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,248\n",
      "Trainable params: 8,195,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,739,952\n",
      "Trainable params: 2,739,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 925,488\n",
      "Trainable params: 925,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3935 - acc: 0.2156\n",
      "Epoch 00001: val_loss improved from inf to 1.84597, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/001-1.8460.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.3935 - acc: 0.2156 - val_loss: 1.8460 - val_acc: 0.4009\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7798 - acc: 0.4215\n",
      "Epoch 00002: val_loss improved from 1.84597 to 1.62328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/002-1.6233.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.7797 - acc: 0.4215 - val_loss: 1.6233 - val_acc: 0.4885\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6170 - acc: 0.4767\n",
      "Epoch 00003: val_loss improved from 1.62328 to 1.48809, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/003-1.4881.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.6171 - acc: 0.4767 - val_loss: 1.4881 - val_acc: 0.5341\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5033 - acc: 0.5179\n",
      "Epoch 00004: val_loss improved from 1.48809 to 1.38371, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/004-1.3837.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.5033 - acc: 0.5179 - val_loss: 1.3837 - val_acc: 0.5688\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4122 - acc: 0.5495\n",
      "Epoch 00005: val_loss improved from 1.38371 to 1.30186, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/005-1.3019.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.4122 - acc: 0.5495 - val_loss: 1.3019 - val_acc: 0.5977\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3287 - acc: 0.5821\n",
      "Epoch 00006: val_loss improved from 1.30186 to 1.26404, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/006-1.2640.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.3286 - acc: 0.5821 - val_loss: 1.2640 - val_acc: 0.6140\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2612 - acc: 0.6038\n",
      "Epoch 00007: val_loss improved from 1.26404 to 1.20269, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/007-1.2027.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2613 - acc: 0.6038 - val_loss: 1.2027 - val_acc: 0.6299\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2060 - acc: 0.6230\n",
      "Epoch 00008: val_loss improved from 1.20269 to 1.16930, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/008-1.1693.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2060 - acc: 0.6230 - val_loss: 1.1693 - val_acc: 0.6457\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1572 - acc: 0.6400\n",
      "Epoch 00009: val_loss improved from 1.16930 to 1.12832, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/009-1.1283.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1573 - acc: 0.6400 - val_loss: 1.1283 - val_acc: 0.6592\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6538\n",
      "Epoch 00010: val_loss improved from 1.12832 to 1.09838, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/010-1.0984.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1133 - acc: 0.6538 - val_loss: 1.0984 - val_acc: 0.6699\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0687 - acc: 0.6710\n",
      "Epoch 00011: val_loss improved from 1.09838 to 1.06126, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/011-1.0613.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0687 - acc: 0.6711 - val_loss: 1.0613 - val_acc: 0.6832\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0253 - acc: 0.6848\n",
      "Epoch 00012: val_loss improved from 1.06126 to 1.04704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/012-1.0470.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0252 - acc: 0.6848 - val_loss: 1.0470 - val_acc: 0.6890\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9905 - acc: 0.6964\n",
      "Epoch 00013: val_loss improved from 1.04704 to 1.00628, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/013-1.0063.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9904 - acc: 0.6964 - val_loss: 1.0063 - val_acc: 0.7056\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9592 - acc: 0.7049\n",
      "Epoch 00014: val_loss improved from 1.00628 to 0.98818, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/014-0.9882.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9593 - acc: 0.7048 - val_loss: 0.9882 - val_acc: 0.7023\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9212 - acc: 0.7190\n",
      "Epoch 00015: val_loss improved from 0.98818 to 0.95602, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/015-0.9560.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9212 - acc: 0.7190 - val_loss: 0.9560 - val_acc: 0.7209\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7273\n",
      "Epoch 00016: val_loss improved from 0.95602 to 0.92291, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/016-0.9229.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8968 - acc: 0.7273 - val_loss: 0.9229 - val_acc: 0.7321\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8682 - acc: 0.7331\n",
      "Epoch 00017: val_loss improved from 0.92291 to 0.90526, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/017-0.9053.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8684 - acc: 0.7331 - val_loss: 0.9053 - val_acc: 0.7333\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7408\n",
      "Epoch 00018: val_loss improved from 0.90526 to 0.88752, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/018-0.8875.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8406 - acc: 0.7408 - val_loss: 0.8875 - val_acc: 0.7431\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8151 - acc: 0.7487\n",
      "Epoch 00019: val_loss improved from 0.88752 to 0.88732, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/019-0.8873.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8151 - acc: 0.7487 - val_loss: 0.8873 - val_acc: 0.7370\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7851 - acc: 0.7590\n",
      "Epoch 00020: val_loss improved from 0.88732 to 0.86828, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/020-0.8683.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7851 - acc: 0.7590 - val_loss: 0.8683 - val_acc: 0.7435\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.7635\n",
      "Epoch 00021: val_loss improved from 0.86828 to 0.86059, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/021-0.8606.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7690 - acc: 0.7634 - val_loss: 0.8606 - val_acc: 0.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7458 - acc: 0.7711\n",
      "Epoch 00022: val_loss improved from 0.86059 to 0.85399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/022-0.8540.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7457 - acc: 0.7711 - val_loss: 0.8540 - val_acc: 0.7501\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7306 - acc: 0.7769\n",
      "Epoch 00023: val_loss improved from 0.85399 to 0.84674, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/023-0.8467.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7305 - acc: 0.7769 - val_loss: 0.8467 - val_acc: 0.7536\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.7813\n",
      "Epoch 00024: val_loss improved from 0.84674 to 0.83537, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/024-0.8354.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7092 - acc: 0.7813 - val_loss: 0.8354 - val_acc: 0.7543\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.7857\n",
      "Epoch 00025: val_loss improved from 0.83537 to 0.83180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/025-0.8318.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6941 - acc: 0.7858 - val_loss: 0.8318 - val_acc: 0.7543\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6810 - acc: 0.7924\n",
      "Epoch 00026: val_loss improved from 0.83180 to 0.81558, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/026-0.8156.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6811 - acc: 0.7924 - val_loss: 0.8156 - val_acc: 0.7612\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.7946\n",
      "Epoch 00027: val_loss did not improve from 0.81558\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6643 - acc: 0.7946 - val_loss: 0.8183 - val_acc: 0.7570\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.7973\n",
      "Epoch 00028: val_loss improved from 0.81558 to 0.80751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/028-0.8075.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6556 - acc: 0.7973 - val_loss: 0.8075 - val_acc: 0.7643\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.7988\n",
      "Epoch 00029: val_loss improved from 0.80751 to 0.78811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/029-0.7881.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6424 - acc: 0.7988 - val_loss: 0.7881 - val_acc: 0.7657\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6301 - acc: 0.8030\n",
      "Epoch 00030: val_loss did not improve from 0.78811\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6301 - acc: 0.8030 - val_loss: 0.8034 - val_acc: 0.7664\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6167 - acc: 0.8064\n",
      "Epoch 00031: val_loss improved from 0.78811 to 0.78117, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/031-0.7812.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6167 - acc: 0.8064 - val_loss: 0.7812 - val_acc: 0.7734\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.8098\n",
      "Epoch 00032: val_loss did not improve from 0.78117\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6083 - acc: 0.8098 - val_loss: 0.7871 - val_acc: 0.7775\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.8112\n",
      "Epoch 00033: val_loss improved from 0.78117 to 0.77544, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/033-0.7754.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5957 - acc: 0.8111 - val_loss: 0.7754 - val_acc: 0.7768\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.8156\n",
      "Epoch 00034: val_loss did not improve from 0.77544\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5874 - acc: 0.8156 - val_loss: 0.7800 - val_acc: 0.7668\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8197\n",
      "Epoch 00035: val_loss did not improve from 0.77544\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5735 - acc: 0.8196 - val_loss: 0.8084 - val_acc: 0.7689\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.8213\n",
      "Epoch 00036: val_loss improved from 0.77544 to 0.77046, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/036-0.7705.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5657 - acc: 0.8212 - val_loss: 0.7705 - val_acc: 0.7750\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8264\n",
      "Epoch 00037: val_loss improved from 0.77046 to 0.76104, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/037-0.7610.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5529 - acc: 0.8264 - val_loss: 0.7610 - val_acc: 0.7801\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8243\n",
      "Epoch 00038: val_loss did not improve from 0.76104\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5519 - acc: 0.8243 - val_loss: 0.7677 - val_acc: 0.7808\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8297\n",
      "Epoch 00039: val_loss improved from 0.76104 to 0.75444, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/039-0.7544.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5378 - acc: 0.8297 - val_loss: 0.7544 - val_acc: 0.7810\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8338\n",
      "Epoch 00040: val_loss did not improve from 0.75444\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5278 - acc: 0.8337 - val_loss: 0.7685 - val_acc: 0.7806\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.8343\n",
      "Epoch 00041: val_loss did not improve from 0.75444\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5292 - acc: 0.8343 - val_loss: 0.7578 - val_acc: 0.7855\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8335\n",
      "Epoch 00042: val_loss did not improve from 0.75444\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5177 - acc: 0.8335 - val_loss: 0.7571 - val_acc: 0.7820\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8349\n",
      "Epoch 00043: val_loss did not improve from 0.75444\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5151 - acc: 0.8350 - val_loss: 0.7813 - val_acc: 0.7778\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8351\n",
      "Epoch 00044: val_loss did not improve from 0.75444\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5104 - acc: 0.8351 - val_loss: 0.7668 - val_acc: 0.7773\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8392\n",
      "Epoch 00045: val_loss improved from 0.75444 to 0.74766, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/045-0.7477.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5058 - acc: 0.8392 - val_loss: 0.7477 - val_acc: 0.7855\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.8404\n",
      "Epoch 00046: val_loss did not improve from 0.74766\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4958 - acc: 0.8404 - val_loss: 0.7513 - val_acc: 0.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8434\n",
      "Epoch 00047: val_loss did not improve from 0.74766\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4881 - acc: 0.8434 - val_loss: 0.7552 - val_acc: 0.7901\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8432\n",
      "Epoch 00048: val_loss did not improve from 0.74766\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4849 - acc: 0.8432 - val_loss: 0.7493 - val_acc: 0.7866\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4907 - acc: 0.8439\n",
      "Epoch 00049: val_loss did not improve from 0.74766\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4906 - acc: 0.8439 - val_loss: 0.7524 - val_acc: 0.7857\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8469\n",
      "Epoch 00050: val_loss improved from 0.74766 to 0.73529, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/050-0.7353.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4742 - acc: 0.8469 - val_loss: 0.7353 - val_acc: 0.7945\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8493\n",
      "Epoch 00051: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4707 - acc: 0.8493 - val_loss: 0.7491 - val_acc: 0.7929\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8487\n",
      "Epoch 00052: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4691 - acc: 0.8487 - val_loss: 0.7448 - val_acc: 0.7890\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8495\n",
      "Epoch 00053: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4620 - acc: 0.8495 - val_loss: 0.7876 - val_acc: 0.7761\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8538\n",
      "Epoch 00054: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4538 - acc: 0.8538 - val_loss: 0.7510 - val_acc: 0.7883\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8549\n",
      "Epoch 00055: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4509 - acc: 0.8549 - val_loss: 0.7537 - val_acc: 0.7869\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8558\n",
      "Epoch 00056: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4453 - acc: 0.8558 - val_loss: 0.7389 - val_acc: 0.7945\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8571\n",
      "Epoch 00057: val_loss did not improve from 0.73529\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4436 - acc: 0.8571 - val_loss: 0.7533 - val_acc: 0.7820\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8582\n",
      "Epoch 00058: val_loss improved from 0.73529 to 0.73373, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/058-0.7337.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4355 - acc: 0.8582 - val_loss: 0.7337 - val_acc: 0.7955\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4345 - acc: 0.8596\n",
      "Epoch 00059: val_loss improved from 0.73373 to 0.73037, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/059-0.7304.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4346 - acc: 0.8596 - val_loss: 0.7304 - val_acc: 0.7925\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8586\n",
      "Epoch 00060: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4346 - acc: 0.8587 - val_loss: 0.7305 - val_acc: 0.7955\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8612\n",
      "Epoch 00061: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4300 - acc: 0.8612 - val_loss: 0.7592 - val_acc: 0.7911\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8610\n",
      "Epoch 00062: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4254 - acc: 0.8610 - val_loss: 0.7411 - val_acc: 0.7955\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8627\n",
      "Epoch 00063: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4223 - acc: 0.8627 - val_loss: 0.7490 - val_acc: 0.7955\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8663\n",
      "Epoch 00064: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4131 - acc: 0.8663 - val_loss: 0.7346 - val_acc: 0.7969\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8665\n",
      "Epoch 00065: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4127 - acc: 0.8665 - val_loss: 0.7673 - val_acc: 0.8020\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8638\n",
      "Epoch 00066: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4151 - acc: 0.8638 - val_loss: 0.7309 - val_acc: 0.8025\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8638\n",
      "Epoch 00067: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4168 - acc: 0.8638 - val_loss: 0.7364 - val_acc: 0.8004\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8700\n",
      "Epoch 00068: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4032 - acc: 0.8700 - val_loss: 0.7460 - val_acc: 0.7969\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8650\n",
      "Epoch 00069: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4090 - acc: 0.8650 - val_loss: 0.7342 - val_acc: 0.8006\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8688\n",
      "Epoch 00070: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4015 - acc: 0.8688 - val_loss: 0.7418 - val_acc: 0.7987\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8691\n",
      "Epoch 00071: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3969 - acc: 0.8691 - val_loss: 0.7357 - val_acc: 0.7950\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8707\n",
      "Epoch 00072: val_loss did not improve from 0.73037\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4004 - acc: 0.8707 - val_loss: 0.7352 - val_acc: 0.8062\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8702\n",
      "Epoch 00073: val_loss improved from 0.73037 to 0.72521, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/073-0.7252.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3949 - acc: 0.8702 - val_loss: 0.7252 - val_acc: 0.7992\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8707\n",
      "Epoch 00074: val_loss did not improve from 0.72521\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3941 - acc: 0.8707 - val_loss: 0.7546 - val_acc: 0.7962\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8720\n",
      "Epoch 00075: val_loss did not improve from 0.72521\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3909 - acc: 0.8721 - val_loss: 0.7356 - val_acc: 0.8020\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8722\n",
      "Epoch 00076: val_loss improved from 0.72521 to 0.70906, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/076-0.7091.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3895 - acc: 0.8722 - val_loss: 0.7091 - val_acc: 0.8057\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8720\n",
      "Epoch 00077: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3855 - acc: 0.8721 - val_loss: 0.7393 - val_acc: 0.8057\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8749\n",
      "Epoch 00078: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3841 - acc: 0.8749 - val_loss: 0.7309 - val_acc: 0.8034\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3782 - acc: 0.8770\n",
      "Epoch 00079: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3781 - acc: 0.8771 - val_loss: 0.7385 - val_acc: 0.8043\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8758\n",
      "Epoch 00080: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3804 - acc: 0.8757 - val_loss: 0.7363 - val_acc: 0.8015\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8766\n",
      "Epoch 00081: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3763 - acc: 0.8765 - val_loss: 0.7404 - val_acc: 0.8050\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8776\n",
      "Epoch 00082: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3727 - acc: 0.8776 - val_loss: 0.7193 - val_acc: 0.8039\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8811\n",
      "Epoch 00083: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3700 - acc: 0.8811 - val_loss: 0.7381 - val_acc: 0.8022\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8756\n",
      "Epoch 00084: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3757 - acc: 0.8756 - val_loss: 0.7340 - val_acc: 0.8050\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3699 - acc: 0.8786\n",
      "Epoch 00085: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3699 - acc: 0.8786 - val_loss: 0.7247 - val_acc: 0.8116\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8797\n",
      "Epoch 00086: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3624 - acc: 0.8797 - val_loss: 0.7415 - val_acc: 0.8064\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8830\n",
      "Epoch 00087: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3567 - acc: 0.8830 - val_loss: 0.7190 - val_acc: 0.8069\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8807\n",
      "Epoch 00088: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3653 - acc: 0.8807 - val_loss: 0.7450 - val_acc: 0.8032\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8792\n",
      "Epoch 00089: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3639 - acc: 0.8792 - val_loss: 0.7313 - val_acc: 0.8050\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8831\n",
      "Epoch 00090: val_loss did not improve from 0.70906\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3558 - acc: 0.8831 - val_loss: 0.7260 - val_acc: 0.8092\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8811\n",
      "Epoch 00091: val_loss improved from 0.70906 to 0.70881, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/091-0.7088.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3625 - acc: 0.8810 - val_loss: 0.7088 - val_acc: 0.8099\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8833\n",
      "Epoch 00092: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3558 - acc: 0.8832 - val_loss: 0.7498 - val_acc: 0.8111\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8836\n",
      "Epoch 00093: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3531 - acc: 0.8836 - val_loss: 0.7273 - val_acc: 0.8078\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8835\n",
      "Epoch 00094: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3527 - acc: 0.8834 - val_loss: 0.7400 - val_acc: 0.8153\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8860\n",
      "Epoch 00095: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3512 - acc: 0.8860 - val_loss: 0.7110 - val_acc: 0.8064\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8845\n",
      "Epoch 00096: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3477 - acc: 0.8844 - val_loss: 0.7307 - val_acc: 0.8130\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8878\n",
      "Epoch 00097: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3439 - acc: 0.8878 - val_loss: 0.7300 - val_acc: 0.8102\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8858\n",
      "Epoch 00098: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3500 - acc: 0.8857 - val_loss: 0.7296 - val_acc: 0.8148\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8885\n",
      "Epoch 00099: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3437 - acc: 0.8885 - val_loss: 0.7356 - val_acc: 0.8104\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8873\n",
      "Epoch 00100: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3412 - acc: 0.8872 - val_loss: 0.7250 - val_acc: 0.8204\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8865\n",
      "Epoch 00101: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3454 - acc: 0.8865 - val_loss: 0.7402 - val_acc: 0.8064\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8866\n",
      "Epoch 00102: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3434 - acc: 0.8866 - val_loss: 0.7358 - val_acc: 0.8069\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8893\n",
      "Epoch 00103: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3413 - acc: 0.8893 - val_loss: 0.7242 - val_acc: 0.8099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8891\n",
      "Epoch 00104: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3407 - acc: 0.8891 - val_loss: 0.7393 - val_acc: 0.8053\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8885\n",
      "Epoch 00105: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3358 - acc: 0.8885 - val_loss: 0.7144 - val_acc: 0.8127\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8892\n",
      "Epoch 00106: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3343 - acc: 0.8892 - val_loss: 0.7320 - val_acc: 0.8118\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8888\n",
      "Epoch 00107: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3358 - acc: 0.8888 - val_loss: 0.7354 - val_acc: 0.8053\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8890\n",
      "Epoch 00108: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3337 - acc: 0.8890 - val_loss: 0.7392 - val_acc: 0.8081\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8898\n",
      "Epoch 00109: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3337 - acc: 0.8898 - val_loss: 0.7183 - val_acc: 0.8174\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8908\n",
      "Epoch 00110: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3315 - acc: 0.8907 - val_loss: 0.7393 - val_acc: 0.8160\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8917\n",
      "Epoch 00111: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3274 - acc: 0.8917 - val_loss: 0.7247 - val_acc: 0.8162\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8903\n",
      "Epoch 00112: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3314 - acc: 0.8903 - val_loss: 0.7256 - val_acc: 0.8192\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8912\n",
      "Epoch 00113: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3275 - acc: 0.8912 - val_loss: 0.7294 - val_acc: 0.8155\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8917\n",
      "Epoch 00114: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3271 - acc: 0.8917 - val_loss: 0.7396 - val_acc: 0.8146\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8924\n",
      "Epoch 00115: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3257 - acc: 0.8924 - val_loss: 0.7255 - val_acc: 0.8155\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8914\n",
      "Epoch 00116: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3284 - acc: 0.8914 - val_loss: 0.7472 - val_acc: 0.8069\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8933\n",
      "Epoch 00117: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3199 - acc: 0.8933 - val_loss: 0.7226 - val_acc: 0.8167\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8935\n",
      "Epoch 00118: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3232 - acc: 0.8935 - val_loss: 0.7288 - val_acc: 0.8102\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8948\n",
      "Epoch 00119: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3196 - acc: 0.8947 - val_loss: 0.7246 - val_acc: 0.8164\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8939\n",
      "Epoch 00120: val_loss did not improve from 0.70881\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3199 - acc: 0.8938 - val_loss: 0.7352 - val_acc: 0.8134\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.8950\n",
      "Epoch 00121: val_loss improved from 0.70881 to 0.68299, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv_checkpoint/121-0.6830.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3200 - acc: 0.8950 - val_loss: 0.6830 - val_acc: 0.8204\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8955\n",
      "Epoch 00122: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3157 - acc: 0.8955 - val_loss: 0.7180 - val_acc: 0.8143\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8965\n",
      "Epoch 00123: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3153 - acc: 0.8965 - val_loss: 0.7178 - val_acc: 0.8139\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.8944\n",
      "Epoch 00124: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3186 - acc: 0.8944 - val_loss: 0.7439 - val_acc: 0.8153\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8972\n",
      "Epoch 00125: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3183 - acc: 0.8972 - val_loss: 0.7189 - val_acc: 0.8178\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8973\n",
      "Epoch 00126: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3121 - acc: 0.8972 - val_loss: 0.7518 - val_acc: 0.8176\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8987\n",
      "Epoch 00127: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3071 - acc: 0.8986 - val_loss: 0.7203 - val_acc: 0.8192\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8971\n",
      "Epoch 00128: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3109 - acc: 0.8971 - val_loss: 0.7000 - val_acc: 0.8213\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8993\n",
      "Epoch 00129: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3109 - acc: 0.8994 - val_loss: 0.7111 - val_acc: 0.8209\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.8958\n",
      "Epoch 00130: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3159 - acc: 0.8958 - val_loss: 0.7198 - val_acc: 0.8206\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8973\n",
      "Epoch 00131: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3105 - acc: 0.8973 - val_loss: 0.7114 - val_acc: 0.8185\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.8991\n",
      "Epoch 00132: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3049 - acc: 0.8991 - val_loss: 0.7577 - val_acc: 0.8153\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.8972\n",
      "Epoch 00133: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3067 - acc: 0.8972 - val_loss: 0.7068 - val_acc: 0.8202\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9008\n",
      "Epoch 00134: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3019 - acc: 0.9008 - val_loss: 0.7235 - val_acc: 0.8230\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.8995\n",
      "Epoch 00135: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3057 - acc: 0.8996 - val_loss: 0.7231 - val_acc: 0.8237\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8986\n",
      "Epoch 00136: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3109 - acc: 0.8986 - val_loss: 0.6961 - val_acc: 0.8195\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8984\n",
      "Epoch 00137: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3105 - acc: 0.8984 - val_loss: 0.7000 - val_acc: 0.8241\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.8974\n",
      "Epoch 00138: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3089 - acc: 0.8974 - val_loss: 0.7227 - val_acc: 0.8183\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.8998\n",
      "Epoch 00139: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3004 - acc: 0.8998 - val_loss: 0.7216 - val_acc: 0.8204\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9004\n",
      "Epoch 00140: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3040 - acc: 0.9004 - val_loss: 0.7143 - val_acc: 0.8218\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9011\n",
      "Epoch 00141: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2999 - acc: 0.9010 - val_loss: 0.7270 - val_acc: 0.8213\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9038\n",
      "Epoch 00142: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2930 - acc: 0.9038 - val_loss: 0.7422 - val_acc: 0.8211\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9018\n",
      "Epoch 00143: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2965 - acc: 0.9018 - val_loss: 0.7243 - val_acc: 0.8227\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9031\n",
      "Epoch 00144: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2898 - acc: 0.9030 - val_loss: 0.6959 - val_acc: 0.8253\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9004\n",
      "Epoch 00145: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2998 - acc: 0.9004 - val_loss: 0.6992 - val_acc: 0.8272\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9027\n",
      "Epoch 00146: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2908 - acc: 0.9027 - val_loss: 0.7174 - val_acc: 0.8232\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9026\n",
      "Epoch 00147: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2924 - acc: 0.9026 - val_loss: 0.7348 - val_acc: 0.8255\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9027\n",
      "Epoch 00148: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2980 - acc: 0.9027 - val_loss: 0.7210 - val_acc: 0.8227\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.9013\n",
      "Epoch 00149: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2955 - acc: 0.9013 - val_loss: 0.6970 - val_acc: 0.8265\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9035\n",
      "Epoch 00150: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2927 - acc: 0.9035 - val_loss: 0.6850 - val_acc: 0.8255\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9033\n",
      "Epoch 00151: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2938 - acc: 0.9032 - val_loss: 0.7210 - val_acc: 0.8209\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9003\n",
      "Epoch 00152: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2980 - acc: 0.9003 - val_loss: 0.7134 - val_acc: 0.8181\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9028\n",
      "Epoch 00153: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2940 - acc: 0.9027 - val_loss: 0.7211 - val_acc: 0.8225\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9033\n",
      "Epoch 00154: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2942 - acc: 0.9033 - val_loss: 0.6862 - val_acc: 0.8290\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9023\n",
      "Epoch 00155: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2875 - acc: 0.9024 - val_loss: 0.7114 - val_acc: 0.8225\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.9063\n",
      "Epoch 00156: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2835 - acc: 0.9063 - val_loss: 0.7325 - val_acc: 0.8288\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.9053\n",
      "Epoch 00157: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2885 - acc: 0.9053 - val_loss: 0.7066 - val_acc: 0.8262\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9054\n",
      "Epoch 00158: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2864 - acc: 0.9054 - val_loss: 0.6958 - val_acc: 0.8302\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2838 - acc: 0.9060\n",
      "Epoch 00159: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2838 - acc: 0.9060 - val_loss: 0.7221 - val_acc: 0.8241\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9057\n",
      "Epoch 00160: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2851 - acc: 0.9057 - val_loss: 0.6983 - val_acc: 0.8316\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9051\n",
      "Epoch 00161: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2874 - acc: 0.9051 - val_loss: 0.6955 - val_acc: 0.8269\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9045\n",
      "Epoch 00162: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2854 - acc: 0.9045 - val_loss: 0.7182 - val_acc: 0.8260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9074\n",
      "Epoch 00163: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2783 - acc: 0.9074 - val_loss: 0.7089 - val_acc: 0.8272\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9065\n",
      "Epoch 00164: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2846 - acc: 0.9065 - val_loss: 0.6971 - val_acc: 0.8332\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9082\n",
      "Epoch 00165: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2794 - acc: 0.9082 - val_loss: 0.7441 - val_acc: 0.8246\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9054\n",
      "Epoch 00166: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2826 - acc: 0.9054 - val_loss: 0.7360 - val_acc: 0.8199\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9076\n",
      "Epoch 00167: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2817 - acc: 0.9075 - val_loss: 0.7076 - val_acc: 0.8286\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9075\n",
      "Epoch 00168: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2803 - acc: 0.9075 - val_loss: 0.7243 - val_acc: 0.8267\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9061\n",
      "Epoch 00169: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2824 - acc: 0.9061 - val_loss: 0.7286 - val_acc: 0.8244\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9060\n",
      "Epoch 00170: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2852 - acc: 0.9060 - val_loss: 0.7187 - val_acc: 0.8253\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9108\n",
      "Epoch 00171: val_loss did not improve from 0.68299\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2710 - acc: 0.9108 - val_loss: 0.7463 - val_acc: 0.8248\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 511us/sample - loss: 0.8153 - acc: 0.7892\n",
      "Loss: 0.8152669871088624 Accuracy: 0.7892004\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4865 - acc: 0.1843\n",
      "Epoch 00001: val_loss improved from inf to 1.94583, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/001-1.9458.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.4864 - acc: 0.1843 - val_loss: 1.9458 - val_acc: 0.3755\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8325 - acc: 0.3976\n",
      "Epoch 00002: val_loss improved from 1.94583 to 1.52103, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/002-1.5210.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.8325 - acc: 0.3977 - val_loss: 1.5210 - val_acc: 0.5257\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5643 - acc: 0.4952\n",
      "Epoch 00003: val_loss improved from 1.52103 to 1.36577, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/003-1.3658.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.5641 - acc: 0.4953 - val_loss: 1.3658 - val_acc: 0.5700\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4353 - acc: 0.5381\n",
      "Epoch 00004: val_loss improved from 1.36577 to 1.25820, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/004-1.2582.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4355 - acc: 0.5380 - val_loss: 1.2582 - val_acc: 0.6112\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3440 - acc: 0.5715\n",
      "Epoch 00005: val_loss improved from 1.25820 to 1.23370, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/005-1.2337.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.3439 - acc: 0.5715 - val_loss: 1.2337 - val_acc: 0.6240\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2697 - acc: 0.6005\n",
      "Epoch 00006: val_loss improved from 1.23370 to 1.11560, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/006-1.1156.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2696 - acc: 0.6005 - val_loss: 1.1156 - val_acc: 0.6709\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1799 - acc: 0.6307\n",
      "Epoch 00007: val_loss improved from 1.11560 to 1.06440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/007-1.0644.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1798 - acc: 0.6307 - val_loss: 1.0644 - val_acc: 0.6723\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1101 - acc: 0.6552\n",
      "Epoch 00008: val_loss improved from 1.06440 to 1.00195, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/008-1.0020.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1100 - acc: 0.6552 - val_loss: 1.0020 - val_acc: 0.6983\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0503 - acc: 0.6739\n",
      "Epoch 00009: val_loss improved from 1.00195 to 0.92258, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/009-0.9226.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0502 - acc: 0.6739 - val_loss: 0.9226 - val_acc: 0.7319\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9938 - acc: 0.6914\n",
      "Epoch 00010: val_loss improved from 0.92258 to 0.90201, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/010-0.9020.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9938 - acc: 0.6914 - val_loss: 0.9020 - val_acc: 0.7433\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9516 - acc: 0.7065\n",
      "Epoch 00011: val_loss did not improve from 0.90201\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9517 - acc: 0.7065 - val_loss: 0.9157 - val_acc: 0.7310\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.7170\n",
      "Epoch 00012: val_loss improved from 0.90201 to 0.86667, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/012-0.8667.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9122 - acc: 0.7170 - val_loss: 0.8667 - val_acc: 0.7405\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8718 - acc: 0.7318\n",
      "Epoch 00013: val_loss improved from 0.86667 to 0.81315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/013-0.8132.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8717 - acc: 0.7318 - val_loss: 0.8132 - val_acc: 0.7685\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8467 - acc: 0.7407\n",
      "Epoch 00014: val_loss did not improve from 0.81315\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8468 - acc: 0.7407 - val_loss: 0.8261 - val_acc: 0.7591\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8171 - acc: 0.7508\n",
      "Epoch 00015: val_loss improved from 0.81315 to 0.77658, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/015-0.7766.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8172 - acc: 0.7508 - val_loss: 0.7766 - val_acc: 0.7799\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7978 - acc: 0.7553\n",
      "Epoch 00016: val_loss improved from 0.77658 to 0.76470, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/016-0.7647.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7977 - acc: 0.7554 - val_loss: 0.7647 - val_acc: 0.7792\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7638\n",
      "Epoch 00017: val_loss improved from 0.76470 to 0.73020, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/017-0.7302.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7658 - acc: 0.7638 - val_loss: 0.7302 - val_acc: 0.7973\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7412 - acc: 0.7726\n",
      "Epoch 00018: val_loss improved from 0.73020 to 0.71716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/018-0.7172.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7413 - acc: 0.7726 - val_loss: 0.7172 - val_acc: 0.7955\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7775\n",
      "Epoch 00019: val_loss improved from 0.71716 to 0.69818, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/019-0.6982.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7230 - acc: 0.7774 - val_loss: 0.6982 - val_acc: 0.8034\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7027 - acc: 0.7847\n",
      "Epoch 00020: val_loss improved from 0.69818 to 0.69126, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/020-0.6913.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7028 - acc: 0.7847 - val_loss: 0.6913 - val_acc: 0.8029\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6866 - acc: 0.7900\n",
      "Epoch 00021: val_loss improved from 0.69126 to 0.67711, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/021-0.6771.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6867 - acc: 0.7900 - val_loss: 0.6771 - val_acc: 0.8113\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6647 - acc: 0.7964\n",
      "Epoch 00022: val_loss did not improve from 0.67711\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6647 - acc: 0.7964 - val_loss: 0.6922 - val_acc: 0.8074\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6534 - acc: 0.8021\n",
      "Epoch 00023: val_loss improved from 0.67711 to 0.67061, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/023-0.6706.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6534 - acc: 0.8021 - val_loss: 0.6706 - val_acc: 0.8141\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.8048\n",
      "Epoch 00024: val_loss improved from 0.67061 to 0.63868, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/024-0.6387.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6429 - acc: 0.8048 - val_loss: 0.6387 - val_acc: 0.8227\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6269 - acc: 0.8100\n",
      "Epoch 00025: val_loss did not improve from 0.63868\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6269 - acc: 0.8100 - val_loss: 0.6793 - val_acc: 0.8099\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6111 - acc: 0.8147\n",
      "Epoch 00026: val_loss improved from 0.63868 to 0.63168, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/026-0.6317.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6110 - acc: 0.8147 - val_loss: 0.6317 - val_acc: 0.8237\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.8168\n",
      "Epoch 00027: val_loss improved from 0.63168 to 0.62788, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/027-0.6279.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5962 - acc: 0.8168 - val_loss: 0.6279 - val_acc: 0.8230\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5829 - acc: 0.8210\n",
      "Epoch 00028: val_loss improved from 0.62788 to 0.60746, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/028-0.6075.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5829 - acc: 0.8210 - val_loss: 0.6075 - val_acc: 0.8304\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5752 - acc: 0.8246\n",
      "Epoch 00029: val_loss did not improve from 0.60746\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5753 - acc: 0.8246 - val_loss: 0.6755 - val_acc: 0.8127\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.8269\n",
      "Epoch 00030: val_loss did not improve from 0.60746\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5658 - acc: 0.8270 - val_loss: 0.6080 - val_acc: 0.8337\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8295\n",
      "Epoch 00031: val_loss improved from 0.60746 to 0.59465, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/031-0.5946.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5587 - acc: 0.8295 - val_loss: 0.5946 - val_acc: 0.8411\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.8337\n",
      "Epoch 00032: val_loss did not improve from 0.59465\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5403 - acc: 0.8337 - val_loss: 0.5984 - val_acc: 0.8353\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5331 - acc: 0.8359\n",
      "Epoch 00033: val_loss improved from 0.59465 to 0.59229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/033-0.5923.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5331 - acc: 0.8359 - val_loss: 0.5923 - val_acc: 0.8367\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8390\n",
      "Epoch 00034: val_loss did not improve from 0.59229\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5223 - acc: 0.8391 - val_loss: 0.5982 - val_acc: 0.8400\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8391\n",
      "Epoch 00035: val_loss improved from 0.59229 to 0.57729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/035-0.5773.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5168 - acc: 0.8391 - val_loss: 0.5773 - val_acc: 0.8381\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8445\n",
      "Epoch 00036: val_loss did not improve from 0.57729\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5074 - acc: 0.8445 - val_loss: 0.6035 - val_acc: 0.8360\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8439\n",
      "Epoch 00037: val_loss did not improve from 0.57729\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5042 - acc: 0.8439 - val_loss: 0.5883 - val_acc: 0.8355\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.8473\n",
      "Epoch 00038: val_loss improved from 0.57729 to 0.57438, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/038-0.5744.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4965 - acc: 0.8472 - val_loss: 0.5744 - val_acc: 0.8456\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8497\n",
      "Epoch 00039: val_loss did not improve from 0.57438\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4834 - acc: 0.8496 - val_loss: 0.5749 - val_acc: 0.8479\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.8500\n",
      "Epoch 00040: val_loss did not improve from 0.57438\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4766 - acc: 0.8499 - val_loss: 0.5991 - val_acc: 0.8351\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8547\n",
      "Epoch 00041: val_loss improved from 0.57438 to 0.56712, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/041-0.5671.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4746 - acc: 0.8547 - val_loss: 0.5671 - val_acc: 0.8505\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8564\n",
      "Epoch 00042: val_loss improved from 0.56712 to 0.56618, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/042-0.5662.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4599 - acc: 0.8564 - val_loss: 0.5662 - val_acc: 0.8519\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8607\n",
      "Epoch 00043: val_loss did not improve from 0.56618\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4539 - acc: 0.8606 - val_loss: 0.5764 - val_acc: 0.8507\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8560\n",
      "Epoch 00044: val_loss improved from 0.56618 to 0.55696, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/044-0.5570.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4560 - acc: 0.8559 - val_loss: 0.5570 - val_acc: 0.8539\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8613\n",
      "Epoch 00045: val_loss did not improve from 0.55696\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4437 - acc: 0.8612 - val_loss: 0.5829 - val_acc: 0.8458\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8622\n",
      "Epoch 00046: val_loss did not improve from 0.55696\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4391 - acc: 0.8622 - val_loss: 0.5869 - val_acc: 0.8488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8640\n",
      "Epoch 00047: val_loss did not improve from 0.55696\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4314 - acc: 0.8640 - val_loss: 0.5656 - val_acc: 0.8500\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8657\n",
      "Epoch 00048: val_loss did not improve from 0.55696\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4254 - acc: 0.8657 - val_loss: 0.5757 - val_acc: 0.8493\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8648\n",
      "Epoch 00049: val_loss improved from 0.55696 to 0.55261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/049-0.5526.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4303 - acc: 0.8648 - val_loss: 0.5526 - val_acc: 0.8535\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4219 - acc: 0.8680\n",
      "Epoch 00050: val_loss did not improve from 0.55261\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4219 - acc: 0.8680 - val_loss: 0.5532 - val_acc: 0.8532\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8703\n",
      "Epoch 00051: val_loss improved from 0.55261 to 0.55260, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/051-0.5526.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4159 - acc: 0.8703 - val_loss: 0.5526 - val_acc: 0.8537\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8716\n",
      "Epoch 00052: val_loss improved from 0.55260 to 0.53554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/052-0.5355.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4099 - acc: 0.8716 - val_loss: 0.5355 - val_acc: 0.8556\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8720\n",
      "Epoch 00053: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4102 - acc: 0.8720 - val_loss: 0.5397 - val_acc: 0.8581\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8735\n",
      "Epoch 00054: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3996 - acc: 0.8734 - val_loss: 0.5714 - val_acc: 0.8523\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8726\n",
      "Epoch 00055: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3994 - acc: 0.8726 - val_loss: 0.5406 - val_acc: 0.8563\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8775\n",
      "Epoch 00056: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3855 - acc: 0.8775 - val_loss: 0.5484 - val_acc: 0.8539\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8743\n",
      "Epoch 00057: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3917 - acc: 0.8743 - val_loss: 0.5424 - val_acc: 0.8544\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8764\n",
      "Epoch 00058: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3927 - acc: 0.8763 - val_loss: 0.5503 - val_acc: 0.8556\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8810\n",
      "Epoch 00059: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3768 - acc: 0.8809 - val_loss: 0.5584 - val_acc: 0.8574\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8802\n",
      "Epoch 00060: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3807 - acc: 0.8802 - val_loss: 0.5500 - val_acc: 0.8535\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8810\n",
      "Epoch 00061: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3757 - acc: 0.8810 - val_loss: 0.5513 - val_acc: 0.8567\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8805\n",
      "Epoch 00062: val_loss did not improve from 0.53554\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3761 - acc: 0.8805 - val_loss: 0.5716 - val_acc: 0.8530\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8815\n",
      "Epoch 00063: val_loss improved from 0.53554 to 0.52496, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/063-0.5250.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3702 - acc: 0.8815 - val_loss: 0.5250 - val_acc: 0.8621\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.8827\n",
      "Epoch 00064: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3665 - acc: 0.8827 - val_loss: 0.5476 - val_acc: 0.8619\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8836\n",
      "Epoch 00065: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3654 - acc: 0.8835 - val_loss: 0.5519 - val_acc: 0.8556\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8859\n",
      "Epoch 00066: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3584 - acc: 0.8859 - val_loss: 0.5503 - val_acc: 0.8567\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8880\n",
      "Epoch 00067: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3516 - acc: 0.8880 - val_loss: 0.5298 - val_acc: 0.8619\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8878\n",
      "Epoch 00068: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3502 - acc: 0.8878 - val_loss: 0.5411 - val_acc: 0.8623\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8854\n",
      "Epoch 00069: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3577 - acc: 0.8854 - val_loss: 0.5431 - val_acc: 0.8605\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8887\n",
      "Epoch 00070: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3473 - acc: 0.8887 - val_loss: 0.5319 - val_acc: 0.8665\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8911\n",
      "Epoch 00071: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3383 - acc: 0.8911 - val_loss: 0.5508 - val_acc: 0.8556\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8915\n",
      "Epoch 00072: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3414 - acc: 0.8915 - val_loss: 0.5743 - val_acc: 0.8553\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8891\n",
      "Epoch 00073: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3419 - acc: 0.8891 - val_loss: 0.5552 - val_acc: 0.8640\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8882\n",
      "Epoch 00074: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3423 - acc: 0.8882 - val_loss: 0.5319 - val_acc: 0.8616\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8943\n",
      "Epoch 00075: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3364 - acc: 0.8943 - val_loss: 0.5297 - val_acc: 0.8628\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8929\n",
      "Epoch 00076: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3322 - acc: 0.8929 - val_loss: 0.5593 - val_acc: 0.8628\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8919\n",
      "Epoch 00077: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3366 - acc: 0.8919 - val_loss: 0.5586 - val_acc: 0.8633\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8952\n",
      "Epoch 00078: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3333 - acc: 0.8952 - val_loss: 0.5454 - val_acc: 0.8658\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8946\n",
      "Epoch 00079: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3214 - acc: 0.8946 - val_loss: 0.5446 - val_acc: 0.8640\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8986\n",
      "Epoch 00080: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3228 - acc: 0.8986 - val_loss: 0.5391 - val_acc: 0.8663\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8962\n",
      "Epoch 00081: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3207 - acc: 0.8962 - val_loss: 0.5396 - val_acc: 0.8668\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8945\n",
      "Epoch 00082: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3185 - acc: 0.8945 - val_loss: 0.5603 - val_acc: 0.8647\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8965\n",
      "Epoch 00083: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3177 - acc: 0.8965 - val_loss: 0.5729 - val_acc: 0.8570\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.8978\n",
      "Epoch 00084: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3162 - acc: 0.8978 - val_loss: 0.5397 - val_acc: 0.8675\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9005\n",
      "Epoch 00085: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3082 - acc: 0.9005 - val_loss: 0.5349 - val_acc: 0.8623\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8976\n",
      "Epoch 00086: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3161 - acc: 0.8976 - val_loss: 0.5319 - val_acc: 0.8724\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8982\n",
      "Epoch 00087: val_loss did not improve from 0.52496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3131 - acc: 0.8982 - val_loss: 0.5438 - val_acc: 0.8707\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9020\n",
      "Epoch 00088: val_loss improved from 0.52496 to 0.52264, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/088-0.5226.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3057 - acc: 0.9021 - val_loss: 0.5226 - val_acc: 0.8724\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9002\n",
      "Epoch 00089: val_loss did not improve from 0.52264\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3108 - acc: 0.9002 - val_loss: 0.5493 - val_acc: 0.8605\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9019\n",
      "Epoch 00090: val_loss did not improve from 0.52264\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3070 - acc: 0.9019 - val_loss: 0.5898 - val_acc: 0.8670\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.8999\n",
      "Epoch 00091: val_loss did not improve from 0.52264\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3072 - acc: 0.8999 - val_loss: 0.5297 - val_acc: 0.8693\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9021\n",
      "Epoch 00092: val_loss did not improve from 0.52264\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3010 - acc: 0.9021 - val_loss: 0.5417 - val_acc: 0.8656\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.8993\n",
      "Epoch 00093: val_loss improved from 0.52264 to 0.52097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/093-0.5210.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3058 - acc: 0.8993 - val_loss: 0.5210 - val_acc: 0.8703\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9033\n",
      "Epoch 00094: val_loss did not improve from 0.52097\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2947 - acc: 0.9033 - val_loss: 0.5404 - val_acc: 0.8696\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9035\n",
      "Epoch 00095: val_loss did not improve from 0.52097\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3000 - acc: 0.9034 - val_loss: 0.5491 - val_acc: 0.8721\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9049\n",
      "Epoch 00096: val_loss did not improve from 0.52097\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2927 - acc: 0.9049 - val_loss: 0.5270 - val_acc: 0.8733\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9063\n",
      "Epoch 00097: val_loss did not improve from 0.52097\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2917 - acc: 0.9063 - val_loss: 0.5258 - val_acc: 0.8698\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9062\n",
      "Epoch 00098: val_loss improved from 0.52097 to 0.51993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/098-0.5199.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2897 - acc: 0.9062 - val_loss: 0.5199 - val_acc: 0.8691\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9057\n",
      "Epoch 00099: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2909 - acc: 0.9057 - val_loss: 0.5409 - val_acc: 0.8721\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9065\n",
      "Epoch 00100: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2864 - acc: 0.9065 - val_loss: 0.5209 - val_acc: 0.8740\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9061\n",
      "Epoch 00101: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2871 - acc: 0.9061 - val_loss: 0.5361 - val_acc: 0.8747\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9068\n",
      "Epoch 00102: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2861 - acc: 0.9068 - val_loss: 0.5439 - val_acc: 0.8719\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9091\n",
      "Epoch 00103: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2774 - acc: 0.9091 - val_loss: 0.5323 - val_acc: 0.8675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9069\n",
      "Epoch 00104: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2824 - acc: 0.9069 - val_loss: 0.5489 - val_acc: 0.8721\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9056\n",
      "Epoch 00105: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2872 - acc: 0.9056 - val_loss: 0.5432 - val_acc: 0.8744\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9079\n",
      "Epoch 00106: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2851 - acc: 0.9079 - val_loss: 0.5494 - val_acc: 0.8682\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9095\n",
      "Epoch 00107: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2791 - acc: 0.9094 - val_loss: 0.5610 - val_acc: 0.8719\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9105\n",
      "Epoch 00108: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2745 - acc: 0.9106 - val_loss: 0.5284 - val_acc: 0.8754\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9089\n",
      "Epoch 00109: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2787 - acc: 0.9088 - val_loss: 0.5282 - val_acc: 0.8726\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9111\n",
      "Epoch 00110: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2753 - acc: 0.9111 - val_loss: 0.5513 - val_acc: 0.8724\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9095\n",
      "Epoch 00111: val_loss did not improve from 0.51993\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2756 - acc: 0.9095 - val_loss: 0.5378 - val_acc: 0.8735\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9148\n",
      "Epoch 00112: val_loss improved from 0.51993 to 0.51908, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/112-0.5191.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2640 - acc: 0.9148 - val_loss: 0.5191 - val_acc: 0.8698\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9132\n",
      "Epoch 00113: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2659 - acc: 0.9132 - val_loss: 0.5242 - val_acc: 0.8742\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9089\n",
      "Epoch 00114: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2793 - acc: 0.9088 - val_loss: 0.5295 - val_acc: 0.8728\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9111\n",
      "Epoch 00115: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2746 - acc: 0.9111 - val_loss: 0.5473 - val_acc: 0.8684\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9145\n",
      "Epoch 00116: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2629 - acc: 0.9145 - val_loss: 0.5439 - val_acc: 0.8758\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9120\n",
      "Epoch 00117: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2711 - acc: 0.9120 - val_loss: 0.5284 - val_acc: 0.8749\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9109\n",
      "Epoch 00118: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2703 - acc: 0.9109 - val_loss: 0.5285 - val_acc: 0.8758\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9150\n",
      "Epoch 00119: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2609 - acc: 0.9150 - val_loss: 0.5317 - val_acc: 0.8740\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9156\n",
      "Epoch 00120: val_loss did not improve from 0.51908\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2604 - acc: 0.9156 - val_loss: 0.5338 - val_acc: 0.8730\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9152\n",
      "Epoch 00121: val_loss improved from 0.51908 to 0.51631, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/121-0.5163.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2599 - acc: 0.9153 - val_loss: 0.5163 - val_acc: 0.8772\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9152\n",
      "Epoch 00122: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2561 - acc: 0.9152 - val_loss: 0.5563 - val_acc: 0.8640\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9188\n",
      "Epoch 00123: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2496 - acc: 0.9188 - val_loss: 0.5554 - val_acc: 0.8768\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9169\n",
      "Epoch 00124: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2577 - acc: 0.9169 - val_loss: 0.5386 - val_acc: 0.8677\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9154\n",
      "Epoch 00125: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2592 - acc: 0.9154 - val_loss: 0.5257 - val_acc: 0.8786\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9154\n",
      "Epoch 00126: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2557 - acc: 0.9154 - val_loss: 0.5440 - val_acc: 0.8772\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9178\n",
      "Epoch 00127: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2550 - acc: 0.9178 - val_loss: 0.5292 - val_acc: 0.8758\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9169\n",
      "Epoch 00128: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2502 - acc: 0.9169 - val_loss: 0.5396 - val_acc: 0.8703\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9165\n",
      "Epoch 00129: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2563 - acc: 0.9165 - val_loss: 0.5236 - val_acc: 0.8798\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9176\n",
      "Epoch 00130: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2516 - acc: 0.9176 - val_loss: 0.5383 - val_acc: 0.8761\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9162\n",
      "Epoch 00131: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2539 - acc: 0.9163 - val_loss: 0.5348 - val_acc: 0.8712\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9195\n",
      "Epoch 00132: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2469 - acc: 0.9195 - val_loss: 0.5213 - val_acc: 0.8763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9173\n",
      "Epoch 00133: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2517 - acc: 0.9173 - val_loss: 0.5587 - val_acc: 0.8714\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9176\n",
      "Epoch 00134: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2489 - acc: 0.9176 - val_loss: 0.5276 - val_acc: 0.8737\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9181\n",
      "Epoch 00135: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2486 - acc: 0.9181 - val_loss: 0.5221 - val_acc: 0.8717\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9184\n",
      "Epoch 00136: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2484 - acc: 0.9184 - val_loss: 0.5380 - val_acc: 0.8763\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9190\n",
      "Epoch 00137: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2465 - acc: 0.9190 - val_loss: 0.5297 - val_acc: 0.8744\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9202\n",
      "Epoch 00138: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2469 - acc: 0.9201 - val_loss: 0.5172 - val_acc: 0.8784\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9212\n",
      "Epoch 00139: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2424 - acc: 0.9212 - val_loss: 0.5333 - val_acc: 0.8749\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9200\n",
      "Epoch 00140: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2411 - acc: 0.9200 - val_loss: 0.5248 - val_acc: 0.8763\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9211\n",
      "Epoch 00141: val_loss did not improve from 0.51631\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2448 - acc: 0.9210 - val_loss: 0.5311 - val_acc: 0.8784\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9217\n",
      "Epoch 00142: val_loss improved from 0.51631 to 0.50015, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/142-0.5001.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2414 - acc: 0.9216 - val_loss: 0.5001 - val_acc: 0.8751\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9195\n",
      "Epoch 00143: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2473 - acc: 0.9195 - val_loss: 0.5230 - val_acc: 0.8761\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9210\n",
      "Epoch 00144: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2396 - acc: 0.9210 - val_loss: 0.5633 - val_acc: 0.8689\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9233\n",
      "Epoch 00145: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2340 - acc: 0.9234 - val_loss: 0.5623 - val_acc: 0.8789\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9240\n",
      "Epoch 00146: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2348 - acc: 0.9240 - val_loss: 0.5476 - val_acc: 0.8751\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9218\n",
      "Epoch 00147: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2439 - acc: 0.9218 - val_loss: 0.5357 - val_acc: 0.8775\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9220\n",
      "Epoch 00148: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2349 - acc: 0.9220 - val_loss: 0.5358 - val_acc: 0.8798\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9227\n",
      "Epoch 00149: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2330 - acc: 0.9227 - val_loss: 0.5564 - val_acc: 0.8779\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9258\n",
      "Epoch 00150: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2308 - acc: 0.9258 - val_loss: 0.5314 - val_acc: 0.8807\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9231\n",
      "Epoch 00151: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2379 - acc: 0.9231 - val_loss: 0.5251 - val_acc: 0.8777\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9248\n",
      "Epoch 00152: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2308 - acc: 0.9248 - val_loss: 0.5070 - val_acc: 0.8803\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9256\n",
      "Epoch 00153: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2305 - acc: 0.9256 - val_loss: 0.5551 - val_acc: 0.8742\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9255\n",
      "Epoch 00154: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2268 - acc: 0.9254 - val_loss: 0.5368 - val_acc: 0.8765\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9247\n",
      "Epoch 00155: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2310 - acc: 0.9247 - val_loss: 0.5127 - val_acc: 0.8789\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9248\n",
      "Epoch 00156: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2275 - acc: 0.9248 - val_loss: 0.5133 - val_acc: 0.8793\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9261\n",
      "Epoch 00157: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2264 - acc: 0.9261 - val_loss: 0.5176 - val_acc: 0.8791\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9246\n",
      "Epoch 00158: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2320 - acc: 0.9245 - val_loss: 0.5190 - val_acc: 0.8777\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9251\n",
      "Epoch 00159: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2311 - acc: 0.9251 - val_loss: 0.5482 - val_acc: 0.8700\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9252\n",
      "Epoch 00160: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2254 - acc: 0.9252 - val_loss: 0.5485 - val_acc: 0.8751\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9261\n",
      "Epoch 00161: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2242 - acc: 0.9262 - val_loss: 0.5321 - val_acc: 0.8772\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9270\n",
      "Epoch 00162: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2241 - acc: 0.9269 - val_loss: 0.5474 - val_acc: 0.8800\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9252\n",
      "Epoch 00163: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2325 - acc: 0.9252 - val_loss: 0.5074 - val_acc: 0.8814\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9274\n",
      "Epoch 00164: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2229 - acc: 0.9274 - val_loss: 0.5404 - val_acc: 0.8782\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9268\n",
      "Epoch 00165: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2224 - acc: 0.9267 - val_loss: 0.5543 - val_acc: 0.8707\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9266\n",
      "Epoch 00166: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2262 - acc: 0.9266 - val_loss: 0.5463 - val_acc: 0.8784\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9287\n",
      "Epoch 00167: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2169 - acc: 0.9287 - val_loss: 0.5205 - val_acc: 0.8805\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9305\n",
      "Epoch 00168: val_loss did not improve from 0.50015\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2143 - acc: 0.9306 - val_loss: 0.5533 - val_acc: 0.8770\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9266\n",
      "Epoch 00169: val_loss improved from 0.50015 to 0.49034, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv_checkpoint/169-0.4903.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2229 - acc: 0.9266 - val_loss: 0.4903 - val_acc: 0.8826\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9290\n",
      "Epoch 00170: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2189 - acc: 0.9289 - val_loss: 0.5074 - val_acc: 0.8740\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9263\n",
      "Epoch 00171: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2204 - acc: 0.9263 - val_loss: 0.5347 - val_acc: 0.8770\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9295\n",
      "Epoch 00172: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2151 - acc: 0.9295 - val_loss: 0.5232 - val_acc: 0.8789\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9282\n",
      "Epoch 00173: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2177 - acc: 0.9282 - val_loss: 0.5510 - val_acc: 0.8747\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9287\n",
      "Epoch 00174: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2189 - acc: 0.9287 - val_loss: 0.5170 - val_acc: 0.8831\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9285\n",
      "Epoch 00175: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2166 - acc: 0.9285 - val_loss: 0.5340 - val_acc: 0.8775\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9277\n",
      "Epoch 00176: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2181 - acc: 0.9277 - val_loss: 0.5278 - val_acc: 0.8847\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9273\n",
      "Epoch 00177: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2203 - acc: 0.9273 - val_loss: 0.4906 - val_acc: 0.8786\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9288\n",
      "Epoch 00178: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2131 - acc: 0.9288 - val_loss: 0.5547 - val_acc: 0.8765\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9294\n",
      "Epoch 00179: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2161 - acc: 0.9294 - val_loss: 0.5234 - val_acc: 0.8763\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9315\n",
      "Epoch 00180: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2101 - acc: 0.9315 - val_loss: 0.5456 - val_acc: 0.8789\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9305\n",
      "Epoch 00181: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2120 - acc: 0.9305 - val_loss: 0.5215 - val_acc: 0.8782\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9283\n",
      "Epoch 00182: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2171 - acc: 0.9283 - val_loss: 0.5028 - val_acc: 0.8852\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9301\n",
      "Epoch 00183: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2105 - acc: 0.9300 - val_loss: 0.5300 - val_acc: 0.8819\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9304\n",
      "Epoch 00184: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2130 - acc: 0.9304 - val_loss: 0.5225 - val_acc: 0.8821\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9301\n",
      "Epoch 00185: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2140 - acc: 0.9301 - val_loss: 0.5481 - val_acc: 0.8814\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9331\n",
      "Epoch 00186: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2074 - acc: 0.9331 - val_loss: 0.5249 - val_acc: 0.8828\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9332\n",
      "Epoch 00187: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2064 - acc: 0.9331 - val_loss: 0.5178 - val_acc: 0.8793\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9320\n",
      "Epoch 00188: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2071 - acc: 0.9320 - val_loss: 0.5578 - val_acc: 0.8756\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9282\n",
      "Epoch 00189: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2166 - acc: 0.9282 - val_loss: 0.5200 - val_acc: 0.8814\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9328\n",
      "Epoch 00190: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2078 - acc: 0.9328 - val_loss: 0.5375 - val_acc: 0.8819\n",
      "Epoch 191/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9302\n",
      "Epoch 00191: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2108 - acc: 0.9302 - val_loss: 0.5148 - val_acc: 0.8793\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9322\n",
      "Epoch 00192: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2064 - acc: 0.9322 - val_loss: 0.5356 - val_acc: 0.8866\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9305\n",
      "Epoch 00193: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2104 - acc: 0.9305 - val_loss: 0.5142 - val_acc: 0.8845\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9342\n",
      "Epoch 00194: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2010 - acc: 0.9342 - val_loss: 0.5418 - val_acc: 0.8796\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9344\n",
      "Epoch 00195: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2026 - acc: 0.9344 - val_loss: 0.5174 - val_acc: 0.8887\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9330\n",
      "Epoch 00196: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2027 - acc: 0.9330 - val_loss: 0.5494 - val_acc: 0.8817\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9338\n",
      "Epoch 00197: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2033 - acc: 0.9338 - val_loss: 0.5435 - val_acc: 0.8810\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9329\n",
      "Epoch 00198: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2028 - acc: 0.9329 - val_loss: 0.5207 - val_acc: 0.8866\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9333\n",
      "Epoch 00199: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2024 - acc: 0.9332 - val_loss: 0.5147 - val_acc: 0.8856\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9341\n",
      "Epoch 00200: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2002 - acc: 0.9341 - val_loss: 0.5334 - val_acc: 0.8875\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9349\n",
      "Epoch 00201: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1992 - acc: 0.9349 - val_loss: 0.5170 - val_acc: 0.8856\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9348\n",
      "Epoch 00202: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1977 - acc: 0.9347 - val_loss: 0.5062 - val_acc: 0.8880\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9364\n",
      "Epoch 00203: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2022 - acc: 0.9364 - val_loss: 0.5341 - val_acc: 0.8817\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9347\n",
      "Epoch 00204: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1996 - acc: 0.9347 - val_loss: 0.5171 - val_acc: 0.8849\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9334\n",
      "Epoch 00205: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2033 - acc: 0.9334 - val_loss: 0.5570 - val_acc: 0.8765\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9298\n",
      "Epoch 00206: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2123 - acc: 0.9298 - val_loss: 0.5233 - val_acc: 0.8861\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9349\n",
      "Epoch 00207: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1981 - acc: 0.9349 - val_loss: 0.5287 - val_acc: 0.8828\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9361\n",
      "Epoch 00208: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1960 - acc: 0.9361 - val_loss: 0.5519 - val_acc: 0.8817\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9352\n",
      "Epoch 00209: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1989 - acc: 0.9352 - val_loss: 0.5191 - val_acc: 0.8840\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9356\n",
      "Epoch 00210: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2002 - acc: 0.9356 - val_loss: 0.5210 - val_acc: 0.8814\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9348\n",
      "Epoch 00211: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1978 - acc: 0.9348 - val_loss: 0.5269 - val_acc: 0.8854\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9342\n",
      "Epoch 00212: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1998 - acc: 0.9342 - val_loss: 0.5548 - val_acc: 0.8807\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9346\n",
      "Epoch 00213: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1946 - acc: 0.9346 - val_loss: 0.5178 - val_acc: 0.8828\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9358\n",
      "Epoch 00214: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1982 - acc: 0.9358 - val_loss: 0.5114 - val_acc: 0.8873\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9357\n",
      "Epoch 00215: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1974 - acc: 0.9357 - val_loss: 0.5089 - val_acc: 0.8840\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9354\n",
      "Epoch 00216: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1968 - acc: 0.9354 - val_loss: 0.5315 - val_acc: 0.8819\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9362\n",
      "Epoch 00217: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1960 - acc: 0.9362 - val_loss: 0.5220 - val_acc: 0.8849\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9344\n",
      "Epoch 00218: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1973 - acc: 0.9344 - val_loss: 0.5052 - val_acc: 0.8882\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9352\n",
      "Epoch 00219: val_loss did not improve from 0.49034\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1952 - acc: 0.9352 - val_loss: 0.5147 - val_acc: 0.8875\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 548us/sample - loss: 0.5928 - acc: 0.8480\n",
      "Loss: 0.5928024519145922 Accuracy: 0.8479751\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6387 - acc: 0.1240\n",
      "Epoch 00001: val_loss improved from inf to 2.34659, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/001-2.3466.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.6386 - acc: 0.1240 - val_loss: 2.3466 - val_acc: 0.2660\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0488 - acc: 0.3254\n",
      "Epoch 00002: val_loss improved from 2.34659 to 1.63222, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/002-1.6322.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.0487 - acc: 0.3254 - val_loss: 1.6322 - val_acc: 0.4761\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6700 - acc: 0.4486\n",
      "Epoch 00003: val_loss improved from 1.63222 to 1.42928, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/003-1.4293.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.6700 - acc: 0.4486 - val_loss: 1.4293 - val_acc: 0.5665\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5278 - acc: 0.4959\n",
      "Epoch 00004: val_loss improved from 1.42928 to 1.32099, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/004-1.3210.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.5278 - acc: 0.4959 - val_loss: 1.3210 - val_acc: 0.6019\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4439 - acc: 0.5279\n",
      "Epoch 00005: val_loss improved from 1.32099 to 1.24315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/005-1.2431.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.4440 - acc: 0.5279 - val_loss: 1.2431 - val_acc: 0.6140\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3709 - acc: 0.5498\n",
      "Epoch 00006: val_loss improved from 1.24315 to 1.20493, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/006-1.2049.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3708 - acc: 0.5498 - val_loss: 1.2049 - val_acc: 0.6217\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3122 - acc: 0.5728\n",
      "Epoch 00007: val_loss improved from 1.20493 to 1.12355, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/007-1.1236.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3121 - acc: 0.5728 - val_loss: 1.1236 - val_acc: 0.6534\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2610 - acc: 0.5948\n",
      "Epoch 00008: val_loss improved from 1.12355 to 1.06667, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/008-1.0667.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2610 - acc: 0.5948 - val_loss: 1.0667 - val_acc: 0.6771\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2014 - acc: 0.6178\n",
      "Epoch 00009: val_loss improved from 1.06667 to 1.04882, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/009-1.0488.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2014 - acc: 0.6178 - val_loss: 1.0488 - val_acc: 0.6809\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1526 - acc: 0.6335\n",
      "Epoch 00010: val_loss did not improve from 1.04882\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1525 - acc: 0.6335 - val_loss: 1.0583 - val_acc: 0.6902\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1019 - acc: 0.6535\n",
      "Epoch 00011: val_loss improved from 1.04882 to 0.91641, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/011-0.9164.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1019 - acc: 0.6536 - val_loss: 0.9164 - val_acc: 0.7291\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0500 - acc: 0.6728\n",
      "Epoch 00012: val_loss did not improve from 0.91641\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0500 - acc: 0.6727 - val_loss: 0.9422 - val_acc: 0.7226\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0020 - acc: 0.6905\n",
      "Epoch 00013: val_loss improved from 0.91641 to 0.85645, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/013-0.8564.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0021 - acc: 0.6904 - val_loss: 0.8564 - val_acc: 0.7491\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9595 - acc: 0.7030\n",
      "Epoch 00014: val_loss improved from 0.85645 to 0.79884, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/014-0.7988.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9595 - acc: 0.7031 - val_loss: 0.7988 - val_acc: 0.7685\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9245 - acc: 0.7177\n",
      "Epoch 00015: val_loss improved from 0.79884 to 0.75531, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/015-0.7553.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9245 - acc: 0.7177 - val_loss: 0.7553 - val_acc: 0.7796\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8931 - acc: 0.7297\n",
      "Epoch 00016: val_loss improved from 0.75531 to 0.74555, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/016-0.7456.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8931 - acc: 0.7297 - val_loss: 0.7456 - val_acc: 0.7834\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8593 - acc: 0.7378\n",
      "Epoch 00017: val_loss improved from 0.74555 to 0.71556, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/017-0.7156.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8593 - acc: 0.7378 - val_loss: 0.7156 - val_acc: 0.7973\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7500\n",
      "Epoch 00018: val_loss improved from 0.71556 to 0.69098, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/018-0.6910.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8297 - acc: 0.7500 - val_loss: 0.6910 - val_acc: 0.8046\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7623\n",
      "Epoch 00019: val_loss improved from 0.69098 to 0.68972, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/019-0.6897.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7945 - acc: 0.7623 - val_loss: 0.6897 - val_acc: 0.7990\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7810 - acc: 0.7653\n",
      "Epoch 00020: val_loss improved from 0.68972 to 0.64078, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/020-0.6408.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7810 - acc: 0.7653 - val_loss: 0.6408 - val_acc: 0.8109\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7772\n",
      "Epoch 00021: val_loss improved from 0.64078 to 0.61066, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/021-0.6107.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7528 - acc: 0.7771 - val_loss: 0.6107 - val_acc: 0.8346\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7296 - acc: 0.7821\n",
      "Epoch 00022: val_loss improved from 0.61066 to 0.59071, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/022-0.5907.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7295 - acc: 0.7821 - val_loss: 0.5907 - val_acc: 0.8330\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7106 - acc: 0.7870\n",
      "Epoch 00023: val_loss did not improve from 0.59071\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7106 - acc: 0.7870 - val_loss: 0.6495 - val_acc: 0.8220\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7028 - acc: 0.7922\n",
      "Epoch 00024: val_loss improved from 0.59071 to 0.56278, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/024-0.5628.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7029 - acc: 0.7921 - val_loss: 0.5628 - val_acc: 0.8430\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.7991\n",
      "Epoch 00025: val_loss did not improve from 0.56278\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6709 - acc: 0.7991 - val_loss: 0.5784 - val_acc: 0.8407\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.8023\n",
      "Epoch 00026: val_loss improved from 0.56278 to 0.53584, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/026-0.5358.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6592 - acc: 0.8023 - val_loss: 0.5358 - val_acc: 0.8472\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6421 - acc: 0.8110\n",
      "Epoch 00027: val_loss improved from 0.53584 to 0.52268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/027-0.5227.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6420 - acc: 0.8110 - val_loss: 0.5227 - val_acc: 0.8567\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6303 - acc: 0.8112\n",
      "Epoch 00028: val_loss improved from 0.52268 to 0.52203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/028-0.5220.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6303 - acc: 0.8112 - val_loss: 0.5220 - val_acc: 0.8558\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6188 - acc: 0.8150\n",
      "Epoch 00029: val_loss improved from 0.52203 to 0.49499, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/029-0.4950.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6188 - acc: 0.8150 - val_loss: 0.4950 - val_acc: 0.8605\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6073 - acc: 0.8181\n",
      "Epoch 00030: val_loss did not improve from 0.49499\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6073 - acc: 0.8181 - val_loss: 0.5194 - val_acc: 0.8570\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.8237\n",
      "Epoch 00031: val_loss did not improve from 0.49499\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5956 - acc: 0.8237 - val_loss: 0.4950 - val_acc: 0.8642\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5794 - acc: 0.8279\n",
      "Epoch 00032: val_loss did not improve from 0.49499\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5794 - acc: 0.8279 - val_loss: 0.5194 - val_acc: 0.8614\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5752 - acc: 0.8302\n",
      "Epoch 00033: val_loss improved from 0.49499 to 0.47244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/033-0.4724.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5752 - acc: 0.8302 - val_loss: 0.4724 - val_acc: 0.8726\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8334\n",
      "Epoch 00034: val_loss improved from 0.47244 to 0.46828, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/034-0.4683.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5628 - acc: 0.8334 - val_loss: 0.4683 - val_acc: 0.8642\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8365\n",
      "Epoch 00035: val_loss did not improve from 0.46828\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5498 - acc: 0.8365 - val_loss: 0.4931 - val_acc: 0.8633\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.8417\n",
      "Epoch 00036: val_loss improved from 0.46828 to 0.45417, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/036-0.4542.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5411 - acc: 0.8417 - val_loss: 0.4542 - val_acc: 0.8742\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8402\n",
      "Epoch 00037: val_loss improved from 0.45417 to 0.42912, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/037-0.4291.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5363 - acc: 0.8402 - val_loss: 0.4291 - val_acc: 0.8859\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5286 - acc: 0.8425\n",
      "Epoch 00038: val_loss did not improve from 0.42912\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5286 - acc: 0.8425 - val_loss: 0.4337 - val_acc: 0.8812\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.8432\n",
      "Epoch 00039: val_loss did not improve from 0.42912\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5208 - acc: 0.8432 - val_loss: 0.4481 - val_acc: 0.8845\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8478\n",
      "Epoch 00040: val_loss improved from 0.42912 to 0.42657, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/040-0.4266.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5116 - acc: 0.8478 - val_loss: 0.4266 - val_acc: 0.8877\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8513\n",
      "Epoch 00041: val_loss did not improve from 0.42657\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5012 - acc: 0.8513 - val_loss: 0.4289 - val_acc: 0.8817\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4971 - acc: 0.8497\n",
      "Epoch 00042: val_loss improved from 0.42657 to 0.41050, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/042-0.4105.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4971 - acc: 0.8497 - val_loss: 0.4105 - val_acc: 0.8896\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8534\n",
      "Epoch 00043: val_loss did not improve from 0.41050\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4937 - acc: 0.8534 - val_loss: 0.4400 - val_acc: 0.8826\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8560\n",
      "Epoch 00044: val_loss improved from 0.41050 to 0.39476, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/044-0.3948.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4854 - acc: 0.8560 - val_loss: 0.3948 - val_acc: 0.8924\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8562\n",
      "Epoch 00045: val_loss did not improve from 0.39476\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4775 - acc: 0.8562 - val_loss: 0.3993 - val_acc: 0.8894\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.8587\n",
      "Epoch 00046: val_loss did not improve from 0.39476\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4757 - acc: 0.8588 - val_loss: 0.4311 - val_acc: 0.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4691 - acc: 0.8585\n",
      "Epoch 00047: val_loss improved from 0.39476 to 0.38690, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/047-0.3869.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4690 - acc: 0.8584 - val_loss: 0.3869 - val_acc: 0.8977\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8585\n",
      "Epoch 00048: val_loss improved from 0.38690 to 0.37999, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/048-0.3800.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4707 - acc: 0.8585 - val_loss: 0.3800 - val_acc: 0.8938\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8641\n",
      "Epoch 00049: val_loss did not improve from 0.37999\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4529 - acc: 0.8641 - val_loss: 0.3888 - val_acc: 0.8945\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8618\n",
      "Epoch 00050: val_loss improved from 0.37999 to 0.37746, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/050-0.3775.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4634 - acc: 0.8618 - val_loss: 0.3775 - val_acc: 0.8975\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8678\n",
      "Epoch 00051: val_loss did not improve from 0.37746\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4456 - acc: 0.8677 - val_loss: 0.3939 - val_acc: 0.8910\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8646\n",
      "Epoch 00052: val_loss did not improve from 0.37746\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4407 - acc: 0.8646 - val_loss: 0.3815 - val_acc: 0.9024\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8696\n",
      "Epoch 00053: val_loss improved from 0.37746 to 0.37645, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/053-0.3765.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4340 - acc: 0.8696 - val_loss: 0.3765 - val_acc: 0.8975\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8696\n",
      "Epoch 00054: val_loss did not improve from 0.37645\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4315 - acc: 0.8696 - val_loss: 0.4080 - val_acc: 0.8963\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8696\n",
      "Epoch 00055: val_loss improved from 0.37645 to 0.37478, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/055-0.3748.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4295 - acc: 0.8696 - val_loss: 0.3748 - val_acc: 0.8968\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8696\n",
      "Epoch 00056: val_loss improved from 0.37478 to 0.36741, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/056-0.3674.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4286 - acc: 0.8696 - val_loss: 0.3674 - val_acc: 0.9012\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8731\n",
      "Epoch 00057: val_loss improved from 0.36741 to 0.35612, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/057-0.3561.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4241 - acc: 0.8731 - val_loss: 0.3561 - val_acc: 0.9029\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8748\n",
      "Epoch 00058: val_loss did not improve from 0.35612\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4155 - acc: 0.8748 - val_loss: 0.3711 - val_acc: 0.9022\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8768\n",
      "Epoch 00059: val_loss did not improve from 0.35612\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4094 - acc: 0.8768 - val_loss: 0.3906 - val_acc: 0.8959\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8752\n",
      "Epoch 00060: val_loss did not improve from 0.35612\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4139 - acc: 0.8752 - val_loss: 0.3576 - val_acc: 0.9017\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8748\n",
      "Epoch 00061: val_loss did not improve from 0.35612\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4073 - acc: 0.8748 - val_loss: 0.3623 - val_acc: 0.8970\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8788\n",
      "Epoch 00062: val_loss improved from 0.35612 to 0.34853, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/062-0.3485.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4038 - acc: 0.8788 - val_loss: 0.3485 - val_acc: 0.9068\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8795\n",
      "Epoch 00063: val_loss did not improve from 0.34853\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3982 - acc: 0.8795 - val_loss: 0.3543 - val_acc: 0.9036\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8788\n",
      "Epoch 00064: val_loss did not improve from 0.34853\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3949 - acc: 0.8788 - val_loss: 0.3555 - val_acc: 0.9059\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8822\n",
      "Epoch 00065: val_loss did not improve from 0.34853\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3855 - acc: 0.8821 - val_loss: 0.3493 - val_acc: 0.8998\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8808\n",
      "Epoch 00066: val_loss did not improve from 0.34853\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3925 - acc: 0.8808 - val_loss: 0.3714 - val_acc: 0.8966\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8825\n",
      "Epoch 00067: val_loss did not improve from 0.34853\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3862 - acc: 0.8825 - val_loss: 0.3514 - val_acc: 0.9071\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8859\n",
      "Epoch 00068: val_loss improved from 0.34853 to 0.33359, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/068-0.3336.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3773 - acc: 0.8859 - val_loss: 0.3336 - val_acc: 0.9122\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8865\n",
      "Epoch 00069: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3732 - acc: 0.8865 - val_loss: 0.3500 - val_acc: 0.9050\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8839\n",
      "Epoch 00070: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3766 - acc: 0.8839 - val_loss: 0.3463 - val_acc: 0.9099\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8867\n",
      "Epoch 00071: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3736 - acc: 0.8867 - val_loss: 0.3437 - val_acc: 0.9066\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8857\n",
      "Epoch 00072: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3730 - acc: 0.8857 - val_loss: 0.3346 - val_acc: 0.9096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8864\n",
      "Epoch 00073: val_loss did not improve from 0.33359\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3714 - acc: 0.8864 - val_loss: 0.3458 - val_acc: 0.9096\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8890\n",
      "Epoch 00074: val_loss improved from 0.33359 to 0.33113, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/074-0.3311.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3610 - acc: 0.8890 - val_loss: 0.3311 - val_acc: 0.9143\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8890\n",
      "Epoch 00075: val_loss did not improve from 0.33113\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3642 - acc: 0.8890 - val_loss: 0.3354 - val_acc: 0.9068\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8909\n",
      "Epoch 00076: val_loss improved from 0.33113 to 0.32496, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/076-0.3250.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3599 - acc: 0.8909 - val_loss: 0.3250 - val_acc: 0.9124\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8926\n",
      "Epoch 00077: val_loss did not improve from 0.32496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3509 - acc: 0.8925 - val_loss: 0.3251 - val_acc: 0.9101\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8893\n",
      "Epoch 00078: val_loss did not improve from 0.32496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3538 - acc: 0.8893 - val_loss: 0.3370 - val_acc: 0.9147\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8934\n",
      "Epoch 00079: val_loss did not improve from 0.32496\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3480 - acc: 0.8934 - val_loss: 0.3327 - val_acc: 0.9089\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8929\n",
      "Epoch 00080: val_loss improved from 0.32496 to 0.32406, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/080-0.3241.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3477 - acc: 0.8929 - val_loss: 0.3241 - val_acc: 0.9168\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8927\n",
      "Epoch 00081: val_loss improved from 0.32406 to 0.31230, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/081-0.3123.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3480 - acc: 0.8927 - val_loss: 0.3123 - val_acc: 0.9154\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8959\n",
      "Epoch 00082: val_loss did not improve from 0.31230\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3396 - acc: 0.8959 - val_loss: 0.3197 - val_acc: 0.9099\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8960\n",
      "Epoch 00083: val_loss did not improve from 0.31230\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3379 - acc: 0.8960 - val_loss: 0.3398 - val_acc: 0.9157\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8974\n",
      "Epoch 00084: val_loss improved from 0.31230 to 0.31199, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/084-0.3120.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3359 - acc: 0.8974 - val_loss: 0.3120 - val_acc: 0.9126\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.8968\n",
      "Epoch 00085: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3307 - acc: 0.8967 - val_loss: 0.3220 - val_acc: 0.9103\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8952\n",
      "Epoch 00086: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3378 - acc: 0.8952 - val_loss: 0.3214 - val_acc: 0.9119\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8952\n",
      "Epoch 00087: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3329 - acc: 0.8953 - val_loss: 0.3260 - val_acc: 0.9150\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8977\n",
      "Epoch 00088: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3304 - acc: 0.8977 - val_loss: 0.3313 - val_acc: 0.9201\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8989\n",
      "Epoch 00089: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3254 - acc: 0.8988 - val_loss: 0.3266 - val_acc: 0.9140\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8987\n",
      "Epoch 00090: val_loss did not improve from 0.31199\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3249 - acc: 0.8987 - val_loss: 0.3277 - val_acc: 0.9133\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8994\n",
      "Epoch 00091: val_loss improved from 0.31199 to 0.31101, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/091-0.3110.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3199 - acc: 0.8994 - val_loss: 0.3110 - val_acc: 0.9164\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9026\n",
      "Epoch 00092: val_loss did not improve from 0.31101\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3132 - acc: 0.9026 - val_loss: 0.3285 - val_acc: 0.9168\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9001\n",
      "Epoch 00093: val_loss improved from 0.31101 to 0.30522, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/093-0.3052.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3222 - acc: 0.9001 - val_loss: 0.3052 - val_acc: 0.9189\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9016\n",
      "Epoch 00094: val_loss did not improve from 0.30522\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3177 - acc: 0.9016 - val_loss: 0.3111 - val_acc: 0.9173\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.9028\n",
      "Epoch 00095: val_loss did not improve from 0.30522\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3104 - acc: 0.9028 - val_loss: 0.3335 - val_acc: 0.9131\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9010\n",
      "Epoch 00096: val_loss improved from 0.30522 to 0.30203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/096-0.3020.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3143 - acc: 0.9010 - val_loss: 0.3020 - val_acc: 0.9201\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9034\n",
      "Epoch 00097: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3128 - acc: 0.9034 - val_loss: 0.3031 - val_acc: 0.9175\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9024\n",
      "Epoch 00098: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3077 - acc: 0.9024 - val_loss: 0.3258 - val_acc: 0.9131\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9052\n",
      "Epoch 00099: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3018 - acc: 0.9053 - val_loss: 0.3043 - val_acc: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9047\n",
      "Epoch 00100: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3057 - acc: 0.9047 - val_loss: 0.3036 - val_acc: 0.9199\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9058\n",
      "Epoch 00101: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2998 - acc: 0.9058 - val_loss: 0.3147 - val_acc: 0.9171\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9062\n",
      "Epoch 00102: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3004 - acc: 0.9062 - val_loss: 0.3054 - val_acc: 0.9189\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9054\n",
      "Epoch 00103: val_loss did not improve from 0.30203\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3017 - acc: 0.9053 - val_loss: 0.3034 - val_acc: 0.9206\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9061\n",
      "Epoch 00104: val_loss improved from 0.30203 to 0.29664, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/104-0.2966.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3018 - acc: 0.9062 - val_loss: 0.2966 - val_acc: 0.9238\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9082\n",
      "Epoch 00105: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2973 - acc: 0.9082 - val_loss: 0.2989 - val_acc: 0.9173\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9078\n",
      "Epoch 00106: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2907 - acc: 0.9078 - val_loss: 0.3081 - val_acc: 0.9199\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9091\n",
      "Epoch 00107: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2897 - acc: 0.9091 - val_loss: 0.3009 - val_acc: 0.9236\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9067\n",
      "Epoch 00108: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2948 - acc: 0.9067 - val_loss: 0.3025 - val_acc: 0.9262\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9085\n",
      "Epoch 00109: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2934 - acc: 0.9085 - val_loss: 0.3153 - val_acc: 0.9203\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9107\n",
      "Epoch 00110: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2905 - acc: 0.9106 - val_loss: 0.3037 - val_acc: 0.9182\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9099\n",
      "Epoch 00111: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2845 - acc: 0.9100 - val_loss: 0.2974 - val_acc: 0.9217\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9097\n",
      "Epoch 00112: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2841 - acc: 0.9097 - val_loss: 0.3140 - val_acc: 0.9231\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9090\n",
      "Epoch 00113: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2894 - acc: 0.9090 - val_loss: 0.2972 - val_acc: 0.9208\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9119\n",
      "Epoch 00114: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2807 - acc: 0.9119 - val_loss: 0.3022 - val_acc: 0.9231\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9095\n",
      "Epoch 00115: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2849 - acc: 0.9095 - val_loss: 0.3195 - val_acc: 0.9210\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9141\n",
      "Epoch 00116: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2749 - acc: 0.9140 - val_loss: 0.2974 - val_acc: 0.9224\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9107\n",
      "Epoch 00117: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2807 - acc: 0.9107 - val_loss: 0.3007 - val_acc: 0.9231\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9119\n",
      "Epoch 00118: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2776 - acc: 0.9119 - val_loss: 0.2977 - val_acc: 0.9250\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9137\n",
      "Epoch 00119: val_loss did not improve from 0.29664\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2744 - acc: 0.9138 - val_loss: 0.3097 - val_acc: 0.9252\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9108\n",
      "Epoch 00120: val_loss improved from 0.29664 to 0.29486, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/120-0.2949.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2804 - acc: 0.9109 - val_loss: 0.2949 - val_acc: 0.9231\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9130\n",
      "Epoch 00121: val_loss improved from 0.29486 to 0.29223, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/121-0.2922.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2746 - acc: 0.9130 - val_loss: 0.2922 - val_acc: 0.9266\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9144\n",
      "Epoch 00122: val_loss did not improve from 0.29223\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2698 - acc: 0.9144 - val_loss: 0.2950 - val_acc: 0.9245\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9155\n",
      "Epoch 00123: val_loss improved from 0.29223 to 0.28463, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/123-0.2846.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2677 - acc: 0.9155 - val_loss: 0.2846 - val_acc: 0.9273\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9129\n",
      "Epoch 00124: val_loss did not improve from 0.28463\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2754 - acc: 0.9129 - val_loss: 0.2986 - val_acc: 0.9283\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9148\n",
      "Epoch 00125: val_loss improved from 0.28463 to 0.27956, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/125-0.2796.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2639 - acc: 0.9148 - val_loss: 0.2796 - val_acc: 0.9294\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.9143\n",
      "Epoch 00126: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2665 - acc: 0.9143 - val_loss: 0.2900 - val_acc: 0.9227\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9172\n",
      "Epoch 00127: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2637 - acc: 0.9172 - val_loss: 0.2971 - val_acc: 0.9257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9177\n",
      "Epoch 00128: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2572 - acc: 0.9178 - val_loss: 0.2934 - val_acc: 0.9278\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9176\n",
      "Epoch 00129: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2604 - acc: 0.9176 - val_loss: 0.3011 - val_acc: 0.9262\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9170\n",
      "Epoch 00130: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2630 - acc: 0.9170 - val_loss: 0.2930 - val_acc: 0.9264\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9174\n",
      "Epoch 00131: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2609 - acc: 0.9174 - val_loss: 0.2922 - val_acc: 0.9290\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9176\n",
      "Epoch 00132: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2586 - acc: 0.9176 - val_loss: 0.3031 - val_acc: 0.9278\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9158\n",
      "Epoch 00133: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2600 - acc: 0.9159 - val_loss: 0.2978 - val_acc: 0.9285\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9191\n",
      "Epoch 00134: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2542 - acc: 0.9191 - val_loss: 0.2890 - val_acc: 0.9269\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9200\n",
      "Epoch 00135: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2559 - acc: 0.9200 - val_loss: 0.2988 - val_acc: 0.9250\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9177\n",
      "Epoch 00136: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2573 - acc: 0.9177 - val_loss: 0.2819 - val_acc: 0.9276\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9180\n",
      "Epoch 00137: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2605 - acc: 0.9180 - val_loss: 0.3058 - val_acc: 0.9243\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9185\n",
      "Epoch 00138: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2539 - acc: 0.9185 - val_loss: 0.3103 - val_acc: 0.9264\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9196\n",
      "Epoch 00139: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2516 - acc: 0.9196 - val_loss: 0.2857 - val_acc: 0.9269\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9225\n",
      "Epoch 00140: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2453 - acc: 0.9225 - val_loss: 0.2929 - val_acc: 0.9259\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9199\n",
      "Epoch 00141: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2491 - acc: 0.9199 - val_loss: 0.2934 - val_acc: 0.9315\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9205\n",
      "Epoch 00142: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2480 - acc: 0.9205 - val_loss: 0.3054 - val_acc: 0.9217\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9226\n",
      "Epoch 00143: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2413 - acc: 0.9226 - val_loss: 0.2892 - val_acc: 0.9313\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9198\n",
      "Epoch 00144: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2503 - acc: 0.9198 - val_loss: 0.2850 - val_acc: 0.9294\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9233\n",
      "Epoch 00145: val_loss did not improve from 0.27956\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2407 - acc: 0.9234 - val_loss: 0.3040 - val_acc: 0.9287\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9226\n",
      "Epoch 00146: val_loss improved from 0.27956 to 0.27774, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/146-0.2777.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2423 - acc: 0.9226 - val_loss: 0.2777 - val_acc: 0.9304\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9230\n",
      "Epoch 00147: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2400 - acc: 0.9231 - val_loss: 0.3041 - val_acc: 0.9248\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9237\n",
      "Epoch 00148: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2365 - acc: 0.9236 - val_loss: 0.2935 - val_acc: 0.9278\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9222\n",
      "Epoch 00149: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2439 - acc: 0.9222 - val_loss: 0.3016 - val_acc: 0.9287\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9236\n",
      "Epoch 00150: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2340 - acc: 0.9236 - val_loss: 0.2984 - val_acc: 0.9250\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9229\n",
      "Epoch 00151: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2381 - acc: 0.9229 - val_loss: 0.2872 - val_acc: 0.9273\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9264\n",
      "Epoch 00152: val_loss did not improve from 0.27774\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2323 - acc: 0.9264 - val_loss: 0.2782 - val_acc: 0.9280\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9249\n",
      "Epoch 00153: val_loss improved from 0.27774 to 0.27610, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/153-0.2761.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2303 - acc: 0.9249 - val_loss: 0.2761 - val_acc: 0.9327\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9245\n",
      "Epoch 00154: val_loss did not improve from 0.27610\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2335 - acc: 0.9244 - val_loss: 0.2883 - val_acc: 0.9285\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9273\n",
      "Epoch 00155: val_loss did not improve from 0.27610\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2312 - acc: 0.9273 - val_loss: 0.2960 - val_acc: 0.9301\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9248\n",
      "Epoch 00156: val_loss did not improve from 0.27610\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2341 - acc: 0.9247 - val_loss: 0.3148 - val_acc: 0.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9243\n",
      "Epoch 00157: val_loss improved from 0.27610 to 0.27399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/157-0.2740.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2362 - acc: 0.9243 - val_loss: 0.2740 - val_acc: 0.9299\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9285\n",
      "Epoch 00158: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2210 - acc: 0.9284 - val_loss: 0.2746 - val_acc: 0.9301\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9248\n",
      "Epoch 00159: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2302 - acc: 0.9248 - val_loss: 0.2845 - val_acc: 0.9285\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9266\n",
      "Epoch 00160: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2297 - acc: 0.9266 - val_loss: 0.2759 - val_acc: 0.9324\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9278\n",
      "Epoch 00161: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2237 - acc: 0.9278 - val_loss: 0.2801 - val_acc: 0.9322\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9271\n",
      "Epoch 00162: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2266 - acc: 0.9272 - val_loss: 0.2996 - val_acc: 0.9317\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9285\n",
      "Epoch 00163: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2246 - acc: 0.9285 - val_loss: 0.2766 - val_acc: 0.9304\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9264\n",
      "Epoch 00164: val_loss did not improve from 0.27399\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2243 - acc: 0.9264 - val_loss: 0.2886 - val_acc: 0.9341\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9281\n",
      "Epoch 00165: val_loss improved from 0.27399 to 0.27336, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/165-0.2734.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2215 - acc: 0.9281 - val_loss: 0.2734 - val_acc: 0.9280\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9262\n",
      "Epoch 00166: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2259 - acc: 0.9262 - val_loss: 0.2745 - val_acc: 0.9327\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9279\n",
      "Epoch 00167: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2239 - acc: 0.9279 - val_loss: 0.2931 - val_acc: 0.9276\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9272\n",
      "Epoch 00168: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2196 - acc: 0.9272 - val_loss: 0.2866 - val_acc: 0.9264\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9282\n",
      "Epoch 00169: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2191 - acc: 0.9282 - val_loss: 0.2937 - val_acc: 0.9317\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9296\n",
      "Epoch 00170: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2158 - acc: 0.9296 - val_loss: 0.2794 - val_acc: 0.9336\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9283\n",
      "Epoch 00171: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2204 - acc: 0.9284 - val_loss: 0.2829 - val_acc: 0.9327\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9289\n",
      "Epoch 00172: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2174 - acc: 0.9289 - val_loss: 0.2801 - val_acc: 0.9345\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9315\n",
      "Epoch 00173: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2127 - acc: 0.9315 - val_loss: 0.2775 - val_acc: 0.9364\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9326\n",
      "Epoch 00174: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2133 - acc: 0.9326 - val_loss: 0.2916 - val_acc: 0.9306\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9287\n",
      "Epoch 00175: val_loss improved from 0.27336 to 0.27271, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/175-0.2727.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2180 - acc: 0.9288 - val_loss: 0.2727 - val_acc: 0.9320\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9320\n",
      "Epoch 00176: val_loss did not improve from 0.27271\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2147 - acc: 0.9320 - val_loss: 0.2823 - val_acc: 0.9331\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9323\n",
      "Epoch 00177: val_loss did not improve from 0.27271\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2083 - acc: 0.9323 - val_loss: 0.2776 - val_acc: 0.9329\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9318\n",
      "Epoch 00178: val_loss did not improve from 0.27271\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2117 - acc: 0.9318 - val_loss: 0.2904 - val_acc: 0.9278\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9335\n",
      "Epoch 00179: val_loss did not improve from 0.27271\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2058 - acc: 0.9335 - val_loss: 0.2892 - val_acc: 0.9324\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9327\n",
      "Epoch 00180: val_loss did not improve from 0.27271\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2080 - acc: 0.9328 - val_loss: 0.2974 - val_acc: 0.9306\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9324\n",
      "Epoch 00181: val_loss improved from 0.27271 to 0.26846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/181-0.2685.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2063 - acc: 0.9324 - val_loss: 0.2685 - val_acc: 0.9320\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9321\n",
      "Epoch 00182: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2091 - acc: 0.9321 - val_loss: 0.2848 - val_acc: 0.9299\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9325\n",
      "Epoch 00183: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2096 - acc: 0.9325 - val_loss: 0.2839 - val_acc: 0.9324\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9323\n",
      "Epoch 00184: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2089 - acc: 0.9323 - val_loss: 0.2917 - val_acc: 0.9299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9318\n",
      "Epoch 00185: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2087 - acc: 0.9318 - val_loss: 0.2716 - val_acc: 0.9315\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9326\n",
      "Epoch 00186: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2017 - acc: 0.9326 - val_loss: 0.2842 - val_acc: 0.9324\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9356\n",
      "Epoch 00187: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1960 - acc: 0.9356 - val_loss: 0.2860 - val_acc: 0.9311\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9326\n",
      "Epoch 00188: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2049 - acc: 0.9326 - val_loss: 0.2834 - val_acc: 0.9322\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9346\n",
      "Epoch 00189: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2047 - acc: 0.9346 - val_loss: 0.2734 - val_acc: 0.9320\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9343\n",
      "Epoch 00190: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2031 - acc: 0.9344 - val_loss: 0.2886 - val_acc: 0.9322\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9340\n",
      "Epoch 00191: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2004 - acc: 0.9340 - val_loss: 0.2822 - val_acc: 0.9345\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9335\n",
      "Epoch 00192: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2028 - acc: 0.9335 - val_loss: 0.2940 - val_acc: 0.9336\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9339\n",
      "Epoch 00193: val_loss did not improve from 0.26846\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2046 - acc: 0.9339 - val_loss: 0.2946 - val_acc: 0.9297\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9324\n",
      "Epoch 00194: val_loss improved from 0.26846 to 0.26471, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/194-0.2647.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2071 - acc: 0.9324 - val_loss: 0.2647 - val_acc: 0.9355\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9336\n",
      "Epoch 00195: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2006 - acc: 0.9336 - val_loss: 0.2770 - val_acc: 0.9343\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9353\n",
      "Epoch 00196: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2011 - acc: 0.9353 - val_loss: 0.2839 - val_acc: 0.9329\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9330\n",
      "Epoch 00197: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2032 - acc: 0.9330 - val_loss: 0.2829 - val_acc: 0.9364\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9356\n",
      "Epoch 00198: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1982 - acc: 0.9356 - val_loss: 0.2711 - val_acc: 0.9308\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9350\n",
      "Epoch 00199: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1952 - acc: 0.9350 - val_loss: 0.2852 - val_acc: 0.9311\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9373\n",
      "Epoch 00200: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1933 - acc: 0.9373 - val_loss: 0.2789 - val_acc: 0.9352\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9378\n",
      "Epoch 00201: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1940 - acc: 0.9378 - val_loss: 0.2972 - val_acc: 0.9301\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9358\n",
      "Epoch 00202: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1928 - acc: 0.9358 - val_loss: 0.2932 - val_acc: 0.9331\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9353\n",
      "Epoch 00203: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1943 - acc: 0.9353 - val_loss: 0.2771 - val_acc: 0.9311\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9385\n",
      "Epoch 00204: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1892 - acc: 0.9384 - val_loss: 0.2732 - val_acc: 0.9364\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9350\n",
      "Epoch 00205: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1914 - acc: 0.9350 - val_loss: 0.2944 - val_acc: 0.9304\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9362\n",
      "Epoch 00206: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1935 - acc: 0.9362 - val_loss: 0.2882 - val_acc: 0.9308\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9383\n",
      "Epoch 00207: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1904 - acc: 0.9384 - val_loss: 0.2759 - val_acc: 0.9359\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9359\n",
      "Epoch 00208: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1962 - acc: 0.9359 - val_loss: 0.2711 - val_acc: 0.9322\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9363\n",
      "Epoch 00209: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1943 - acc: 0.9363 - val_loss: 0.2681 - val_acc: 0.9317\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9377\n",
      "Epoch 00210: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1920 - acc: 0.9377 - val_loss: 0.2803 - val_acc: 0.9297\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9388\n",
      "Epoch 00211: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1851 - acc: 0.9388 - val_loss: 0.2789 - val_acc: 0.9341\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9386\n",
      "Epoch 00212: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1875 - acc: 0.9386 - val_loss: 0.2823 - val_acc: 0.9320\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9374\n",
      "Epoch 00213: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1875 - acc: 0.9374 - val_loss: 0.2819 - val_acc: 0.9315\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9393\n",
      "Epoch 00214: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1895 - acc: 0.9393 - val_loss: 0.2723 - val_acc: 0.9336\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9404\n",
      "Epoch 00215: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1866 - acc: 0.9404 - val_loss: 0.2741 - val_acc: 0.9336\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9385\n",
      "Epoch 00216: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1875 - acc: 0.9385 - val_loss: 0.2801 - val_acc: 0.9376\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9395\n",
      "Epoch 00217: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1843 - acc: 0.9395 - val_loss: 0.2796 - val_acc: 0.9322\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9402\n",
      "Epoch 00218: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1806 - acc: 0.9402 - val_loss: 0.2748 - val_acc: 0.9350\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9379\n",
      "Epoch 00219: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1896 - acc: 0.9378 - val_loss: 0.2696 - val_acc: 0.9341\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9382\n",
      "Epoch 00220: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1850 - acc: 0.9382 - val_loss: 0.2819 - val_acc: 0.9322\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9400\n",
      "Epoch 00221: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1829 - acc: 0.9400 - val_loss: 0.2714 - val_acc: 0.9341\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9391\n",
      "Epoch 00222: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1835 - acc: 0.9391 - val_loss: 0.2950 - val_acc: 0.9301\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9405\n",
      "Epoch 00223: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1772 - acc: 0.9405 - val_loss: 0.2777 - val_acc: 0.9348\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9414\n",
      "Epoch 00224: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1817 - acc: 0.9414 - val_loss: 0.2748 - val_acc: 0.9338\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9412\n",
      "Epoch 00225: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1824 - acc: 0.9412 - val_loss: 0.2834 - val_acc: 0.9355\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9415\n",
      "Epoch 00226: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1774 - acc: 0.9414 - val_loss: 0.2649 - val_acc: 0.9336\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9418\n",
      "Epoch 00227: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1807 - acc: 0.9418 - val_loss: 0.2940 - val_acc: 0.9327\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9400\n",
      "Epoch 00228: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1793 - acc: 0.9400 - val_loss: 0.2759 - val_acc: 0.9348\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9418\n",
      "Epoch 00229: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1761 - acc: 0.9418 - val_loss: 0.2727 - val_acc: 0.9329\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9411\n",
      "Epoch 00230: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1756 - acc: 0.9410 - val_loss: 0.2814 - val_acc: 0.9345\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9404\n",
      "Epoch 00231: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1788 - acc: 0.9404 - val_loss: 0.2774 - val_acc: 0.9352\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9401\n",
      "Epoch 00232: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1816 - acc: 0.9401 - val_loss: 0.2758 - val_acc: 0.9359\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9417\n",
      "Epoch 00233: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1790 - acc: 0.9416 - val_loss: 0.2806 - val_acc: 0.9345\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9407\n",
      "Epoch 00234: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1799 - acc: 0.9407 - val_loss: 0.2681 - val_acc: 0.9385\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9426\n",
      "Epoch 00235: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1757 - acc: 0.9426 - val_loss: 0.2666 - val_acc: 0.9359\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9405\n",
      "Epoch 00236: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1747 - acc: 0.9405 - val_loss: 0.2927 - val_acc: 0.9315\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9424\n",
      "Epoch 00237: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1722 - acc: 0.9424 - val_loss: 0.2854 - val_acc: 0.9362\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9423\n",
      "Epoch 00238: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1756 - acc: 0.9423 - val_loss: 0.2818 - val_acc: 0.9343\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9454\n",
      "Epoch 00239: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1702 - acc: 0.9453 - val_loss: 0.2746 - val_acc: 0.9320\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9405\n",
      "Epoch 00240: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1791 - acc: 0.9405 - val_loss: 0.2927 - val_acc: 0.9336\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9435\n",
      "Epoch 00241: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1705 - acc: 0.9434 - val_loss: 0.2751 - val_acc: 0.9348\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9435\n",
      "Epoch 00242: val_loss did not improve from 0.26471\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1737 - acc: 0.9435 - val_loss: 0.2783 - val_acc: 0.9350\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9435\n",
      "Epoch 00243: val_loss improved from 0.26471 to 0.26445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/243-0.2644.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1697 - acc: 0.9435 - val_loss: 0.2644 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9446\n",
      "Epoch 00244: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1684 - acc: 0.9446 - val_loss: 0.2791 - val_acc: 0.9350\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9435\n",
      "Epoch 00245: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1725 - acc: 0.9434 - val_loss: 0.2991 - val_acc: 0.9313\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9434\n",
      "Epoch 00246: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1711 - acc: 0.9434 - val_loss: 0.2778 - val_acc: 0.9348\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9463\n",
      "Epoch 00247: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1682 - acc: 0.9463 - val_loss: 0.2697 - val_acc: 0.9380\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9424\n",
      "Epoch 00248: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1701 - acc: 0.9424 - val_loss: 0.2692 - val_acc: 0.9394\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9434\n",
      "Epoch 00249: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1694 - acc: 0.9434 - val_loss: 0.2751 - val_acc: 0.9366\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9446\n",
      "Epoch 00250: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1692 - acc: 0.9446 - val_loss: 0.2847 - val_acc: 0.9341\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9428\n",
      "Epoch 00251: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1685 - acc: 0.9428 - val_loss: 0.2757 - val_acc: 0.9348\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9450\n",
      "Epoch 00252: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1665 - acc: 0.9450 - val_loss: 0.2730 - val_acc: 0.9331\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9460\n",
      "Epoch 00253: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1636 - acc: 0.9460 - val_loss: 0.2922 - val_acc: 0.9348\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9461\n",
      "Epoch 00254: val_loss did not improve from 0.26445\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1638 - acc: 0.9461 - val_loss: 0.2722 - val_acc: 0.9355\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9455\n",
      "Epoch 00255: val_loss improved from 0.26445 to 0.26072, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/255-0.2607.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1635 - acc: 0.9455 - val_loss: 0.2607 - val_acc: 0.9373\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9457\n",
      "Epoch 00256: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1643 - acc: 0.9457 - val_loss: 0.2829 - val_acc: 0.9352\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9456\n",
      "Epoch 00257: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1636 - acc: 0.9456 - val_loss: 0.2748 - val_acc: 0.9359\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9454\n",
      "Epoch 00258: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1643 - acc: 0.9454 - val_loss: 0.2719 - val_acc: 0.9341\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9454\n",
      "Epoch 00259: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1627 - acc: 0.9454 - val_loss: 0.2933 - val_acc: 0.9336\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9468\n",
      "Epoch 00260: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1609 - acc: 0.9468 - val_loss: 0.2677 - val_acc: 0.9352\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9464\n",
      "Epoch 00261: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1622 - acc: 0.9464 - val_loss: 0.2707 - val_acc: 0.9334\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9465\n",
      "Epoch 00262: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1643 - acc: 0.9466 - val_loss: 0.2695 - val_acc: 0.9366\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9482\n",
      "Epoch 00263: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1561 - acc: 0.9482 - val_loss: 0.2640 - val_acc: 0.9343\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9474\n",
      "Epoch 00264: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1572 - acc: 0.9474 - val_loss: 0.2917 - val_acc: 0.9357\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9469\n",
      "Epoch 00265: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1632 - acc: 0.9469 - val_loss: 0.2855 - val_acc: 0.9378\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9468\n",
      "Epoch 00266: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1633 - acc: 0.9469 - val_loss: 0.2853 - val_acc: 0.9373\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9477\n",
      "Epoch 00267: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1576 - acc: 0.9477 - val_loss: 0.2899 - val_acc: 0.9366\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9501\n",
      "Epoch 00268: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1545 - acc: 0.9501 - val_loss: 0.2626 - val_acc: 0.9404\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9463\n",
      "Epoch 00269: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1606 - acc: 0.9463 - val_loss: 0.2975 - val_acc: 0.9364\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9482\n",
      "Epoch 00270: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1554 - acc: 0.9482 - val_loss: 0.2789 - val_acc: 0.9366\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9467\n",
      "Epoch 00271: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1590 - acc: 0.9467 - val_loss: 0.2721 - val_acc: 0.9336\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9472\n",
      "Epoch 00272: val_loss did not improve from 0.26072\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1541 - acc: 0.9472 - val_loss: 0.2789 - val_acc: 0.9373\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9474\n",
      "Epoch 00273: val_loss improved from 0.26072 to 0.25716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv_checkpoint/273-0.2572.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1591 - acc: 0.9474 - val_loss: 0.2572 - val_acc: 0.9394\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9464\n",
      "Epoch 00274: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1591 - acc: 0.9464 - val_loss: 0.3014 - val_acc: 0.9334\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9472\n",
      "Epoch 00275: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1595 - acc: 0.9472 - val_loss: 0.2686 - val_acc: 0.9390\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9493\n",
      "Epoch 00276: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1535 - acc: 0.9493 - val_loss: 0.2724 - val_acc: 0.9378\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9480\n",
      "Epoch 00277: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1545 - acc: 0.9480 - val_loss: 0.2775 - val_acc: 0.9387\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9474\n",
      "Epoch 00278: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1579 - acc: 0.9474 - val_loss: 0.2809 - val_acc: 0.9343\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9486\n",
      "Epoch 00279: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1530 - acc: 0.9486 - val_loss: 0.2935 - val_acc: 0.9362\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9477\n",
      "Epoch 00280: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1542 - acc: 0.9477 - val_loss: 0.2778 - val_acc: 0.9362\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9481\n",
      "Epoch 00281: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1553 - acc: 0.9481 - val_loss: 0.2865 - val_acc: 0.9390\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9495\n",
      "Epoch 00282: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1567 - acc: 0.9495 - val_loss: 0.2823 - val_acc: 0.9392\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9484\n",
      "Epoch 00283: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1561 - acc: 0.9483 - val_loss: 0.2955 - val_acc: 0.9362\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9492\n",
      "Epoch 00284: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1543 - acc: 0.9492 - val_loss: 0.2677 - val_acc: 0.9359\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9510\n",
      "Epoch 00285: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1512 - acc: 0.9510 - val_loss: 0.2869 - val_acc: 0.9364\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9486\n",
      "Epoch 00286: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1508 - acc: 0.9486 - val_loss: 0.2784 - val_acc: 0.9345\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9496\n",
      "Epoch 00287: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1525 - acc: 0.9497 - val_loss: 0.3054 - val_acc: 0.9324\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9486\n",
      "Epoch 00288: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1487 - acc: 0.9486 - val_loss: 0.2818 - val_acc: 0.9380\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9505\n",
      "Epoch 00289: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1493 - acc: 0.9504 - val_loss: 0.2940 - val_acc: 0.9324\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9495\n",
      "Epoch 00290: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1522 - acc: 0.9495 - val_loss: 0.2824 - val_acc: 0.9376\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9501\n",
      "Epoch 00291: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1510 - acc: 0.9501 - val_loss: 0.2712 - val_acc: 0.9371\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9496\n",
      "Epoch 00292: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1514 - acc: 0.9496 - val_loss: 0.2952 - val_acc: 0.9364\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9485\n",
      "Epoch 00293: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1529 - acc: 0.9485 - val_loss: 0.2644 - val_acc: 0.9380\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9505\n",
      "Epoch 00294: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1502 - acc: 0.9505 - val_loss: 0.2641 - val_acc: 0.9355\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9523\n",
      "Epoch 00295: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1458 - acc: 0.9523 - val_loss: 0.2849 - val_acc: 0.9376\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9514\n",
      "Epoch 00296: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1482 - acc: 0.9514 - val_loss: 0.2726 - val_acc: 0.9322\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9494\n",
      "Epoch 00297: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1491 - acc: 0.9494 - val_loss: 0.2892 - val_acc: 0.9383\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9514\n",
      "Epoch 00298: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1467 - acc: 0.9514 - val_loss: 0.2761 - val_acc: 0.9364\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9525\n",
      "Epoch 00299: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1437 - acc: 0.9525 - val_loss: 0.2877 - val_acc: 0.9345\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9514\n",
      "Epoch 00300: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1475 - acc: 0.9514 - val_loss: 0.2654 - val_acc: 0.9387\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9522\n",
      "Epoch 00301: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1430 - acc: 0.9522 - val_loss: 0.2717 - val_acc: 0.9399\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9491\n",
      "Epoch 00302: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1524 - acc: 0.9491 - val_loss: 0.2733 - val_acc: 0.9390\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9511\n",
      "Epoch 00303: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1470 - acc: 0.9511 - val_loss: 0.2790 - val_acc: 0.9373\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9513\n",
      "Epoch 00304: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1446 - acc: 0.9513 - val_loss: 0.2825 - val_acc: 0.9399\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9529\n",
      "Epoch 00305: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1396 - acc: 0.9529 - val_loss: 0.2782 - val_acc: 0.9411\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9520\n",
      "Epoch 00306: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1447 - acc: 0.9520 - val_loss: 0.2758 - val_acc: 0.9383\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9524\n",
      "Epoch 00307: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1436 - acc: 0.9525 - val_loss: 0.2703 - val_acc: 0.9366\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9526\n",
      "Epoch 00308: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1444 - acc: 0.9526 - val_loss: 0.2749 - val_acc: 0.9350\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9532\n",
      "Epoch 00309: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1425 - acc: 0.9532 - val_loss: 0.2639 - val_acc: 0.9385\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9524\n",
      "Epoch 00310: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1427 - acc: 0.9524 - val_loss: 0.2913 - val_acc: 0.9357\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9526\n",
      "Epoch 00311: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1434 - acc: 0.9525 - val_loss: 0.2826 - val_acc: 0.9366\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9525\n",
      "Epoch 00312: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1420 - acc: 0.9525 - val_loss: 0.2913 - val_acc: 0.9341\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9536\n",
      "Epoch 00313: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1373 - acc: 0.9536 - val_loss: 0.2870 - val_acc: 0.9355\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9527\n",
      "Epoch 00314: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1413 - acc: 0.9528 - val_loss: 0.2689 - val_acc: 0.9397\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9535\n",
      "Epoch 00315: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1393 - acc: 0.9535 - val_loss: 0.2727 - val_acc: 0.9394\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9535\n",
      "Epoch 00316: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1373 - acc: 0.9534 - val_loss: 0.2818 - val_acc: 0.9350\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9515\n",
      "Epoch 00317: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1422 - acc: 0.9515 - val_loss: 0.2733 - val_acc: 0.9390\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9530\n",
      "Epoch 00318: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1396 - acc: 0.9530 - val_loss: 0.2739 - val_acc: 0.9376\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9504\n",
      "Epoch 00319: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1464 - acc: 0.9504 - val_loss: 0.2708 - val_acc: 0.9390\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9537\n",
      "Epoch 00320: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1374 - acc: 0.9537 - val_loss: 0.3056 - val_acc: 0.9334\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9512\n",
      "Epoch 00321: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1482 - acc: 0.9512 - val_loss: 0.2771 - val_acc: 0.9369\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9548\n",
      "Epoch 00322: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1350 - acc: 0.9548 - val_loss: 0.2686 - val_acc: 0.9394\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9539\n",
      "Epoch 00323: val_loss did not improve from 0.25716\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1384 - acc: 0.9539 - val_loss: 0.2708 - val_acc: 0.9397\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmSUzmex7QgIEBFnCvokiAtqquG+I1pXW7ftTW2trpdpaW9t+KdXWutW6L7WKX3CtVq3KJi5ssovskITs60xmySzn98dJwpYAQoYA87xfr3nNzJ17z33uTHKfe8+59xyltUYIIYQAsHR1AEIIIY4ekhSEEEK0kaQghBCijSQFIYQQbSQpCCGEaCNJQQghRBtJCkIIIdpIUhBCCNFGkoIQQog2tq4O4LvKzMzUhYWFXR2GEEIcU5YtW1attc460HzHXFIoLCxk6dKlXR2GEEIcU5RS2w9mPqk+EkII0UaSghBCiDaSFIQQQrQ55toU2hMMBikpKcHv93d1KMcsp9NJQUEBdru9q0MRQnSh4yIplJSUkJSURGFhIUqprg7nmKO1pqamhpKSEnr16tXV4QghutBxUX3k9/vJyMiQhHCIlFJkZGTImZYQ4vhICoAkhMMk358QAo6jpHAg4bCPQKCUSCTY1aEIIcRRK2aSQiTio7m5DK1DnV52fX09TzzxxCEte84551BfX3/Q899///08+OCDh7QuIYQ4kJhJCrs2VXd6yftLCqHQ/pPQ+++/T2pqaqfHJIQQhyJmksKuOvNIp5c9ffp0Nm/ezLBhw7jrrruYN28e48eP54ILLmDgwIEAXHTRRYwcOZKioiKeeuqptmULCwuprq5m27ZtDBgwgBtvvJGioiLOPPNMfD7ffte7YsUKxo4dy5AhQ7j44oupq6sD4JFHHmHgwIEMGTKEK664AoD58+czbNgwhg0bxvDhw3G73Z3+PQghjn3HxSWpu9u48Q48nhX7TNc6TCTixWJxoZT1O5WZmDiMvn0f7vDzGTNmsGbNGlasMOudN28ey5cvZ82aNW2XeD733HOkp6fj8/kYPXo0l156KRkZGXvFvpFXX32Vp59+mssvv5w5c+Zw9dVXd7jea6+9lkcffZQJEyZw33338dvf/paHH36YGTNmsHXrVhwOR1vV1IMPPsjjjz/OuHHj8Hg8OJ3O7/QdCCFiQ8ycKRxpY8aM2eOa/0ceeYShQ4cyduxYiouL2bhx4z7L9OrVi2HDhgEwcuRItm3b1mH5DQ0N1NfXM2HCBACuu+46FixYAMCQIUO46qqr+Oc//4nNZvL+uHHjuPPOO3nkkUeor69vmy6EELs77vYMHR3Rh0IefL71xMf3xWZLiXocCQkJba/nzZvHxx9/zBdffIHL5WLixInt3hPgcDjaXlut1gNWH3XkvffeY8GCBbz77rv84Q9/YPXq1UyfPp1zzz2X999/n3HjxvHhhx/Sv3//QypfCHH8ipkzhdY2Ba07v6E5KSlpv3X0DQ0NpKWl4XK5WL9+PV9++eVhrzMlJYW0tDQWLlwIwMsvv8yECROIRCIUFxczadIk/vSnP9HQ0IDH42Hz5s0MHjyYu+++m9GjR7N+/frDjkEIcfyJ2pmCUqo78BKQg7nk5ymt9d/2mmci8DawtWXSG1rr30Upopbnzk8KGRkZjBs3jkGDBjF58mTOPffcPT4/++yzefLJJxkwYAD9+vVj7NixnbLeF198kVtuuQWv10vv3r15/vnnCYfDXH311TQ0NKC15sc//jGpqan8+te/Zu7cuVgsFoqKipg8eXKnxCCEOL6oaBw5Ayil8oA8rfVypVQSsAy4SGu9brd5JgI/11qfd7Dljho1Su89yM4333zDgAED9rtcOOzD612L09kbuz39O2xJ7DiY71EIcWxSSi3TWo860HxRqz7SWpdprZe3vHYD3wD50VrfgUXvTEEIIY4XR6RNQSlVCAwHvmrn45OVUiuVUv9RShVFMQYgOm0KQghxvIj61UdKqURgDnCH1rpxr4+XAz211h6l1DnAW0Dfdsq4CbgJoEePHocaScuzJAUhhOhIVM8UlFJ2TEJ4RWv9xt6fa60btdaeltfvA3alVGY78z2ltR6ltR6VlZV1qNG0lnaIywshxPEvaklBmfqaZ4FvtNZ/6WCe3Jb5UEqNaYmnJkoRtTxLUhBCiI5Es/poHHANsFop1drvxD1ADwCt9ZPAZcD/KKVCgA+4Qkep0l/aFIQQ4sCilhS01p+x6/C8o3keAx6LVgx7OrrOFBITE/F4PAc9XQghjoSYuaP5aEsKQghxNIqZpLCr6+zOTwrTp0/n8ccfb3vfOhCOx+PhjDPOYMSIEQwePJi33377oMvUWnPXXXcxaNAgBg8ezKxZswAoKyvjtNNOY9iwYQwaNIiFCxcSDoe5/vrr2+b961//2unbKISIDcddh3jccQes2LfrbID4sBuLigOLo93POzRsGDzccdfZU6dO5Y477uDWW28F4PXXX+fDDz/E6XTy5ptvkpycTHV1NWPHjuWCCy44qPGQ33jjDVasWMHKlSuprq5m9OjRnHbaafzrX//irLPO4t577yUcDuP1elmxYgWlpaWsWbMG4DuN5CaEELs7/pLCfkVncPrhw4dTWVnJzp07qaqqIi0tje7duxMMBrnnnntYsGABFouF0tJSKioqyM3NPWCZn332GVdeeSVWq5WcnBwmTJjAkiVLGD16ND/84Q8JBoNcdNFFDBs2jN69e7NlyxZuv/12zj33XM4888yobKcQ4vh3/CWF/RzR+9wrsNvTcDp7dvpqp0yZwuzZsykvL2fq1KkAvPLKK1RVVbFs2TLsdjuFhYXtdpn9XZx22mksWLCA9957j+uvv54777yTa6+9lpUrV/Lhhx/y5JNP8vrrr/Pcc891xmYJIWJMzLQpQGu7QnQamqdOncprr73G7NmzmTJlCmC6zM7OzsZutzN37ly2b99+0OWNHz+eWbNmEQ6HqaqqYsGCBYwZM4bt27eTk5PDjTfeyA033MDy5cuprq4mEolw6aWX8vvf/57ly5dHZRuFEMe/4+9MoSNeL3EVYSLZYYjCSJRFRUW43W7y8/PJy8sD4KqrruL8889n8ODBjBo16jsNanPxxRfzxRdfMHToUJRSzJw5k9zcXF588UX+/Oc/Y7fbSUxM5KWXXqK0tJRp06YRiZjxp//3f/+38zdQCBETotZ1drQcatfZ1NbCli34+6TgTN2neyWBdJ0txPGsy7vOPuq0XvETObaSoBBCHEmxlxTk5jUhhOhQ7CWFY6y6TAghjiRJCkIIIdpIUhBCCNEmBpNCpGvjEEKIo1gMJoXOL7q+vp4nnnjikJY955xzpK8iIcRRI3aSQqsoVB/tLymEQqH9Lvv++++Tmpra6TEJIcShiJ2kEMU2henTp7N582aGDRvGXXfdxbx58xg/fjwXXHABAwcOBOCiiy5i5MiRFBUV8dRTT7UtW1hYSHV1Ndu2bWPAgAHceOONFBUVceaZZ+Lz+fZZ17vvvstJJ53E8OHD+d73vkdFRQUAHo+HadOmMXjwYIYMGcKcOXMA+OCDDxgxYgRDhw7ljDPO6PRtF0IcX467bi467Dk74oCmfkQcCkvcdyvzAD1nM2PGDNasWcOKlhXPmzeP5cuXs2bNGnr16gXAc889R3p6Oj6fj9GjR3PppZeSkZGxRzkbN27k1Vdf5emnn+byyy9nzpw5XH311XvMc+qpp/Lll1+ilOKZZ55h5syZPPTQQzzwwAOkpKSwevVqAOrq6qiqquLGG29kwYIF9OrVi9ra2u+24UKImHPcJYWORafb7I6MGTOmLSEAPPLII7z55psAFBcXs3Hjxn2SQq9evRg2bBgAI0eOZNu2bfuUW1JSwtSpUykrK6O5ubltHR9//DGvvfZa23xpaWm8++67nHbaaW3zpKend+o2CiGOP8ddUujwiD4QhNXf4s+z4swfHvU4EhIS2l7PmzePjz/+mC+++AKXy8XEiRPb7ULb4dg1+I/Vam23+uj222/nzjvv5IILLmDevHncf//9UYlfCBGbpE2hEyQlJeF2uzv8vKGhgbS0NFwuF+vXr+fLL7885HU1NDSQn58PwIsvvtg2/fvf//4eQ4LW1dUxduxYFixYwNatWwGk+kgIcUAxmBQ6v+iMjAzGjRvHoEGDuOuuu/b5/OyzzyYUCjFgwACmT5/O2LFjD3ld999/P1OmTGHkyJFkZma2Tf/Vr35FXV0dgwYNYujQocydO5esrCyeeuopLrnkEoYOHdo2+I8QQnQkdrrODgZh5Ur82Qpnj5FRjPDYJV1nC3H8kq6z9ya9pAohxAHFXlKQnCCEEB2KuaSgNBxrVWZCCHGkxFxSMGcKkhSEEKI9MZUUdqUCSQpCCNGe2EkKYG5qluojIYToUGwlBdRRU32UmJjY1SEIIcQ+YispKNXSA1LXJwUhhDgaRS0pKKW6K6XmKqXWKaXWKqV+0s48Sin1iFJqk1JqlVJqRLTiMSukpfqoc0dfmz59+h5dTNx///08+OCDeDwezjjjDEaMGMHgwYN5++23D1hWR11st9cFdkfdZQshxKGKZod4IeBnWuvlSqkkYJlS6r9a63W7zTMZ6NvyOAn4e8vzIbvjgztYUd5e39mAx4O2anAmoNTB58NhucN4+OyO+86eOnUqd9xxB7feeisAr7/+Oh9++CFOp5M333yT5ORkqqurGTt2LBdccAFKddxja3tdbEcikXa7wG6vu2whhDgcUUsKWusyoKzltVsp9Q2QD+yeFC4EXtKm5fdLpVSqUiqvZdko6tzqo+HDh1NZWcnOnTupqqoiLS2N7t27EwwGueeee1iwYAEWi4XS0lIqKirIzc3tsKz2utiuqqpqtwvs9rrLFkKIw3FEus5WShUCw4Gv9vooHyje7X1Jy7Q9koJS6ibgJoAePXrsd137O6LXq1YScgZRvfthsyUdXPAHacqUKcyePZvy8vK2judeeeUVqqqqWLZsGXa7ncLCwna7zG51sF1sCyFEtES9oVkplQjMAe7QWjceShla66e01qO01qOysrIOIxiidvXR1KlTee2115g9ezZTpkwBTDfX2dnZ2O125s6dy/bt2/dbRkddbHfUBXZ73WULIcThiGpSUErZMQnhFa31G+3MUgp03+19Qcu0aAUEdH5DM0BRURFut5v8/Hzy8vIAuOqqq1i6dCmDBw/mpZdeon///vsto6MutjvqAru97rKFEOJwRK3rbGVaU18EarXWd3Qwz7nAbcA5mAbmR7TWY/ZX7iF3nQ3oNasJ2QJwQm/sdhmacm/SdbYQx6+D7To7mm0K44BrgNVKqdbLge4BegBorZ8E3sckhE2AF5gWxXik+2whhDiAaF599BnQ8bWXZh4N3BqtGPahFEpDJArVR0IIcTw4bu5oPqhqMGVpOUmQpLA36Q9KCAHHSVJwOp3U1NQceMfW1tAsO8Ddaa2pqanB6XR2dShCiC52RO5TiLaCggJKSkqoqqra/4wVFUTCfiLhIDabXL65O6fTSUFBQVeHIYToYsdFUrDb7W13++7XHXfQWPIR1e/+kt69/xj9wIQQ4hhzXFQfHTSbDRW2EInIXcJCCNGeGEwKSpKCEEJ0ILaSgt2ORZKCEEJ0KLaSgs2GikhSEEKIjsReUghJUhBCiI7EVlKw21FhJCkIIUQHYisp2GySFIQQYj9iLymENJGIr6sjEUKIo1LsJQU5UxBCiA7FVlKw21FhLUlBCCE6EFtJoa36SJKCEEK0J/aSQjgiSUEIIToQW0nBbjdnCmFpaBZCiPbEVlKwmU5hI0E5UxBCiPbEZFIgFJCBdoQQoh2xlRTsdoCWy1IDXRyMEEIcfWIrKbScKagwhMOeLg5GCCGOPrGZFEIQDFZ3cTBCCHH0ia2ksFv1UTB4gPGchRAiBsVWUoiLA8DSDMFgZRcHI4QQR5/YSgrJyQBYm6C5WZKCEELsLbaSQkoKALYmqT4SQoj2xGRScPgT5ExBCCHaEVtJITUVgDhfkrQpCCFEO2IrKbSdKbik+kgIIdoRk0khzueU6iMhhGhH1JKCUuo5pVSlUmpNB59PVEo1KKVWtDzui1YsbRwOcDiwN9nlTEEIIdphi2LZLwCPAS/tZ56FWuvzohjDvlJSsHmtBIPVRCJBLBb7EV29EEIczaJ2pqC1XgDURqv8Q5aair3JBmj8/m1dHY0QQhxVurpN4WSl1Eql1H+UUkVHZI0pKdi8CgCfb+MRWaUQQhwrujIpLAd6aq2HAo8Cb3U0o1LqJqXUUqXU0qqqw2wLSEnB6gkDkhSEEGJvXZYUtNaNWmtPy+v3AbtSKrODeZ/SWo/SWo/Kyso6vBWnpKAavVitSXi9khSEEGJ3B5UUlFI/UUolK+NZpdRypdSZh7NipVSuUkq1vB7TEkvN4ZR5UFJSUA0NxMf3xefbFPXVCSHEseRgzxR+qLVuBM4E0oBrgBn7W0Ap9SrwBdBPKVWilPqRUuoWpdQtLbNcBqxRSq0EHgGu0EdijMzUVGhLCnKmIIQQuzvYS1JVy/M5wMta67WtR/kd0VpfeYDPH8NcsnpkpaSAx4Mrrg9V/v8jHPZjtTqPeBhCCHE0OtgzhWVKqY8wSeFDpVQSEIleWFGUng5AQqA7EMHn29C18QghxFHkYM8UfgQMA7Zorb1KqXRgWvTCiqL8fAASG0zneE1N60hMHNKVEQkhxFHjYM8UTga+1VrXK6WuBn4FNEQvrCjq3h0AR5UVsOD1ruvaeIQQ4ihysEnh74BXKTUU+Bmwmf13X3H0KigAwLqzkvj4PjQ1SVIQQohWB5sUQi1XBl0IPKa1fhxIil5YUZSdDXY7FBeTkDAQr3dtV0ckhBBHjYNNCm6l1C8xl6K+p5SyAMdmT3IWi2lXKCkhIWEIXu8GQiFPV0clhBBHhYNNClOBAOZ+hXKgAPhz1KKKtoICKC4mKWk0EMHjWdHVEQkhxFHhoJJCSyJ4BUhRSp0H+LXWx2abApjG5pISkpJGAuB2L+nigIQQ4uhwsN1cXA4sBqYAlwNfKaUui2ZgUVVQACUlOOw5xMXl43Yv7eqIhBDiqHCw9yncC4zWWlcCKKWygI+B2dEKLKpOOAECASgtJTl5NG734q6OSAghjgoH26ZgaU0ILWq+w7JHn/79zfP69SQnj8Pn20QgUN61MQkhxFHgYHfsHyilPlRKXa+Uuh54D3g/emFFWb9+5nn9elJSTgWgsXFRFwYkhBBHh4NtaL4LeAoY0vJ4Smt9dzQDi6qcHEhOhm+/JSlpBBZLPPX1C7s6KiGE6HIH26aA1noOMCeKsRw5SpkqpPXrsVjiSEoaI2cKQgjBAc4UlFJupVRjOw+3UqrxSAUZFf36wTffAJCcPAaPZxWRSHMXByWEEF1rv0lBa52ktU5u55GktU4+UkFGxdixsHMnfPMNSUmj0LqZpqY1XR2VEEJ0qWP3CqLDdeGF5vmtt0hKGgUg9ysIIWJe7CaF/HwYMwbeegunsxc2Wxpu97KujkoIIbpU7CYFgIkT4euvUZFIS2Pz510dkRBCdKnYTgonngjBIGzfTmrqaTQ1raG5ubqroxJCiC4jSQFg40ZSUycC0NAg9ysIIWKXJAWADRtIShrVchPbvC4NSQghulJsJ4XsbHNn84YNWCxxpKSMk6QghIhpsZ0UlIK+fWHDBgBSUyfS1LSaYLC2iwMTQoiuEdtJAUx3F2vNOM0pKRMATX39gq6NSQghuogkhdGjobQUdu4kOXm0tCsIIWKaJIUxY8zzkiVYLA6Sk0+moWF+18YkhBBdRJLCsGFgs8FXXwGmXcHjWUkwWNfFgQkhxJEnSSE+HoYMgY8/hvJy0tQIQMv9CkKImCRJAeCmm2DJEsjLI+mef2KxOKVdQQgRk6KWFJRSzymlKpVS7fZHrYxHlFKblFKrlFIjohXLAd10E0yZAoBl2dckJ58sSUEIEZOieabwAnD2fj6fDPRtedwE/D2KseyfUvD66zB1KmhNauoEPJ4VBIP1XRaSEEJ0haglBa31AmB/d4FdCLykjS+BVKVUXrTiOSjZ2VBZ2dIPkrQrCCFiT1e2KeQDxbu9L2mZ1nWysqC+niTHcJRySBWSECLmHBMNzUqpm5RSS5VSS6uqqqK3ouxsAKx1HpKTx1JfL/crCCFiS1cmhVKg+27vC1qm7UNr/ZTWepTWelRWVlb0ImpJCqYKaQIez9fSriCEiCldmRTeAa5tuQppLNCgtS7rwnhM9RFAVVVLu0KEhobPujIiIcRxTmsIh6G5Gfx+87pVOAyRCNTWQlkZBALRj8cWrYKVUq8CE4FMpVQJ8BvADqC1fhJ4HzgH2AR4gWnRiuWgtZ4pbNlC8oSrsFhc1NS8Q2bmeV0blxBHCa137bwCAfM6KclM9/shMRFcLqirA4sFKivNsVZVFdTXw44dMHIkFBebZcBc/AewebMpq1s3s1xamllm+3azrnB4105y9+f2XodCsG0b9Ohh4khKMusJhczrQAA8HjPwotbgdoPdDl6vmRYOmy7R0tIgLs58Vl0NDgdYrabM+HjYtMnEmp9vtnv7dvD5zPxZWWb+ujooqwrQ7Isjzq7Q2sTYGvPe7HZTtt1u4tTaxKU1/Pzn8Oc/R/c3jlpS0FpfeYDPNXBrtNZ/SFqTwi23YP3mG7JuvoSqqv+jT59HsFqdXRub2IPWGnezmwZ/AwXJBSil0FqjlCIcCVPjqyHBnoBSCpfd1e7yX5V+xYDMAWys3UhER8hyZdEUbKIoq4hAOEAoEiIxLpGIjvBVyVfU+evom96XrIQsmsPNbK0tJt2VwqqKlQRCQVKdKQQCiq2ebxiTdzIV3lLOKDyTBn8js1d8QI23HqwBVhR/i82Xz+huJ1Hp3UmjJ0Rvx0nUBEuwEU9zxEeiNR23LifL0Z3GYDXFlR789akkpftocAdospYQtLjJsQyi1vINARqgKRufawNJDWOxNWeyybcEZ80oknNraY74KQ+tZ5D1IrZ61+CPK6abGkVzTR71ajOBUABXJBe3pZSGQC22HkuwVIygWftIc6aDLx1f2EOgOo9w5ipQEbD7oKE7WJtbvlQrJJaDsw5CTshdAe5usGM8ZK+Bhh5Q3xMSKyCh0pShIuDJhaALTvgIstaCLwNKxkJzIvjSwdEAKTsgocqUh4ambAjHY4lrAmsYFV+LJZABriq0qwosYfTw1UQa8rE2dSdS0gudvQpSt6HXjYVuy7AoBxa7HVvlaKyJtQTjSwnnbccWyCbeM5DEngmUehKxBbJoTFtIcu4JNMV/iyYCETtBSz0pvbsR72ymzl3AFsfXZJx6AhZ7NVW25dSFLTiauxGJa8Bv34FDJ5MRGUKSzqfWsp4UemBRVlJVD6pYSymLSVQ5JIQKaKCEUMRCSqQHyeRT7liAV1VRXfhT4HdR/d9SujVdHyNGjRqlly5dGp3CtTaHAAAZGdRueIVVq86mqGg2WVmXRmedR6FAKEBlUyXdkrq17WytFisA7oCbjzZ/xEkFJ5GflE+pu5Sqpip2unfSJ70PVouVJaVLOKngJMo95SzbuYxeab0AWFG+gnVV6xhbMBa7xc787fNZXbma8088n8HZg1m6cykNgQbS49Np8DeQ6cqk2ltNRVMFlU2VJMQlUNpYikVZ2OneSUOgAYBhOcNp9PrZ7tlAsj0DX9iDP+xt255Uazc0YWxWKw6VSFOokWAogldV4iId715XTjtCWQRsVShtwRnshs9eCmrX/4lqTgRbM9rSfOAvM2K+Nyy7HRK68yChAiyRQ/l5divbYsqIWMxO1eEBfzI4G9udXUXi2mJWEQfa0nFdRFKoN27bFmzaSUj59/jMih2FhTiVgFfXojCH+hpNPKk4dTrNlkZyLANwq1KqQltItGTgjdQTwXwPCdYUrMqGUgp3sJYIEfokF3FiwmgqvGVs9C6mOdyMP9KEw+qge3IPMuIzKG8qw6IsVHmr8AV9OG1ONJpMVyaNgUayE7LJcplq4N5pvSltLKXGV8P66vUMzhlMTkIO87bN4/Rep2O32qn2VrO2ci25iblkujI5MeNEttVvY0fDDjzNHhoDjbib3XRP7k6pu5T+mf3JiM8gEA6QGJfITvdOfEEfpe5STu1xKtvrt+OwOZhUOAmAksYSkhxJ9E3vS2VTJasqVlHcWEzf9L6Ue8oJ6zA7GnZQmFrIxJ4T2Vy3mTJPGf0y+pmDkdKv8AV9TCicQGFKIaf3Op1zTzz3kP5clFLLtNajDjifJIW9tJ7L9u9PZO1qvvyyO8nJYxk06M3orfMgtR4J7/7eG/QSb4/Hokwy21a/jU21m0iMSyQvMY+eqT15b8N7PPv1swzLHYan2UNOQg47GnZQ3FjMyoqVTCqcxIaaDfhDfqq8VUR0hB0NO8hPyicYCeINehndbTTuZjerKlbRHG7GZrGRGJdIvX/PhniFQtPx31SaLZe6UDkASTqfXHtfNgbng9JYI/HEkURA1WELpRK01mHxZxEfziHkzsIS34A9mEmT206oLgeXvy+B5jDhga9CYwFUFoGz3uwg6wvB5gdrENI3mZVbA+ao1peOxRYiUtkfhj+PfdPFBLecQmp+FQ5XM97seTgaBxAIBVHpW0gI9KF3Sl+a67MgqYyqhLk0NyUwyHkW9c01pIX70TMrg2ZLA/Z4P4GaXOoSP0e5u1OXPB+wMCnzSnLi8/F5LZzQPZHkHtv5+ttK8pPzKSiAheX/oUdSbxy2OJzWeKq8laQ7cin37CQjPpvu2YmEbY2ocDyZKU4yXZkkO5JZXLqYLFcOPZN7E9AeLKEEvm1YSTDSTP/M/qwoX0H35O4EwgFyE3NZtnMZBckF9EjpwY6GHZQ0lpDpyiQ7IZtqbzW5ibmkOlOxW+3U+mpJdaYSCAWo9dVitVjZVr+N0d1GY1EWlFI0NTehlGr7+3Pa9jyj1lpT3FhMQXIBwXCQMk8ZOQk5xNvj2+ap9lYTCAXIT973inR/yI/D6tjj775VREdQKCI60nbQEg31/nqSHcn4Q37ibfH7xBKOhPGH/CTEJXT6ulv3z+1t/3clSeFQrVoFDz4Is2aB18umrb+gtPRRTjmlDLtCKMvHAAAgAElEQVQ9I2qr1VrjafZQ2VRJ77TePLb4MWp9teQl5bGmcg15iXnM/HwmI/JGMChrEGWeMj7Y9AHuZjdpzjSG5AyhpLGEiqYKPM0eAKzKyindT2HhjoVtR90WZSGiIyTYE8hwZZCbmMuS0iUMTB+BjiiSHcnUuD2ckXUVC8reJ+CzkRgupEwtxhZOJtU/nCz3Gey0fUaTriZcNpjKzblkOPKoc6wg7CrFuvlCwvlmR8/Ka1p2ygrKh5oqgW5LzY65sTeRsMKaVEPuicVYqovweuxYbRHyci2EI5qePRR1ddCzp6mX1toMrZ2XZ+qnU1NNne/AgZCSYup7IxEzn89n6pBHjTL1xw0NZvTVpCRT1+z3m/lSUqCx0XzWCf97QhyVJCkcjqefNv0hbd+OO62GZctG0LfvE+Tn/89hF72yfCUzFs1gcPZgFpcubqsWWV+9nnAkTJmnjKKsItZWrW1bxmaxEYqE6JfRr63qJDEukcl9JtMnvQ+fFX/G2sq19Envgw0n38+4AbcnwrKqz1hS/z6FTCL364fZ4dlEr8x8isubSCCb+jor36zX+IMBdLD9NhObzTSWORxm59v67HJBQgJkZJjB68rLzY7a5TI749xc87pHD7OjTk01O2in0yyTmWlel5WZphy7/bC/WiHEfkhSOBxz58Lpp8Mnn6AnTWLp0iFYrUmMGPH5dy6q3l/PfXPv44J+F5BgT+Ccf52DO+AmrMMUphbitDkpbiimb0ZfPM0ebhxxIw8seIDuyd357zX/JazD5CTk8sQHn3BSt1NYtTiFpiZTdk0NLFoE69ebKyMyM83RcHuXrfXsaR5bt0L3lrtDkpJgwADzfMIJJgHU15txh0Ihs7Pu3VuOnoU4HhxsUoja1UfHtD59zPOmTajTTycn5xq2bLkbr3cjLlffDhdbXLqY5WXLWbpzKVpr/rPpPzQEGvAGvTy6+FFTdHofvr75a1x2V1uDGEAgoPj8c822bxU/bLqGxuI4fvvzDN5+2+zkGxom77M+i8WMJnr++ZCTY5JEQgKceiqkp5tqkdRU80hJkZ27EOLAJCm0p/Wi43XrAMjJuYotW6ZTUfEyvXrteTlYuaec5WXL+bLkS/6w8A9EdIRkRzLhSJhhucM4MeNELht4GZtrN1PdVMvYuJup35bL8i3mCH/2bPjmG1O3HQiYvbbVmkdGhqkbHzvW7PDHjDFJ4JRTdh3p2+2mCkYIITqLJIX2WCwwfLgZeAdwOPJJS/seFRUvU1h4P6sqVjNj0QyG5Qzj3Q3vsqh4EQBXDb6KByY9QG5iLk6bE6UU27fDggWw5jN4/334Xcmeqxo7Fm6+2dS7T5wIgwebahvXvpfWCyFE1ElS6MiYMfDkk6Zy3WYjJ+ca1q+/loaGRdzx4X18Xvw5r615DYD7TruPSwdeypCcIYC5E3LWLHjttba8QnIyTJgAM2aYI/w+faCgYNf9ckIIcTSQpNCR0aPhr3+FtWth6FCS085lTaOTpV/9hnnb5vG3s/9Ggj2BL0u+5L4J91FfZ2XaNFi4ELZsMdVBI0bAzJlw9tnmkklr9C6lFkKITiFJoSOjR5vnr75iW88UJr8ymfXVfmAuI/OGc9PIm3DanIxz/YgrpsKbb5qG3AsvhOuuM4O4nXhil26BEEJ8Z5IUOnLCCei8XP5vyQv8uvkhKpsq+evpd7Fk45/5nyHdeP+dOJ54Aj75xNT///SncM01MGRIVwcuhBCHTpJCBzTwkytSeTTlC/Kb83lr6ltMKJzA6tTu/OQnWcyda6F7d/j97+GGG8wVQkIIcayTpNCB2etm82jKeu74Ah76zetYCk/h2WfhlltuIxRS/PjHz/PQQ9OwyTcohDiOHBPDcR5p/pCfOz68gxFZQ/jzF4lYLpvCCzMrufFGmDRJ8cYbz3HxxT/E71/V1aEKIUSnkqSwl+Vly7n3k3vZ6d7Jnyf/FTVvEXdX/JQf3p3J974H77wD5513MVZrClu3/rqrwxVCiE4llR+7aQ43c9IzJxGKhBiaM5SJPSdx8x8Vz4SHcIP9Rf72ymU4nQlAGj163MXWrb+itvYj0tPP7OrQhRCiU8iZwm7mbZtHKBJibMFYnj7/ae69V/HMM3DPNcU8Hbwe1+svtM1bUPAz4uP7sX79NOrr53dd0EII0YkkKezm7fVv47K7+PTaT1k4azQzZpgetH//QgGMG2duR/7kEwCsVidFRbOwWl2sXn0+4XBTF0cvhBCHT5ICUNpYyvSPp/PU8qc4/8TzWfZVPD//OVxyCTzxBCiLggcegIoK+N732vquSEwcSr9+zxMOu6mqmtPFWyGEEIcv5pNCOBLmzH+eycxFM7lkwCU8/L1/MG0aFBbCCy/s1jXFpEltvaby6adty6ekjMPpPIHS0kcJh/17Fy+EEMeUmE8Kr699nXVV63j10leZddksnvhLCps2wTPPmMFn9tCnjxlmbH5LG0I4jPL56NXrAdzuZaxbNwWtD3MwdiGE6EIxnxT+8uVfGJA5gClFU/j2W/jTn+Cqq8zAa+2aMAE++8z0nvqnP0G/fuRkX0GfPg9TU/NvSkr+dkTjF0KIzhTTSWFN5RqW7lzKTSNvQkcs3HwzxMfDQw/tZ6EzzgC32wzZ+cknUFICFRXk599ORsYFbN58Jzt3PnXEtkEIITpTzCaFck85P/ngJ9gtdjM4zgOmVujhhw/Qj9H555uR5//xD2gdK3rTJpRSDBw4i/T0c9iw4WZKSh45ItshhBCdKSaTwk73Tk597lS+KP6CRyc/irc6i//9X/jBD+D66w+wsNNp+saeMwcaG820zZsBc5nqoEFvkpl5MZs2/YQdO/4U1e0QQojOFpNJ4YH5D1DqLuXT6z7l5lE3c//9ZiyEGTMOsoCf/WzP95s2tb20WOIYOHAW2dlXsGXLdGljEEIcU2IuKYQiIWZ/M5sL+13I2IKxrF0LL70Et90G3bsfZCHdusFjj5kBlQsKTAFvvtn2scViZ8CAf5KZeRGbN/+ckpLH5HJVIcQxIeaSwqdbP6XaW83UoqkA3HMPJCbCL3/5HQu69VZYtQpsNtixw9zp1tzc9rFSVvr1e57ExBFs2nQ7a9ZcSCQS6MQtEUKIzhdzSeHz4s9RKM7qcxaLFpleT+++27QdH5KLL96t8M/3+MhuT2XEiC/p1+8Z6uo+Yt26K4lEgocevBBCRFnMJYUtdVsoSC4g3uZi+nTIzYWf/OQwCvzTn6C4GOx2+OCDfT5WSpGX9yP69HmE6uo3+frrU6mt/Qit9WGsVAghoiOqSUEpdbZS6lul1Cal1PR2Pr9eKVWllFrR8rghmvGASQq903rz/vvmHrTf/AYSEg6jQLvdtCucdho8/zwsXgzBfc8GCgpuZ+DA1/H7t7Fq1VmsXXspzc2Vh7FiIYTofFFLCkopK/A4MBkYCFyplBrYzqyztNbDWh7PRCueVq1J4eGHoWdP+NGPOqngRx4xyeCkk+Cyy+Ccc8xZxKmnwrvvApCdPYWTT95B794zqal5jyVLBlFV9VYnBSCEEIcvmmcKY4BNWustWutm4DXgwiiu74C8QS9lnjIyLCfwyScwbZo50O8UAwfC8uWm0Hfegf/8B6ZPh0WLzHMLi8VBjx53MXLkMhyOAtauvZjNm3+Bz7e1kwIRQohDF82kkA8U7/a+pGXa3i5VSq1SSs1WSrV7UahS6ial1FKl1NKqqqpDDmhb/TYTyOreaA3XXnvIRbWvsNDc7OBwQF7erumVlaY7jIkT4dtvAUhMHMSIEV+Sm3s9xcV/5quvevP11xPwer/t5KCEEOLgdXVD87tAodZ6CPBf4MX2ZtJaP6W1HqW1HpWVlXXIK9tStwWAzUt6M2QI9Op1yEV1LDvb3LPwn//ArFnw619DdbXpZW/+fNMfdwuLJY7+/Z/npJM20bv3TJqa1rB06TA2bLiV2toPCYe90uuqEOKIimZSKAV2P/IvaJnWRmtdo7VuvXj/GWBkFOOhpLEEgNWLujNpUhRXNHkyDB0Kl18ON95oBmVYsMB89vbb+8weH38CPXrcxejRa8nIOI/y8hdYtepsFi5MYP36aZSVPY/fvyOKAQshhGGLYtlLgL5KqV6YZHAF8IPdZ1BK5Wmty1reXgB8E8V4aAyYvor89alMnBjNNe2me3dzmdNbb+3qS2P9eujdGzZuhKKitlkdjlyKiv6PSKSZ8vKXqKv7kIqKl6ioeInExBGMGPElFktnNYIIIcS+opYUtNYhpdRtwIeAFXhOa71WKfU7YKnW+h3gx0qpC4AQUAtcH614ABr8DViwEgnGc+qp0VzTXsaONY+yMvjLX8yjstKcNSxcaK5Q+uAD07HerbdiscTRrdsN5OZeh8USj1J2ysufY+nSIfTocS+ZmRdis+09ApAQQhw+dazdRDVq1Ci9tLXL6u/otvdv49mvXsXyYA0ejzlwP+JuvhmeahlvIT4e0tJg/Hgz7nNJCdTVmXny8mDmzLbFKitnsWPHDDyeFQA4nYU4HD3Jzp5KXt6NWCzRPOkTQhzrlFLLtNajDjRfTO1JGgINWEPJ5OZ1UUIAc7ec1uY+hkgE7r3XNEi3evll+Oc/ITPTVDVZTLNPdvZUsrKmUFf3X5qKF+C2b6WpaS0bN/4/KipeJivrUsJhH9263UJcXGYXbZwQ4lgXU2cKF752IZ8s3c6IxSva2n2PCq1XJu3caRJGq2XLYMQI8zoSMQli0SJz9/THH8OkSVRUvMKmTXcSDJq7o5Wyk55+Fnl5N5OWdgZWa3wXbJAQ4mgjZwrtaPA3EPGm7HELwVHhxRfB54MLLzTDfJ5xhhnq89VX4Y03zK3X991nbr/evt0kiGefhUmTyMm5iszF8ejX/0Xg7/dTXvES5eUvUFPzb5zOE4iLy8Juz+aEE2bicvXr6i0VQhzlYupMYfg/hrN6UQG3pb/Lww93cmCdYds28xg/Hs4+25wNtLJaIRw2r+1289iwAVwu6N/fNFy3NFqHw37q6j5iw4ZbcFTbCLgaCcUF6dbtJoLBapzO3jidvXA6e5KaOhHVZXVpQogjRc4U2tHgayTclEJe0YHn7RKFheYB8O9/m7OBwkJzM9yVV8K6deb1TTeZIUEHDoRhw6CmxtxFfeaZ8PjjWKdNIzP9PNJOWIzl3MHogf1Y9/c8SkoexW5PJxisBszBQFramdjt6WRmXkJW1mWSIISIcTF1ppA+I5O6hVN5cerjnd/FxZG2aZOpblq3Dh58EFavNtVQYEaEW7fOnEW43Wba448TqalEpWcROf1UmnskUPv5w2xxvIjF6iIYrMTl6o/VmkRm5sVYLA6SkkaSmjqh67ZRCNFp5ExhL1prGpsbIJB89LUpHIo+fcxNcYsWwbnnQmMj3H47/PGP8PXXcOed8OmncPrppsH6ttuwtBwAWIH4oiLy166l289+BieNoWTUDrwL/0XCZ9soPe0efAVmNQNe7InqdQLqf27D4/kah6OAdPtpODP77XsJl89n7rUYNOjIfhdCiE4TM0nBH/IT1iEIHIUNzYcqLQ3OO8+8TkmBkSNhzpx959u0yVQzXXYZ/OpX5j6Jhx6CgQNRDz0EQPeTT4ZVG6CpifwX44j8+QEqk78m54XXCCVuZ539UxzVkLIQnEsg0M1BODuFyJWX4np7OXWPTiPtsYVYXp5l2kVWrTLrvf32Q9u2cNgkurS0Q1teCHFotNbH1GPkyJH6UJS5yzT3oxn9uC4vP6Qijm21tVpHIrvee71a+3xav/mm1n/7m9Z9+2p9+ulaL1+u9XnnaQ1aO51aZ2WZ1y2PcLds3XDjeF13WqoOpKm26bXD0RGLeV1+fYEOpTi0Bh365CMd+dEPtf7pT806W331ldZut9aff671KadoPW+eeX3ttVo3NGh9/vlm3TU1Zv7Fi7Ves+bwvgOPR+tXXtF65cpd0wKB71ZGJKL17bdr/dhjhxeLOHb4/Xv+7xxJzc1aB4OdUhSmJ4kD7mO7fCf/XR+HmhTWV603SWHwP7Xff0hFxI5QSOv/9/+0vuQSrbdtMzvqn/5U682b9/gDDc/7VDf3zdMN4zK1Bt2cE6/dJ1rbEkVz0q5kokFHbErXnp6mGyb3Nu+zs3UkId68djq1zsgw8/bps2u5MWO0vvxyrW02rdPSTDJZtswkjkcf1XrHDq3r67V+6SWtn39e60svNcnld7/TevBgrf/xD63fe0/rWbO0PukkU+YJJ2hdV6f1O+9onZSk9W9/q/XSpVoPGKD1X/6idc+eWv/851qfdZbWDz5ovo9Wr766K7bddxRNTaac0tI9v0uPx/xj19buOb29nYzPp/XLL5tkeTDq6sy21te3/7nXq/Vzz2m9caN5HwhovXXrwZXdnlDI/A3srrGx4/W3am7e//tPP9X6jTc6Xj4c3ndaJHLg9Wptyt39NykuNn8jVVV7lt/YuOt9SYluO3KsrzcHJ7/7ndZlZWa9kYj5GzwSiWL8eK2nTOmUoiQp7GVxyWLN/Whb0buHtLzYD49H69dfN8+LFml99dU6+PTDuuTfN+uam0fqrY+P0d/8vbeu+EE3HYq36pADXXwJuvpUuy47C730SXTV6XE6lOLU9ZcO0GGnRXtuOlt7p03WkaQkHcnJ1nrSJB1JT9+1Q05MNM9Dh2qdm7trunVXUtI9e+6RlDRoPXHinu8dDq2V0rqgYN95u3c3z5dfbrbtgQe0tlh2fb5ypdZz5mi9aZPWv/ylmTZpkvkn3rRJ6z/+0cTTvbtJPm+9Zc6IystNHNOmab19u9ZLlpiE1bp9P/uZ1k8/rfWNN5ptmz9f6xkztP7Vr7ResEDrhx/W+vrrd233+eebHdvbb5vfoa5O68cfN8m89Tv56CNTHmj9gx+YBKS1SfI1NWYn9+mnZju1Nkm3dacbDmu9YYPWv/iF+a4+/dTsXFes0PqMM0yS9XhM4qurM8vU1Jid5tKlWsfFaT11qtn2hx4yyX3tWjNfWZnWycla2+27klckYpL4t99qvXq12c7XXjNxLFpkktP995vvdOfOXX+HV1+tdVGR1nfdpfWpp2p9991me8eN25VYZs4002bONO+9Xq2//33z3T/xhNa//rXWqalaDxtmDjrGjNFtZ81Kmb+Ff/zDTPv3v01y+de/zBnoFVeYz/7yl10xBYPmQKa4eM//mRUrtO7Rw5SXn6/1zTebZOl2m78Jrc32g1nv3sn4EEhS2Mt/N/9Xcz86dej8Q1pedBK/Xwe8ZXrLlt/ob775kS4re0nv3PmsXrXqPD33U/S8eTa9cH6qnjsXPXcuev48l547V+klS4bpL+e49IZ7knT178/V4ew07RmVrTXoSN++Wn/8sdnxVVRo/eyzZqcSDpt/0l/8wlSP9e9vjpbPOkvriy4y/8ilpWYnmZpqduhgjvjnzjU7p5kzzT9lcrL57KqrtF63rv3E0nqm03q2Exdn1nnaaVp367Zv0oFdSSY52ZQdF7frM6XMzmjveVsfublaX3mleb17+a2xgkk8RUVmR6zUrp3c+PFm+/v127PME080Sa01uU6YoPXFF++5/rQ0s0PePdaTT9Y6Pt7s3Pv3N+u66iqtL7yw/e1OTjZncpMnm3ISEkxifPfdXcu4XFr3NmeVOivLJMXWs8fW7+W668xv/c47u8q22fZd3/jx+/4+06ZpnZJi3ick7PrMbt9z2V69TJmt35VqqTa94IKOt++xx8zfXevvopTWgwaZg4drr93zex8/3jxfe61Zl92u9TXXmO1vTeoXXmi2c+nSQ/7XO9ikEDOXpM5ZN4fL/u8yer6/gm1fDY1CZOJwhcNNLc8+vN511NfPw+NZSWLiEOrr5xMXl0so1Eht7Xu03GZBxiILDUMgLrc/ubnX4ffvwOfbiMvVH4cjH6s1kdTUifgql+NtWENcbhE5OT/ADCG+l8ZGePRRuOMOSEjYNX3+fHO57wknwC9/abobue46+OormDoVysshP9/ccf7Xv0K/fqYzw/x8eP11M/DSxo3w5ZeQkWGW69fP3IeSnm66Tz/3XBg9Gj76CM46C372M9NP1muvwd13m67XR42Cn/4Uiovh6afB6QSPZ9e9LS++aG5yvOEGsw2FhXDNNVBaCj/+semld+5c0zvv3XebGx6TkszrwkKIizPrDQbNWCDp6WZQqG3b4JRT4JtvzDpeftncVf/pp9DUBL/4hZk2aJB5rFtn7puZPdvEdc895vHcc/Dee/CDH8Arr5ir57xe02tw6/fn8ZhLqadPN5dZL1oEP/yh+T6bm035fr8ZuGr4cLM9uysrg9xcE9sZZ5i4zznHjKFeV2e2LTfX/GbJyXDRRWYIXYcDVqwwl3lHIqbstDTz3Ywfbz7v1s38ZnfeaS7qWLTI7NZPOsl8D7fdZn7nF14w9w5ZrTBhgvk9tmwxvRTsHu9Pf2p+px49zHfy6qvmdxg1ytyIWllp7j2aNMn83Wltfse//e2Q/r8O9pLUmEkKayrXcNG9r5O07sd8vUg6jDuW+XxbaWpahcvVn507/4FSNurr5+N2L8ZiScDl6ovXu4FIxNvu8lZrMpmZF2GxOABITZ1AVdUbdOt2CwkJg4iLyyUSCeD3byUhYcCR3DRj3Tpzl3pLZ4iEw2YH05GXXjLDCI4fb94HAmYntj/BoOmZt3t382il9Z6XGofD5jLjvn3NznL3OObONTvXK69sfx2ffGKGn73mGpN89rZggdkR3367WWdlpblLf8AAkzx399FHZsf44osmMUQiZif/xRcm/v/8x3QiecMNu7bjttvM1XmTJ5tpr71mYv3wQxPP8OEmsbZnzRozT8+e7X93NTVmVMWMDPjtb/f8vufPh61bzY4+Lm7PZSsrze/z17+axJedbab7/Wadw4fv+o69XrDZTBnr15t5+vfvOOYDkKTQjlNOMQeA//1vJwclupzWGr9/Kw5HDywWG1qHCYeb8Hq/xetdh8tVRELCAGpr/0tNzb+prHwFpcw/cjjcsEdZdnsOFkscgUAx6ennYLHEkZNzdcu8TWgdxO1eTnr6ZJKTRxMKNRAf3welunp0W3FAO3eaI/4YJEmhHUVF5iCk9axWxK5wuAml4tA6RFXVbFJSTqGm5n1A43YvIRAoIy4um7q6jwELwWDFXiVYsFji0DqC1s3Ex/dBa01cXDZ2ewZ2exYez0pcrn6kp5+N09kTsBIMVpCSMoG4uEz8/hIsljji4rKP/BcgYo7c0dyO+npITe3qKMTRwGptbTOwk5t7DQAFBe3faBeJBHC7lwMKmy0ZpWzYbCl8/fWpuFwDyMg4l8rKWdhs6YRCdfj926ivn09S0ijq6j6hsvLVPcpTyo7T2QufbwMA6emTsdlSSEoahdWaiMXiwufbhNZhlLJit2eSm3stFosTn28zNlsqdns2Slnx+824GgkJg4iP70UkEpQhW8Vhiamk0NBgbvwV4ruwWBykpJy8z/QxYza0dSDYrdvN7S4biYTw+Tbi820iHG7C6exJdfUb+Hxbycm5Bq2DlJY+jlIWKitf221JBVgA0zPutm33Y7dn4PNtbInJDNMaDptxx5WyYbdn0dxcRlLSaNLTzyYhYTBKWbBYEtA6SGrqafh8W4lEvASDVfh8m0lIGERy8lhstuRO+77EsS1mkkIoZC4QkKQgOsvB9ChrsdhISBiwR4P13gmmsPB+ALzeb7Fa41uSR2+sVidaazye5WzZci9e7zpOPPFJtI7g9X6L1kESE4ficvWnpubfBIM12O1Z1NV9xPbtfwAiB7UdNls6LtcAQqE6AoFSHI5uWCzxJCYOIy4uh/Lyl0hJOZmEhEFYrSmkpJxKU9NKbLY04uP7EArV43INJC4uk+bmahobv8BuT0cpBwkJRXsM9KR1hGCwVkYHPIrFTFJoNAdUUn0kjjqtySUhoX+7nyUljWTo0A/MNeQdJKLU1NN2ezejpZF9A0pZCIc9hEJu3O4luFwnolQcoEhJOQWPZyWlpY+07Nj7k5p6GoFACZFIM1VVcwiHG0hOPoWGhi+oqtp/Y5zdnkUwWLXHNIslgfj43jgc3QkEdmCxJOB2LyEr61LAQmrqBJqbK2hu3onLNRCrNR6tQ9hsGW1tM6b33j2vuNnfdyEOT8wkhfp68yxnCuJY9V12glZrAklJw/eYlpFx9j7zpad/n/T077dbRiQSIhDYgdPZC6UUkUiIpqbVeDzLzb0fvk00N5cTF5eH272UQKAYh6M7KSmnEgo1Eon4aWhYgM+3Bb9/C3Z7Bl7vBtLTz6au7mMsFgdVVbMAhd2eSTD4dAfbbWu7KEDrMBBua5ex2VLw+TYTF5dNXt7NVFe/QVPTWpKTx5Cd/QMaGhbhdPYgHPbgcvXH7V5GOOzBYoknO/tygsFqmpsrSUkZ1/J6J1pHcDi64XIV4XDkEg57W6rrFJFIAKXiiER8RCJ+7Pb0g/5NjhUxc/XR11+b4Y7ffNPcryKE6FpaRwgESrFYnMTFZREI7ETrMBaLg2CwmmCwhubmMjyeFUQizS3JwYpSViKRZvz+zYRCDTidvair+2/bJcnp6WdRXf02wWAlFouTSMTftk6LxdUy0FRth/ex7M5icRGJeHE4umO1JuD1fsvubT0OR09SUsYRCBSjlJWkpDFABK3D1NZ+QCTiJytrCunpZ+H3byUSCaJ1qKWKztV20UJCQhGBQClOZ0+CwVqam8twuxejlAOvdy0u10Byc68jEgkc8rjrcknqXubNMzcG/v/27jVGrrKO4/j3t7uzu91uLSW0pXIpLdRoSSpWJcgtRrzBm0KAUEUkhoREIZEXJEJQRN5hoiQmRMBAUpAIgjQ2BINcKgYTLgUWWsDCgiS2VqpAC7vpZZn9++J5dpxOd3aXwuyZ0/19ksmc88zZyW+emdlnzjNn/uexx9K1mR08Rkf3MjLyH7q7D8+DxgjDwxvp7V1KtfoeXV3zGB7eRH//CogSlX0AAAfRSURBVDo7ZzMy8i47dvyF7u6FdHb2Mzy8ie7uRfT0fBIQe/Zs5f33n2Xv3m10dc1leHgjEVX6+pYDo3R2fiL/aHI9Q0PPMWvWMkZH9zA09DzpIIGgUllAX98ydu78GxEjkzyCDmAUqYv0P7m63xaVymEceeQVLF58zQH1kQ9JbTA2feTvFMwOPh0d3fT0HFG3XmHOnJUAVCrpTV//BX+lMo/588+prff3r9jn/vr6PsW8eZN/ejz66Cv3Wa9WdwPVvMfTS0dHN9XqMG+//QC9vcfS07MI6GTv3n8xOrqHiA/YvftNhoYGmD37eHbteh2pg76+5VQq89m161W6uubyzjsPEVFlzpwvftiu+dBmzKCwYAGcey4sXFh0EjM7WDV+IZ7aZrNgwQX7tPX0HF63dhpwUZN7/CoACxde+PEEnIIZMyicfHK6mJlZcy7WYmZmNR4UzMysxoOCmZnVeFAwM7Oalg4Kkr4pabOkQUlXjXN7j6R78u1PSTqmlXnMzGxiLRsUlM53eBNwJrAc+Jak5Q2bXQK8GxHHATcCN7Qqj5mZTa6VewonAoMR8UZE7AXuBlY1bLMKWJOX7wPOkKtcmZkVppWDwhHAP+vWt+S2cbeJiA+AnUDDyVnNzGy6lOLHa5IuBS7Nq0OSNh/gXR0G/PfjSVWIMud39mKUOTuUO3+7ZV88lY1aOShsBY6qWz8yt423zRZJXcBc4O3GO4qIW4FbP2ogSRumUhCqXZU5v7MXo8zZodz5y5q9ldNHzwDLJC1ROqvHamBdwzbrgIvz8nnAY1G2sq1mZgeRlu0pRMQHki4HHgI6gdsj4iVJ1wMbImIdcBtwp6RB4B3SwGFmZgVp6XcKEfEg8GBD27V1y7uB81uZocFHnoIqWJnzO3sxypwdyp2/lNlLd5IdMzNrHZe5MDOzmhkzKExWcqPdSHpT0kZJA5I25LZDJT0s6bV8Pa/onGMk3S5pu6RNdW3j5lXyq/xcvChpZXHJm2a/TtLW3P8Dks6qu+3qnH2zpG8Uk7qW5ShJ6yW9LOklST/M7W3f9xNkb/u+l9Qr6WlJL+TsP8vtS3LJnsFcwqc7t5enpE9EHPQX0hfdrwNLgW7gBWB50bkmyfwmcFhD28+Bq/LyVcANReesy3Y6sBLYNFle4CzgT6ST2Z4EPNWG2a8Drhxn2+X59dMDLMmvq84Csy8CVublOcCrOWPb9/0E2du+73P/9eflCvBU7s/fA6tz+83A9/PyD4Cb8/Jq4J6i+n2yy0zZU5hKyY0yqC8LsgY4u8As+4iIv5KOIKvXLO8q4I5IngQOkbRoepLur0n2ZlYBd0fEnoj4BzBIen0VIiK2RcRzefl94BVSpYC27/sJsjfTNn2f+28or1byJYCvkEr2wP79XoqSPjNlUJhKyY12E8CfJT2bf9ENsDAituXlfwPtfsbpZnnL8nxcnqdYbq+bqmvb7HlK4nOkT62l6vuG7FCCvpfUKWkA2A48TNpz2RGpZE9jvtKU9Jkpg0IZnRoRK0lVZi+TdHr9jZH2Q0tz6FjZ8gK/Bo4FTgC2Ab8oNs7EJPUDfwCuiIj36m9r974fJ3sp+j4iqhFxAqlaw4nApwuO9LGYKYPCVEputJWI2JqvtwNrSS+6t8Z29fP19uISTkmzvG3/fETEW/lNPwr8hv9PU7RddkkV0j/VuyLi/txcir4fL3uZ+h4gInYA64Evkabjxn7/VZ+vll0TlPRpBzNlUJhKyY22IWm2pDljy8DXgU3sWxbkYuCPxSScsmZ51wHfzUfCnATsrJvqaAsN8+znkPofUvbV+WiSJcAy4Onpzjcmz0vfBrwSEb+su6nt+75Z9jL0vaT5kg7Jy7OAr5G+E1lPKtkD+/d7OUr6FP1N93RdSEddvEqa97um6DyTZF1KOsriBeClsbykOchHgdeAR4BDi85al/l3pF39EdJc6iXN8pKO3LgpPxcbgS+0YfY7c7YXSW/oRXXbX5OzbwbOLDj7qaSpoReBgXw5qwx9P0H2tu97YAXwfM64Cbg2ty8lDVSDwL1AT27vzeuD+falRb5uJrr4F81mZlYzU6aPzMxsCjwomJlZjQcFMzOr8aBgZmY1HhTMzKzGg4LZNJL0ZUkPFJ3DrBkPCmZmVuNBwWwckr6T6+UPSLolFz8bknRjrp//qKT5edsTJD2ZC7itrTt3wXGSHsk195+TdGy++35J90n6u6S72rVaps1MHhTMGkj6DHABcEqkgmdV4EJgNrAhIo4HHgd+mv/kDuBHEbGC9Evcsfa7gJsi4rPAyaRfTUOqBnoF6fwAS4FTWv6gzKaoa/JNzGacM4DPA8/kD/GzSAXlRoF78ja/Be6XNBc4JCIez+1rgHtz7aojImItQETsBsj393REbMnrA8AxwBOtf1hmk/OgYLY/AWsi4up9GqWfNGx3oDVi9tQtV/H70NqIp4/M9vcocJ6kBVA73/Fi0vtlrALmt4EnImIn8K6k03L7RcDjkc4ktkXS2fk+eiT1TeujMDsA/oRi1iAiXpb0Y9KZ7zpI1VMvA4aBE/Nt20nfO0AqiXxz/qf/BvC93H4RcIuk6/N9nD+ND8PsgLhKqtkUSRqKiP6ic5i1kqePzMysxnsKZmZW4z0FMzOr8aBgZmY1HhTMzKzGg4KZmdV4UDAzsxoPCmZmVvM/keyB3RpefIMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 555us/sample - loss: 0.3019 - acc: 0.9209\n",
      "Loss: 0.3019223801010247 Accuracy: 0.9208723\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7039 - acc: 0.0886\n",
      "Epoch 00001: val_loss improved from inf to 2.53641, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/001-2.5364.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.7039 - acc: 0.0885 - val_loss: 2.5364 - val_acc: 0.1798\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3065 - acc: 0.2388\n",
      "Epoch 00002: val_loss improved from 2.53641 to 1.81648, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/002-1.8165.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.3065 - acc: 0.2389 - val_loss: 1.8165 - val_acc: 0.4333\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8642 - acc: 0.3789\n",
      "Epoch 00003: val_loss improved from 1.81648 to 1.51090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/003-1.5109.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.8641 - acc: 0.3789 - val_loss: 1.5109 - val_acc: 0.5267\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6762 - acc: 0.4428\n",
      "Epoch 00004: val_loss improved from 1.51090 to 1.36222, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/004-1.3622.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.6761 - acc: 0.4428 - val_loss: 1.3622 - val_acc: 0.5914\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5578 - acc: 0.4845\n",
      "Epoch 00005: val_loss improved from 1.36222 to 1.25825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/005-1.2583.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.5578 - acc: 0.4845 - val_loss: 1.2583 - val_acc: 0.6150\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4645 - acc: 0.5189\n",
      "Epoch 00006: val_loss improved from 1.25825 to 1.22808, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/006-1.2281.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.4645 - acc: 0.5189 - val_loss: 1.2281 - val_acc: 0.6403\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3872 - acc: 0.5442\n",
      "Epoch 00007: val_loss improved from 1.22808 to 1.11832, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/007-1.1183.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3872 - acc: 0.5441 - val_loss: 1.1183 - val_acc: 0.6583\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3184 - acc: 0.5691\n",
      "Epoch 00008: val_loss improved from 1.11832 to 1.05017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/008-1.0502.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3184 - acc: 0.5691 - val_loss: 1.0502 - val_acc: 0.6956\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2561 - acc: 0.5948\n",
      "Epoch 00009: val_loss improved from 1.05017 to 0.98304, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/009-0.9830.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2561 - acc: 0.5948 - val_loss: 0.9830 - val_acc: 0.7056\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1960 - acc: 0.6167\n",
      "Epoch 00010: val_loss improved from 0.98304 to 0.95167, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/010-0.9517.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1961 - acc: 0.6167 - val_loss: 0.9517 - val_acc: 0.7084\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6368\n",
      "Epoch 00011: val_loss improved from 0.95167 to 0.87200, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/011-0.8720.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1347 - acc: 0.6368 - val_loss: 0.8720 - val_acc: 0.7384\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0730 - acc: 0.6597\n",
      "Epoch 00012: val_loss improved from 0.87200 to 0.79906, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/012-0.7991.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0731 - acc: 0.6597 - val_loss: 0.7991 - val_acc: 0.7727\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0185 - acc: 0.6785\n",
      "Epoch 00013: val_loss improved from 0.79906 to 0.75772, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/013-0.7577.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0186 - acc: 0.6785 - val_loss: 0.7577 - val_acc: 0.7859\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9783 - acc: 0.6912\n",
      "Epoch 00014: val_loss improved from 0.75772 to 0.71360, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/014-0.7136.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9783 - acc: 0.6912 - val_loss: 0.7136 - val_acc: 0.8036\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9306 - acc: 0.7120\n",
      "Epoch 00015: val_loss improved from 0.71360 to 0.66385, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/015-0.6638.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9306 - acc: 0.7119 - val_loss: 0.6638 - val_acc: 0.8130\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8916 - acc: 0.7195\n",
      "Epoch 00016: val_loss improved from 0.66385 to 0.60364, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/016-0.6036.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8917 - acc: 0.7195 - val_loss: 0.6036 - val_acc: 0.8311\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8578 - acc: 0.7354\n",
      "Epoch 00017: val_loss improved from 0.60364 to 0.60157, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/017-0.6016.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8578 - acc: 0.7354 - val_loss: 0.6016 - val_acc: 0.8309\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8130 - acc: 0.7478\n",
      "Epoch 00018: val_loss did not improve from 0.60157\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8129 - acc: 0.7478 - val_loss: 0.6330 - val_acc: 0.8116\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7925 - acc: 0.7573\n",
      "Epoch 00019: val_loss improved from 0.60157 to 0.57668, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/019-0.5767.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7926 - acc: 0.7573 - val_loss: 0.5767 - val_acc: 0.8341\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7584 - acc: 0.7691\n",
      "Epoch 00020: val_loss improved from 0.57668 to 0.52088, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/020-0.5209.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7585 - acc: 0.7691 - val_loss: 0.5209 - val_acc: 0.8609\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7758\n",
      "Epoch 00021: val_loss improved from 0.52088 to 0.47699, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/021-0.4770.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7239 - acc: 0.7757 - val_loss: 0.4770 - val_acc: 0.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.7824\n",
      "Epoch 00022: val_loss did not improve from 0.47699\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7110 - acc: 0.7824 - val_loss: 0.4780 - val_acc: 0.8698\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.7930\n",
      "Epoch 00023: val_loss improved from 0.47699 to 0.46923, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/023-0.4692.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6837 - acc: 0.7930 - val_loss: 0.4692 - val_acc: 0.8696\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.8024\n",
      "Epoch 00024: val_loss improved from 0.46923 to 0.44642, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/024-0.4464.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6542 - acc: 0.8024 - val_loss: 0.4464 - val_acc: 0.8763\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6394 - acc: 0.8044\n",
      "Epoch 00025: val_loss improved from 0.44642 to 0.39715, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/025-0.3972.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6394 - acc: 0.8044 - val_loss: 0.3972 - val_acc: 0.8924\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6276 - acc: 0.8086\n",
      "Epoch 00026: val_loss did not improve from 0.39715\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6276 - acc: 0.8087 - val_loss: 0.4169 - val_acc: 0.8859\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.8185\n",
      "Epoch 00027: val_loss improved from 0.39715 to 0.38767, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/027-0.3877.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5989 - acc: 0.8185 - val_loss: 0.3877 - val_acc: 0.8942\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5903 - acc: 0.8203\n",
      "Epoch 00028: val_loss improved from 0.38767 to 0.38611, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/028-0.3861.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5903 - acc: 0.8203 - val_loss: 0.3861 - val_acc: 0.8931\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8244\n",
      "Epoch 00029: val_loss improved from 0.38611 to 0.34977, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/029-0.3498.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5771 - acc: 0.8244 - val_loss: 0.3498 - val_acc: 0.9008\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.8299\n",
      "Epoch 00030: val_loss did not improve from 0.34977\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5570 - acc: 0.8299 - val_loss: 0.3574 - val_acc: 0.9061\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.8313\n",
      "Epoch 00031: val_loss did not improve from 0.34977\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5525 - acc: 0.8313 - val_loss: 0.4220 - val_acc: 0.8765\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8338\n",
      "Epoch 00032: val_loss improved from 0.34977 to 0.33869, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/032-0.3387.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5450 - acc: 0.8338 - val_loss: 0.3387 - val_acc: 0.9073\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8399\n",
      "Epoch 00033: val_loss did not improve from 0.33869\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5250 - acc: 0.8399 - val_loss: 0.3457 - val_acc: 0.9059\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.8414\n",
      "Epoch 00034: val_loss improved from 0.33869 to 0.31259, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/034-0.3126.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5184 - acc: 0.8415 - val_loss: 0.3126 - val_acc: 0.9154\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5060 - acc: 0.8471\n",
      "Epoch 00035: val_loss improved from 0.31259 to 0.30334, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/035-0.3033.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5060 - acc: 0.8471 - val_loss: 0.3033 - val_acc: 0.9185\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8479\n",
      "Epoch 00036: val_loss did not improve from 0.30334\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4929 - acc: 0.8479 - val_loss: 0.3148 - val_acc: 0.9157\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8504\n",
      "Epoch 00037: val_loss did not improve from 0.30334\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4929 - acc: 0.8504 - val_loss: 0.3078 - val_acc: 0.9136\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8536\n",
      "Epoch 00038: val_loss did not improve from 0.30334\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4842 - acc: 0.8536 - val_loss: 0.3286 - val_acc: 0.9040\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8580\n",
      "Epoch 00039: val_loss did not improve from 0.30334\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4697 - acc: 0.8581 - val_loss: 0.3039 - val_acc: 0.9145\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8584\n",
      "Epoch 00040: val_loss improved from 0.30334 to 0.28249, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/040-0.2825.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4606 - acc: 0.8584 - val_loss: 0.2825 - val_acc: 0.9280\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.8583\n",
      "Epoch 00041: val_loss did not improve from 0.28249\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4642 - acc: 0.8583 - val_loss: 0.2991 - val_acc: 0.9164\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8610\n",
      "Epoch 00042: val_loss improved from 0.28249 to 0.27248, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/042-0.2725.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4543 - acc: 0.8609 - val_loss: 0.2725 - val_acc: 0.9255\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8626\n",
      "Epoch 00043: val_loss did not improve from 0.27248\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4445 - acc: 0.8627 - val_loss: 0.2728 - val_acc: 0.9278\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8645\n",
      "Epoch 00044: val_loss did not improve from 0.27248\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4409 - acc: 0.8645 - val_loss: 0.2742 - val_acc: 0.9292\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8663\n",
      "Epoch 00045: val_loss did not improve from 0.27248\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4346 - acc: 0.8663 - val_loss: 0.2726 - val_acc: 0.9238\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8712\n",
      "Epoch 00046: val_loss improved from 0.27248 to 0.25596, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/046-0.2560.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4229 - acc: 0.8713 - val_loss: 0.2560 - val_acc: 0.9311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8724\n",
      "Epoch 00047: val_loss did not improve from 0.25596\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4181 - acc: 0.8725 - val_loss: 0.2742 - val_acc: 0.9266\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8760\n",
      "Epoch 00048: val_loss did not improve from 0.25596\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4088 - acc: 0.8760 - val_loss: 0.2960 - val_acc: 0.9192\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8721\n",
      "Epoch 00049: val_loss did not improve from 0.25596\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4111 - acc: 0.8721 - val_loss: 0.2600 - val_acc: 0.9259\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8800\n",
      "Epoch 00050: val_loss did not improve from 0.25596\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3974 - acc: 0.8799 - val_loss: 0.2650 - val_acc: 0.9317\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8800\n",
      "Epoch 00051: val_loss improved from 0.25596 to 0.25582, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/051-0.2558.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3935 - acc: 0.8800 - val_loss: 0.2558 - val_acc: 0.9348\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8815\n",
      "Epoch 00052: val_loss improved from 0.25582 to 0.24247, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/052-0.2425.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3924 - acc: 0.8815 - val_loss: 0.2425 - val_acc: 0.9336\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8808\n",
      "Epoch 00053: val_loss did not improve from 0.24247\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3877 - acc: 0.8808 - val_loss: 0.2472 - val_acc: 0.9338\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8803\n",
      "Epoch 00054: val_loss improved from 0.24247 to 0.23856, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/054-0.2386.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3841 - acc: 0.8803 - val_loss: 0.2386 - val_acc: 0.9376\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8835\n",
      "Epoch 00055: val_loss did not improve from 0.23856\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3757 - acc: 0.8835 - val_loss: 0.2568 - val_acc: 0.9304\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8848\n",
      "Epoch 00056: val_loss did not improve from 0.23856\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3732 - acc: 0.8848 - val_loss: 0.2510 - val_acc: 0.9357\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8881\n",
      "Epoch 00057: val_loss improved from 0.23856 to 0.23440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/057-0.2344.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3646 - acc: 0.8881 - val_loss: 0.2344 - val_acc: 0.9352\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8875\n",
      "Epoch 00058: val_loss did not improve from 0.23440\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3619 - acc: 0.8875 - val_loss: 0.2393 - val_acc: 0.9331\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8893\n",
      "Epoch 00059: val_loss did not improve from 0.23440\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3623 - acc: 0.8893 - val_loss: 0.2393 - val_acc: 0.9343\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8907\n",
      "Epoch 00060: val_loss improved from 0.23440 to 0.22723, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/060-0.2272.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3538 - acc: 0.8907 - val_loss: 0.2272 - val_acc: 0.9348\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8935\n",
      "Epoch 00061: val_loss did not improve from 0.22723\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3471 - acc: 0.8934 - val_loss: 0.2372 - val_acc: 0.9357\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8908\n",
      "Epoch 00062: val_loss improved from 0.22723 to 0.22255, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/062-0.2226.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3507 - acc: 0.8908 - val_loss: 0.2226 - val_acc: 0.9422\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8955\n",
      "Epoch 00063: val_loss did not improve from 0.22255\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3362 - acc: 0.8955 - val_loss: 0.2255 - val_acc: 0.9390\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8955\n",
      "Epoch 00064: val_loss improved from 0.22255 to 0.22173, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/064-0.2217.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3416 - acc: 0.8955 - val_loss: 0.2217 - val_acc: 0.9411\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8985\n",
      "Epoch 00065: val_loss improved from 0.22173 to 0.21884, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/065-0.2188.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3287 - acc: 0.8985 - val_loss: 0.2188 - val_acc: 0.9399\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8965\n",
      "Epoch 00066: val_loss did not improve from 0.21884\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3320 - acc: 0.8965 - val_loss: 0.2226 - val_acc: 0.9436\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8992\n",
      "Epoch 00067: val_loss did not improve from 0.21884\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3266 - acc: 0.8992 - val_loss: 0.2610 - val_acc: 0.9322\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.9011\n",
      "Epoch 00068: val_loss improved from 0.21884 to 0.20685, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/068-0.2068.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3246 - acc: 0.9011 - val_loss: 0.2068 - val_acc: 0.9415\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9032\n",
      "Epoch 00069: val_loss did not improve from 0.20685\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3125 - acc: 0.9031 - val_loss: 0.2100 - val_acc: 0.9392\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.9011\n",
      "Epoch 00070: val_loss did not improve from 0.20685\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3151 - acc: 0.9011 - val_loss: 0.2270 - val_acc: 0.9408\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9011\n",
      "Epoch 00071: val_loss did not improve from 0.20685\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3162 - acc: 0.9011 - val_loss: 0.2506 - val_acc: 0.9313\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9032\n",
      "Epoch 00072: val_loss did not improve from 0.20685\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3118 - acc: 0.9032 - val_loss: 0.2213 - val_acc: 0.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9042\n",
      "Epoch 00073: val_loss did not improve from 0.20685\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3062 - acc: 0.9042 - val_loss: 0.2524 - val_acc: 0.9322\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9045\n",
      "Epoch 00074: val_loss improved from 0.20685 to 0.20288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/074-0.2029.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3061 - acc: 0.9046 - val_loss: 0.2029 - val_acc: 0.9453\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9056\n",
      "Epoch 00075: val_loss did not improve from 0.20288\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3043 - acc: 0.9056 - val_loss: 0.2073 - val_acc: 0.9420\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9076\n",
      "Epoch 00076: val_loss improved from 0.20288 to 0.20080, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/076-0.2008.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2973 - acc: 0.9075 - val_loss: 0.2008 - val_acc: 0.9450\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9057\n",
      "Epoch 00077: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3005 - acc: 0.9057 - val_loss: 0.2094 - val_acc: 0.9453\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9064\n",
      "Epoch 00078: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2988 - acc: 0.9064 - val_loss: 0.2187 - val_acc: 0.9383\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9094\n",
      "Epoch 00079: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2937 - acc: 0.9094 - val_loss: 0.2246 - val_acc: 0.9345\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9087\n",
      "Epoch 00080: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2933 - acc: 0.9087 - val_loss: 0.2133 - val_acc: 0.9434\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.9098\n",
      "Epoch 00081: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2912 - acc: 0.9098 - val_loss: 0.2017 - val_acc: 0.9478\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9123\n",
      "Epoch 00082: val_loss did not improve from 0.20080\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2805 - acc: 0.9123 - val_loss: 0.2154 - val_acc: 0.9408\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9101\n",
      "Epoch 00083: val_loss improved from 0.20080 to 0.19228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/083-0.1923.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2830 - acc: 0.9101 - val_loss: 0.1923 - val_acc: 0.9490\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9114\n",
      "Epoch 00084: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2821 - acc: 0.9114 - val_loss: 0.2082 - val_acc: 0.9436\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9129\n",
      "Epoch 00085: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2808 - acc: 0.9129 - val_loss: 0.2002 - val_acc: 0.9469\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9149\n",
      "Epoch 00086: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2714 - acc: 0.9149 - val_loss: 0.2341 - val_acc: 0.9408\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9133\n",
      "Epoch 00087: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2731 - acc: 0.9133 - val_loss: 0.2122 - val_acc: 0.9439\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9154\n",
      "Epoch 00088: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2706 - acc: 0.9154 - val_loss: 0.2048 - val_acc: 0.9469\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9182\n",
      "Epoch 00089: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2675 - acc: 0.9182 - val_loss: 0.1964 - val_acc: 0.9474\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9157\n",
      "Epoch 00090: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2668 - acc: 0.9157 - val_loss: 0.2002 - val_acc: 0.9462\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9172\n",
      "Epoch 00091: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2652 - acc: 0.9172 - val_loss: 0.2081 - val_acc: 0.9429\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9160\n",
      "Epoch 00092: val_loss did not improve from 0.19228\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2621 - acc: 0.9160 - val_loss: 0.2004 - val_acc: 0.9469\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9157\n",
      "Epoch 00093: val_loss improved from 0.19228 to 0.18774, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/093-0.1877.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2675 - acc: 0.9157 - val_loss: 0.1877 - val_acc: 0.9490\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9186\n",
      "Epoch 00094: val_loss improved from 0.18774 to 0.18627, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/094-0.1863.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2574 - acc: 0.9186 - val_loss: 0.1863 - val_acc: 0.9476\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9185\n",
      "Epoch 00095: val_loss did not improve from 0.18627\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2590 - acc: 0.9185 - val_loss: 0.1915 - val_acc: 0.9483\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9182\n",
      "Epoch 00096: val_loss did not improve from 0.18627\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2591 - acc: 0.9182 - val_loss: 0.2058 - val_acc: 0.9462\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9210\n",
      "Epoch 00097: val_loss did not improve from 0.18627\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2519 - acc: 0.9210 - val_loss: 0.1883 - val_acc: 0.9497\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9207\n",
      "Epoch 00098: val_loss improved from 0.18627 to 0.18002, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/098-0.1800.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2475 - acc: 0.9207 - val_loss: 0.1800 - val_acc: 0.9536\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9204\n",
      "Epoch 00099: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2532 - acc: 0.9204 - val_loss: 0.1818 - val_acc: 0.9478\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9215\n",
      "Epoch 00100: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2489 - acc: 0.9215 - val_loss: 0.1867 - val_acc: 0.9515\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9245\n",
      "Epoch 00101: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2441 - acc: 0.9245 - val_loss: 0.1899 - val_acc: 0.9504\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9226\n",
      "Epoch 00102: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2442 - acc: 0.9226 - val_loss: 0.2115 - val_acc: 0.9469\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9227\n",
      "Epoch 00103: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2429 - acc: 0.9228 - val_loss: 0.1836 - val_acc: 0.9518\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9241\n",
      "Epoch 00104: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2402 - acc: 0.9241 - val_loss: 0.1817 - val_acc: 0.9513\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9244\n",
      "Epoch 00105: val_loss did not improve from 0.18002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2398 - acc: 0.9244 - val_loss: 0.1825 - val_acc: 0.9481\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9251\n",
      "Epoch 00106: val_loss improved from 0.18002 to 0.17594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/106-0.1759.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2377 - acc: 0.9251 - val_loss: 0.1759 - val_acc: 0.9534\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9261\n",
      "Epoch 00107: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2340 - acc: 0.9261 - val_loss: 0.1787 - val_acc: 0.9504\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9290\n",
      "Epoch 00108: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2277 - acc: 0.9290 - val_loss: 0.2025 - val_acc: 0.9439\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9268\n",
      "Epoch 00109: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2319 - acc: 0.9268 - val_loss: 0.1825 - val_acc: 0.9502\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9261\n",
      "Epoch 00110: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2330 - acc: 0.9261 - val_loss: 0.1885 - val_acc: 0.9481\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9258\n",
      "Epoch 00111: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2303 - acc: 0.9258 - val_loss: 0.1772 - val_acc: 0.9509\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9275\n",
      "Epoch 00112: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2254 - acc: 0.9275 - val_loss: 0.1826 - val_acc: 0.9518\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9270\n",
      "Epoch 00113: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2284 - acc: 0.9270 - val_loss: 0.1830 - val_acc: 0.9497\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9302\n",
      "Epoch 00114: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2212 - acc: 0.9302 - val_loss: 0.1943 - val_acc: 0.9515\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9287\n",
      "Epoch 00115: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2208 - acc: 0.9287 - val_loss: 0.1899 - val_acc: 0.9490\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9299\n",
      "Epoch 00116: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2218 - acc: 0.9299 - val_loss: 0.1973 - val_acc: 0.9497\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9319\n",
      "Epoch 00117: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2169 - acc: 0.9319 - val_loss: 0.1851 - val_acc: 0.9499\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9294\n",
      "Epoch 00118: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2234 - acc: 0.9294 - val_loss: 0.2018 - val_acc: 0.9455\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9314\n",
      "Epoch 00119: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2170 - acc: 0.9314 - val_loss: 0.2226 - val_acc: 0.9415\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9323\n",
      "Epoch 00120: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2159 - acc: 0.9323 - val_loss: 0.1882 - val_acc: 0.9506\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9320\n",
      "Epoch 00121: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2143 - acc: 0.9320 - val_loss: 0.1853 - val_acc: 0.9509\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9308\n",
      "Epoch 00122: val_loss did not improve from 0.17594\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2153 - acc: 0.9308 - val_loss: 0.1820 - val_acc: 0.9522\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9338\n",
      "Epoch 00123: val_loss improved from 0.17594 to 0.17134, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/123-0.1713.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2083 - acc: 0.9338 - val_loss: 0.1713 - val_acc: 0.9541\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9335\n",
      "Epoch 00124: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2078 - acc: 0.9335 - val_loss: 0.1819 - val_acc: 0.9541\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9323\n",
      "Epoch 00125: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2115 - acc: 0.9323 - val_loss: 0.1790 - val_acc: 0.9541\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9340\n",
      "Epoch 00126: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2099 - acc: 0.9341 - val_loss: 0.1819 - val_acc: 0.9499\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9351\n",
      "Epoch 00127: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2050 - acc: 0.9351 - val_loss: 0.1748 - val_acc: 0.9534\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9347\n",
      "Epoch 00128: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2064 - acc: 0.9347 - val_loss: 0.1910 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9350\n",
      "Epoch 00129: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2033 - acc: 0.9350 - val_loss: 0.1778 - val_acc: 0.9536\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9354\n",
      "Epoch 00130: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2044 - acc: 0.9354 - val_loss: 0.1886 - val_acc: 0.9469\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9354\n",
      "Epoch 00131: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2027 - acc: 0.9354 - val_loss: 0.1928 - val_acc: 0.9527\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9365\n",
      "Epoch 00132: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1983 - acc: 0.9365 - val_loss: 0.1837 - val_acc: 0.9534\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9370\n",
      "Epoch 00133: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2001 - acc: 0.9370 - val_loss: 0.1963 - val_acc: 0.9509\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9369\n",
      "Epoch 00134: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1937 - acc: 0.9369 - val_loss: 0.1872 - val_acc: 0.9520\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9372\n",
      "Epoch 00135: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2009 - acc: 0.9372 - val_loss: 0.1753 - val_acc: 0.9532\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9367\n",
      "Epoch 00136: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1986 - acc: 0.9367 - val_loss: 0.1871 - val_acc: 0.9511\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9401\n",
      "Epoch 00137: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1885 - acc: 0.9401 - val_loss: 0.1790 - val_acc: 0.9555\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9370\n",
      "Epoch 00138: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1959 - acc: 0.9369 - val_loss: 0.1837 - val_acc: 0.9504\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9376\n",
      "Epoch 00139: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1947 - acc: 0.9376 - val_loss: 0.1803 - val_acc: 0.9497\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1934 - acc: 0.9385\n",
      "Epoch 00140: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1933 - acc: 0.9385 - val_loss: 0.1865 - val_acc: 0.9515\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9370\n",
      "Epoch 00141: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1926 - acc: 0.9370 - val_loss: 0.1908 - val_acc: 0.9497\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9388\n",
      "Epoch 00142: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1888 - acc: 0.9388 - val_loss: 0.1992 - val_acc: 0.9469\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9395\n",
      "Epoch 00143: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1847 - acc: 0.9395 - val_loss: 0.1886 - val_acc: 0.9534\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9395\n",
      "Epoch 00144: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1869 - acc: 0.9395 - val_loss: 0.1826 - val_acc: 0.9488\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9408\n",
      "Epoch 00145: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1874 - acc: 0.9408 - val_loss: 0.1775 - val_acc: 0.9539\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9392\n",
      "Epoch 00146: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1873 - acc: 0.9392 - val_loss: 0.1916 - val_acc: 0.9495\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9409\n",
      "Epoch 00147: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1854 - acc: 0.9409 - val_loss: 0.1870 - val_acc: 0.9536\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9408\n",
      "Epoch 00148: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1810 - acc: 0.9408 - val_loss: 0.1856 - val_acc: 0.9536\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9430\n",
      "Epoch 00149: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1818 - acc: 0.9430 - val_loss: 0.1872 - val_acc: 0.9509\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9407\n",
      "Epoch 00150: val_loss did not improve from 0.17134\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1834 - acc: 0.9407 - val_loss: 0.1809 - val_acc: 0.9553\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9418\n",
      "Epoch 00151: val_loss improved from 0.17134 to 0.16962, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/151-0.1696.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1786 - acc: 0.9418 - val_loss: 0.1696 - val_acc: 0.9560\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9418\n",
      "Epoch 00152: val_loss did not improve from 0.16962\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1812 - acc: 0.9419 - val_loss: 0.2050 - val_acc: 0.9499\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9438\n",
      "Epoch 00153: val_loss did not improve from 0.16962\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1750 - acc: 0.9437 - val_loss: 0.1814 - val_acc: 0.9548\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9454\n",
      "Epoch 00154: val_loss did not improve from 0.16962\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1721 - acc: 0.9454 - val_loss: 0.1815 - val_acc: 0.9520\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9437\n",
      "Epoch 00155: val_loss did not improve from 0.16962\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1754 - acc: 0.9437 - val_loss: 0.1834 - val_acc: 0.9515\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9437\n",
      "Epoch 00156: val_loss did not improve from 0.16962\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1760 - acc: 0.9437 - val_loss: 0.1741 - val_acc: 0.9543\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9432\n",
      "Epoch 00157: val_loss improved from 0.16962 to 0.16761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv_checkpoint/157-0.1676.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1755 - acc: 0.9432 - val_loss: 0.1676 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9440\n",
      "Epoch 00158: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1707 - acc: 0.9440 - val_loss: 0.1928 - val_acc: 0.9511\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9417\n",
      "Epoch 00159: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1771 - acc: 0.9417 - val_loss: 0.1678 - val_acc: 0.9555\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9460\n",
      "Epoch 00160: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1721 - acc: 0.9460 - val_loss: 0.1758 - val_acc: 0.9581\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9451\n",
      "Epoch 00161: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1691 - acc: 0.9450 - val_loss: 0.1829 - val_acc: 0.9525\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9432\n",
      "Epoch 00162: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1713 - acc: 0.9431 - val_loss: 0.1704 - val_acc: 0.9583\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9454\n",
      "Epoch 00163: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1692 - acc: 0.9454 - val_loss: 0.1817 - val_acc: 0.9560\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9467\n",
      "Epoch 00164: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1638 - acc: 0.9467 - val_loss: 0.1694 - val_acc: 0.9576\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9470\n",
      "Epoch 00165: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1643 - acc: 0.9470 - val_loss: 0.1796 - val_acc: 0.9548\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9470\n",
      "Epoch 00166: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1632 - acc: 0.9470 - val_loss: 0.1742 - val_acc: 0.9571\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9458\n",
      "Epoch 00167: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1679 - acc: 0.9458 - val_loss: 0.1900 - val_acc: 0.9520\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9470\n",
      "Epoch 00168: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1631 - acc: 0.9470 - val_loss: 0.1754 - val_acc: 0.9562\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1587 - acc: 0.9479\n",
      "Epoch 00169: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1586 - acc: 0.9479 - val_loss: 0.1840 - val_acc: 0.9548\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9482\n",
      "Epoch 00170: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1614 - acc: 0.9482 - val_loss: 0.1869 - val_acc: 0.9555\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9470\n",
      "Epoch 00171: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1599 - acc: 0.9470 - val_loss: 0.1713 - val_acc: 0.9597\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9496\n",
      "Epoch 00172: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1564 - acc: 0.9496 - val_loss: 0.1965 - val_acc: 0.9518\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9495\n",
      "Epoch 00173: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1584 - acc: 0.9495 - val_loss: 0.1838 - val_acc: 0.9557\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9506\n",
      "Epoch 00174: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1518 - acc: 0.9506 - val_loss: 0.1908 - val_acc: 0.9534\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9497\n",
      "Epoch 00175: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1548 - acc: 0.9497 - val_loss: 0.1859 - val_acc: 0.9543\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9492\n",
      "Epoch 00176: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1539 - acc: 0.9492 - val_loss: 0.1775 - val_acc: 0.9564\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9506\n",
      "Epoch 00177: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1530 - acc: 0.9506 - val_loss: 0.1757 - val_acc: 0.9588\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9491\n",
      "Epoch 00178: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1573 - acc: 0.9491 - val_loss: 0.1765 - val_acc: 0.9574\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9508\n",
      "Epoch 00179: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1499 - acc: 0.9508 - val_loss: 0.1726 - val_acc: 0.9564\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9511\n",
      "Epoch 00180: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1522 - acc: 0.9511 - val_loss: 0.1801 - val_acc: 0.9562\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9504\n",
      "Epoch 00181: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1529 - acc: 0.9504 - val_loss: 0.1831 - val_acc: 0.9520\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9482\n",
      "Epoch 00182: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1554 - acc: 0.9482 - val_loss: 0.1770 - val_acc: 0.9557\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9507\n",
      "Epoch 00183: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1498 - acc: 0.9507 - val_loss: 0.1770 - val_acc: 0.9581\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9498\n",
      "Epoch 00184: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1552 - acc: 0.9497 - val_loss: 0.1755 - val_acc: 0.9581\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9504\n",
      "Epoch 00185: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1530 - acc: 0.9504 - val_loss: 0.1828 - val_acc: 0.9560\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9524\n",
      "Epoch 00186: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1479 - acc: 0.9524 - val_loss: 0.1914 - val_acc: 0.9550\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9511\n",
      "Epoch 00187: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1472 - acc: 0.9511 - val_loss: 0.1871 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9514\n",
      "Epoch 00188: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1455 - acc: 0.9514 - val_loss: 0.1984 - val_acc: 0.9529\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9526\n",
      "Epoch 00189: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1446 - acc: 0.9526 - val_loss: 0.1754 - val_acc: 0.9553\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9520\n",
      "Epoch 00190: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1446 - acc: 0.9520 - val_loss: 0.1766 - val_acc: 0.9562\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9531\n",
      "Epoch 00191: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1428 - acc: 0.9531 - val_loss: 0.1842 - val_acc: 0.9553\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9523\n",
      "Epoch 00192: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1389 - acc: 0.9523 - val_loss: 0.1869 - val_acc: 0.9588\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9516\n",
      "Epoch 00193: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1472 - acc: 0.9516 - val_loss: 0.1865 - val_acc: 0.9539\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9520\n",
      "Epoch 00194: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1470 - acc: 0.9520 - val_loss: 0.1918 - val_acc: 0.9543\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9533\n",
      "Epoch 00195: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1418 - acc: 0.9532 - val_loss: 0.1852 - val_acc: 0.9539\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9530\n",
      "Epoch 00196: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1393 - acc: 0.9530 - val_loss: 0.1848 - val_acc: 0.9546\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9550\n",
      "Epoch 00197: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1379 - acc: 0.9550 - val_loss: 0.1878 - val_acc: 0.9511\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9538\n",
      "Epoch 00198: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1391 - acc: 0.9538 - val_loss: 0.1820 - val_acc: 0.9576\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9530\n",
      "Epoch 00199: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1409 - acc: 0.9530 - val_loss: 0.1869 - val_acc: 0.9557\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9551\n",
      "Epoch 00200: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1381 - acc: 0.9551 - val_loss: 0.1877 - val_acc: 0.9536\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9548\n",
      "Epoch 00201: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1389 - acc: 0.9548 - val_loss: 0.1923 - val_acc: 0.9592\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9546\n",
      "Epoch 00202: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1366 - acc: 0.9547 - val_loss: 0.1868 - val_acc: 0.9602\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9536\n",
      "Epoch 00203: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1403 - acc: 0.9536 - val_loss: 0.1810 - val_acc: 0.9578\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9555\n",
      "Epoch 00204: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1362 - acc: 0.9555 - val_loss: 0.1748 - val_acc: 0.9592\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9545\n",
      "Epoch 00205: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1373 - acc: 0.9545 - val_loss: 0.1678 - val_acc: 0.9602\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9555\n",
      "Epoch 00206: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1351 - acc: 0.9555 - val_loss: 0.1802 - val_acc: 0.9562\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9573\n",
      "Epoch 00207: val_loss did not improve from 0.16761\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1305 - acc: 0.9573 - val_loss: 0.1776 - val_acc: 0.9602\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 571us/sample - loss: 0.1951 - acc: 0.9427\n",
      "Loss: 0.19514510566417054 Accuracy: 0.9426791\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7180 - acc: 0.0865\n",
      "Epoch 00001: val_loss improved from inf to 2.62728, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/001-2.6273.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.7180 - acc: 0.0865 - val_loss: 2.6273 - val_acc: 0.1363\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4570 - acc: 0.1927\n",
      "Epoch 00002: val_loss improved from 2.62728 to 2.01774, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/002-2.0177.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.4569 - acc: 0.1927 - val_loss: 2.0177 - val_acc: 0.3615\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0445 - acc: 0.3229\n",
      "Epoch 00003: val_loss improved from 2.01774 to 1.62393, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/003-1.6239.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.0445 - acc: 0.3229 - val_loss: 1.6239 - val_acc: 0.4803\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7473 - acc: 0.4124\n",
      "Epoch 00004: val_loss improved from 1.62393 to 1.33408, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/004-1.3341.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.7474 - acc: 0.4124 - val_loss: 1.3341 - val_acc: 0.5721\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5424 - acc: 0.4837\n",
      "Epoch 00005: val_loss improved from 1.33408 to 1.17901, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/005-1.1790.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.5424 - acc: 0.4837 - val_loss: 1.1790 - val_acc: 0.6292\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4202 - acc: 0.5219\n",
      "Epoch 00006: val_loss improved from 1.17901 to 1.05358, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/006-1.0536.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4202 - acc: 0.5219 - val_loss: 1.0536 - val_acc: 0.6646\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2998 - acc: 0.5645\n",
      "Epoch 00007: val_loss improved from 1.05358 to 0.96957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/007-0.9696.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2997 - acc: 0.5645 - val_loss: 0.9696 - val_acc: 0.6886\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2131 - acc: 0.5938\n",
      "Epoch 00008: val_loss did not improve from 0.96957\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2131 - acc: 0.5938 - val_loss: 0.9871 - val_acc: 0.6771\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1423 - acc: 0.6186\n",
      "Epoch 00009: val_loss improved from 0.96957 to 0.85124, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/009-0.8512.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1423 - acc: 0.6186 - val_loss: 0.8512 - val_acc: 0.7331\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6469\n",
      "Epoch 00010: val_loss improved from 0.85124 to 0.76575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/010-0.7658.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0697 - acc: 0.6468 - val_loss: 0.7658 - val_acc: 0.7589\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0113 - acc: 0.6683\n",
      "Epoch 00011: val_loss improved from 0.76575 to 0.71463, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/011-0.7146.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0114 - acc: 0.6682 - val_loss: 0.7146 - val_acc: 0.7747\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9619 - acc: 0.6858\n",
      "Epoch 00012: val_loss improved from 0.71463 to 0.65040, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/012-0.6504.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9619 - acc: 0.6858 - val_loss: 0.6504 - val_acc: 0.7985\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9036 - acc: 0.7058\n",
      "Epoch 00013: val_loss improved from 0.65040 to 0.58982, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/013-0.5898.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9036 - acc: 0.7058 - val_loss: 0.5898 - val_acc: 0.8272\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8572 - acc: 0.7233\n",
      "Epoch 00014: val_loss improved from 0.58982 to 0.54658, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/014-0.5466.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8572 - acc: 0.7233 - val_loss: 0.5466 - val_acc: 0.8339\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8161 - acc: 0.7374\n",
      "Epoch 00015: val_loss improved from 0.54658 to 0.53088, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/015-0.5309.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8161 - acc: 0.7374 - val_loss: 0.5309 - val_acc: 0.8500\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7494\n",
      "Epoch 00016: val_loss improved from 0.53088 to 0.49376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/016-0.4938.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7777 - acc: 0.7494 - val_loss: 0.4938 - val_acc: 0.8544\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7472 - acc: 0.7652\n",
      "Epoch 00017: val_loss improved from 0.49376 to 0.46435, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/017-0.4643.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7471 - acc: 0.7652 - val_loss: 0.4643 - val_acc: 0.8635\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.7745\n",
      "Epoch 00018: val_loss improved from 0.46435 to 0.43819, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/018-0.4382.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7134 - acc: 0.7745 - val_loss: 0.4382 - val_acc: 0.8707\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6854 - acc: 0.7835\n",
      "Epoch 00019: val_loss improved from 0.43819 to 0.40129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/019-0.4013.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6853 - acc: 0.7835 - val_loss: 0.4013 - val_acc: 0.8737\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6566 - acc: 0.7927\n",
      "Epoch 00020: val_loss improved from 0.40129 to 0.36848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/020-0.3685.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6565 - acc: 0.7928 - val_loss: 0.3685 - val_acc: 0.8940\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.8040\n",
      "Epoch 00021: val_loss improved from 0.36848 to 0.36132, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/021-0.3613.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6300 - acc: 0.8039 - val_loss: 0.3613 - val_acc: 0.8989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.8101\n",
      "Epoch 00022: val_loss improved from 0.36132 to 0.34973, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/022-0.3497.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6104 - acc: 0.8101 - val_loss: 0.3497 - val_acc: 0.9022\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5826 - acc: 0.8198\n",
      "Epoch 00023: val_loss improved from 0.34973 to 0.33598, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/023-0.3360.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5827 - acc: 0.8198 - val_loss: 0.3360 - val_acc: 0.9078\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.8223\n",
      "Epoch 00024: val_loss improved from 0.33598 to 0.32297, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/024-0.3230.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5667 - acc: 0.8223 - val_loss: 0.3230 - val_acc: 0.9045\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8323\n",
      "Epoch 00025: val_loss improved from 0.32297 to 0.30524, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/025-0.3052.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5425 - acc: 0.8322 - val_loss: 0.3052 - val_acc: 0.9143\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8370\n",
      "Epoch 00026: val_loss improved from 0.30524 to 0.28519, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/026-0.2852.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5238 - acc: 0.8370 - val_loss: 0.2852 - val_acc: 0.9185\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8410\n",
      "Epoch 00027: val_loss improved from 0.28519 to 0.28032, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/027-0.2803.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5113 - acc: 0.8409 - val_loss: 0.2803 - val_acc: 0.9210\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8460\n",
      "Epoch 00028: val_loss did not improve from 0.28032\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4985 - acc: 0.8460 - val_loss: 0.2860 - val_acc: 0.9187\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4871 - acc: 0.8505\n",
      "Epoch 00029: val_loss improved from 0.28032 to 0.25721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/029-0.2572.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4871 - acc: 0.8505 - val_loss: 0.2572 - val_acc: 0.9271\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8563\n",
      "Epoch 00030: val_loss improved from 0.25721 to 0.25428, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/030-0.2543.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4727 - acc: 0.8564 - val_loss: 0.2543 - val_acc: 0.9259\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8561\n",
      "Epoch 00031: val_loss improved from 0.25428 to 0.24815, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/031-0.2482.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4630 - acc: 0.8561 - val_loss: 0.2482 - val_acc: 0.9259\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8637\n",
      "Epoch 00032: val_loss improved from 0.24815 to 0.24693, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/032-0.2469.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4444 - acc: 0.8637 - val_loss: 0.2469 - val_acc: 0.9278\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8669\n",
      "Epoch 00033: val_loss improved from 0.24693 to 0.23391, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/033-0.2339.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4321 - acc: 0.8669 - val_loss: 0.2339 - val_acc: 0.9331\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8675\n",
      "Epoch 00034: val_loss improved from 0.23391 to 0.22294, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/034-0.2229.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4281 - acc: 0.8675 - val_loss: 0.2229 - val_acc: 0.9359\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4173 - acc: 0.8715\n",
      "Epoch 00035: val_loss improved from 0.22294 to 0.20933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/035-0.2093.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4173 - acc: 0.8715 - val_loss: 0.2093 - val_acc: 0.9443\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8737\n",
      "Epoch 00036: val_loss did not improve from 0.20933\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4047 - acc: 0.8738 - val_loss: 0.2208 - val_acc: 0.9350\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8780\n",
      "Epoch 00037: val_loss did not improve from 0.20933\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3987 - acc: 0.8780 - val_loss: 0.2156 - val_acc: 0.9352\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8799\n",
      "Epoch 00038: val_loss did not improve from 0.20933\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3898 - acc: 0.8799 - val_loss: 0.2132 - val_acc: 0.9380\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8800\n",
      "Epoch 00039: val_loss improved from 0.20933 to 0.20790, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/039-0.2079.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3824 - acc: 0.8800 - val_loss: 0.2079 - val_acc: 0.9429\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8835\n",
      "Epoch 00040: val_loss improved from 0.20790 to 0.19506, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/040-0.1951.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3808 - acc: 0.8835 - val_loss: 0.1951 - val_acc: 0.9432\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8846\n",
      "Epoch 00041: val_loss did not improve from 0.19506\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3690 - acc: 0.8846 - val_loss: 0.2011 - val_acc: 0.9422\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8894\n",
      "Epoch 00042: val_loss did not improve from 0.19506\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3574 - acc: 0.8894 - val_loss: 0.2017 - val_acc: 0.9432\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3522 - acc: 0.8916\n",
      "Epoch 00043: val_loss improved from 0.19506 to 0.19082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/043-0.1908.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3521 - acc: 0.8916 - val_loss: 0.1908 - val_acc: 0.9483\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8961\n",
      "Epoch 00044: val_loss did not improve from 0.19082\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3387 - acc: 0.8962 - val_loss: 0.2015 - val_acc: 0.9441\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8953\n",
      "Epoch 00045: val_loss improved from 0.19082 to 0.18623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/045-0.1862.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3385 - acc: 0.8953 - val_loss: 0.1862 - val_acc: 0.9481\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8965\n",
      "Epoch 00046: val_loss improved from 0.18623 to 0.18409, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/046-0.1841.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3392 - acc: 0.8966 - val_loss: 0.1841 - val_acc: 0.9485\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8971\n",
      "Epoch 00047: val_loss improved from 0.18409 to 0.17894, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/047-0.1789.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3348 - acc: 0.8971 - val_loss: 0.1789 - val_acc: 0.9518\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.9006\n",
      "Epoch 00048: val_loss did not improve from 0.17894\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3242 - acc: 0.9005 - val_loss: 0.2084 - val_acc: 0.9394\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9024\n",
      "Epoch 00049: val_loss improved from 0.17894 to 0.17392, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/049-0.1739.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3178 - acc: 0.9024 - val_loss: 0.1739 - val_acc: 0.9499\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.9053\n",
      "Epoch 00050: val_loss improved from 0.17392 to 0.16788, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/050-0.1679.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3084 - acc: 0.9053 - val_loss: 0.1679 - val_acc: 0.9518\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9050\n",
      "Epoch 00051: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3074 - acc: 0.9050 - val_loss: 0.1841 - val_acc: 0.9490\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9066\n",
      "Epoch 00052: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2996 - acc: 0.9066 - val_loss: 0.1689 - val_acc: 0.9504\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9074\n",
      "Epoch 00053: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2975 - acc: 0.9074 - val_loss: 0.1791 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9095\n",
      "Epoch 00054: val_loss did not improve from 0.16788\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2952 - acc: 0.9096 - val_loss: 0.1700 - val_acc: 0.9481\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9117\n",
      "Epoch 00055: val_loss improved from 0.16788 to 0.16260, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/055-0.1626.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2839 - acc: 0.9117 - val_loss: 0.1626 - val_acc: 0.9553\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9109\n",
      "Epoch 00056: val_loss improved from 0.16260 to 0.15536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/056-0.1554.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2857 - acc: 0.9109 - val_loss: 0.1554 - val_acc: 0.9557\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9131\n",
      "Epoch 00057: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2793 - acc: 0.9131 - val_loss: 0.1972 - val_acc: 0.9443\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9132\n",
      "Epoch 00058: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2773 - acc: 0.9132 - val_loss: 0.1643 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9161\n",
      "Epoch 00059: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2699 - acc: 0.9161 - val_loss: 0.1592 - val_acc: 0.9525\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9163\n",
      "Epoch 00060: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2707 - acc: 0.9163 - val_loss: 0.1606 - val_acc: 0.9543\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9196\n",
      "Epoch 00061: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2625 - acc: 0.9196 - val_loss: 0.1592 - val_acc: 0.9534\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9190\n",
      "Epoch 00062: val_loss did not improve from 0.15536\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2610 - acc: 0.9190 - val_loss: 0.1589 - val_acc: 0.9560\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9186\n",
      "Epoch 00063: val_loss improved from 0.15536 to 0.15245, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/063-0.1524.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2602 - acc: 0.9187 - val_loss: 0.1524 - val_acc: 0.9548\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9213\n",
      "Epoch 00064: val_loss did not improve from 0.15245\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2495 - acc: 0.9212 - val_loss: 0.1541 - val_acc: 0.9529\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9240\n",
      "Epoch 00065: val_loss did not improve from 0.15245\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2450 - acc: 0.9240 - val_loss: 0.1841 - val_acc: 0.9481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9207\n",
      "Epoch 00066: val_loss did not improve from 0.15245\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2539 - acc: 0.9207 - val_loss: 0.1574 - val_acc: 0.9532\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9221\n",
      "Epoch 00067: val_loss did not improve from 0.15245\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2440 - acc: 0.9220 - val_loss: 0.1815 - val_acc: 0.9427\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9242\n",
      "Epoch 00068: val_loss did not improve from 0.15245\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2435 - acc: 0.9242 - val_loss: 0.1573 - val_acc: 0.9529\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9273\n",
      "Epoch 00069: val_loss improved from 0.15245 to 0.14030, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/069-0.1403.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2365 - acc: 0.9273 - val_loss: 0.1403 - val_acc: 0.9602\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9282\n",
      "Epoch 00070: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2327 - acc: 0.9282 - val_loss: 0.1572 - val_acc: 0.9557\n",
      "Epoch 71/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9264\n",
      "Epoch 00071: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2354 - acc: 0.9264 - val_loss: 0.1491 - val_acc: 0.9543\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9284\n",
      "Epoch 00072: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2309 - acc: 0.9284 - val_loss: 0.1488 - val_acc: 0.9541\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9286\n",
      "Epoch 00073: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2290 - acc: 0.9286 - val_loss: 0.1448 - val_acc: 0.9578\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9313\n",
      "Epoch 00074: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2197 - acc: 0.9313 - val_loss: 0.1567 - val_acc: 0.9511\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9314\n",
      "Epoch 00075: val_loss improved from 0.14030 to 0.13898, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/075-0.1390.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2193 - acc: 0.9314 - val_loss: 0.1390 - val_acc: 0.9599\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9329\n",
      "Epoch 00076: val_loss did not improve from 0.13898\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2170 - acc: 0.9329 - val_loss: 0.1537 - val_acc: 0.9564\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9326\n",
      "Epoch 00077: val_loss did not improve from 0.13898\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2104 - acc: 0.9326 - val_loss: 0.1632 - val_acc: 0.9536\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9341\n",
      "Epoch 00078: val_loss improved from 0.13898 to 0.13010, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/078-0.1301.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2121 - acc: 0.9341 - val_loss: 0.1301 - val_acc: 0.9634\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9338\n",
      "Epoch 00079: val_loss did not improve from 0.13010\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2114 - acc: 0.9338 - val_loss: 0.1368 - val_acc: 0.9616\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9361\n",
      "Epoch 00080: val_loss did not improve from 0.13010\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2068 - acc: 0.9361 - val_loss: 0.1411 - val_acc: 0.9604\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9334\n",
      "Epoch 00081: val_loss did not improve from 0.13010\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2083 - acc: 0.9334 - val_loss: 0.1333 - val_acc: 0.9602\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9344\n",
      "Epoch 00082: val_loss did not improve from 0.13010\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2052 - acc: 0.9344 - val_loss: 0.1392 - val_acc: 0.9597\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9368\n",
      "Epoch 00083: val_loss did not improve from 0.13010\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1994 - acc: 0.9368 - val_loss: 0.1456 - val_acc: 0.9597\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9370\n",
      "Epoch 00084: val_loss improved from 0.13010 to 0.12834, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/084-0.1283.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1986 - acc: 0.9370 - val_loss: 0.1283 - val_acc: 0.9618\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9359\n",
      "Epoch 00085: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2009 - acc: 0.9359 - val_loss: 0.1506 - val_acc: 0.9567\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9379\n",
      "Epoch 00086: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1950 - acc: 0.9379 - val_loss: 0.1308 - val_acc: 0.9618\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9393\n",
      "Epoch 00087: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1930 - acc: 0.9393 - val_loss: 0.1586 - val_acc: 0.9602\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9407\n",
      "Epoch 00088: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1895 - acc: 0.9407 - val_loss: 0.1613 - val_acc: 0.9511\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9393\n",
      "Epoch 00089: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1898 - acc: 0.9393 - val_loss: 0.1485 - val_acc: 0.9592\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9402\n",
      "Epoch 00090: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1842 - acc: 0.9402 - val_loss: 0.1329 - val_acc: 0.9634\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9417\n",
      "Epoch 00091: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1822 - acc: 0.9417 - val_loss: 0.1304 - val_acc: 0.9599\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9430\n",
      "Epoch 00092: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1789 - acc: 0.9431 - val_loss: 0.1378 - val_acc: 0.9637\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9423\n",
      "Epoch 00093: val_loss did not improve from 0.12834\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1783 - acc: 0.9423 - val_loss: 0.1630 - val_acc: 0.9550\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9439\n",
      "Epoch 00094: val_loss improved from 0.12834 to 0.12546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/094-0.1255.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1774 - acc: 0.9439 - val_loss: 0.1255 - val_acc: 0.9616\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9443\n",
      "Epoch 00095: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1762 - acc: 0.9443 - val_loss: 0.1298 - val_acc: 0.9627\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9436\n",
      "Epoch 00096: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1753 - acc: 0.9436 - val_loss: 0.1267 - val_acc: 0.9630\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9456\n",
      "Epoch 00097: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1718 - acc: 0.9456 - val_loss: 0.1304 - val_acc: 0.9623\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9449\n",
      "Epoch 00098: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1736 - acc: 0.9450 - val_loss: 0.1378 - val_acc: 0.9613\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9452\n",
      "Epoch 00099: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1727 - acc: 0.9452 - val_loss: 0.1382 - val_acc: 0.9613\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9474\n",
      "Epoch 00100: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1672 - acc: 0.9474 - val_loss: 0.1448 - val_acc: 0.9623\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9455\n",
      "Epoch 00101: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1700 - acc: 0.9455 - val_loss: 0.1311 - val_acc: 0.9634\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9483\n",
      "Epoch 00102: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1624 - acc: 0.9483 - val_loss: 0.1261 - val_acc: 0.9651\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9478\n",
      "Epoch 00103: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1601 - acc: 0.9478 - val_loss: 0.1355 - val_acc: 0.9632\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9494\n",
      "Epoch 00104: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1616 - acc: 0.9494 - val_loss: 0.1376 - val_acc: 0.9641\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9507\n",
      "Epoch 00105: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1565 - acc: 0.9507 - val_loss: 0.1354 - val_acc: 0.9611\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9491\n",
      "Epoch 00106: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1560 - acc: 0.9491 - val_loss: 0.1402 - val_acc: 0.9604\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9503\n",
      "Epoch 00107: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1554 - acc: 0.9503 - val_loss: 0.1346 - val_acc: 0.9625\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9493\n",
      "Epoch 00108: val_loss did not improve from 0.12546\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1570 - acc: 0.9493 - val_loss: 0.1277 - val_acc: 0.9637\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9505\n",
      "Epoch 00109: val_loss improved from 0.12546 to 0.12003, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/109-0.1200.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1529 - acc: 0.9506 - val_loss: 0.1200 - val_acc: 0.9660\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9515\n",
      "Epoch 00110: val_loss did not improve from 0.12003\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1517 - acc: 0.9516 - val_loss: 0.1428 - val_acc: 0.9611\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9507\n",
      "Epoch 00111: val_loss did not improve from 0.12003\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1504 - acc: 0.9507 - val_loss: 0.1544 - val_acc: 0.9583\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9513\n",
      "Epoch 00112: val_loss did not improve from 0.12003\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1505 - acc: 0.9513 - val_loss: 0.1255 - val_acc: 0.9637\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.9543\n",
      "Epoch 00113: val_loss improved from 0.12003 to 0.11916, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/113-0.1192.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1468 - acc: 0.9543 - val_loss: 0.1192 - val_acc: 0.9683\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9519\n",
      "Epoch 00114: val_loss improved from 0.11916 to 0.11841, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/114-0.1184.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1515 - acc: 0.9519 - val_loss: 0.1184 - val_acc: 0.9658\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9533\n",
      "Epoch 00115: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1432 - acc: 0.9533 - val_loss: 0.1187 - val_acc: 0.9658\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9538\n",
      "Epoch 00116: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1414 - acc: 0.9538 - val_loss: 0.1311 - val_acc: 0.9646\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9530\n",
      "Epoch 00117: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1451 - acc: 0.9530 - val_loss: 0.1274 - val_acc: 0.9655\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9546\n",
      "Epoch 00118: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1386 - acc: 0.9547 - val_loss: 0.1336 - val_acc: 0.9641\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9546\n",
      "Epoch 00119: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1404 - acc: 0.9545 - val_loss: 0.1307 - val_acc: 0.9651\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9549\n",
      "Epoch 00120: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1368 - acc: 0.9549 - val_loss: 0.1482 - val_acc: 0.9609\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9562\n",
      "Epoch 00121: val_loss did not improve from 0.11841\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1346 - acc: 0.9562 - val_loss: 0.1255 - val_acc: 0.9665\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9568\n",
      "Epoch 00122: val_loss improved from 0.11841 to 0.11793, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/122-0.1179.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1340 - acc: 0.9568 - val_loss: 0.1179 - val_acc: 0.9667\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9568\n",
      "Epoch 00123: val_loss did not improve from 0.11793\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1323 - acc: 0.9568 - val_loss: 0.1307 - val_acc: 0.9639\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9565\n",
      "Epoch 00124: val_loss improved from 0.11793 to 0.11613, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/124-0.1161.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1346 - acc: 0.9565 - val_loss: 0.1161 - val_acc: 0.9646\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9589\n",
      "Epoch 00125: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1277 - acc: 0.9589 - val_loss: 0.1300 - val_acc: 0.9641\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9574\n",
      "Epoch 00126: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1330 - acc: 0.9574 - val_loss: 0.1529 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9583\n",
      "Epoch 00127: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1295 - acc: 0.9583 - val_loss: 0.1275 - val_acc: 0.9683\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9584\n",
      "Epoch 00128: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1316 - acc: 0.9584 - val_loss: 0.1180 - val_acc: 0.9667\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9568\n",
      "Epoch 00129: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1316 - acc: 0.9569 - val_loss: 0.1256 - val_acc: 0.9662\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9577\n",
      "Epoch 00130: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1272 - acc: 0.9577 - val_loss: 0.1210 - val_acc: 0.9660\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9590\n",
      "Epoch 00131: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1265 - acc: 0.9590 - val_loss: 0.1264 - val_acc: 0.9669\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9572\n",
      "Epoch 00132: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1306 - acc: 0.9572 - val_loss: 0.1299 - val_acc: 0.9676\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9619\n",
      "Epoch 00133: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1192 - acc: 0.9619 - val_loss: 0.1244 - val_acc: 0.9683\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9607\n",
      "Epoch 00134: val_loss did not improve from 0.11613\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1209 - acc: 0.9607 - val_loss: 0.1408 - val_acc: 0.9630\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9615\n",
      "Epoch 00135: val_loss improved from 0.11613 to 0.11285, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv_checkpoint/135-0.1128.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1205 - acc: 0.9616 - val_loss: 0.1128 - val_acc: 0.9674\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9596\n",
      "Epoch 00136: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1224 - acc: 0.9596 - val_loss: 0.1208 - val_acc: 0.9658\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9612\n",
      "Epoch 00137: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1239 - acc: 0.9612 - val_loss: 0.1293 - val_acc: 0.9653\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9595\n",
      "Epoch 00138: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1198 - acc: 0.9595 - val_loss: 0.1229 - val_acc: 0.9665\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9605\n",
      "Epoch 00139: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1183 - acc: 0.9605 - val_loss: 0.1260 - val_acc: 0.9690\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9620\n",
      "Epoch 00140: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1159 - acc: 0.9620 - val_loss: 0.1353 - val_acc: 0.9655\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9612\n",
      "Epoch 00141: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1160 - acc: 0.9612 - val_loss: 0.1248 - val_acc: 0.9665\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9617\n",
      "Epoch 00142: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1166 - acc: 0.9617 - val_loss: 0.1183 - val_acc: 0.9672\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9634\n",
      "Epoch 00143: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1116 - acc: 0.9633 - val_loss: 0.1283 - val_acc: 0.9632\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9625\n",
      "Epoch 00144: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1145 - acc: 0.9625 - val_loss: 0.1207 - val_acc: 0.9690\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9640\n",
      "Epoch 00145: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1104 - acc: 0.9640 - val_loss: 0.1283 - val_acc: 0.9651\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9643\n",
      "Epoch 00146: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1070 - acc: 0.9643 - val_loss: 0.1289 - val_acc: 0.9672\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9638\n",
      "Epoch 00147: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1113 - acc: 0.9638 - val_loss: 0.1442 - val_acc: 0.9620\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9636\n",
      "Epoch 00148: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1120 - acc: 0.9636 - val_loss: 0.1251 - val_acc: 0.9648\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9635\n",
      "Epoch 00149: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1094 - acc: 0.9635 - val_loss: 0.1307 - val_acc: 0.9618\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9641\n",
      "Epoch 00150: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1100 - acc: 0.9641 - val_loss: 0.1191 - val_acc: 0.9676\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9670\n",
      "Epoch 00151: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1023 - acc: 0.9670 - val_loss: 0.1377 - val_acc: 0.9651\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9644\n",
      "Epoch 00152: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1072 - acc: 0.9644 - val_loss: 0.1334 - val_acc: 0.9655\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9655\n",
      "Epoch 00153: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1023 - acc: 0.9655 - val_loss: 0.1132 - val_acc: 0.9697\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9667\n",
      "Epoch 00154: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1047 - acc: 0.9667 - val_loss: 0.1235 - val_acc: 0.9693\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9654\n",
      "Epoch 00155: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1033 - acc: 0.9654 - val_loss: 0.1213 - val_acc: 0.9681\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9662\n",
      "Epoch 00156: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1027 - acc: 0.9662 - val_loss: 0.1376 - val_acc: 0.9641\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9655\n",
      "Epoch 00157: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1043 - acc: 0.9655 - val_loss: 0.1400 - val_acc: 0.9625\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9673\n",
      "Epoch 00158: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1025 - acc: 0.9673 - val_loss: 0.1463 - val_acc: 0.9613\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9673\n",
      "Epoch 00159: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0982 - acc: 0.9673 - val_loss: 0.1474 - val_acc: 0.9599\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9679\n",
      "Epoch 00160: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1004 - acc: 0.9679 - val_loss: 0.1237 - val_acc: 0.9690\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9673\n",
      "Epoch 00161: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0987 - acc: 0.9673 - val_loss: 0.1259 - val_acc: 0.9697\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9658\n",
      "Epoch 00162: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1029 - acc: 0.9658 - val_loss: 0.1160 - val_acc: 0.9693\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9671\n",
      "Epoch 00163: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0979 - acc: 0.9671 - val_loss: 0.1458 - val_acc: 0.9616\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9679\n",
      "Epoch 00164: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0944 - acc: 0.9679 - val_loss: 0.1176 - val_acc: 0.9676\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9693\n",
      "Epoch 00165: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0965 - acc: 0.9694 - val_loss: 0.1310 - val_acc: 0.9686\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9695\n",
      "Epoch 00166: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0945 - acc: 0.9695 - val_loss: 0.1256 - val_acc: 0.9683\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9705\n",
      "Epoch 00167: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0916 - acc: 0.9705 - val_loss: 0.1342 - val_acc: 0.9660\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9693\n",
      "Epoch 00168: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0925 - acc: 0.9693 - val_loss: 0.1443 - val_acc: 0.9653\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9682\n",
      "Epoch 00169: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0962 - acc: 0.9682 - val_loss: 0.1377 - val_acc: 0.9646\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9699\n",
      "Epoch 00170: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0932 - acc: 0.9699 - val_loss: 0.1323 - val_acc: 0.9651\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9683\n",
      "Epoch 00171: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0966 - acc: 0.9683 - val_loss: 0.1233 - val_acc: 0.9681\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9707\n",
      "Epoch 00172: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0871 - acc: 0.9707 - val_loss: 0.1493 - val_acc: 0.9660\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9698\n",
      "Epoch 00173: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0906 - acc: 0.9698 - val_loss: 0.1267 - val_acc: 0.9651\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9716\n",
      "Epoch 00174: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0872 - acc: 0.9716 - val_loss: 0.1459 - val_acc: 0.9641\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9692\n",
      "Epoch 00175: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0924 - acc: 0.9691 - val_loss: 0.1515 - val_acc: 0.9653\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9698\n",
      "Epoch 00176: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0931 - acc: 0.9698 - val_loss: 0.1261 - val_acc: 0.9669\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9723\n",
      "Epoch 00177: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0816 - acc: 0.9723 - val_loss: 0.1284 - val_acc: 0.9653\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9714\n",
      "Epoch 00178: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0871 - acc: 0.9714 - val_loss: 0.1331 - val_acc: 0.9632\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9704\n",
      "Epoch 00179: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0882 - acc: 0.9704 - val_loss: 0.1171 - val_acc: 0.9690\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9714\n",
      "Epoch 00180: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0845 - acc: 0.9714 - val_loss: 0.1370 - val_acc: 0.9669\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9721\n",
      "Epoch 00181: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0848 - acc: 0.9721 - val_loss: 0.1441 - val_acc: 0.9658\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9718\n",
      "Epoch 00182: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0829 - acc: 0.9718 - val_loss: 0.1456 - val_acc: 0.9634\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9722\n",
      "Epoch 00183: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0836 - acc: 0.9722 - val_loss: 0.1268 - val_acc: 0.9679\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9727\n",
      "Epoch 00184: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0848 - acc: 0.9727 - val_loss: 0.1265 - val_acc: 0.9669\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9721\n",
      "Epoch 00185: val_loss did not improve from 0.11285\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0829 - acc: 0.9721 - val_loss: 0.1324 - val_acc: 0.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 598us/sample - loss: 0.1752 - acc: 0.9531\n",
      "Loss: 0.1752099445780211 Accuracy: 0.95306337\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5807 - acc: 0.1458\n",
      "Epoch 00001: val_loss improved from inf to 2.18212, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/001-2.1821.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.5807 - acc: 0.1458 - val_loss: 2.1821 - val_acc: 0.2986\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1768 - acc: 0.2863\n",
      "Epoch 00002: val_loss improved from 2.18212 to 1.84814, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/002-1.8481.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.1770 - acc: 0.2863 - val_loss: 1.8481 - val_acc: 0.4156\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8978 - acc: 0.3785\n",
      "Epoch 00003: val_loss improved from 1.84814 to 1.46799, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/003-1.4680.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.8977 - acc: 0.3785 - val_loss: 1.4680 - val_acc: 0.5218\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5794 - acc: 0.4823\n",
      "Epoch 00004: val_loss improved from 1.46799 to 1.21302, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/004-1.2130.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.5795 - acc: 0.4822 - val_loss: 1.2130 - val_acc: 0.6105\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3256 - acc: 0.5689\n",
      "Epoch 00005: val_loss improved from 1.21302 to 0.89955, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/005-0.8996.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.3255 - acc: 0.5689 - val_loss: 0.8996 - val_acc: 0.7121\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1531 - acc: 0.6237\n",
      "Epoch 00006: val_loss improved from 0.89955 to 0.76112, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/006-0.7611.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1530 - acc: 0.6237 - val_loss: 0.7611 - val_acc: 0.7622\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0316 - acc: 0.6659\n",
      "Epoch 00007: val_loss improved from 0.76112 to 0.67701, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/007-0.6770.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0317 - acc: 0.6659 - val_loss: 0.6770 - val_acc: 0.7813\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9458 - acc: 0.6945\n",
      "Epoch 00008: val_loss improved from 0.67701 to 0.62207, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/008-0.6221.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9457 - acc: 0.6946 - val_loss: 0.6221 - val_acc: 0.7976\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8611 - acc: 0.7217\n",
      "Epoch 00009: val_loss improved from 0.62207 to 0.53427, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/009-0.5343.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8610 - acc: 0.7218 - val_loss: 0.5343 - val_acc: 0.8279\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.7422\n",
      "Epoch 00010: val_loss did not improve from 0.53427\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8019 - acc: 0.7422 - val_loss: 0.5612 - val_acc: 0.8148\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7596\n",
      "Epoch 00011: val_loss improved from 0.53427 to 0.47650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/011-0.4765.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7523 - acc: 0.7597 - val_loss: 0.4765 - val_acc: 0.8507\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7753\n",
      "Epoch 00012: val_loss improved from 0.47650 to 0.42594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/012-0.4259.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7004 - acc: 0.7753 - val_loss: 0.4259 - val_acc: 0.8626\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7891\n",
      "Epoch 00013: val_loss improved from 0.42594 to 0.39528, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/013-0.3953.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6623 - acc: 0.7891 - val_loss: 0.3953 - val_acc: 0.8754\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.8016\n",
      "Epoch 00014: val_loss did not improve from 0.39528\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6274 - acc: 0.8016 - val_loss: 0.4218 - val_acc: 0.8616\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8103\n",
      "Epoch 00015: val_loss improved from 0.39528 to 0.38658, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/015-0.3866.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6004 - acc: 0.8103 - val_loss: 0.3866 - val_acc: 0.8782\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.8195\n",
      "Epoch 00016: val_loss improved from 0.38658 to 0.35575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/016-0.3557.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5698 - acc: 0.8195 - val_loss: 0.3557 - val_acc: 0.8884\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8268\n",
      "Epoch 00017: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5464 - acc: 0.8268 - val_loss: 0.3632 - val_acc: 0.8861\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8383\n",
      "Epoch 00018: val_loss did not improve from 0.35575\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5099 - acc: 0.8384 - val_loss: 0.3689 - val_acc: 0.8856\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8428\n",
      "Epoch 00019: val_loss improved from 0.35575 to 0.34309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/019-0.3431.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5032 - acc: 0.8428 - val_loss: 0.3431 - val_acc: 0.8945\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4760 - acc: 0.8490\n",
      "Epoch 00020: val_loss improved from 0.34309 to 0.29396, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/020-0.2940.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4760 - acc: 0.8490 - val_loss: 0.2940 - val_acc: 0.9101\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8570\n",
      "Epoch 00021: val_loss improved from 0.29396 to 0.26837, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/021-0.2684.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4568 - acc: 0.8570 - val_loss: 0.2684 - val_acc: 0.9154\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.8611\n",
      "Epoch 00022: val_loss did not improve from 0.26837\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4456 - acc: 0.8611 - val_loss: 0.2793 - val_acc: 0.9122\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8667\n",
      "Epoch 00023: val_loss improved from 0.26837 to 0.26662, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/023-0.2666.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4305 - acc: 0.8666 - val_loss: 0.2666 - val_acc: 0.9126\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8716\n",
      "Epoch 00024: val_loss improved from 0.26662 to 0.25079, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/024-0.2508.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4115 - acc: 0.8715 - val_loss: 0.2508 - val_acc: 0.9217\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8749\n",
      "Epoch 00025: val_loss did not improve from 0.25079\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4039 - acc: 0.8749 - val_loss: 0.2557 - val_acc: 0.9178\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8786\n",
      "Epoch 00026: val_loss did not improve from 0.25079\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3865 - acc: 0.8785 - val_loss: 0.2633 - val_acc: 0.9178\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8837\n",
      "Epoch 00027: val_loss did not improve from 0.25079\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3753 - acc: 0.8837 - val_loss: 0.2675 - val_acc: 0.9145\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8863\n",
      "Epoch 00028: val_loss improved from 0.25079 to 0.22680, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/028-0.2268.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3714 - acc: 0.8863 - val_loss: 0.2268 - val_acc: 0.9271\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8907\n",
      "Epoch 00029: val_loss did not improve from 0.22680\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3560 - acc: 0.8906 - val_loss: 0.2534 - val_acc: 0.9206\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8936\n",
      "Epoch 00030: val_loss improved from 0.22680 to 0.22479, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/030-0.2248.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3444 - acc: 0.8937 - val_loss: 0.2248 - val_acc: 0.9292\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.8983\n",
      "Epoch 00031: val_loss did not improve from 0.22479\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3302 - acc: 0.8984 - val_loss: 0.2326 - val_acc: 0.9278\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8992\n",
      "Epoch 00032: val_loss did not improve from 0.22479\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3246 - acc: 0.8992 - val_loss: 0.2442 - val_acc: 0.9280\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9021\n",
      "Epoch 00033: val_loss improved from 0.22479 to 0.19916, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/033-0.1992.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3117 - acc: 0.9021 - val_loss: 0.1992 - val_acc: 0.9373\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.9066\n",
      "Epoch 00034: val_loss did not improve from 0.19916\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3004 - acc: 0.9066 - val_loss: 0.2003 - val_acc: 0.9383\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9086\n",
      "Epoch 00035: val_loss improved from 0.19916 to 0.19740, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/035-0.1974.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2952 - acc: 0.9086 - val_loss: 0.1974 - val_acc: 0.9371\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9101\n",
      "Epoch 00036: val_loss improved from 0.19740 to 0.19179, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/036-0.1918.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2925 - acc: 0.9101 - val_loss: 0.1918 - val_acc: 0.9413\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9130\n",
      "Epoch 00037: val_loss did not improve from 0.19179\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2825 - acc: 0.9130 - val_loss: 0.2006 - val_acc: 0.9418\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9141\n",
      "Epoch 00038: val_loss improved from 0.19179 to 0.18022, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/038-0.1802.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2743 - acc: 0.9141 - val_loss: 0.1802 - val_acc: 0.9436\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9172\n",
      "Epoch 00039: val_loss did not improve from 0.18022\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2684 - acc: 0.9172 - val_loss: 0.1841 - val_acc: 0.9427\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9163\n",
      "Epoch 00040: val_loss improved from 0.18022 to 0.17877, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/040-0.1788.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2695 - acc: 0.9163 - val_loss: 0.1788 - val_acc: 0.9464\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9182\n",
      "Epoch 00041: val_loss did not improve from 0.17877\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2590 - acc: 0.9182 - val_loss: 0.1945 - val_acc: 0.9390\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9219\n",
      "Epoch 00042: val_loss did not improve from 0.17877\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2507 - acc: 0.9219 - val_loss: 0.2270 - val_acc: 0.9373\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9220\n",
      "Epoch 00043: val_loss improved from 0.17877 to 0.17653, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/043-0.1765.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2500 - acc: 0.9220 - val_loss: 0.1765 - val_acc: 0.9469\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9267\n",
      "Epoch 00044: val_loss improved from 0.17653 to 0.17169, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/044-0.1717.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2347 - acc: 0.9267 - val_loss: 0.1717 - val_acc: 0.9506\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9281\n",
      "Epoch 00045: val_loss improved from 0.17169 to 0.16188, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/045-0.1619.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2324 - acc: 0.9281 - val_loss: 0.1619 - val_acc: 0.9506\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9291\n",
      "Epoch 00046: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2277 - acc: 0.9291 - val_loss: 0.1991 - val_acc: 0.9404\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9292\n",
      "Epoch 00047: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2242 - acc: 0.9292 - val_loss: 0.1928 - val_acc: 0.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9314\n",
      "Epoch 00048: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2185 - acc: 0.9314 - val_loss: 0.1629 - val_acc: 0.9511\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9326\n",
      "Epoch 00049: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2145 - acc: 0.9326 - val_loss: 0.1663 - val_acc: 0.9485\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9341\n",
      "Epoch 00050: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2078 - acc: 0.9341 - val_loss: 0.1772 - val_acc: 0.9495\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9358\n",
      "Epoch 00051: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2041 - acc: 0.9358 - val_loss: 0.1702 - val_acc: 0.9485\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9390\n",
      "Epoch 00052: val_loss improved from 0.16188 to 0.15673, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/052-0.1567.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1955 - acc: 0.9390 - val_loss: 0.1567 - val_acc: 0.9539\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9389\n",
      "Epoch 00053: val_loss did not improve from 0.15673\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1948 - acc: 0.9389 - val_loss: 0.1702 - val_acc: 0.9499\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9355\n",
      "Epoch 00054: val_loss did not improve from 0.15673\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2059 - acc: 0.9355 - val_loss: 0.1636 - val_acc: 0.9495\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9424\n",
      "Epoch 00055: val_loss did not improve from 0.15673\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1812 - acc: 0.9424 - val_loss: 0.1606 - val_acc: 0.9532\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9422\n",
      "Epoch 00056: val_loss improved from 0.15673 to 0.14842, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/056-0.1484.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1839 - acc: 0.9422 - val_loss: 0.1484 - val_acc: 0.9546\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9447\n",
      "Epoch 00057: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1792 - acc: 0.9447 - val_loss: 0.1518 - val_acc: 0.9513\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9443\n",
      "Epoch 00058: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1750 - acc: 0.9443 - val_loss: 0.1755 - val_acc: 0.9509\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9439\n",
      "Epoch 00059: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1754 - acc: 0.9438 - val_loss: 0.1491 - val_acc: 0.9557\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9467\n",
      "Epoch 00060: val_loss did not improve from 0.14842\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1645 - acc: 0.9466 - val_loss: 0.1536 - val_acc: 0.9539\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9480\n",
      "Epoch 00061: val_loss improved from 0.14842 to 0.14471, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/061-0.1447.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1645 - acc: 0.9479 - val_loss: 0.1447 - val_acc: 0.9569\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9485\n",
      "Epoch 00062: val_loss did not improve from 0.14471\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1628 - acc: 0.9485 - val_loss: 0.1566 - val_acc: 0.9536\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9499\n",
      "Epoch 00063: val_loss improved from 0.14471 to 0.14460, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/063-0.1446.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1568 - acc: 0.9499 - val_loss: 0.1446 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9510\n",
      "Epoch 00064: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1565 - acc: 0.9510 - val_loss: 0.1553 - val_acc: 0.9532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9517\n",
      "Epoch 00065: val_loss did not improve from 0.14460\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1521 - acc: 0.9517 - val_loss: 0.1675 - val_acc: 0.9515\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9514\n",
      "Epoch 00066: val_loss improved from 0.14460 to 0.14270, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/066-0.1427.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1523 - acc: 0.9514 - val_loss: 0.1427 - val_acc: 0.9588\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9521\n",
      "Epoch 00067: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1503 - acc: 0.9521 - val_loss: 0.1507 - val_acc: 0.9571\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9542\n",
      "Epoch 00068: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1450 - acc: 0.9542 - val_loss: 0.1430 - val_acc: 0.9602\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9549\n",
      "Epoch 00069: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1412 - acc: 0.9548 - val_loss: 0.1579 - val_acc: 0.9560\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9532\n",
      "Epoch 00070: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1439 - acc: 0.9532 - val_loss: 0.1524 - val_acc: 0.9585\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9549\n",
      "Epoch 00071: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1375 - acc: 0.9549 - val_loss: 0.1519 - val_acc: 0.9574\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9578\n",
      "Epoch 00072: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1304 - acc: 0.9578 - val_loss: 0.1502 - val_acc: 0.9564\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9549\n",
      "Epoch 00073: val_loss did not improve from 0.14270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1380 - acc: 0.9550 - val_loss: 0.1452 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9576\n",
      "Epoch 00074: val_loss improved from 0.14270 to 0.13745, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/074-0.1374.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1307 - acc: 0.9576 - val_loss: 0.1374 - val_acc: 0.9595\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9596\n",
      "Epoch 00075: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1233 - acc: 0.9597 - val_loss: 0.1577 - val_acc: 0.9550\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9600\n",
      "Epoch 00076: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1233 - acc: 0.9600 - val_loss: 0.1556 - val_acc: 0.9597\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9595\n",
      "Epoch 00077: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1240 - acc: 0.9595 - val_loss: 0.1445 - val_acc: 0.9609\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9606\n",
      "Epoch 00078: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1212 - acc: 0.9605 - val_loss: 0.1557 - val_acc: 0.9606\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9606\n",
      "Epoch 00079: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1210 - acc: 0.9606 - val_loss: 0.1591 - val_acc: 0.9569\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9618\n",
      "Epoch 00080: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1160 - acc: 0.9618 - val_loss: 0.1574 - val_acc: 0.9595\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9621\n",
      "Epoch 00081: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1182 - acc: 0.9621 - val_loss: 0.1646 - val_acc: 0.9541\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9636\n",
      "Epoch 00082: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1120 - acc: 0.9636 - val_loss: 0.1585 - val_acc: 0.9578\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9627\n",
      "Epoch 00083: val_loss did not improve from 0.13745\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1155 - acc: 0.9627 - val_loss: 0.1540 - val_acc: 0.9590\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9632\n",
      "Epoch 00084: val_loss improved from 0.13745 to 0.13714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/084-0.1371.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1094 - acc: 0.9632 - val_loss: 0.1371 - val_acc: 0.9623\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9633\n",
      "Epoch 00085: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1099 - acc: 0.9633 - val_loss: 0.1521 - val_acc: 0.9602\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9640\n",
      "Epoch 00086: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1096 - acc: 0.9640 - val_loss: 0.1567 - val_acc: 0.9585\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9657\n",
      "Epoch 00087: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1033 - acc: 0.9657 - val_loss: 0.1645 - val_acc: 0.9583\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9657\n",
      "Epoch 00088: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1045 - acc: 0.9657 - val_loss: 0.1579 - val_acc: 0.9630\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9658\n",
      "Epoch 00089: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1037 - acc: 0.9658 - val_loss: 0.1615 - val_acc: 0.9602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9658\n",
      "Epoch 00090: val_loss did not improve from 0.13714\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1026 - acc: 0.9658 - val_loss: 0.1657 - val_acc: 0.9532\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9666\n",
      "Epoch 00091: val_loss improved from 0.13714 to 0.13656, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv_checkpoint/091-0.1366.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1019 - acc: 0.9666 - val_loss: 0.1366 - val_acc: 0.9658\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9699\n",
      "Epoch 00092: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0926 - acc: 0.9699 - val_loss: 0.1423 - val_acc: 0.9625\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9673\n",
      "Epoch 00093: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0978 - acc: 0.9673 - val_loss: 0.1475 - val_acc: 0.9637\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9691\n",
      "Epoch 00094: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0965 - acc: 0.9691 - val_loss: 0.1406 - val_acc: 0.9646\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9693\n",
      "Epoch 00095: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0902 - acc: 0.9693 - val_loss: 0.1735 - val_acc: 0.9576\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9706\n",
      "Epoch 00096: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0888 - acc: 0.9706 - val_loss: 0.1648 - val_acc: 0.9623\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9706\n",
      "Epoch 00097: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0901 - acc: 0.9706 - val_loss: 0.1494 - val_acc: 0.9620\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9709\n",
      "Epoch 00098: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0878 - acc: 0.9709 - val_loss: 0.1698 - val_acc: 0.9606\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9711\n",
      "Epoch 00099: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0863 - acc: 0.9711 - val_loss: 0.1478 - val_acc: 0.9630\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9712\n",
      "Epoch 00100: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0847 - acc: 0.9713 - val_loss: 0.1742 - val_acc: 0.9597\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9723\n",
      "Epoch 00101: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0837 - acc: 0.9723 - val_loss: 0.1515 - val_acc: 0.9613\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9718\n",
      "Epoch 00102: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0839 - acc: 0.9718 - val_loss: 0.1605 - val_acc: 0.9620\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9717\n",
      "Epoch 00103: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0850 - acc: 0.9717 - val_loss: 0.1722 - val_acc: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9742\n",
      "Epoch 00104: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0798 - acc: 0.9742 - val_loss: 0.1617 - val_acc: 0.9595\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9738\n",
      "Epoch 00105: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0770 - acc: 0.9738 - val_loss: 0.1446 - val_acc: 0.9639\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9728\n",
      "Epoch 00106: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0812 - acc: 0.9728 - val_loss: 0.1751 - val_acc: 0.9604\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9727\n",
      "Epoch 00107: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0815 - acc: 0.9727 - val_loss: 0.1537 - val_acc: 0.9634\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9748\n",
      "Epoch 00108: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0750 - acc: 0.9748 - val_loss: 0.1495 - val_acc: 0.9625\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9751\n",
      "Epoch 00109: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0739 - acc: 0.9751 - val_loss: 0.1959 - val_acc: 0.9585\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9755\n",
      "Epoch 00110: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0742 - acc: 0.9755 - val_loss: 0.1810 - val_acc: 0.9632\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9741\n",
      "Epoch 00111: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0788 - acc: 0.9741 - val_loss: 0.1587 - val_acc: 0.9623\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9768\n",
      "Epoch 00112: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0681 - acc: 0.9768 - val_loss: 0.1761 - val_acc: 0.9627\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9755\n",
      "Epoch 00113: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0735 - acc: 0.9755 - val_loss: 0.1741 - val_acc: 0.9602\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9764\n",
      "Epoch 00114: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0695 - acc: 0.9764 - val_loss: 0.1629 - val_acc: 0.9648\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9758\n",
      "Epoch 00115: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0731 - acc: 0.9758 - val_loss: 0.1837 - val_acc: 0.9602\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9776\n",
      "Epoch 00116: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0677 - acc: 0.9776 - val_loss: 0.1950 - val_acc: 0.9606\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9790\n",
      "Epoch 00117: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0637 - acc: 0.9790 - val_loss: 0.1990 - val_acc: 0.9595\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9765\n",
      "Epoch 00118: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0703 - acc: 0.9765 - val_loss: 0.1734 - val_acc: 0.9583\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9786\n",
      "Epoch 00119: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0665 - acc: 0.9786 - val_loss: 0.2055 - val_acc: 0.9588\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9789\n",
      "Epoch 00120: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0627 - acc: 0.9789 - val_loss: 0.1633 - val_acc: 0.9604\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9779\n",
      "Epoch 00121: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0638 - acc: 0.9779 - val_loss: 0.1625 - val_acc: 0.9627\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9790\n",
      "Epoch 00122: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0625 - acc: 0.9790 - val_loss: 0.1789 - val_acc: 0.9648\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9794\n",
      "Epoch 00123: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0614 - acc: 0.9794 - val_loss: 0.1753 - val_acc: 0.9648\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9786\n",
      "Epoch 00124: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0664 - acc: 0.9786 - val_loss: 0.1553 - val_acc: 0.9648\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9801\n",
      "Epoch 00125: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0611 - acc: 0.9801 - val_loss: 0.2016 - val_acc: 0.9613\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9786\n",
      "Epoch 00126: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0635 - acc: 0.9785 - val_loss: 0.1651 - val_acc: 0.9637\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9798\n",
      "Epoch 00127: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0615 - acc: 0.9798 - val_loss: 0.1593 - val_acc: 0.9662\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9806\n",
      "Epoch 00128: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0575 - acc: 0.9805 - val_loss: 0.1933 - val_acc: 0.9630\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9799\n",
      "Epoch 00129: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0603 - acc: 0.9799 - val_loss: 0.2051 - val_acc: 0.9609\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9792\n",
      "Epoch 00130: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0632 - acc: 0.9792 - val_loss: 0.1678 - val_acc: 0.9667\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9811\n",
      "Epoch 00131: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0562 - acc: 0.9811 - val_loss: 0.1721 - val_acc: 0.9630\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9806\n",
      "Epoch 00132: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0590 - acc: 0.9806 - val_loss: 0.1779 - val_acc: 0.9620\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9814\n",
      "Epoch 00133: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0541 - acc: 0.9814 - val_loss: 0.1605 - val_acc: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9812\n",
      "Epoch 00134: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0555 - acc: 0.9812 - val_loss: 0.1884 - val_acc: 0.9639\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9820\n",
      "Epoch 00135: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0557 - acc: 0.9820 - val_loss: 0.1665 - val_acc: 0.9651\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9823\n",
      "Epoch 00136: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0530 - acc: 0.9823 - val_loss: 0.2141 - val_acc: 0.9623\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9798\n",
      "Epoch 00137: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0615 - acc: 0.9798 - val_loss: 0.1704 - val_acc: 0.9634\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9817\n",
      "Epoch 00138: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0530 - acc: 0.9817 - val_loss: 0.2035 - val_acc: 0.9592\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9817\n",
      "Epoch 00139: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0559 - acc: 0.9817 - val_loss: 0.1923 - val_acc: 0.9623\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9824\n",
      "Epoch 00140: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0520 - acc: 0.9824 - val_loss: 0.2129 - val_acc: 0.9618\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9822\n",
      "Epoch 00141: val_loss did not improve from 0.13656\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0521 - acc: 0.9822 - val_loss: 0.1773 - val_acc: 0.9648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 623us/sample - loss: 0.2159 - acc: 0.9481\n",
      "Loss: 0.21589078600532913 Accuracy: 0.94807893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO'\n",
    "    \n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv Model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 652us/sample - loss: 0.8153 - acc: 0.7892\n",
      "Loss: 0.8152669871088624 Accuracy: 0.7892004\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 495us/sample - loss: 0.5928 - acc: 0.8480\n",
      "Loss: 0.5928024519145922 Accuracy: 0.8479751\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 592us/sample - loss: 0.3019 - acc: 0.9209\n",
      "Loss: 0.3019223801010247 Accuracy: 0.9208723\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 568us/sample - loss: 0.1951 - acc: 0.9427\n",
      "Loss: 0.19514510566417054 Accuracy: 0.9426791\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 562us/sample - loss: 0.1752 - acc: 0.9531\n",
      "Loss: 0.1752099445780211 Accuracy: 0.95306337\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 597us/sample - loss: 0.2159 - acc: 0.9481\n",
      "Loss: 0.21589078600532913 Accuracy: 0.94807893\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 324,976\n",
      "Trainable params: 324,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 572us/sample - loss: 0.8878 - acc: 0.7913\n",
      "Loss: 0.8877872370732783 Accuracy: 0.7912772\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_98 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 242,160\n",
      "Trainable params: 242,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 578us/sample - loss: 0.6063 - acc: 0.8555\n",
      "Loss: 0.6062740863916906 Accuracy: 0.8554517\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_108 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 131,696\n",
      "Trainable params: 131,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 603us/sample - loss: 0.3083 - acc: 0.9205\n",
      "Loss: 0.3082796956272264 Accuracy: 0.9204569\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 111,344\n",
      "Trainable params: 111,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 624us/sample - loss: 0.1969 - acc: 0.9477\n",
      "Loss: 0.19687764250546663 Accuracy: 0.94766355\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 121,712\n",
      "Trainable params: 121,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 637us/sample - loss: 0.1920 - acc: 0.9547\n",
      "Loss: 0.19200982882281975 Accuracy: 0.9547248\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_ch_32_DO_075_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_150 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 5333, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 1777, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 592, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 592, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 197, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 65, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 21, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 7, 128)            24704     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 7, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 192,624\n",
      "Trainable params: 192,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 657us/sample - loss: 0.2662 - acc: 0.9533\n",
      "Loss: 0.2662208026789781 Accuracy: 0.95327103\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
