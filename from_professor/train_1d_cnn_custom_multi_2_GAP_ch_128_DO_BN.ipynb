{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3332 - acc: 0.2568\n",
      "Epoch 00001: val_loss improved from inf to 2.22321, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/001-2.2232.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 2.3331 - acc: 0.2568 - val_loss: 2.2232 - val_acc: 0.3103\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0156 - acc: 0.3562\n",
      "Epoch 00002: val_loss improved from 2.22321 to 1.80722, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/002-1.8072.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 2.0156 - acc: 0.3562 - val_loss: 1.8072 - val_acc: 0.5022\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8671 - acc: 0.4109\n",
      "Epoch 00003: val_loss improved from 1.80722 to 1.67069, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/003-1.6707.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.8670 - acc: 0.4109 - val_loss: 1.6707 - val_acc: 0.5229\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7619 - acc: 0.4396\n",
      "Epoch 00004: val_loss improved from 1.67069 to 1.55130, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/004-1.5513.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.7619 - acc: 0.4396 - val_loss: 1.5513 - val_acc: 0.5791\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6802 - acc: 0.4695\n",
      "Epoch 00005: val_loss improved from 1.55130 to 1.51727, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/005-1.5173.hdf5\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.6802 - acc: 0.4695 - val_loss: 1.5173 - val_acc: 0.5900\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6095 - acc: 0.4936\n",
      "Epoch 00006: val_loss improved from 1.51727 to 1.41268, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/006-1.4127.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.6095 - acc: 0.4936 - val_loss: 1.4127 - val_acc: 0.6201\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5527 - acc: 0.5132\n",
      "Epoch 00007: val_loss did not improve from 1.41268\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.5527 - acc: 0.5132 - val_loss: 1.4192 - val_acc: 0.5754\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5027 - acc: 0.5302\n",
      "Epoch 00008: val_loss improved from 1.41268 to 1.30468, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/008-1.3047.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.5026 - acc: 0.5302 - val_loss: 1.3047 - val_acc: 0.6413\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4592 - acc: 0.5458\n",
      "Epoch 00009: val_loss improved from 1.30468 to 1.30450, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/009-1.3045.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.4592 - acc: 0.5458 - val_loss: 1.3045 - val_acc: 0.6203\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4290 - acc: 0.5566\n",
      "Epoch 00010: val_loss did not improve from 1.30450\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.4290 - acc: 0.5566 - val_loss: 1.3724 - val_acc: 0.5686\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3926 - acc: 0.5671\n",
      "Epoch 00011: val_loss did not improve from 1.30450\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.3926 - acc: 0.5670 - val_loss: 1.3691 - val_acc: 0.5595\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3615 - acc: 0.5804\n",
      "Epoch 00012: val_loss improved from 1.30450 to 1.20418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/012-1.2042.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.3614 - acc: 0.5804 - val_loss: 1.2042 - val_acc: 0.6571\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3418 - acc: 0.5864\n",
      "Epoch 00013: val_loss did not improve from 1.20418\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.3419 - acc: 0.5863 - val_loss: 1.2334 - val_acc: 0.6329\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3159 - acc: 0.5929\n",
      "Epoch 00014: val_loss did not improve from 1.20418\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.3158 - acc: 0.5929 - val_loss: 1.2849 - val_acc: 0.6150\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2968 - acc: 0.6005\n",
      "Epoch 00015: val_loss did not improve from 1.20418\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2968 - acc: 0.6005 - val_loss: 1.2304 - val_acc: 0.5940\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2724 - acc: 0.6053\n",
      "Epoch 00016: val_loss improved from 1.20418 to 1.11362, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/016-1.1136.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2724 - acc: 0.6053 - val_loss: 1.1136 - val_acc: 0.6583\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2606 - acc: 0.6137\n",
      "Epoch 00017: val_loss did not improve from 1.11362\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2611 - acc: 0.6136 - val_loss: 1.3337 - val_acc: 0.5460\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2438 - acc: 0.6182\n",
      "Epoch 00018: val_loss improved from 1.11362 to 1.10280, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/018-1.1028.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2438 - acc: 0.6181 - val_loss: 1.1028 - val_acc: 0.6758\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2269 - acc: 0.6235\n",
      "Epoch 00019: val_loss did not improve from 1.10280\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.2270 - acc: 0.6234 - val_loss: 1.2624 - val_acc: 0.5952\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2162 - acc: 0.6282\n",
      "Epoch 00020: val_loss improved from 1.10280 to 1.09713, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/020-1.0971.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2161 - acc: 0.6282 - val_loss: 1.0971 - val_acc: 0.6690\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2074 - acc: 0.6293\n",
      "Epoch 00021: val_loss did not improve from 1.09713\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.2073 - acc: 0.6293 - val_loss: 1.1575 - val_acc: 0.6515\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1900 - acc: 0.6352\n",
      "Epoch 00022: val_loss did not improve from 1.09713\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1901 - acc: 0.6351 - val_loss: 1.1170 - val_acc: 0.6692\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1826 - acc: 0.6392\n",
      "Epoch 00023: val_loss improved from 1.09713 to 1.06898, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/023-1.0690.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1826 - acc: 0.6391 - val_loss: 1.0690 - val_acc: 0.6697\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6429\n",
      "Epoch 00024: val_loss did not improve from 1.06898\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1739 - acc: 0.6429 - val_loss: 1.1319 - val_acc: 0.6492\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1577 - acc: 0.6428\n",
      "Epoch 00025: val_loss did not improve from 1.06898\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1577 - acc: 0.6428 - val_loss: 1.1809 - val_acc: 0.6315\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1548 - acc: 0.6496\n",
      "Epoch 00026: val_loss improved from 1.06898 to 0.97906, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/026-0.9791.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1548 - acc: 0.6496 - val_loss: 0.9791 - val_acc: 0.7214\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1377 - acc: 0.6536\n",
      "Epoch 00027: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1380 - acc: 0.6535 - val_loss: 1.0886 - val_acc: 0.6441\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1403 - acc: 0.6518\n",
      "Epoch 00028: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1404 - acc: 0.6518 - val_loss: 3.3610 - val_acc: 0.2844\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1235 - acc: 0.6593\n",
      "Epoch 00029: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1240 - acc: 0.6593 - val_loss: 1.2596 - val_acc: 0.5833\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1177 - acc: 0.6610\n",
      "Epoch 00030: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1178 - acc: 0.6610 - val_loss: 2.4381 - val_acc: 0.3720\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1050 - acc: 0.6631\n",
      "Epoch 00031: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1050 - acc: 0.6631 - val_loss: 5.5056 - val_acc: 0.2618\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.6686\n",
      "Epoch 00032: val_loss improved from 0.97906 to 0.97361, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/032-0.9736.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.1013 - acc: 0.6686 - val_loss: 0.9736 - val_acc: 0.7044\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0986 - acc: 0.6653\n",
      "Epoch 00033: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0986 - acc: 0.6653 - val_loss: 0.9795 - val_acc: 0.7116\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0878 - acc: 0.6669\n",
      "Epoch 00034: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0878 - acc: 0.6669 - val_loss: 1.1004 - val_acc: 0.6413\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0845 - acc: 0.6693\n",
      "Epoch 00035: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0845 - acc: 0.6693 - val_loss: 1.0279 - val_acc: 0.6716\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0771 - acc: 0.6730\n",
      "Epoch 00036: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0776 - acc: 0.6730 - val_loss: 0.9741 - val_acc: 0.7032\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0726 - acc: 0.6721\n",
      "Epoch 00037: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0727 - acc: 0.6721 - val_loss: 1.2457 - val_acc: 0.6003\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0608 - acc: 0.6793\n",
      "Epoch 00038: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 1.0608 - acc: 0.6793 - val_loss: 0.9985 - val_acc: 0.6839\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0567 - acc: 0.6803\n",
      "Epoch 00039: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0566 - acc: 0.6804 - val_loss: 1.3062 - val_acc: 0.5770\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0512 - acc: 0.6802\n",
      "Epoch 00040: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0512 - acc: 0.6802 - val_loss: 1.3463 - val_acc: 0.5691\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.6810\n",
      "Epoch 00041: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0520 - acc: 0.6810 - val_loss: 1.0573 - val_acc: 0.6613\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0438 - acc: 0.6857\n",
      "Epoch 00042: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0439 - acc: 0.6856 - val_loss: 1.0835 - val_acc: 0.6641\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0341 - acc: 0.6854\n",
      "Epoch 00043: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0341 - acc: 0.6854 - val_loss: 2.2474 - val_acc: 0.4365\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0301 - acc: 0.6876\n",
      "Epoch 00044: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0300 - acc: 0.6876 - val_loss: 1.1783 - val_acc: 0.6056\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0242 - acc: 0.6901\n",
      "Epoch 00045: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0242 - acc: 0.6901 - val_loss: 1.1374 - val_acc: 0.6210\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0270 - acc: 0.6890\n",
      "Epoch 00046: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0269 - acc: 0.6891 - val_loss: 1.1623 - val_acc: 0.6191\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0218 - acc: 0.6946\n",
      "Epoch 00047: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0219 - acc: 0.6946 - val_loss: 1.0790 - val_acc: 0.6720\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0097 - acc: 0.6960\n",
      "Epoch 00048: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0096 - acc: 0.6961 - val_loss: 2.1187 - val_acc: 0.3930\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0050 - acc: 0.6962\n",
      "Epoch 00049: val_loss did not improve from 0.97361\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0051 - acc: 0.6962 - val_loss: 1.0136 - val_acc: 0.6725\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0045 - acc: 0.6971\n",
      "Epoch 00050: val_loss improved from 0.97361 to 0.95261, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/050-0.9526.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0045 - acc: 0.6971 - val_loss: 0.9526 - val_acc: 0.6967\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9999 - acc: 0.6993\n",
      "Epoch 00051: val_loss did not improve from 0.95261\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 1.0000 - acc: 0.6993 - val_loss: 1.4363 - val_acc: 0.5332\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9992 - acc: 0.6983\n",
      "Epoch 00052: val_loss did not improve from 0.95261\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9992 - acc: 0.6983 - val_loss: 1.3848 - val_acc: 0.5292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9889 - acc: 0.7027\n",
      "Epoch 00053: val_loss improved from 0.95261 to 0.94100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/053-0.9410.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9890 - acc: 0.7027 - val_loss: 0.9410 - val_acc: 0.6888\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9912 - acc: 0.7004\n",
      "Epoch 00054: val_loss did not improve from 0.94100\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9913 - acc: 0.7004 - val_loss: 1.0620 - val_acc: 0.6562\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9835 - acc: 0.7039\n",
      "Epoch 00055: val_loss did not improve from 0.94100\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9836 - acc: 0.7039 - val_loss: 1.0435 - val_acc: 0.6746\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.7049\n",
      "Epoch 00056: val_loss did not improve from 0.94100\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9831 - acc: 0.7049 - val_loss: 1.8667 - val_acc: 0.4826\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9923 - acc: 0.7021\n",
      "Epoch 00057: val_loss did not improve from 0.94100\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9926 - acc: 0.7021 - val_loss: 1.0217 - val_acc: 0.6762\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9739 - acc: 0.7074\n",
      "Epoch 00058: val_loss did not improve from 0.94100\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9740 - acc: 0.7074 - val_loss: 1.1281 - val_acc: 0.6539\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9753 - acc: 0.7068\n",
      "Epoch 00059: val_loss improved from 0.94100 to 0.90343, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/059-0.9034.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9755 - acc: 0.7068 - val_loss: 0.9034 - val_acc: 0.7165\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9652 - acc: 0.7117\n",
      "Epoch 00060: val_loss did not improve from 0.90343\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9651 - acc: 0.7117 - val_loss: 0.9107 - val_acc: 0.7335\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9669 - acc: 0.7083\n",
      "Epoch 00061: val_loss did not improve from 0.90343\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9669 - acc: 0.7083 - val_loss: 8.9488 - val_acc: 0.1982\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9637 - acc: 0.7121\n",
      "Epoch 00062: val_loss did not improve from 0.90343\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9637 - acc: 0.7121 - val_loss: 1.0405 - val_acc: 0.6744\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.7119\n",
      "Epoch 00063: val_loss did not improve from 0.90343\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9614 - acc: 0.7119 - val_loss: 1.7478 - val_acc: 0.4805\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9589 - acc: 0.7143\n",
      "Epoch 00064: val_loss did not improve from 0.90343\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9588 - acc: 0.7143 - val_loss: 1.1815 - val_acc: 0.6194\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9541 - acc: 0.7151\n",
      "Epoch 00065: val_loss improved from 0.90343 to 0.82296, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv_checkpoint/065-0.8230.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9541 - acc: 0.7151 - val_loss: 0.8230 - val_acc: 0.7577\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9526 - acc: 0.7146\n",
      "Epoch 00066: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9525 - acc: 0.7146 - val_loss: 0.8501 - val_acc: 0.7501\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9528 - acc: 0.7161\n",
      "Epoch 00067: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9527 - acc: 0.7161 - val_loss: 1.0631 - val_acc: 0.6499\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9531 - acc: 0.7139\n",
      "Epoch 00068: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9534 - acc: 0.7138 - val_loss: 1.2586 - val_acc: 0.6140\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.7175\n",
      "Epoch 00069: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9409 - acc: 0.7175 - val_loss: 2.0139 - val_acc: 0.4698\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.7212\n",
      "Epoch 00070: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9336 - acc: 0.7212 - val_loss: 1.0847 - val_acc: 0.6697\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7171\n",
      "Epoch 00071: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9354 - acc: 0.7171 - val_loss: 1.7801 - val_acc: 0.5052\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.7218\n",
      "Epoch 00072: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9339 - acc: 0.7218 - val_loss: 1.2843 - val_acc: 0.5809\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9257 - acc: 0.7237\n",
      "Epoch 00073: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9256 - acc: 0.7237 - val_loss: 3.3812 - val_acc: 0.3061\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9322 - acc: 0.7212\n",
      "Epoch 00074: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9322 - acc: 0.7213 - val_loss: 1.8594 - val_acc: 0.4768\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9236 - acc: 0.7234\n",
      "Epoch 00075: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9236 - acc: 0.7234 - val_loss: 1.0910 - val_acc: 0.6434\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9250 - acc: 0.7268\n",
      "Epoch 00076: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9249 - acc: 0.7269 - val_loss: 1.3343 - val_acc: 0.5653\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9223 - acc: 0.7224\n",
      "Epoch 00077: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9222 - acc: 0.7224 - val_loss: 0.8794 - val_acc: 0.7405\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9124 - acc: 0.7282\n",
      "Epoch 00078: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9129 - acc: 0.7282 - val_loss: 1.4499 - val_acc: 0.5537\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9153 - acc: 0.7259\n",
      "Epoch 00079: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9154 - acc: 0.7259 - val_loss: 1.3799 - val_acc: 0.5418\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9154 - acc: 0.7271\n",
      "Epoch 00080: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9153 - acc: 0.7272 - val_loss: 0.8598 - val_acc: 0.7331\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9155 - acc: 0.7291\n",
      "Epoch 00081: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9154 - acc: 0.7291 - val_loss: 1.3542 - val_acc: 0.5474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7286\n",
      "Epoch 00082: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9120 - acc: 0.7286 - val_loss: 1.4936 - val_acc: 0.5723\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9133 - acc: 0.7304\n",
      "Epoch 00083: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9133 - acc: 0.7304 - val_loss: 3.6964 - val_acc: 0.3487\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7303\n",
      "Epoch 00084: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9047 - acc: 0.7303 - val_loss: 0.9181 - val_acc: 0.7223\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9068 - acc: 0.7292\n",
      "Epoch 00085: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.9068 - acc: 0.7292 - val_loss: 1.5514 - val_acc: 0.5698\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8992 - acc: 0.7323\n",
      "Epoch 00086: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8994 - acc: 0.7323 - val_loss: 1.0002 - val_acc: 0.6830\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7317\n",
      "Epoch 00087: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8978 - acc: 0.7317 - val_loss: 0.8948 - val_acc: 0.7212\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8917 - acc: 0.7348\n",
      "Epoch 00088: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8916 - acc: 0.7348 - val_loss: 1.1845 - val_acc: 0.6711\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.7323\n",
      "Epoch 00089: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8992 - acc: 0.7323 - val_loss: 1.0305 - val_acc: 0.6564\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.7380\n",
      "Epoch 00090: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8848 - acc: 0.7379 - val_loss: 3.8107 - val_acc: 0.3832\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8854 - acc: 0.7391\n",
      "Epoch 00091: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8854 - acc: 0.7391 - val_loss: 1.1175 - val_acc: 0.6189\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8888 - acc: 0.7333\n",
      "Epoch 00092: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8890 - acc: 0.7332 - val_loss: 1.2214 - val_acc: 0.5938\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8884 - acc: 0.7363\n",
      "Epoch 00093: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8883 - acc: 0.7363 - val_loss: 1.1091 - val_acc: 0.6371\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7361\n",
      "Epoch 00094: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8908 - acc: 0.7361 - val_loss: 0.8972 - val_acc: 0.7223\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8860 - acc: 0.7371\n",
      "Epoch 00095: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8860 - acc: 0.7371 - val_loss: 4.0984 - val_acc: 0.2399\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8753 - acc: 0.7413\n",
      "Epoch 00096: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8755 - acc: 0.7413 - val_loss: 3.9751 - val_acc: 0.3152\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7401\n",
      "Epoch 00097: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8732 - acc: 0.7401 - val_loss: 1.4263 - val_acc: 0.5770\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8760 - acc: 0.7439\n",
      "Epoch 00098: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8760 - acc: 0.7439 - val_loss: 4.0063 - val_acc: 0.2546\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8742 - acc: 0.7418\n",
      "Epoch 00099: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8744 - acc: 0.7417 - val_loss: 1.2491 - val_acc: 0.6040\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7410\n",
      "Epoch 00100: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8780 - acc: 0.7410 - val_loss: 0.8328 - val_acc: 0.7477\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8760 - acc: 0.7417\n",
      "Epoch 00101: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8760 - acc: 0.7417 - val_loss: 1.0873 - val_acc: 0.6355\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.7414\n",
      "Epoch 00102: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8702 - acc: 0.7414 - val_loss: 1.7912 - val_acc: 0.4642\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8625 - acc: 0.7415\n",
      "Epoch 00103: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8625 - acc: 0.7416 - val_loss: 2.1291 - val_acc: 0.4614\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.7415\n",
      "Epoch 00104: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 0.8720 - acc: 0.7415 - val_loss: 1.7037 - val_acc: 0.4724\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8607 - acc: 0.7451\n",
      "Epoch 00105: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8607 - acc: 0.7451 - val_loss: 1.0280 - val_acc: 0.6541\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8644 - acc: 0.7451\n",
      "Epoch 00106: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8643 - acc: 0.7450 - val_loss: 1.2879 - val_acc: 0.5931\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8617 - acc: 0.7448\n",
      "Epoch 00107: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8616 - acc: 0.7448 - val_loss: 6.4716 - val_acc: 0.2383\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7409\n",
      "Epoch 00108: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8679 - acc: 0.7409 - val_loss: 0.9505 - val_acc: 0.6830\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8622 - acc: 0.7458\n",
      "Epoch 00109: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8622 - acc: 0.7458 - val_loss: 1.2588 - val_acc: 0.5719\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7457\n",
      "Epoch 00110: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8571 - acc: 0.7457 - val_loss: 0.9550 - val_acc: 0.6883\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8515 - acc: 0.7470\n",
      "Epoch 00111: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8515 - acc: 0.7471 - val_loss: 1.2151 - val_acc: 0.5954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7480\n",
      "Epoch 00112: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8595 - acc: 0.7481 - val_loss: 1.6267 - val_acc: 0.4596\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8494 - acc: 0.7512\n",
      "Epoch 00113: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8493 - acc: 0.7512 - val_loss: 0.9148 - val_acc: 0.7014\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8495 - acc: 0.7493\n",
      "Epoch 00114: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8497 - acc: 0.7492 - val_loss: 6.8045 - val_acc: 0.2884\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8500 - acc: 0.7479\n",
      "Epoch 00115: val_loss did not improve from 0.82296\n",
      "36805/36805 [==============================] - 102s 3ms/sample - loss: 0.8501 - acc: 0.7479 - val_loss: 0.8866 - val_acc: 0.7256\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 976us/sample - loss: 0.8767 - acc: 0.7310\n",
      "Loss: 0.8767301782764502 Accuracy: 0.7310488\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1676 - acc: 0.3118\n",
      "Epoch 00001: val_loss improved from inf to 2.00298, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/001-2.0030.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 2.1675 - acc: 0.3118 - val_loss: 2.0030 - val_acc: 0.3969\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7470 - acc: 0.4530\n",
      "Epoch 00002: val_loss improved from 2.00298 to 1.47085, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/002-1.4708.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.7470 - acc: 0.4530 - val_loss: 1.4708 - val_acc: 0.5842\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5598 - acc: 0.5183\n",
      "Epoch 00003: val_loss improved from 1.47085 to 1.31885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/003-1.3189.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.5597 - acc: 0.5183 - val_loss: 1.3189 - val_acc: 0.6338\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4343 - acc: 0.5583\n",
      "Epoch 00004: val_loss improved from 1.31885 to 1.23058, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/004-1.2306.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.4344 - acc: 0.5582 - val_loss: 1.2306 - val_acc: 0.6709\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3401 - acc: 0.5935\n",
      "Epoch 00005: val_loss improved from 1.23058 to 1.12874, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/005-1.1287.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.3400 - acc: 0.5935 - val_loss: 1.1287 - val_acc: 0.6820\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2711 - acc: 0.6119\n",
      "Epoch 00006: val_loss improved from 1.12874 to 1.09368, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/006-1.0937.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.2711 - acc: 0.6119 - val_loss: 1.0937 - val_acc: 0.6900\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2086 - acc: 0.6374\n",
      "Epoch 00007: val_loss improved from 1.09368 to 1.00458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/007-1.0046.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.2086 - acc: 0.6373 - val_loss: 1.0046 - val_acc: 0.7375\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1604 - acc: 0.6517\n",
      "Epoch 00008: val_loss improved from 1.00458 to 0.97350, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/008-0.9735.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.1603 - acc: 0.6517 - val_loss: 0.9735 - val_acc: 0.7421\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1186 - acc: 0.6636\n",
      "Epoch 00009: val_loss did not improve from 0.97350\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.1186 - acc: 0.6636 - val_loss: 0.9807 - val_acc: 0.7289\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0832 - acc: 0.6798\n",
      "Epoch 00010: val_loss improved from 0.97350 to 0.94239, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/010-0.9424.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0832 - acc: 0.6797 - val_loss: 0.9424 - val_acc: 0.7396\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0497 - acc: 0.6862\n",
      "Epoch 00011: val_loss improved from 0.94239 to 0.93150, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/011-0.9315.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0498 - acc: 0.6862 - val_loss: 0.9315 - val_acc: 0.7300\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0312 - acc: 0.6921\n",
      "Epoch 00012: val_loss did not improve from 0.93150\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0313 - acc: 0.6921 - val_loss: 1.2283 - val_acc: 0.5982\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0031 - acc: 0.7025\n",
      "Epoch 00013: val_loss did not improve from 0.93150\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0031 - acc: 0.7025 - val_loss: 0.9718 - val_acc: 0.6883\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9789 - acc: 0.7074\n",
      "Epoch 00014: val_loss did not improve from 0.93150\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9790 - acc: 0.7074 - val_loss: 0.9563 - val_acc: 0.7105\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9578 - acc: 0.7144\n",
      "Epoch 00015: val_loss improved from 0.93150 to 0.92017, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/015-0.9202.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9578 - acc: 0.7144 - val_loss: 0.9202 - val_acc: 0.7319\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9420 - acc: 0.7220\n",
      "Epoch 00016: val_loss improved from 0.92017 to 0.84592, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/016-0.8459.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9421 - acc: 0.7219 - val_loss: 0.8459 - val_acc: 0.7650\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9222 - acc: 0.7260\n",
      "Epoch 00017: val_loss did not improve from 0.84592\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9223 - acc: 0.7260 - val_loss: 0.9050 - val_acc: 0.7135\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9103 - acc: 0.7298\n",
      "Epoch 00018: val_loss improved from 0.84592 to 0.77047, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/018-0.7705.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.9105 - acc: 0.7297 - val_loss: 0.7705 - val_acc: 0.7801\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8944 - acc: 0.7324\n",
      "Epoch 00019: val_loss improved from 0.77047 to 0.74010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/019-0.7401.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8944 - acc: 0.7324 - val_loss: 0.7401 - val_acc: 0.8046\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7387\n",
      "Epoch 00020: val_loss did not improve from 0.74010\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8835 - acc: 0.7387 - val_loss: 0.7506 - val_acc: 0.8018\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8753 - acc: 0.7410\n",
      "Epoch 00021: val_loss improved from 0.74010 to 0.71792, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/021-0.7179.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8754 - acc: 0.7410 - val_loss: 0.7179 - val_acc: 0.8090\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8611 - acc: 0.7442\n",
      "Epoch 00022: val_loss did not improve from 0.71792\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8610 - acc: 0.7442 - val_loss: 0.7879 - val_acc: 0.7787\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8535 - acc: 0.7473\n",
      "Epoch 00023: val_loss did not improve from 0.71792\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8535 - acc: 0.7473 - val_loss: 1.1484 - val_acc: 0.6597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8378 - acc: 0.7505\n",
      "Epoch 00024: val_loss improved from 0.71792 to 0.70544, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/024-0.7054.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8378 - acc: 0.7506 - val_loss: 0.7054 - val_acc: 0.8036\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8304 - acc: 0.7537\n",
      "Epoch 00025: val_loss did not improve from 0.70544\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8303 - acc: 0.7537 - val_loss: 0.8141 - val_acc: 0.7729\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7561\n",
      "Epoch 00026: val_loss did not improve from 0.70544\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8211 - acc: 0.7561 - val_loss: 0.7900 - val_acc: 0.7815\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8180 - acc: 0.7573\n",
      "Epoch 00027: val_loss did not improve from 0.70544\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8180 - acc: 0.7572 - val_loss: 0.8686 - val_acc: 0.7345\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8112 - acc: 0.7600\n",
      "Epoch 00028: val_loss improved from 0.70544 to 0.65450, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/028-0.6545.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8112 - acc: 0.7600 - val_loss: 0.6545 - val_acc: 0.8216\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7962 - acc: 0.7640\n",
      "Epoch 00029: val_loss did not improve from 0.65450\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7962 - acc: 0.7641 - val_loss: 0.8200 - val_acc: 0.7349\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7898 - acc: 0.7658\n",
      "Epoch 00030: val_loss did not improve from 0.65450\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7897 - acc: 0.7658 - val_loss: 0.7171 - val_acc: 0.7997\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7697\n",
      "Epoch 00031: val_loss did not improve from 0.65450\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7836 - acc: 0.7697 - val_loss: 0.7055 - val_acc: 0.7862\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7692\n",
      "Epoch 00032: val_loss improved from 0.65450 to 0.64693, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/032-0.6469.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7825 - acc: 0.7692 - val_loss: 0.6469 - val_acc: 0.8325\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7667 - acc: 0.7752\n",
      "Epoch 00033: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7667 - acc: 0.7752 - val_loss: 0.6985 - val_acc: 0.8146\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7685 - acc: 0.7739\n",
      "Epoch 00034: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7685 - acc: 0.7739 - val_loss: 0.8549 - val_acc: 0.7510\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7572 - acc: 0.7767\n",
      "Epoch 00035: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7572 - acc: 0.7767 - val_loss: 2.6213 - val_acc: 0.4118\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7538 - acc: 0.7768\n",
      "Epoch 00036: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7540 - acc: 0.7767 - val_loss: 0.7349 - val_acc: 0.7796\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7805\n",
      "Epoch 00037: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7442 - acc: 0.7804 - val_loss: 0.8712 - val_acc: 0.7529\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7801\n",
      "Epoch 00038: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7438 - acc: 0.7800 - val_loss: 0.6619 - val_acc: 0.8230\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.7826\n",
      "Epoch 00039: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7354 - acc: 0.7826 - val_loss: 1.8322 - val_acc: 0.5446\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7292 - acc: 0.7840\n",
      "Epoch 00040: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7291 - acc: 0.7841 - val_loss: 0.9074 - val_acc: 0.7091\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7270 - acc: 0.7865\n",
      "Epoch 00041: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7271 - acc: 0.7865 - val_loss: 1.7948 - val_acc: 0.5416\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7863\n",
      "Epoch 00042: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7229 - acc: 0.7864 - val_loss: 0.7108 - val_acc: 0.7911\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7912\n",
      "Epoch 00043: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7141 - acc: 0.7912 - val_loss: 0.7889 - val_acc: 0.7449\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.7894\n",
      "Epoch 00044: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7180 - acc: 0.7894 - val_loss: 0.6736 - val_acc: 0.7997\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.7915\n",
      "Epoch 00045: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7046 - acc: 0.7915 - val_loss: 0.6619 - val_acc: 0.8104\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7901\n",
      "Epoch 00046: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7032 - acc: 0.7901 - val_loss: 0.7100 - val_acc: 0.7792\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7008 - acc: 0.7928\n",
      "Epoch 00047: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.7008 - acc: 0.7928 - val_loss: 0.6736 - val_acc: 0.7999\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6944 - acc: 0.7965\n",
      "Epoch 00048: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6944 - acc: 0.7965 - val_loss: 0.6519 - val_acc: 0.8150\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6944 - acc: 0.7952\n",
      "Epoch 00049: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6945 - acc: 0.7952 - val_loss: 0.8086 - val_acc: 0.7591\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6852 - acc: 0.7977\n",
      "Epoch 00050: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6854 - acc: 0.7976 - val_loss: 0.6480 - val_acc: 0.8218\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.7939\n",
      "Epoch 00051: val_loss did not improve from 0.64693\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6997 - acc: 0.7938 - val_loss: 1.0959 - val_acc: 0.6636\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.8022\n",
      "Epoch 00052: val_loss improved from 0.64693 to 0.63029, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/052-0.6303.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6757 - acc: 0.8021 - val_loss: 0.6303 - val_acc: 0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6729 - acc: 0.8032\n",
      "Epoch 00053: val_loss did not improve from 0.63029\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6731 - acc: 0.8032 - val_loss: 0.7197 - val_acc: 0.7845\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6708 - acc: 0.8035\n",
      "Epoch 00054: val_loss did not improve from 0.63029\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6709 - acc: 0.8034 - val_loss: 0.7910 - val_acc: 0.7612\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.8032\n",
      "Epoch 00055: val_loss did not improve from 0.63029\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6676 - acc: 0.8032 - val_loss: 0.9153 - val_acc: 0.7167\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6632 - acc: 0.8027\n",
      "Epoch 00056: val_loss improved from 0.63029 to 0.59902, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/056-0.5990.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6632 - acc: 0.8027 - val_loss: 0.5990 - val_acc: 0.8360\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.8040\n",
      "Epoch 00057: val_loss did not improve from 0.59902\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6655 - acc: 0.8040 - val_loss: 1.0297 - val_acc: 0.6799\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.8089\n",
      "Epoch 00058: val_loss did not improve from 0.59902\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6539 - acc: 0.8089 - val_loss: 0.6042 - val_acc: 0.8344\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6555 - acc: 0.8084\n",
      "Epoch 00059: val_loss did not improve from 0.59902\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6555 - acc: 0.8084 - val_loss: 0.6780 - val_acc: 0.7997\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6480 - acc: 0.8123\n",
      "Epoch 00060: val_loss improved from 0.59902 to 0.58905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/060-0.5890.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6481 - acc: 0.8122 - val_loss: 0.5890 - val_acc: 0.8302\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6521 - acc: 0.8052\n",
      "Epoch 00061: val_loss did not improve from 0.58905\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6522 - acc: 0.8052 - val_loss: 0.6660 - val_acc: 0.8118\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6394 - acc: 0.8093\n",
      "Epoch 00062: val_loss did not improve from 0.58905\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6395 - acc: 0.8092 - val_loss: 1.8727 - val_acc: 0.5539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6459 - acc: 0.8107\n",
      "Epoch 00063: val_loss did not improve from 0.58905\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6458 - acc: 0.8108 - val_loss: 0.6578 - val_acc: 0.8013\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.8144\n",
      "Epoch 00064: val_loss did not improve from 0.58905\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6366 - acc: 0.8144 - val_loss: 0.6216 - val_acc: 0.8260\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6279 - acc: 0.8143\n",
      "Epoch 00065: val_loss improved from 0.58905 to 0.54849, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/065-0.5485.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6280 - acc: 0.8143 - val_loss: 0.5485 - val_acc: 0.8470\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6330 - acc: 0.8118\n",
      "Epoch 00066: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6330 - acc: 0.8118 - val_loss: 0.6403 - val_acc: 0.8269\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6317 - acc: 0.8145\n",
      "Epoch 00067: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6317 - acc: 0.8145 - val_loss: 1.0121 - val_acc: 0.6928\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8138\n",
      "Epoch 00068: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6307 - acc: 0.8138 - val_loss: 0.6803 - val_acc: 0.7911\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6184 - acc: 0.8176\n",
      "Epoch 00069: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6184 - acc: 0.8177 - val_loss: 0.6899 - val_acc: 0.7929\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.8153\n",
      "Epoch 00070: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6212 - acc: 0.8153 - val_loss: 0.5696 - val_acc: 0.8318\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6273 - acc: 0.8139\n",
      "Epoch 00071: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6274 - acc: 0.8139 - val_loss: 0.7503 - val_acc: 0.7729\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8163\n",
      "Epoch 00072: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6164 - acc: 0.8163 - val_loss: 0.6489 - val_acc: 0.8029\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.8214\n",
      "Epoch 00073: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6078 - acc: 0.8214 - val_loss: 2.6899 - val_acc: 0.4929\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.8212\n",
      "Epoch 00074: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6093 - acc: 0.8212 - val_loss: 0.9585 - val_acc: 0.7123\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6069 - acc: 0.8216\n",
      "Epoch 00075: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6069 - acc: 0.8216 - val_loss: 0.7173 - val_acc: 0.7773\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8213\n",
      "Epoch 00076: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6060 - acc: 0.8212 - val_loss: 0.5642 - val_acc: 0.8418\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8249\n",
      "Epoch 00077: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6002 - acc: 0.8249 - val_loss: 1.8259 - val_acc: 0.5577\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6009 - acc: 0.8236\n",
      "Epoch 00078: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6009 - acc: 0.8237 - val_loss: 0.6636 - val_acc: 0.8097\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8221\n",
      "Epoch 00079: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6004 - acc: 0.8220 - val_loss: 1.8663 - val_acc: 0.5604\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.8238\n",
      "Epoch 00080: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5948 - acc: 0.8237 - val_loss: 0.5902 - val_acc: 0.8297\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5921 - acc: 0.8240\n",
      "Epoch 00081: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5921 - acc: 0.8240 - val_loss: 0.8074 - val_acc: 0.7587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.8253\n",
      "Epoch 00082: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5919 - acc: 0.8253 - val_loss: 0.9179 - val_acc: 0.7128\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5900 - acc: 0.8251\n",
      "Epoch 00083: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5902 - acc: 0.8250 - val_loss: 0.7052 - val_acc: 0.7980\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.8284\n",
      "Epoch 00084: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5857 - acc: 0.8284 - val_loss: 1.1052 - val_acc: 0.6508\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5818 - acc: 0.8269\n",
      "Epoch 00085: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5819 - acc: 0.8269 - val_loss: 4.2483 - val_acc: 0.3792\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.8301\n",
      "Epoch 00086: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5785 - acc: 0.8301 - val_loss: 0.6596 - val_acc: 0.7936\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5756 - acc: 0.8315\n",
      "Epoch 00087: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5756 - acc: 0.8316 - val_loss: 0.5889 - val_acc: 0.8316\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5711 - acc: 0.8321\n",
      "Epoch 00088: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5711 - acc: 0.8321 - val_loss: 1.0669 - val_acc: 0.7021\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8280\n",
      "Epoch 00089: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5738 - acc: 0.8280 - val_loss: 0.6400 - val_acc: 0.8176\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.8314\n",
      "Epoch 00090: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5698 - acc: 0.8314 - val_loss: 0.6742 - val_acc: 0.7994\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8343\n",
      "Epoch 00091: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5637 - acc: 0.8342 - val_loss: 0.6783 - val_acc: 0.7966\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5665 - acc: 0.8321\n",
      "Epoch 00092: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5666 - acc: 0.8321 - val_loss: 1.6478 - val_acc: 0.5758\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.8324\n",
      "Epoch 00093: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5653 - acc: 0.8324 - val_loss: 0.7328 - val_acc: 0.7717\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.8347\n",
      "Epoch 00094: val_loss did not improve from 0.54849\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5608 - acc: 0.8347 - val_loss: 0.5636 - val_acc: 0.8418\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8344\n",
      "Epoch 00095: val_loss improved from 0.54849 to 0.51303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/095-0.5130.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5628 - acc: 0.8344 - val_loss: 0.5130 - val_acc: 0.8544\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8358\n",
      "Epoch 00096: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5552 - acc: 0.8358 - val_loss: 1.9405 - val_acc: 0.5758\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8371\n",
      "Epoch 00097: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5564 - acc: 0.8371 - val_loss: 0.5758 - val_acc: 0.8295\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.8369\n",
      "Epoch 00098: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5510 - acc: 0.8368 - val_loss: 2.7478 - val_acc: 0.4677\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.8354\n",
      "Epoch 00099: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5525 - acc: 0.8354 - val_loss: 1.4627 - val_acc: 0.6322\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8394\n",
      "Epoch 00100: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5496 - acc: 0.8394 - val_loss: 1.9907 - val_acc: 0.5549\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8390\n",
      "Epoch 00101: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5455 - acc: 0.8390 - val_loss: 0.6555 - val_acc: 0.8095\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8397\n",
      "Epoch 00102: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5441 - acc: 0.8396 - val_loss: 0.7785 - val_acc: 0.7515\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.8405\n",
      "Epoch 00103: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5442 - acc: 0.8405 - val_loss: 4.4064 - val_acc: 0.3645\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.8429\n",
      "Epoch 00104: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5360 - acc: 0.8428 - val_loss: 0.5249 - val_acc: 0.8521\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8416\n",
      "Epoch 00105: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5385 - acc: 0.8416 - val_loss: 1.2795 - val_acc: 0.6548\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8409\n",
      "Epoch 00106: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5387 - acc: 0.8409 - val_loss: 0.5133 - val_acc: 0.8595\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8410\n",
      "Epoch 00107: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5383 - acc: 0.8411 - val_loss: 1.0296 - val_acc: 0.6797\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.8406\n",
      "Epoch 00108: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5335 - acc: 0.8406 - val_loss: 1.4928 - val_acc: 0.6108\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.8425\n",
      "Epoch 00109: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5336 - acc: 0.8425 - val_loss: 1.5766 - val_acc: 0.5691\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8448\n",
      "Epoch 00110: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5253 - acc: 0.8448 - val_loss: 0.7727 - val_acc: 0.7666\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8420\n",
      "Epoch 00111: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5321 - acc: 0.8419 - val_loss: 0.7146 - val_acc: 0.7750\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8446\n",
      "Epoch 00112: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5251 - acc: 0.8445 - val_loss: 0.5870 - val_acc: 0.8239\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8446\n",
      "Epoch 00113: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5203 - acc: 0.8446 - val_loss: 0.9798 - val_acc: 0.7140\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8433\n",
      "Epoch 00114: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5237 - acc: 0.8432 - val_loss: 0.5707 - val_acc: 0.8209\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5195 - acc: 0.8442\n",
      "Epoch 00115: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5195 - acc: 0.8442 - val_loss: 0.7492 - val_acc: 0.7848\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8457\n",
      "Epoch 00116: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5155 - acc: 0.8457 - val_loss: 1.1204 - val_acc: 0.7014\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8472\n",
      "Epoch 00117: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5140 - acc: 0.8471 - val_loss: 0.7103 - val_acc: 0.7810\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8465\n",
      "Epoch 00118: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5169 - acc: 0.8465 - val_loss: 0.6642 - val_acc: 0.7985\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8481\n",
      "Epoch 00119: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5116 - acc: 0.8480 - val_loss: 0.5928 - val_acc: 0.8265\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.8493\n",
      "Epoch 00120: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5129 - acc: 0.8493 - val_loss: 1.1592 - val_acc: 0.6806\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8503\n",
      "Epoch 00121: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5017 - acc: 0.8503 - val_loss: 0.9684 - val_acc: 0.6953\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8498\n",
      "Epoch 00122: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5083 - acc: 0.8498 - val_loss: 0.5464 - val_acc: 0.8460\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8523\n",
      "Epoch 00123: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5029 - acc: 0.8523 - val_loss: 1.2817 - val_acc: 0.6655\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8510\n",
      "Epoch 00124: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.5035 - acc: 0.8510 - val_loss: 0.5595 - val_acc: 0.8402\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.8520\n",
      "Epoch 00125: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5021 - acc: 0.8519 - val_loss: 0.7214 - val_acc: 0.7824\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8522\n",
      "Epoch 00126: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4951 - acc: 0.8522 - val_loss: 0.8692 - val_acc: 0.7398\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.8554\n",
      "Epoch 00127: val_loss did not improve from 0.51303\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4906 - acc: 0.8553 - val_loss: 0.7210 - val_acc: 0.7624\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8520\n",
      "Epoch 00128: val_loss improved from 0.51303 to 0.48314, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/128-0.4831.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4925 - acc: 0.8521 - val_loss: 0.4831 - val_acc: 0.8724\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8535\n",
      "Epoch 00129: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4938 - acc: 0.8535 - val_loss: 0.5273 - val_acc: 0.8456\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4916 - acc: 0.8528\n",
      "Epoch 00130: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4919 - acc: 0.8528 - val_loss: 0.5616 - val_acc: 0.8430\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8535\n",
      "Epoch 00131: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4920 - acc: 0.8535 - val_loss: 0.5249 - val_acc: 0.8430\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4858 - acc: 0.8532\n",
      "Epoch 00132: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4859 - acc: 0.8531 - val_loss: 4.2159 - val_acc: 0.4198\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8568\n",
      "Epoch 00133: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4877 - acc: 0.8568 - val_loss: 1.6509 - val_acc: 0.5577\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8544\n",
      "Epoch 00134: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4879 - acc: 0.8544 - val_loss: 0.6740 - val_acc: 0.8078\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8570\n",
      "Epoch 00135: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4842 - acc: 0.8570 - val_loss: 0.8490 - val_acc: 0.7181\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8578\n",
      "Epoch 00136: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4775 - acc: 0.8578 - val_loss: 0.7305 - val_acc: 0.7596\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8549\n",
      "Epoch 00137: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4826 - acc: 0.8550 - val_loss: 1.1915 - val_acc: 0.6778\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8568\n",
      "Epoch 00138: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4802 - acc: 0.8568 - val_loss: 0.6629 - val_acc: 0.8181\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8548\n",
      "Epoch 00139: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4824 - acc: 0.8547 - val_loss: 1.3575 - val_acc: 0.6408\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8578\n",
      "Epoch 00140: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4805 - acc: 0.8578 - val_loss: 2.8640 - val_acc: 0.4435\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8582\n",
      "Epoch 00141: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4752 - acc: 0.8581 - val_loss: 0.5584 - val_acc: 0.8227\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4773 - acc: 0.8602\n",
      "Epoch 00142: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4774 - acc: 0.8601 - val_loss: 0.5372 - val_acc: 0.8465\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8581\n",
      "Epoch 00143: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4783 - acc: 0.8581 - val_loss: 1.0476 - val_acc: 0.6755\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4675 - acc: 0.8612\n",
      "Epoch 00144: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4676 - acc: 0.8612 - val_loss: 0.5572 - val_acc: 0.8435\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4689 - acc: 0.8596\n",
      "Epoch 00145: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4691 - acc: 0.8596 - val_loss: 1.9282 - val_acc: 0.5854\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8599\n",
      "Epoch 00146: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4714 - acc: 0.8599 - val_loss: 0.5417 - val_acc: 0.8467\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8626\n",
      "Epoch 00147: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4635 - acc: 0.8626 - val_loss: 1.0124 - val_acc: 0.6816\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.8612\n",
      "Epoch 00148: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4682 - acc: 0.8612 - val_loss: 0.5828 - val_acc: 0.8227\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4687 - acc: 0.8603\n",
      "Epoch 00149: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4688 - acc: 0.8603 - val_loss: 0.7436 - val_acc: 0.7764\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8628\n",
      "Epoch 00150: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4607 - acc: 0.8628 - val_loss: 1.9052 - val_acc: 0.5942\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8640\n",
      "Epoch 00151: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4578 - acc: 0.8640 - val_loss: 1.7940 - val_acc: 0.6035\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.8643\n",
      "Epoch 00152: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4614 - acc: 0.8643 - val_loss: 1.8734 - val_acc: 0.6124\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8631\n",
      "Epoch 00153: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4597 - acc: 0.8631 - val_loss: 0.5815 - val_acc: 0.8309\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8654\n",
      "Epoch 00154: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4586 - acc: 0.8654 - val_loss: 1.3794 - val_acc: 0.6634\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8655\n",
      "Epoch 00155: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4482 - acc: 0.8655 - val_loss: 0.6100 - val_acc: 0.8195\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4570 - acc: 0.8630\n",
      "Epoch 00156: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4571 - acc: 0.8629 - val_loss: 3.5376 - val_acc: 0.4649\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8672\n",
      "Epoch 00157: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4484 - acc: 0.8672 - val_loss: 0.7244 - val_acc: 0.7801\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8654\n",
      "Epoch 00158: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4496 - acc: 0.8653 - val_loss: 2.0683 - val_acc: 0.5439\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.8656\n",
      "Epoch 00159: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4516 - acc: 0.8656 - val_loss: 0.6341 - val_acc: 0.7976\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8670\n",
      "Epoch 00160: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4466 - acc: 0.8670 - val_loss: 1.1865 - val_acc: 0.7133\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8658\n",
      "Epoch 00161: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4481 - acc: 0.8658 - val_loss: 0.5825 - val_acc: 0.8192\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8670\n",
      "Epoch 00162: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4478 - acc: 0.8669 - val_loss: 2.3081 - val_acc: 0.5432\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8677\n",
      "Epoch 00163: val_loss did not improve from 0.48314\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4468 - acc: 0.8677 - val_loss: 0.4860 - val_acc: 0.8598\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8676\n",
      "Epoch 00164: val_loss improved from 0.48314 to 0.47411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv_checkpoint/164-0.4741.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4463 - acc: 0.8675 - val_loss: 0.4741 - val_acc: 0.8728\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8691\n",
      "Epoch 00165: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4406 - acc: 0.8691 - val_loss: 0.7794 - val_acc: 0.7673\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4390 - acc: 0.8676\n",
      "Epoch 00166: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4391 - acc: 0.8676 - val_loss: 1.9343 - val_acc: 0.5933\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.8683\n",
      "Epoch 00167: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4380 - acc: 0.8683 - val_loss: 0.5250 - val_acc: 0.8502\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8688\n",
      "Epoch 00168: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4365 - acc: 0.8688 - val_loss: 1.5717 - val_acc: 0.6243\n",
      "Epoch 169/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8694\n",
      "Epoch 00169: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4346 - acc: 0.8694 - val_loss: 0.5000 - val_acc: 0.8595\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8687\n",
      "Epoch 00170: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4338 - acc: 0.8687 - val_loss: 0.7142 - val_acc: 0.7785\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8717\n",
      "Epoch 00171: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4296 - acc: 0.8718 - val_loss: 0.8390 - val_acc: 0.7482\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8718\n",
      "Epoch 00172: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4272 - acc: 0.8718 - val_loss: 0.9567 - val_acc: 0.7489\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8703\n",
      "Epoch 00173: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4283 - acc: 0.8703 - val_loss: 0.5170 - val_acc: 0.8498\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8678\n",
      "Epoch 00174: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4365 - acc: 0.8678 - val_loss: 0.6190 - val_acc: 0.8071\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8737\n",
      "Epoch 00175: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4273 - acc: 0.8737 - val_loss: 0.6781 - val_acc: 0.7873\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8713\n",
      "Epoch 00176: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4220 - acc: 0.8713 - val_loss: 0.5761 - val_acc: 0.8279\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8726\n",
      "Epoch 00177: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4235 - acc: 0.8726 - val_loss: 3.3275 - val_acc: 0.4584\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8739\n",
      "Epoch 00178: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4220 - acc: 0.8739 - val_loss: 1.2476 - val_acc: 0.6676\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8745\n",
      "Epoch 00179: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4188 - acc: 0.8744 - val_loss: 0.9788 - val_acc: 0.7184\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8734\n",
      "Epoch 00180: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4235 - acc: 0.8734 - val_loss: 1.8223 - val_acc: 0.6252\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8730\n",
      "Epoch 00181: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4207 - acc: 0.8730 - val_loss: 2.3025 - val_acc: 0.5313\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8718\n",
      "Epoch 00182: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4294 - acc: 0.8718 - val_loss: 1.1483 - val_acc: 0.6876\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4104 - acc: 0.8758\n",
      "Epoch 00183: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4105 - acc: 0.8758 - val_loss: 1.3098 - val_acc: 0.6028\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8755\n",
      "Epoch 00184: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4125 - acc: 0.8756 - val_loss: 1.1853 - val_acc: 0.6487\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8767\n",
      "Epoch 00185: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4156 - acc: 0.8767 - val_loss: 0.5560 - val_acc: 0.8281\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8750\n",
      "Epoch 00186: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4132 - acc: 0.8750 - val_loss: 0.7368 - val_acc: 0.7773\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8779\n",
      "Epoch 00187: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4115 - acc: 0.8779 - val_loss: 0.5476 - val_acc: 0.8477\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8786\n",
      "Epoch 00188: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4057 - acc: 0.8787 - val_loss: 1.1006 - val_acc: 0.7186\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8768\n",
      "Epoch 00189: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4071 - acc: 0.8768 - val_loss: 1.6518 - val_acc: 0.5740\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8773\n",
      "Epoch 00190: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4051 - acc: 0.8773 - val_loss: 2.7761 - val_acc: 0.5504\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8794\n",
      "Epoch 00191: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4048 - acc: 0.8794 - val_loss: 1.5380 - val_acc: 0.6215\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8778\n",
      "Epoch 00192: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4025 - acc: 0.8777 - val_loss: 0.9883 - val_acc: 0.7109\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8770\n",
      "Epoch 00193: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4105 - acc: 0.8771 - val_loss: 1.1927 - val_acc: 0.7112\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4042 - acc: 0.8797\n",
      "Epoch 00194: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4042 - acc: 0.8797 - val_loss: 1.0147 - val_acc: 0.7445\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8814\n",
      "Epoch 00195: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3987 - acc: 0.8814 - val_loss: 1.6023 - val_acc: 0.6429\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8796\n",
      "Epoch 00196: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3951 - acc: 0.8796 - val_loss: 6.6284 - val_acc: 0.3464\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8776\n",
      "Epoch 00197: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3988 - acc: 0.8776 - val_loss: 2.1957 - val_acc: 0.5819\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8794\n",
      "Epoch 00198: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4015 - acc: 0.8794 - val_loss: 0.5088 - val_acc: 0.8556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8791\n",
      "Epoch 00199: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3988 - acc: 0.8791 - val_loss: 1.4087 - val_acc: 0.6816\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8824\n",
      "Epoch 00200: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3880 - acc: 0.8824 - val_loss: 1.8129 - val_acc: 0.5924\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8817\n",
      "Epoch 00201: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3982 - acc: 0.8817 - val_loss: 1.7782 - val_acc: 0.5840\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8842\n",
      "Epoch 00202: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3872 - acc: 0.8842 - val_loss: 2.6804 - val_acc: 0.5597\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8837\n",
      "Epoch 00203: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.3895 - acc: 0.8837 - val_loss: 0.6390 - val_acc: 0.8244\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8815\n",
      "Epoch 00204: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3902 - acc: 0.8815 - val_loss: 0.8082 - val_acc: 0.7675\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8817\n",
      "Epoch 00205: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3925 - acc: 0.8818 - val_loss: 0.5985 - val_acc: 0.8109\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8850\n",
      "Epoch 00206: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3845 - acc: 0.8850 - val_loss: 0.6710 - val_acc: 0.8097\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8832\n",
      "Epoch 00207: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3877 - acc: 0.8831 - val_loss: 1.5649 - val_acc: 0.6322\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8824\n",
      "Epoch 00208: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3868 - acc: 0.8824 - val_loss: 2.6808 - val_acc: 0.5367\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8836\n",
      "Epoch 00209: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3854 - acc: 0.8835 - val_loss: 1.0901 - val_acc: 0.7179\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3825 - acc: 0.8850\n",
      "Epoch 00210: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3825 - acc: 0.8850 - val_loss: 0.7015 - val_acc: 0.7757\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8861\n",
      "Epoch 00211: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3796 - acc: 0.8862 - val_loss: 0.5164 - val_acc: 0.8602\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8850\n",
      "Epoch 00212: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3845 - acc: 0.8850 - val_loss: 3.2618 - val_acc: 0.4624\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8857\n",
      "Epoch 00213: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3839 - acc: 0.8857 - val_loss: 1.4653 - val_acc: 0.6522\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8848\n",
      "Epoch 00214: val_loss did not improve from 0.47411\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3796 - acc: 0.8848 - val_loss: 4.2945 - val_acc: 0.4153\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5631 - acc: 0.8309\n",
      "Loss: 0.5630885163324025 Accuracy: 0.83094496\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8295 - acc: 0.4199\n",
      "Epoch 00001: val_loss improved from inf to 1.60169, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/001-1.6017.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 1.8294 - acc: 0.4199 - val_loss: 1.6017 - val_acc: 0.5625\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3160 - acc: 0.6009\n",
      "Epoch 00002: val_loss improved from 1.60169 to 1.22121, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/002-1.2212.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.3159 - acc: 0.6009 - val_loss: 1.2212 - val_acc: 0.6208\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1323 - acc: 0.6651\n",
      "Epoch 00003: val_loss improved from 1.22121 to 0.96524, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/003-0.9652.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.1323 - acc: 0.6650 - val_loss: 0.9652 - val_acc: 0.7235\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0144 - acc: 0.7017\n",
      "Epoch 00004: val_loss improved from 0.96524 to 0.89445, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/004-0.8945.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.0144 - acc: 0.7017 - val_loss: 0.8945 - val_acc: 0.7522\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.7301\n",
      "Epoch 00005: val_loss improved from 0.89445 to 0.83469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/005-0.8347.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9294 - acc: 0.7300 - val_loss: 0.8347 - val_acc: 0.7857\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8684 - acc: 0.7474\n",
      "Epoch 00006: val_loss improved from 0.83469 to 0.80630, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/006-0.8063.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8684 - acc: 0.7474 - val_loss: 0.8063 - val_acc: 0.7848\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8164 - acc: 0.7639\n",
      "Epoch 00007: val_loss improved from 0.80630 to 0.78003, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/007-0.7800.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8165 - acc: 0.7639 - val_loss: 0.7800 - val_acc: 0.7925\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7753 - acc: 0.7787\n",
      "Epoch 00008: val_loss improved from 0.78003 to 0.66104, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/008-0.6610.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7753 - acc: 0.7787 - val_loss: 0.6610 - val_acc: 0.8293\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7890\n",
      "Epoch 00009: val_loss did not improve from 0.66104\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7397 - acc: 0.7890 - val_loss: 0.7375 - val_acc: 0.8018\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7950\n",
      "Epoch 00010: val_loss did not improve from 0.66104\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7120 - acc: 0.7949 - val_loss: 0.7507 - val_acc: 0.7694\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.8050\n",
      "Epoch 00011: val_loss improved from 0.66104 to 0.60745, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/011-0.6075.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6828 - acc: 0.8050 - val_loss: 0.6075 - val_acc: 0.8437\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.8126\n",
      "Epoch 00012: val_loss did not improve from 0.60745\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6538 - acc: 0.8125 - val_loss: 0.6233 - val_acc: 0.8309\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.8162\n",
      "Epoch 00013: val_loss did not improve from 0.60745\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6349 - acc: 0.8162 - val_loss: 0.7041 - val_acc: 0.7862\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.8245\n",
      "Epoch 00014: val_loss did not improve from 0.60745\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6187 - acc: 0.8245 - val_loss: 0.6354 - val_acc: 0.8290\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8311\n",
      "Epoch 00015: val_loss improved from 0.60745 to 0.58721, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/015-0.5872.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5955 - acc: 0.8311 - val_loss: 0.5872 - val_acc: 0.8416\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.8343\n",
      "Epoch 00016: val_loss improved from 0.58721 to 0.52046, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/016-0.5205.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5815 - acc: 0.8343 - val_loss: 0.5205 - val_acc: 0.8672\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.8392\n",
      "Epoch 00017: val_loss did not improve from 0.52046\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5659 - acc: 0.8391 - val_loss: 0.6050 - val_acc: 0.8286\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5435 - acc: 0.8432\n",
      "Epoch 00018: val_loss improved from 0.52046 to 0.49040, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/018-0.4904.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5435 - acc: 0.8432 - val_loss: 0.4904 - val_acc: 0.8707\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.8503\n",
      "Epoch 00019: val_loss improved from 0.49040 to 0.49010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/019-0.4901.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5293 - acc: 0.8503 - val_loss: 0.4901 - val_acc: 0.8665\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8521\n",
      "Epoch 00020: val_loss did not improve from 0.49010\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5149 - acc: 0.8521 - val_loss: 0.5113 - val_acc: 0.8623\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8540\n",
      "Epoch 00021: val_loss did not improve from 0.49010\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5066 - acc: 0.8540 - val_loss: 0.5574 - val_acc: 0.8558\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4911 - acc: 0.8599\n",
      "Epoch 00022: val_loss did not improve from 0.49010\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4914 - acc: 0.8599 - val_loss: 0.5433 - val_acc: 0.8395\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8620\n",
      "Epoch 00023: val_loss did not improve from 0.49010\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4809 - acc: 0.8619 - val_loss: 0.5437 - val_acc: 0.8465\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8630\n",
      "Epoch 00024: val_loss improved from 0.49010 to 0.45894, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/024-0.4589.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4818 - acc: 0.8629 - val_loss: 0.4589 - val_acc: 0.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8685\n",
      "Epoch 00025: val_loss did not improve from 0.45894\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4598 - acc: 0.8685 - val_loss: 0.5581 - val_acc: 0.8425\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8708\n",
      "Epoch 00026: val_loss did not improve from 0.45894\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4489 - acc: 0.8708 - val_loss: 0.4817 - val_acc: 0.8672\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8735\n",
      "Epoch 00027: val_loss did not improve from 0.45894\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4350 - acc: 0.8734 - val_loss: 0.5215 - val_acc: 0.8472\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8781\n",
      "Epoch 00028: val_loss improved from 0.45894 to 0.42303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/028-0.4230.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4281 - acc: 0.8781 - val_loss: 0.4230 - val_acc: 0.8870\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8787\n",
      "Epoch 00029: val_loss did not improve from 0.42303\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4216 - acc: 0.8787 - val_loss: 0.4531 - val_acc: 0.8828\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8804\n",
      "Epoch 00030: val_loss did not improve from 0.42303\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4111 - acc: 0.8804 - val_loss: 0.4876 - val_acc: 0.8621\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8828\n",
      "Epoch 00031: val_loss improved from 0.42303 to 0.41295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/031-0.4130.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4057 - acc: 0.8827 - val_loss: 0.4130 - val_acc: 0.8942\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8868\n",
      "Epoch 00032: val_loss did not improve from 0.41295\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3937 - acc: 0.8867 - val_loss: 0.5279 - val_acc: 0.8484\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8847\n",
      "Epoch 00033: val_loss improved from 0.41295 to 0.40203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/033-0.4020.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3969 - acc: 0.8847 - val_loss: 0.4020 - val_acc: 0.8998\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8876\n",
      "Epoch 00034: val_loss did not improve from 0.40203\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3828 - acc: 0.8876 - val_loss: 0.4057 - val_acc: 0.8898\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8924\n",
      "Epoch 00035: val_loss improved from 0.40203 to 0.40150, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/035-0.4015.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3698 - acc: 0.8924 - val_loss: 0.4015 - val_acc: 0.9001\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8914\n",
      "Epoch 00036: val_loss did not improve from 0.40150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3718 - acc: 0.8913 - val_loss: 1.3122 - val_acc: 0.6485\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8965\n",
      "Epoch 00037: val_loss did not improve from 0.40150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3575 - acc: 0.8966 - val_loss: 0.4490 - val_acc: 0.8805\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8979\n",
      "Epoch 00038: val_loss did not improve from 0.40150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3520 - acc: 0.8979 - val_loss: 0.4602 - val_acc: 0.8719\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8977\n",
      "Epoch 00039: val_loss improved from 0.40150 to 0.37150, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/039-0.3715.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3491 - acc: 0.8977 - val_loss: 0.3715 - val_acc: 0.9066\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8995\n",
      "Epoch 00040: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3428 - acc: 0.8996 - val_loss: 0.4519 - val_acc: 0.8749\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.9020\n",
      "Epoch 00041: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3340 - acc: 0.9020 - val_loss: 0.6305 - val_acc: 0.8192\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.9032\n",
      "Epoch 00042: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3318 - acc: 0.9032 - val_loss: 0.7491 - val_acc: 0.7761\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.9039\n",
      "Epoch 00043: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3232 - acc: 0.9039 - val_loss: 0.3910 - val_acc: 0.8968\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9080\n",
      "Epoch 00044: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3180 - acc: 0.9080 - val_loss: 0.4593 - val_acc: 0.8747\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9097\n",
      "Epoch 00045: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3134 - acc: 0.9097 - val_loss: 0.4306 - val_acc: 0.8949\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9088\n",
      "Epoch 00046: val_loss did not improve from 0.37150\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3103 - acc: 0.9088 - val_loss: 0.3881 - val_acc: 0.8952\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9109\n",
      "Epoch 00047: val_loss improved from 0.37150 to 0.34450, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/047-0.3445.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3040 - acc: 0.9109 - val_loss: 0.3445 - val_acc: 0.9068\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9117\n",
      "Epoch 00048: val_loss did not improve from 0.34450\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3013 - acc: 0.9117 - val_loss: 0.6217 - val_acc: 0.8204\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9099\n",
      "Epoch 00049: val_loss did not improve from 0.34450\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3015 - acc: 0.9100 - val_loss: 0.4096 - val_acc: 0.8931\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2857 - acc: 0.9150\n",
      "Epoch 00050: val_loss did not improve from 0.34450\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2858 - acc: 0.9150 - val_loss: 0.4377 - val_acc: 0.8807\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9172\n",
      "Epoch 00051: val_loss did not improve from 0.34450\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2810 - acc: 0.9172 - val_loss: 0.4513 - val_acc: 0.8821\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9168\n",
      "Epoch 00052: val_loss did not improve from 0.34450\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2793 - acc: 0.9168 - val_loss: 0.3920 - val_acc: 0.9015\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9167\n",
      "Epoch 00053: val_loss improved from 0.34450 to 0.33699, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/053-0.3370.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2818 - acc: 0.9167 - val_loss: 0.3370 - val_acc: 0.9122\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9203\n",
      "Epoch 00054: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2716 - acc: 0.9203 - val_loss: 0.3596 - val_acc: 0.9087\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9196\n",
      "Epoch 00055: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2688 - acc: 0.9196 - val_loss: 0.4484 - val_acc: 0.8849\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9202\n",
      "Epoch 00056: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2678 - acc: 0.9201 - val_loss: 0.4625 - val_acc: 0.8796\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9209\n",
      "Epoch 00057: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2703 - acc: 0.9209 - val_loss: 0.4019 - val_acc: 0.8935\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9249\n",
      "Epoch 00058: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2525 - acc: 0.9249 - val_loss: 0.4075 - val_acc: 0.9005\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9255\n",
      "Epoch 00059: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2519 - acc: 0.9255 - val_loss: 0.3377 - val_acc: 0.9126\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9262\n",
      "Epoch 00060: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2491 - acc: 0.9262 - val_loss: 0.6135 - val_acc: 0.8304\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9258\n",
      "Epoch 00061: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2490 - acc: 0.9259 - val_loss: 0.4480 - val_acc: 0.8721\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9286\n",
      "Epoch 00062: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2405 - acc: 0.9287 - val_loss: 0.4183 - val_acc: 0.8926\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9299\n",
      "Epoch 00063: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2353 - acc: 0.9298 - val_loss: 0.4054 - val_acc: 0.8973\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9280\n",
      "Epoch 00064: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2379 - acc: 0.9281 - val_loss: 0.3484 - val_acc: 0.9154\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9311\n",
      "Epoch 00065: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2289 - acc: 0.9311 - val_loss: 0.3748 - val_acc: 0.9003\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9328\n",
      "Epoch 00066: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2282 - acc: 0.9328 - val_loss: 0.6444 - val_acc: 0.8269\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9335\n",
      "Epoch 00067: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2273 - acc: 0.9334 - val_loss: 0.4088 - val_acc: 0.8963\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9335\n",
      "Epoch 00068: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2232 - acc: 0.9335 - val_loss: 0.3972 - val_acc: 0.8968\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9343\n",
      "Epoch 00069: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2221 - acc: 0.9343 - val_loss: 0.4056 - val_acc: 0.8910\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9361\n",
      "Epoch 00070: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2150 - acc: 0.9360 - val_loss: 0.8175 - val_acc: 0.7824\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9368\n",
      "Epoch 00071: val_loss did not improve from 0.33699\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2132 - acc: 0.9368 - val_loss: 0.7868 - val_acc: 0.7911\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9374\n",
      "Epoch 00072: val_loss improved from 0.33699 to 0.32109, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/072-0.3211.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2121 - acc: 0.9375 - val_loss: 0.3211 - val_acc: 0.9173\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9381\n",
      "Epoch 00073: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2042 - acc: 0.9381 - val_loss: 0.3933 - val_acc: 0.9019\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9405\n",
      "Epoch 00074: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2020 - acc: 0.9404 - val_loss: 0.3925 - val_acc: 0.8959\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9392\n",
      "Epoch 00075: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2033 - acc: 0.9392 - val_loss: 0.5449 - val_acc: 0.8498\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9405\n",
      "Epoch 00076: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2001 - acc: 0.9405 - val_loss: 0.3401 - val_acc: 0.9140\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9428\n",
      "Epoch 00077: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1951 - acc: 0.9428 - val_loss: 0.3901 - val_acc: 0.9108\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9415\n",
      "Epoch 00078: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1948 - acc: 0.9415 - val_loss: 0.5286 - val_acc: 0.8570\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9387\n",
      "Epoch 00079: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2046 - acc: 0.9387 - val_loss: 0.3881 - val_acc: 0.9015\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9434\n",
      "Epoch 00080: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1905 - acc: 0.9434 - val_loss: 0.4495 - val_acc: 0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9447\n",
      "Epoch 00081: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1849 - acc: 0.9447 - val_loss: 0.3534 - val_acc: 0.9122\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9457\n",
      "Epoch 00082: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1843 - acc: 0.9456 - val_loss: 0.3264 - val_acc: 0.9213\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9466\n",
      "Epoch 00083: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1817 - acc: 0.9466 - val_loss: 0.3311 - val_acc: 0.9171\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9458\n",
      "Epoch 00084: val_loss did not improve from 0.32109\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1806 - acc: 0.9458 - val_loss: 0.3722 - val_acc: 0.9101\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9455\n",
      "Epoch 00085: val_loss improved from 0.32109 to 0.32088, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv_checkpoint/085-0.3209.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1782 - acc: 0.9455 - val_loss: 0.3209 - val_acc: 0.9194\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9480\n",
      "Epoch 00086: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1764 - acc: 0.9480 - val_loss: 0.3835 - val_acc: 0.9145\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9482\n",
      "Epoch 00087: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1752 - acc: 0.9482 - val_loss: 0.4937 - val_acc: 0.8810\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9500\n",
      "Epoch 00088: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1661 - acc: 0.9500 - val_loss: 0.5029 - val_acc: 0.8786\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9463\n",
      "Epoch 00089: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1813 - acc: 0.9463 - val_loss: 0.4162 - val_acc: 0.8873\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9503\n",
      "Epoch 00090: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1655 - acc: 0.9502 - val_loss: 0.5578 - val_acc: 0.8663\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9506\n",
      "Epoch 00091: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1655 - acc: 0.9505 - val_loss: 0.4434 - val_acc: 0.8952\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9482\n",
      "Epoch 00092: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1754 - acc: 0.9482 - val_loss: 0.5811 - val_acc: 0.8486\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9502\n",
      "Epoch 00093: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1672 - acc: 0.9502 - val_loss: 0.5157 - val_acc: 0.8698\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9516\n",
      "Epoch 00094: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1629 - acc: 0.9516 - val_loss: 0.3508 - val_acc: 0.9119\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9530\n",
      "Epoch 00095: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1567 - acc: 0.9530 - val_loss: 0.4179 - val_acc: 0.8859\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9564\n",
      "Epoch 00096: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1479 - acc: 0.9564 - val_loss: 0.3715 - val_acc: 0.9099\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9551\n",
      "Epoch 00097: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1500 - acc: 0.9551 - val_loss: 0.3355 - val_acc: 0.9208\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9519\n",
      "Epoch 00098: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1605 - acc: 0.9519 - val_loss: 0.4670 - val_acc: 0.8847\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9527\n",
      "Epoch 00099: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1569 - acc: 0.9527 - val_loss: 0.3899 - val_acc: 0.9064\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9566\n",
      "Epoch 00100: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1473 - acc: 0.9566 - val_loss: 0.5686 - val_acc: 0.8591\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9565\n",
      "Epoch 00101: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1452 - acc: 0.9565 - val_loss: 0.6261 - val_acc: 0.8472\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9568\n",
      "Epoch 00102: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1423 - acc: 0.9568 - val_loss: 0.3760 - val_acc: 0.9115\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9580\n",
      "Epoch 00103: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1393 - acc: 0.9580 - val_loss: 0.3939 - val_acc: 0.8975\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9592\n",
      "Epoch 00104: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1372 - acc: 0.9592 - val_loss: 0.3591 - val_acc: 0.9103\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9589\n",
      "Epoch 00105: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1369 - acc: 0.9589 - val_loss: 0.5066 - val_acc: 0.8626\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9594\n",
      "Epoch 00106: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1371 - acc: 0.9594 - val_loss: 1.3285 - val_acc: 0.7100\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9589\n",
      "Epoch 00107: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1390 - acc: 0.9588 - val_loss: 1.0769 - val_acc: 0.7384\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9620\n",
      "Epoch 00108: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1291 - acc: 0.9620 - val_loss: 0.3638 - val_acc: 0.9129\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9580\n",
      "Epoch 00109: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1387 - acc: 0.9580 - val_loss: 0.4161 - val_acc: 0.9019\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9618\n",
      "Epoch 00110: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1302 - acc: 0.9617 - val_loss: 0.6736 - val_acc: 0.8383\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9575\n",
      "Epoch 00111: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1398 - acc: 0.9575 - val_loss: 0.3787 - val_acc: 0.9131\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9640\n",
      "Epoch 00112: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1213 - acc: 0.9640 - val_loss: 0.4208 - val_acc: 0.9015\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9633\n",
      "Epoch 00113: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1239 - acc: 0.9633 - val_loss: 1.5918 - val_acc: 0.6827\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9620\n",
      "Epoch 00114: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1251 - acc: 0.9620 - val_loss: 0.4991 - val_acc: 0.8726\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9624\n",
      "Epoch 00115: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1243 - acc: 0.9624 - val_loss: 0.3481 - val_acc: 0.9178\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9646\n",
      "Epoch 00116: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1177 - acc: 0.9646 - val_loss: 0.4153 - val_acc: 0.8952\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9640\n",
      "Epoch 00117: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1183 - acc: 0.9640 - val_loss: 0.3753 - val_acc: 0.9073\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9644\n",
      "Epoch 00118: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1190 - acc: 0.9644 - val_loss: 0.4440 - val_acc: 0.8956\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9649\n",
      "Epoch 00119: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1200 - acc: 0.9648 - val_loss: 0.3998 - val_acc: 0.9050\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9611\n",
      "Epoch 00120: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1270 - acc: 0.9611 - val_loss: 0.4301 - val_acc: 0.8896\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9693\n",
      "Epoch 00121: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1076 - acc: 0.9693 - val_loss: 0.3844 - val_acc: 0.9040\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9650\n",
      "Epoch 00122: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1173 - acc: 0.9650 - val_loss: 0.4269 - val_acc: 0.8933\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9658\n",
      "Epoch 00123: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1179 - acc: 0.9657 - val_loss: 0.3609 - val_acc: 0.9201\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9669\n",
      "Epoch 00124: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1092 - acc: 0.9669 - val_loss: 0.3732 - val_acc: 0.9119\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9658\n",
      "Epoch 00125: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1142 - acc: 0.9658 - val_loss: 0.6962 - val_acc: 0.8265\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9682\n",
      "Epoch 00126: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1079 - acc: 0.9681 - val_loss: 0.4125 - val_acc: 0.8991\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9670\n",
      "Epoch 00127: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1099 - acc: 0.9670 - val_loss: 0.3352 - val_acc: 0.9178\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9696\n",
      "Epoch 00128: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1035 - acc: 0.9696 - val_loss: 0.4815 - val_acc: 0.8859\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9676\n",
      "Epoch 00129: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1096 - acc: 0.9676 - val_loss: 0.3559 - val_acc: 0.9145\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9694\n",
      "Epoch 00130: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1024 - acc: 0.9693 - val_loss: 0.4871 - val_acc: 0.8866\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9703\n",
      "Epoch 00131: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1005 - acc: 0.9703 - val_loss: 0.3455 - val_acc: 0.9185\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9714\n",
      "Epoch 00132: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0994 - acc: 0.9714 - val_loss: 0.4088 - val_acc: 0.9045\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9702\n",
      "Epoch 00133: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0985 - acc: 0.9702 - val_loss: 0.4143 - val_acc: 0.9064\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9683\n",
      "Epoch 00134: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1048 - acc: 0.9683 - val_loss: 0.4305 - val_acc: 0.8963\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9702\n",
      "Epoch 00135: val_loss did not improve from 0.32088\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1002 - acc: 0.9702 - val_loss: 0.3843 - val_acc: 0.9052\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3918 - acc: 0.8868\n",
      "Loss: 0.39176324451823846 Accuracy: 0.88681203\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5835 - acc: 0.5129\n",
      "Epoch 00001: val_loss improved from inf to 1.31985, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/001-1.3199.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 1.5835 - acc: 0.5129 - val_loss: 1.3199 - val_acc: 0.6580\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9662 - acc: 0.7257\n",
      "Epoch 00002: val_loss improved from 1.31985 to 0.85587, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/002-0.8559.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.9665 - acc: 0.7256 - val_loss: 0.8559 - val_acc: 0.7594\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7726 - acc: 0.7892\n",
      "Epoch 00003: val_loss improved from 0.85587 to 0.60831, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/003-0.6083.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7727 - acc: 0.7892 - val_loss: 0.6083 - val_acc: 0.8404\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.8257\n",
      "Epoch 00004: val_loss improved from 0.60831 to 0.57931, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/004-0.5793.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6396 - acc: 0.8256 - val_loss: 0.5793 - val_acc: 0.8481\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.8502\n",
      "Epoch 00005: val_loss improved from 0.57931 to 0.50765, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/005-0.5077.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5560 - acc: 0.8502 - val_loss: 0.5077 - val_acc: 0.8654\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8658\n",
      "Epoch 00006: val_loss did not improve from 0.50765\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4920 - acc: 0.8658 - val_loss: 0.6165 - val_acc: 0.8130\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8761\n",
      "Epoch 00007: val_loss improved from 0.50765 to 0.48060, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/007-0.4806.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4536 - acc: 0.8762 - val_loss: 0.4806 - val_acc: 0.8798\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8844\n",
      "Epoch 00008: val_loss improved from 0.48060 to 0.36324, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/008-0.3632.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4186 - acc: 0.8844 - val_loss: 0.3632 - val_acc: 0.9040\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8947\n",
      "Epoch 00009: val_loss improved from 0.36324 to 0.30262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/009-0.3026.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3817 - acc: 0.8947 - val_loss: 0.3026 - val_acc: 0.9234\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8989\n",
      "Epoch 00010: val_loss did not improve from 0.30262\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3613 - acc: 0.8989 - val_loss: 0.3407 - val_acc: 0.9124\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.9049\n",
      "Epoch 00011: val_loss did not improve from 0.30262\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3407 - acc: 0.9049 - val_loss: 0.3245 - val_acc: 0.9187\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9103\n",
      "Epoch 00012: val_loss did not improve from 0.30262\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3221 - acc: 0.9103 - val_loss: 0.3122 - val_acc: 0.9117\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9129\n",
      "Epoch 00013: val_loss did not improve from 0.30262\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3069 - acc: 0.9129 - val_loss: 0.3133 - val_acc: 0.9217\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9179\n",
      "Epoch 00014: val_loss improved from 0.30262 to 0.27064, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/014-0.2706.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2885 - acc: 0.9179 - val_loss: 0.2706 - val_acc: 0.9278\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9221\n",
      "Epoch 00015: val_loss did not improve from 0.27064\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2734 - acc: 0.9220 - val_loss: 0.2998 - val_acc: 0.9217\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9256\n",
      "Epoch 00016: val_loss improved from 0.27064 to 0.25744, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/016-0.2574.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2628 - acc: 0.9256 - val_loss: 0.2574 - val_acc: 0.9315\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9254\n",
      "Epoch 00017: val_loss did not improve from 0.25744\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2565 - acc: 0.9254 - val_loss: 0.2622 - val_acc: 0.9294\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9304\n",
      "Epoch 00018: val_loss did not improve from 0.25744\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2419 - acc: 0.9304 - val_loss: 0.3034 - val_acc: 0.9145\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9329\n",
      "Epoch 00019: val_loss did not improve from 0.25744\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2341 - acc: 0.9329 - val_loss: 0.2612 - val_acc: 0.9315\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9372\n",
      "Epoch 00020: val_loss did not improve from 0.25744\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2201 - acc: 0.9372 - val_loss: 0.2606 - val_acc: 0.9299\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9367\n",
      "Epoch 00021: val_loss did not improve from 0.25744\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2187 - acc: 0.9367 - val_loss: 0.2931 - val_acc: 0.9182\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9408\n",
      "Epoch 00022: val_loss improved from 0.25744 to 0.22311, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/022-0.2231.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2065 - acc: 0.9408 - val_loss: 0.2231 - val_acc: 0.9394\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9420\n",
      "Epoch 00023: val_loss improved from 0.22311 to 0.21815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/023-0.2181.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1991 - acc: 0.9420 - val_loss: 0.2181 - val_acc: 0.9434\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9440\n",
      "Epoch 00024: val_loss did not improve from 0.21815\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1916 - acc: 0.9440 - val_loss: 0.2604 - val_acc: 0.9334\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9454\n",
      "Epoch 00025: val_loss improved from 0.21815 to 0.21586, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/025-0.2159.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1879 - acc: 0.9453 - val_loss: 0.2159 - val_acc: 0.9392\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9469\n",
      "Epoch 00026: val_loss did not improve from 0.21586\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1823 - acc: 0.9469 - val_loss: 0.2770 - val_acc: 0.9248\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9486\n",
      "Epoch 00027: val_loss did not improve from 0.21586\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1746 - acc: 0.9485 - val_loss: 0.2527 - val_acc: 0.9297\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9495\n",
      "Epoch 00028: val_loss did not improve from 0.21586\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1731 - acc: 0.9495 - val_loss: 0.2514 - val_acc: 0.9355\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9522\n",
      "Epoch 00029: val_loss did not improve from 0.21586\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1640 - acc: 0.9522 - val_loss: 0.3403 - val_acc: 0.9119\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9544\n",
      "Epoch 00030: val_loss improved from 0.21586 to 0.19565, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/030-0.1957.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1583 - acc: 0.9544 - val_loss: 0.1957 - val_acc: 0.9474\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9545\n",
      "Epoch 00031: val_loss did not improve from 0.19565\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1568 - acc: 0.9545 - val_loss: 0.2026 - val_acc: 0.9404\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9566\n",
      "Epoch 00032: val_loss improved from 0.19565 to 0.18933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/032-0.1893.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1482 - acc: 0.9566 - val_loss: 0.1893 - val_acc: 0.9492\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9580\n",
      "Epoch 00033: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1436 - acc: 0.9580 - val_loss: 0.2205 - val_acc: 0.9399\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9599\n",
      "Epoch 00034: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1392 - acc: 0.9599 - val_loss: 0.2534 - val_acc: 0.9327\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9608\n",
      "Epoch 00035: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1349 - acc: 0.9608 - val_loss: 0.2098 - val_acc: 0.9420\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9619\n",
      "Epoch 00036: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1301 - acc: 0.9619 - val_loss: 0.2127 - val_acc: 0.9464\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9646\n",
      "Epoch 00037: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1224 - acc: 0.9646 - val_loss: 0.2608 - val_acc: 0.9315\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9627\n",
      "Epoch 00038: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1277 - acc: 0.9627 - val_loss: 0.2130 - val_acc: 0.9450\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9654\n",
      "Epoch 00039: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1171 - acc: 0.9653 - val_loss: 0.2669 - val_acc: 0.9376\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9619\n",
      "Epoch 00040: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1295 - acc: 0.9619 - val_loss: 0.2747 - val_acc: 0.9236\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9663\n",
      "Epoch 00041: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1123 - acc: 0.9663 - val_loss: 0.2622 - val_acc: 0.9320\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9682\n",
      "Epoch 00042: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1087 - acc: 0.9682 - val_loss: 0.2201 - val_acc: 0.9420\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9684\n",
      "Epoch 00043: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1076 - acc: 0.9684 - val_loss: 0.2050 - val_acc: 0.9492\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9686\n",
      "Epoch 00044: val_loss did not improve from 0.18933\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1044 - acc: 0.9686 - val_loss: 0.2978 - val_acc: 0.9234\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9701\n",
      "Epoch 00045: val_loss improved from 0.18933 to 0.18451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv_checkpoint/045-0.1845.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1027 - acc: 0.9700 - val_loss: 0.1845 - val_acc: 0.9492\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9705\n",
      "Epoch 00046: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1003 - acc: 0.9705 - val_loss: 0.2791 - val_acc: 0.9343\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9726\n",
      "Epoch 00047: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0939 - acc: 0.9725 - val_loss: 0.2401 - val_acc: 0.9413\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9736\n",
      "Epoch 00048: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0905 - acc: 0.9736 - val_loss: 0.2081 - val_acc: 0.9455\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9703\n",
      "Epoch 00049: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0990 - acc: 0.9703 - val_loss: 0.2417 - val_acc: 0.9413\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9733\n",
      "Epoch 00050: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0900 - acc: 0.9733 - val_loss: 0.2437 - val_acc: 0.9439\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9758\n",
      "Epoch 00051: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0831 - acc: 0.9758 - val_loss: 0.3936 - val_acc: 0.8966\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9717\n",
      "Epoch 00052: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0946 - acc: 0.9717 - val_loss: 0.2037 - val_acc: 0.9432\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9791\n",
      "Epoch 00053: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0761 - acc: 0.9791 - val_loss: 0.2140 - val_acc: 0.9441\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9787\n",
      "Epoch 00054: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0735 - acc: 0.9787 - val_loss: 0.2341 - val_acc: 0.9446\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9765\n",
      "Epoch 00055: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0773 - acc: 0.9765 - val_loss: 0.2005 - val_acc: 0.9471\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9779\n",
      "Epoch 00056: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0744 - acc: 0.9779 - val_loss: 0.2718 - val_acc: 0.9313\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9793\n",
      "Epoch 00057: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0722 - acc: 0.9793 - val_loss: 0.2446 - val_acc: 0.9390\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9766\n",
      "Epoch 00058: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0795 - acc: 0.9766 - val_loss: 0.2108 - val_acc: 0.9509\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9789\n",
      "Epoch 00059: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0698 - acc: 0.9789 - val_loss: 0.2516 - val_acc: 0.9390\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9808\n",
      "Epoch 00060: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0669 - acc: 0.9808 - val_loss: 0.2586 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9805\n",
      "Epoch 00061: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0647 - acc: 0.9805 - val_loss: 0.2139 - val_acc: 0.9481\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9817\n",
      "Epoch 00062: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0623 - acc: 0.9817 - val_loss: 0.2088 - val_acc: 0.9527\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9804\n",
      "Epoch 00063: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0656 - acc: 0.9804 - val_loss: 0.2610 - val_acc: 0.9390\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9818\n",
      "Epoch 00064: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0622 - acc: 0.9818 - val_loss: 0.2342 - val_acc: 0.9443\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9833\n",
      "Epoch 00065: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0595 - acc: 0.9832 - val_loss: 0.2477 - val_acc: 0.9418\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9824\n",
      "Epoch 00066: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0638 - acc: 0.9824 - val_loss: 0.2335 - val_acc: 0.9478\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9842\n",
      "Epoch 00067: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0557 - acc: 0.9842 - val_loss: 0.2540 - val_acc: 0.9467\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9841\n",
      "Epoch 00068: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0571 - acc: 0.9841 - val_loss: 0.2257 - val_acc: 0.9457\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9846\n",
      "Epoch 00069: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0546 - acc: 0.9846 - val_loss: 0.2367 - val_acc: 0.9455\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9831\n",
      "Epoch 00070: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0597 - acc: 0.9831 - val_loss: 0.2222 - val_acc: 0.9462\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9872\n",
      "Epoch 00071: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0465 - acc: 0.9872 - val_loss: 0.3081 - val_acc: 0.9352\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9839\n",
      "Epoch 00072: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0572 - acc: 0.9839 - val_loss: 0.2328 - val_acc: 0.9446\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9873\n",
      "Epoch 00073: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0451 - acc: 0.9872 - val_loss: 0.2768 - val_acc: 0.9334\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9830\n",
      "Epoch 00074: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0600 - acc: 0.9830 - val_loss: 0.2075 - val_acc: 0.9520\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9887\n",
      "Epoch 00075: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0419 - acc: 0.9887 - val_loss: 0.2346 - val_acc: 0.9457\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9883\n",
      "Epoch 00076: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0436 - acc: 0.9883 - val_loss: 0.2544 - val_acc: 0.9450\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9861\n",
      "Epoch 00077: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0472 - acc: 0.9861 - val_loss: 0.2700 - val_acc: 0.9441\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9882\n",
      "Epoch 00078: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0423 - acc: 0.9882 - val_loss: 0.2550 - val_acc: 0.9441\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9871\n",
      "Epoch 00079: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0455 - acc: 0.9871 - val_loss: 0.2344 - val_acc: 0.9485\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9893\n",
      "Epoch 00080: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0405 - acc: 0.9893 - val_loss: 0.2732 - val_acc: 0.9311\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9872\n",
      "Epoch 00081: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0461 - acc: 0.9872 - val_loss: 0.2324 - val_acc: 0.9474\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9883\n",
      "Epoch 00082: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0420 - acc: 0.9882 - val_loss: 0.2571 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9862\n",
      "Epoch 00083: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0485 - acc: 0.9862 - val_loss: 0.2498 - val_acc: 0.9399\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9899\n",
      "Epoch 00084: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0383 - acc: 0.9899 - val_loss: 0.2207 - val_acc: 0.9436\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9859\n",
      "Epoch 00085: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0457 - acc: 0.9859 - val_loss: 0.2358 - val_acc: 0.9448\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9912\n",
      "Epoch 00086: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0354 - acc: 0.9911 - val_loss: 0.2109 - val_acc: 0.9504\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9885\n",
      "Epoch 00087: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0421 - acc: 0.9885 - val_loss: 0.1952 - val_acc: 0.9536\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9911\n",
      "Epoch 00088: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0334 - acc: 0.9911 - val_loss: 0.3023 - val_acc: 0.9313\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9903\n",
      "Epoch 00089: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0351 - acc: 0.9903 - val_loss: 0.2854 - val_acc: 0.9373\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9885\n",
      "Epoch 00090: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0393 - acc: 0.9885 - val_loss: 0.2231 - val_acc: 0.9483\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9913\n",
      "Epoch 00091: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0342 - acc: 0.9913 - val_loss: 0.2105 - val_acc: 0.9518\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9905\n",
      "Epoch 00092: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0356 - acc: 0.9905 - val_loss: 0.2174 - val_acc: 0.9490\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9927\n",
      "Epoch 00093: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0286 - acc: 0.9927 - val_loss: 0.2321 - val_acc: 0.9485\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9891\n",
      "Epoch 00094: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0381 - acc: 0.9891 - val_loss: 0.2864 - val_acc: 0.9373\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9920\n",
      "Epoch 00095: val_loss did not improve from 0.18451\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0302 - acc: 0.9920 - val_loss: 0.2342 - val_acc: 0.9513\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2225 - acc: 0.9385\n",
      "Loss: 0.22249647387586774 Accuracy: 0.93852544\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2767 - acc: 0.6105\n",
      "Epoch 00001: val_loss improved from inf to 0.92527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/001-0.9253.hdf5\n",
      "36805/36805 [==============================] - 121s 3ms/sample - loss: 1.2766 - acc: 0.6105 - val_loss: 0.9253 - val_acc: 0.7911\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8367\n",
      "Epoch 00002: val_loss improved from 0.92527 to 0.38528, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/002-0.3853.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.5939 - acc: 0.8367 - val_loss: 0.3853 - val_acc: 0.9106\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8840\n",
      "Epoch 00003: val_loss improved from 0.38528 to 0.29685, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/003-0.2968.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4251 - acc: 0.8840 - val_loss: 0.2968 - val_acc: 0.9210\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3429 - acc: 0.9052\n",
      "Epoch 00004: val_loss improved from 0.29685 to 0.28936, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/004-0.2894.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3429 - acc: 0.9052 - val_loss: 0.2894 - val_acc: 0.9234\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9188\n",
      "Epoch 00005: val_loss improved from 0.28936 to 0.24256, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/005-0.2426.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2908 - acc: 0.9188 - val_loss: 0.2426 - val_acc: 0.9378\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9267\n",
      "Epoch 00006: val_loss improved from 0.24256 to 0.20927, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/006-0.2093.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2603 - acc: 0.9267 - val_loss: 0.2093 - val_acc: 0.9446\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9340\n",
      "Epoch 00007: val_loss improved from 0.20927 to 0.20236, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/007-0.2024.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2308 - acc: 0.9340 - val_loss: 0.2024 - val_acc: 0.9474\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9392\n",
      "Epoch 00008: val_loss improved from 0.20236 to 0.18626, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/008-0.1863.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2123 - acc: 0.9391 - val_loss: 0.1863 - val_acc: 0.9520\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9447\n",
      "Epoch 00009: val_loss improved from 0.18626 to 0.18543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/009-0.1854.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1901 - acc: 0.9447 - val_loss: 0.1854 - val_acc: 0.9513\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9501\n",
      "Epoch 00010: val_loss did not improve from 0.18543\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1743 - acc: 0.9501 - val_loss: 0.1955 - val_acc: 0.9443\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9539\n",
      "Epoch 00011: val_loss did not improve from 0.18543\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1613 - acc: 0.9539 - val_loss: 0.2172 - val_acc: 0.9371\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9547\n",
      "Epoch 00012: val_loss did not improve from 0.18543\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1536 - acc: 0.9547 - val_loss: 0.1974 - val_acc: 0.9448\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9601\n",
      "Epoch 00013: val_loss improved from 0.18543 to 0.17320, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/013-0.1732.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1377 - acc: 0.9601 - val_loss: 0.1732 - val_acc: 0.9485\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9623\n",
      "Epoch 00014: val_loss improved from 0.17320 to 0.15753, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/014-0.1575.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1305 - acc: 0.9622 - val_loss: 0.1575 - val_acc: 0.9590\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9631\n",
      "Epoch 00015: val_loss did not improve from 0.15753\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1259 - acc: 0.9631 - val_loss: 0.1583 - val_acc: 0.9534\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9665\n",
      "Epoch 00016: val_loss improved from 0.15753 to 0.14257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/016-0.1426.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1151 - acc: 0.9665 - val_loss: 0.1426 - val_acc: 0.9604\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9698\n",
      "Epoch 00017: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1038 - acc: 0.9698 - val_loss: 0.1552 - val_acc: 0.9555\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9682\n",
      "Epoch 00018: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1041 - acc: 0.9682 - val_loss: 0.1823 - val_acc: 0.9476\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9734\n",
      "Epoch 00019: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0926 - acc: 0.9734 - val_loss: 0.1714 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9740\n",
      "Epoch 00020: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0878 - acc: 0.9740 - val_loss: 0.1649 - val_acc: 0.9548\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9746\n",
      "Epoch 00021: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0882 - acc: 0.9745 - val_loss: 0.1631 - val_acc: 0.9569\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9745\n",
      "Epoch 00022: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0873 - acc: 0.9745 - val_loss: 0.2222 - val_acc: 0.9401\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9785\n",
      "Epoch 00023: val_loss did not improve from 0.14257\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0760 - acc: 0.9785 - val_loss: 0.1681 - val_acc: 0.9553\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9807\n",
      "Epoch 00024: val_loss improved from 0.14257 to 0.12670, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv_checkpoint/024-0.1267.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0687 - acc: 0.9807 - val_loss: 0.1267 - val_acc: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9811\n",
      "Epoch 00025: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0655 - acc: 0.9811 - val_loss: 0.1542 - val_acc: 0.9613\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9824\n",
      "Epoch 00026: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0613 - acc: 0.9824 - val_loss: 0.1365 - val_acc: 0.9660\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9808\n",
      "Epoch 00027: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0668 - acc: 0.9808 - val_loss: 0.1579 - val_acc: 0.9553\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9831\n",
      "Epoch 00028: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0589 - acc: 0.9831 - val_loss: 0.1487 - val_acc: 0.9618\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9829\n",
      "Epoch 00029: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0601 - acc: 0.9829 - val_loss: 0.1577 - val_acc: 0.9576\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9863\n",
      "Epoch 00030: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0501 - acc: 0.9863 - val_loss: 0.1665 - val_acc: 0.9555\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9870\n",
      "Epoch 00031: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0472 - acc: 0.9870 - val_loss: 0.1943 - val_acc: 0.9415\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9836\n",
      "Epoch 00032: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0574 - acc: 0.9836 - val_loss: 0.1542 - val_acc: 0.9592\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9855\n",
      "Epoch 00033: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0509 - acc: 0.9855 - val_loss: 0.2054 - val_acc: 0.9439\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9884\n",
      "Epoch 00034: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0419 - acc: 0.9884 - val_loss: 0.2011 - val_acc: 0.9502\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9899\n",
      "Epoch 00035: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0369 - acc: 0.9899 - val_loss: 0.1596 - val_acc: 0.9595\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9907\n",
      "Epoch 00036: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0361 - acc: 0.9907 - val_loss: 0.1423 - val_acc: 0.9585\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9872\n",
      "Epoch 00037: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0420 - acc: 0.9872 - val_loss: 0.1635 - val_acc: 0.9574\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9905\n",
      "Epoch 00038: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0340 - acc: 0.9905 - val_loss: 0.1883 - val_acc: 0.9562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9880\n",
      "Epoch 00039: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0429 - acc: 0.9880 - val_loss: 0.1303 - val_acc: 0.9665\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9925\n",
      "Epoch 00040: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0287 - acc: 0.9925 - val_loss: 0.2181 - val_acc: 0.9492\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9916\n",
      "Epoch 00041: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0309 - acc: 0.9916 - val_loss: 0.2087 - val_acc: 0.9427\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9902\n",
      "Epoch 00042: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0372 - acc: 0.9902 - val_loss: 0.1769 - val_acc: 0.9585\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9936\n",
      "Epoch 00043: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0250 - acc: 0.9936 - val_loss: 0.2250 - val_acc: 0.9509\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9901\n",
      "Epoch 00044: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0337 - acc: 0.9901 - val_loss: 0.1324 - val_acc: 0.9618\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9921\n",
      "Epoch 00045: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0308 - acc: 0.9921 - val_loss: 0.1790 - val_acc: 0.9543\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9920\n",
      "Epoch 00046: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0294 - acc: 0.9920 - val_loss: 0.1770 - val_acc: 0.9588\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9883\n",
      "Epoch 00047: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0379 - acc: 0.9883 - val_loss: 0.1580 - val_acc: 0.9574\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9939\n",
      "Epoch 00048: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0228 - acc: 0.9939 - val_loss: 0.1662 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9914\n",
      "Epoch 00049: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0302 - acc: 0.9914 - val_loss: 0.1369 - val_acc: 0.9667\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9943\n",
      "Epoch 00050: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0221 - acc: 0.9943 - val_loss: 0.1438 - val_acc: 0.9634\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0198 - acc: 0.9948 - val_loss: 0.1584 - val_acc: 0.9616\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9933\n",
      "Epoch 00052: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0249 - acc: 0.9933 - val_loss: 0.1667 - val_acc: 0.9620\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9908\n",
      "Epoch 00053: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0315 - acc: 0.9907 - val_loss: 0.1596 - val_acc: 0.9602\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9950\n",
      "Epoch 00054: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0193 - acc: 0.9950 - val_loss: 0.1828 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9948\n",
      "Epoch 00055: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0192 - acc: 0.9948 - val_loss: 0.1450 - val_acc: 0.9669\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9954\n",
      "Epoch 00056: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0180 - acc: 0.9954 - val_loss: 0.1637 - val_acc: 0.9644\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9945\n",
      "Epoch 00057: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0198 - acc: 0.9945 - val_loss: 0.2701 - val_acc: 0.9420\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9947\n",
      "Epoch 00058: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0194 - acc: 0.9947 - val_loss: 0.1596 - val_acc: 0.9611\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958\n",
      "Epoch 00059: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0158 - acc: 0.9958 - val_loss: 0.1982 - val_acc: 0.9506\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0173 - acc: 0.9955 - val_loss: 0.1999 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9949\n",
      "Epoch 00061: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0185 - acc: 0.9949 - val_loss: 0.2067 - val_acc: 0.9499\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9884\n",
      "Epoch 00062: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0386 - acc: 0.9883 - val_loss: 0.1618 - val_acc: 0.9555\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 00063: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0168 - acc: 0.9956 - val_loss: 0.1813 - val_acc: 0.9583\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9972\n",
      "Epoch 00064: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.1893 - val_acc: 0.9557\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9978\n",
      "Epoch 00065: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0108 - acc: 0.9978 - val_loss: 0.2035 - val_acc: 0.9520\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9953\n",
      "Epoch 00066: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0178 - acc: 0.9953 - val_loss: 0.1775 - val_acc: 0.9606\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9947\n",
      "Epoch 00067: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0188 - acc: 0.9947 - val_loss: 0.1658 - val_acc: 0.9662\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9972\n",
      "Epoch 00068: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.1666 - val_acc: 0.9676\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9970\n",
      "Epoch 00069: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0125 - acc: 0.9970 - val_loss: 0.1731 - val_acc: 0.9599\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9950\n",
      "Epoch 00070: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.1533 - val_acc: 0.9606\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9928\n",
      "Epoch 00071: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0253 - acc: 0.9928 - val_loss: 0.1359 - val_acc: 0.9667\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9982\n",
      "Epoch 00072: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0092 - acc: 0.9982 - val_loss: 0.1658 - val_acc: 0.9620\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9972\n",
      "Epoch 00073: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.1688 - val_acc: 0.9613\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9951\n",
      "Epoch 00074: val_loss did not improve from 0.12670\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.2097 - val_acc: 0.9515\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1590 - acc: 0.9531\n",
      "Loss: 0.1590009427949523 Accuracy: 0.95306337\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1994 - acc: 0.6295\n",
      "Epoch 00001: val_loss improved from inf to 0.65761, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/001-0.6576.hdf5\n",
      "36805/36805 [==============================] - 124s 3ms/sample - loss: 1.1994 - acc: 0.6295 - val_loss: 0.6576 - val_acc: 0.7983\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8611\n",
      "Epoch 00002: val_loss improved from 0.65761 to 0.27624, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/002-0.2762.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.4620 - acc: 0.8611 - val_loss: 0.2762 - val_acc: 0.9236\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9017\n",
      "Epoch 00003: val_loss improved from 0.27624 to 0.20146, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/003-0.2015.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.3285 - acc: 0.9017 - val_loss: 0.2015 - val_acc: 0.9406\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9216\n",
      "Epoch 00004: val_loss improved from 0.20146 to 0.19418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/004-0.1942.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2619 - acc: 0.9216 - val_loss: 0.1942 - val_acc: 0.9469\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9328\n",
      "Epoch 00005: val_loss did not improve from 0.19418\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2193 - acc: 0.9328 - val_loss: 0.1945 - val_acc: 0.9411\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9431\n",
      "Epoch 00006: val_loss did not improve from 0.19418\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1898 - acc: 0.9431 - val_loss: 0.1950 - val_acc: 0.9434\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9483\n",
      "Epoch 00007: val_loss did not improve from 0.19418\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1680 - acc: 0.9483 - val_loss: 0.2043 - val_acc: 0.9383\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9556\n",
      "Epoch 00008: val_loss improved from 0.19418 to 0.16004, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/008-0.1600.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1467 - acc: 0.9556 - val_loss: 0.1600 - val_acc: 0.9532\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9601\n",
      "Epoch 00009: val_loss improved from 0.16004 to 0.15174, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/009-0.1517.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1324 - acc: 0.9601 - val_loss: 0.1517 - val_acc: 0.9515\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9620\n",
      "Epoch 00010: val_loss did not improve from 0.15174\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1216 - acc: 0.9620 - val_loss: 0.2359 - val_acc: 0.9306\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9626\n",
      "Epoch 00011: val_loss did not improve from 0.15174\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1179 - acc: 0.9626 - val_loss: 0.2592 - val_acc: 0.9217\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9710\n",
      "Epoch 00012: val_loss improved from 0.15174 to 0.12989, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/012-0.1299.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0961 - acc: 0.9710 - val_loss: 0.1299 - val_acc: 0.9588\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9720\n",
      "Epoch 00013: val_loss improved from 0.12989 to 0.11782, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/013-0.1178.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0912 - acc: 0.9720 - val_loss: 0.1178 - val_acc: 0.9662\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9747\n",
      "Epoch 00014: val_loss did not improve from 0.11782\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0837 - acc: 0.9747 - val_loss: 0.1493 - val_acc: 0.9529\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9778\n",
      "Epoch 00015: val_loss did not improve from 0.11782\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0765 - acc: 0.9777 - val_loss: 0.1476 - val_acc: 0.9536\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9760\n",
      "Epoch 00016: val_loss improved from 0.11782 to 0.11466, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/016-0.1147.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0791 - acc: 0.9759 - val_loss: 0.1147 - val_acc: 0.9637\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9779\n",
      "Epoch 00017: val_loss improved from 0.11466 to 0.10365, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/017-0.1036.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0722 - acc: 0.9779 - val_loss: 0.1036 - val_acc: 0.9697\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9837\n",
      "Epoch 00018: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0553 - acc: 0.9837 - val_loss: 0.1331 - val_acc: 0.9597\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9832\n",
      "Epoch 00019: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0565 - acc: 0.9832 - val_loss: 0.1203 - val_acc: 0.9641\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9851\n",
      "Epoch 00020: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0505 - acc: 0.9851 - val_loss: 0.1329 - val_acc: 0.9576\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9855\n",
      "Epoch 00021: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0472 - acc: 0.9855 - val_loss: 0.2371 - val_acc: 0.9315\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9866\n",
      "Epoch 00022: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0457 - acc: 0.9866 - val_loss: 0.1469 - val_acc: 0.9553\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9852\n",
      "Epoch 00023: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0491 - acc: 0.9852 - val_loss: 0.1146 - val_acc: 0.9653\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9889\n",
      "Epoch 00024: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0386 - acc: 0.9889 - val_loss: 0.1528 - val_acc: 0.9553\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9878\n",
      "Epoch 00025: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0413 - acc: 0.9878 - val_loss: 0.1321 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9903\n",
      "Epoch 00026: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0332 - acc: 0.9903 - val_loss: 0.2229 - val_acc: 0.9434\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9912\n",
      "Epoch 00027: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0316 - acc: 0.9912 - val_loss: 0.1448 - val_acc: 0.9576\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9863\n",
      "Epoch 00028: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0445 - acc: 0.9863 - val_loss: 0.1209 - val_acc: 0.9655\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9916\n",
      "Epoch 00029: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0288 - acc: 0.9916 - val_loss: 0.1213 - val_acc: 0.9660\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9920\n",
      "Epoch 00030: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0272 - acc: 0.9920 - val_loss: 0.1098 - val_acc: 0.9690\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9920\n",
      "Epoch 00031: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0270 - acc: 0.9920 - val_loss: 0.1332 - val_acc: 0.9665\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9929\n",
      "Epoch 00032: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0247 - acc: 0.9929 - val_loss: 0.1181 - val_acc: 0.9672\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9911\n",
      "Epoch 00033: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0291 - acc: 0.9911 - val_loss: 0.1402 - val_acc: 0.9623\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9891\n",
      "Epoch 00034: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0346 - acc: 0.9891 - val_loss: 0.1340 - val_acc: 0.9620\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9957\n",
      "Epoch 00035: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0172 - acc: 0.9957 - val_loss: 0.1097 - val_acc: 0.9690\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9948\n",
      "Epoch 00036: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0185 - acc: 0.9947 - val_loss: 0.1142 - val_acc: 0.9686\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9893\n",
      "Epoch 00037: val_loss did not improve from 0.10365\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0363 - acc: 0.9892 - val_loss: 0.1361 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9941\n",
      "Epoch 00038: val_loss improved from 0.10365 to 0.09905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv_checkpoint/038-0.0990.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0213 - acc: 0.9941 - val_loss: 0.0990 - val_acc: 0.9737\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9968\n",
      "Epoch 00039: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0129 - acc: 0.9968 - val_loss: 0.1173 - val_acc: 0.9672\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9956\n",
      "Epoch 00040: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0173 - acc: 0.9956 - val_loss: 0.1438 - val_acc: 0.9618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9949\n",
      "Epoch 00041: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0182 - acc: 0.9949 - val_loss: 0.1758 - val_acc: 0.9550\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9954\n",
      "Epoch 00042: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0163 - acc: 0.9954 - val_loss: 0.1453 - val_acc: 0.9641\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 00043: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.1498 - val_acc: 0.9632\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9959\n",
      "Epoch 00044: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0155 - acc: 0.9959 - val_loss: 0.1496 - val_acc: 0.9627\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9953\n",
      "Epoch 00045: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0164 - acc: 0.9953 - val_loss: 0.1709 - val_acc: 0.9588\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0180 - acc: 0.9951 - val_loss: 0.1540 - val_acc: 0.9641\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9969\n",
      "Epoch 00047: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0120 - acc: 0.9969 - val_loss: 0.1625 - val_acc: 0.9630\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9957\n",
      "Epoch 00048: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.1198 - val_acc: 0.9700\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9952\n",
      "Epoch 00049: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0166 - acc: 0.9952 - val_loss: 0.1360 - val_acc: 0.9634\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 00050: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0137 - acc: 0.9959 - val_loss: 0.2511 - val_acc: 0.9439\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9961\n",
      "Epoch 00051: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0134 - acc: 0.9961 - val_loss: 0.1386 - val_acc: 0.9665\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9969\n",
      "Epoch 00052: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0107 - acc: 0.9969 - val_loss: 0.1307 - val_acc: 0.9646\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9961\n",
      "Epoch 00053: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0138 - acc: 0.9961 - val_loss: 0.1359 - val_acc: 0.9667\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9973\n",
      "Epoch 00054: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0101 - acc: 0.9973 - val_loss: 0.1581 - val_acc: 0.9632\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.1055 - val_acc: 0.9697\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 00056: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0096 - acc: 0.9971 - val_loss: 0.1289 - val_acc: 0.9651\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9962\n",
      "Epoch 00057: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.1765 - val_acc: 0.9567\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9930\n",
      "Epoch 00058: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0232 - acc: 0.9930 - val_loss: 0.1110 - val_acc: 0.9681\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00059: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1107 - val_acc: 0.9723\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n",
      "Epoch 00060: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1233 - val_acc: 0.9700\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00061: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.1756 - val_acc: 0.9611\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9944\n",
      "Epoch 00062: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0184 - acc: 0.9944 - val_loss: 0.1210 - val_acc: 0.9697\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9978\n",
      "Epoch 00063: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0081 - acc: 0.9978 - val_loss: 0.1142 - val_acc: 0.9704\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9959\n",
      "Epoch 00064: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0136 - acc: 0.9959 - val_loss: 0.1221 - val_acc: 0.9695\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00065: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0151 - acc: 0.9959 - val_loss: 0.1252 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9984\n",
      "Epoch 00066: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0059 - acc: 0.9984 - val_loss: 0.1235 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 00067: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0059 - acc: 0.9985 - val_loss: 0.1175 - val_acc: 0.9690\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00068: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1434 - val_acc: 0.9651\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 00069: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0109 - acc: 0.9968 - val_loss: 0.1605 - val_acc: 0.9646\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n",
      "Epoch 00070: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0062 - acc: 0.9985 - val_loss: 0.1289 - val_acc: 0.9690\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9976\n",
      "Epoch 00071: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1628 - val_acc: 0.9618\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 00072: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0109 - acc: 0.9968 - val_loss: 0.1481 - val_acc: 0.9653\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.1345 - val_acc: 0.9704\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9971\n",
      "Epoch 00074: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0094 - acc: 0.9971 - val_loss: 0.1579 - val_acc: 0.9660\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985\n",
      "Epoch 00075: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0054 - acc: 0.9985 - val_loss: 0.1571 - val_acc: 0.9634\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 00076: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0119 - acc: 0.9962 - val_loss: 0.1428 - val_acc: 0.9667\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9976\n",
      "Epoch 00077: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0084 - acc: 0.9976 - val_loss: 0.1483 - val_acc: 0.9660\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00078: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1841 - val_acc: 0.9653\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9972\n",
      "Epoch 00079: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.1812 - val_acc: 0.9564\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.1816 - val_acc: 0.9604\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Epoch 00081: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0086 - acc: 0.9977 - val_loss: 0.1388 - val_acc: 0.9706\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978\n",
      "Epoch 00082: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0084 - acc: 0.9978 - val_loss: 0.2125 - val_acc: 0.9550\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9955\n",
      "Epoch 00083: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0159 - acc: 0.9955 - val_loss: 0.1443 - val_acc: 0.9676\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00084: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1480 - val_acc: 0.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9963\n",
      "Epoch 00085: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0129 - acc: 0.9963 - val_loss: 0.1046 - val_acc: 0.9723\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00086: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1117 - val_acc: 0.9730\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 00087: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.1914 - val_acc: 0.9609\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9976\n",
      "Epoch 00088: val_loss did not improve from 0.09905\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0073 - acc: 0.9976 - val_loss: 0.1267 - val_acc: 0.9690\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1413 - acc: 0.9624\n",
      "Loss: 0.1413172302948483 Accuracy: 0.96240914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8767 - acc: 0.7310\n",
      "Loss: 0.8767301782764502 Accuracy: 0.7310488\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5631 - acc: 0.8309\n",
      "Loss: 0.5630885163324025 Accuracy: 0.83094496\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3918 - acc: 0.8868\n",
      "Loss: 0.39176324451823846 Accuracy: 0.88681203\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2225 - acc: 0.9385\n",
      "Loss: 0.22249647387586774 Accuracy: 0.93852544\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1590 - acc: 0.9531\n",
      "Loss: 0.1590009427949523 Accuracy: 0.95306337\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1413 - acc: 0.9624\n",
      "Loss: 0.1413172302948483 Accuracy: 0.96240914\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 170,512\n",
      "Trainable params: 169,744\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.9959 - acc: 0.6808\n",
      "Loss: 0.9958535069245787 Accuracy: 0.6807892\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 128)   512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 128)    512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 128)     512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 253,072\n",
      "Trainable params: 252,048\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 4.4683 - acc: 0.3894\n",
      "Loss: 4.468301856455897 Accuracy: 0.3894081\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 128)   512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 128)    512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 128)     512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 256)     1024        conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 420,240\n",
      "Trainable params: 418,704\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4350 - acc: 0.8847\n",
      "Loss: 0.43498788475371347 Accuracy: 0.8847352\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 128)   512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 128)    512         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 128)    512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 128)     512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 256)     1024        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 256)      1024        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 751,248\n",
      "Trainable params: 749,200\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2752 - acc: 0.9348\n",
      "Loss: 0.2752117615127365 Accuracy: 0.9347871\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 128)   512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 128)    512         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 128)    512         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 128)     512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 256)     1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 256)      1024        conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 256)      1024        conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,080,208\n",
      "Trainable params: 1,077,648\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2234 - acc: 0.9433\n",
      "Loss: 0.2233687987761922 Accuracy: 0.94330215\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 128)   512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 128)    512         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 128)    512         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 128)     512         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 256)     1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 256)      1024        conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 256)      1024        conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 256)       1024        conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,409,168\n",
      "Trainable params: 1,406,096\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1742 - acc: 0.9599\n",
      "Loss: 0.17419907384018218 Accuracy: 0.95991695\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
