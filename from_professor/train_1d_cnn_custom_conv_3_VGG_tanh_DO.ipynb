{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7269 - acc: 0.0979\n",
      "Epoch 00001: val_loss improved from inf to 2.72619, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_1_conv_checkpoint/001-2.7262.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.7269 - acc: 0.0979 - val_loss: 2.7262 - val_acc: 0.0964\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4789 - acc: 0.2557\n",
      "Epoch 00002: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.4790 - acc: 0.2557 - val_loss: 2.8644 - val_acc: 0.1214\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9882 - acc: 0.4138\n",
      "Epoch 00003: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.9882 - acc: 0.4139 - val_loss: 3.1931 - val_acc: 0.1230\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6458 - acc: 0.5180\n",
      "Epoch 00004: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.6461 - acc: 0.5180 - val_loss: 3.5993 - val_acc: 0.1179\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4098 - acc: 0.5869\n",
      "Epoch 00005: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.4098 - acc: 0.5869 - val_loss: 4.0591 - val_acc: 0.1176\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2298 - acc: 0.6409\n",
      "Epoch 00006: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2299 - acc: 0.6409 - val_loss: 4.5251 - val_acc: 0.1176\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0969 - acc: 0.6820\n",
      "Epoch 00007: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0968 - acc: 0.6820 - val_loss: 5.0176 - val_acc: 0.1146\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9853 - acc: 0.7143\n",
      "Epoch 00008: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9852 - acc: 0.7144 - val_loss: 5.4424 - val_acc: 0.1139\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8882 - acc: 0.7422\n",
      "Epoch 00009: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8882 - acc: 0.7422 - val_loss: 5.8829 - val_acc: 0.1106\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8181 - acc: 0.7623\n",
      "Epoch 00010: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8181 - acc: 0.7623 - val_loss: 6.2761 - val_acc: 0.1127\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7447 - acc: 0.7838\n",
      "Epoch 00011: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7446 - acc: 0.7838 - val_loss: 6.6292 - val_acc: 0.1113\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.7968\n",
      "Epoch 00012: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6930 - acc: 0.7968 - val_loss: 6.9775 - val_acc: 0.1081\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6472 - acc: 0.8099\n",
      "Epoch 00013: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6472 - acc: 0.8099 - val_loss: 7.3021 - val_acc: 0.1099\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.8207\n",
      "Epoch 00014: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6031 - acc: 0.8206 - val_loss: 7.6058 - val_acc: 0.1060\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8310\n",
      "Epoch 00015: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5678 - acc: 0.8310 - val_loss: 7.8465 - val_acc: 0.1118\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8415\n",
      "Epoch 00016: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5321 - acc: 0.8416 - val_loss: 8.1384 - val_acc: 0.1088\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8489\n",
      "Epoch 00017: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5065 - acc: 0.8489 - val_loss: 8.3565 - val_acc: 0.1095\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8551\n",
      "Epoch 00018: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4860 - acc: 0.8550 - val_loss: 8.5692 - val_acc: 0.1058\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4486 - acc: 0.8648\n",
      "Epoch 00019: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4485 - acc: 0.8648 - val_loss: 8.7873 - val_acc: 0.1046\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8723\n",
      "Epoch 00020: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4254 - acc: 0.8723 - val_loss: 8.9457 - val_acc: 0.1097\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8759\n",
      "Epoch 00021: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4123 - acc: 0.8759 - val_loss: 9.1243 - val_acc: 0.1092\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8826\n",
      "Epoch 00022: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3871 - acc: 0.8825 - val_loss: 9.2790 - val_acc: 0.1116\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8839\n",
      "Epoch 00023: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3799 - acc: 0.8839 - val_loss: 9.4491 - val_acc: 0.1092\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8935\n",
      "Epoch 00024: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3507 - acc: 0.8935 - val_loss: 9.5484 - val_acc: 0.1090\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8939\n",
      "Epoch 00025: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3446 - acc: 0.8940 - val_loss: 9.7119 - val_acc: 0.1039\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.9007\n",
      "Epoch 00026: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3275 - acc: 0.9007 - val_loss: 9.8135 - val_acc: 0.1060\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.9035\n",
      "Epoch 00027: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3201 - acc: 0.9034 - val_loss: 9.9614 - val_acc: 0.1037\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9037\n",
      "Epoch 00028: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3140 - acc: 0.9037 - val_loss: 10.0701 - val_acc: 0.1058\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.9078\n",
      "Epoch 00029: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3038 - acc: 0.9078 - val_loss: 10.1887 - val_acc: 0.1053\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9127\n",
      "Epoch 00030: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2852 - acc: 0.9126 - val_loss: 10.2863 - val_acc: 0.1051\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9151\n",
      "Epoch 00031: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2726 - acc: 0.9151 - val_loss: 10.3630 - val_acc: 0.1048\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9175\n",
      "Epoch 00032: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2699 - acc: 0.9175 - val_loss: 10.4720 - val_acc: 0.1076\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9201\n",
      "Epoch 00033: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2624 - acc: 0.9200 - val_loss: 10.5766 - val_acc: 0.1067\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9220\n",
      "Epoch 00034: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2532 - acc: 0.9220 - val_loss: 10.5937 - val_acc: 0.1069\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9248\n",
      "Epoch 00035: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2422 - acc: 0.9248 - val_loss: 10.6773 - val_acc: 0.1060\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9297\n",
      "Epoch 00036: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2285 - acc: 0.9297 - val_loss: 10.7604 - val_acc: 0.1099\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9312\n",
      "Epoch 00037: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2272 - acc: 0.9312 - val_loss: 10.8647 - val_acc: 0.1076\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9321\n",
      "Epoch 00038: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2238 - acc: 0.9321 - val_loss: 10.9256 - val_acc: 0.1069\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9322\n",
      "Epoch 00039: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2189 - acc: 0.9322 - val_loss: 11.0140 - val_acc: 0.1072\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9357\n",
      "Epoch 00040: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2131 - acc: 0.9357 - val_loss: 11.0374 - val_acc: 0.1076\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9336\n",
      "Epoch 00041: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2166 - acc: 0.9335 - val_loss: 11.1126 - val_acc: 0.1072\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9375\n",
      "Epoch 00042: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2032 - acc: 0.9375 - val_loss: 11.1462 - val_acc: 0.1055\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9399\n",
      "Epoch 00043: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1959 - acc: 0.9398 - val_loss: 11.2079 - val_acc: 0.1048\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9405\n",
      "Epoch 00044: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1929 - acc: 0.9406 - val_loss: 11.2278 - val_acc: 0.1085\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9419\n",
      "Epoch 00045: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1880 - acc: 0.9419 - val_loss: 11.2631 - val_acc: 0.1058\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9448\n",
      "Epoch 00046: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1843 - acc: 0.9448 - val_loss: 11.3087 - val_acc: 0.1074\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9433\n",
      "Epoch 00047: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1817 - acc: 0.9433 - val_loss: 11.3622 - val_acc: 0.1062\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9436\n",
      "Epoch 00048: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1882 - acc: 0.9436 - val_loss: 11.4069 - val_acc: 0.1092\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9468\n",
      "Epoch 00049: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1761 - acc: 0.9468 - val_loss: 11.4755 - val_acc: 0.1067\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9464\n",
      "Epoch 00050: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1772 - acc: 0.9464 - val_loss: 11.4468 - val_acc: 0.1072\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9475\n",
      "Epoch 00051: val_loss did not improve from 2.72619\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1744 - acc: 0.9475 - val_loss: 11.5196 - val_acc: 0.1092\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 477us/sample - loss: 2.7204 - acc: 0.1001\n",
      "Loss: 2.7204171821954715 Accuracy: 0.10010384\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3266 - acc: 0.2604\n",
      "Epoch 00001: val_loss improved from inf to 2.09603, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_2_conv_checkpoint/001-2.0960.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 2.3266 - acc: 0.2604 - val_loss: 2.0960 - val_acc: 0.3452\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7804 - acc: 0.4453\n",
      "Epoch 00002: val_loss improved from 2.09603 to 2.05232, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_2_conv_checkpoint/002-2.0523.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.7804 - acc: 0.4453 - val_loss: 2.0523 - val_acc: 0.3592\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4411 - acc: 0.5498\n",
      "Epoch 00003: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.4411 - acc: 0.5498 - val_loss: 2.1270 - val_acc: 0.3594\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1930 - acc: 0.6277\n",
      "Epoch 00004: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.1930 - acc: 0.6276 - val_loss: 2.2615 - val_acc: 0.3683\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0098 - acc: 0.6854\n",
      "Epoch 00005: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.0098 - acc: 0.6853 - val_loss: 2.3602 - val_acc: 0.3594\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8502 - acc: 0.7330\n",
      "Epoch 00006: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.8503 - acc: 0.7330 - val_loss: 2.5194 - val_acc: 0.3627\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7326 - acc: 0.7693\n",
      "Epoch 00007: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.7326 - acc: 0.7693 - val_loss: 2.7061 - val_acc: 0.3590\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.8032\n",
      "Epoch 00008: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.6310 - acc: 0.8031 - val_loss: 2.8160 - val_acc: 0.3622\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.8274\n",
      "Epoch 00009: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.5472 - acc: 0.8274 - val_loss: 3.0024 - val_acc: 0.3557\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4756 - acc: 0.8492\n",
      "Epoch 00010: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4756 - acc: 0.8493 - val_loss: 3.0969 - val_acc: 0.3671\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8677\n",
      "Epoch 00011: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.4179 - acc: 0.8677 - val_loss: 3.2473 - val_acc: 0.3692\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8813\n",
      "Epoch 00012: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3726 - acc: 0.8813 - val_loss: 3.3560 - val_acc: 0.3634\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8933\n",
      "Epoch 00013: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3315 - acc: 0.8933 - val_loss: 3.4490 - val_acc: 0.3687\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9045\n",
      "Epoch 00014: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.3016 - acc: 0.9045 - val_loss: 3.5068 - val_acc: 0.3713\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9161\n",
      "Epoch 00015: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2669 - acc: 0.9161 - val_loss: 3.6130 - val_acc: 0.3750\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9265\n",
      "Epoch 00016: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2364 - acc: 0.9265 - val_loss: 3.7907 - val_acc: 0.3818\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9324\n",
      "Epoch 00017: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2209 - acc: 0.9325 - val_loss: 3.8332 - val_acc: 0.3753\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9375\n",
      "Epoch 00018: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.2026 - acc: 0.9375 - val_loss: 3.8455 - val_acc: 0.3778\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9402\n",
      "Epoch 00019: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1939 - acc: 0.9402 - val_loss: 3.9263 - val_acc: 0.3829\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9450\n",
      "Epoch 00020: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1785 - acc: 0.9450 - val_loss: 4.0138 - val_acc: 0.3829\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9471\n",
      "Epoch 00021: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1714 - acc: 0.9471 - val_loss: 4.0753 - val_acc: 0.3888\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9501\n",
      "Epoch 00022: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1626 - acc: 0.9501 - val_loss: 4.1285 - val_acc: 0.3806\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9542\n",
      "Epoch 00023: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1493 - acc: 0.9542 - val_loss: 4.1453 - val_acc: 0.3888\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9573\n",
      "Epoch 00024: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1427 - acc: 0.9573 - val_loss: 4.2559 - val_acc: 0.3864\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9577\n",
      "Epoch 00025: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1404 - acc: 0.9577 - val_loss: 4.2434 - val_acc: 0.3881\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9625\n",
      "Epoch 00026: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1247 - acc: 0.9625 - val_loss: 4.2593 - val_acc: 0.3899\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9648\n",
      "Epoch 00027: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1187 - acc: 0.9648 - val_loss: 4.3980 - val_acc: 0.3867\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9648\n",
      "Epoch 00028: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1187 - acc: 0.9648 - val_loss: 4.4086 - val_acc: 0.3934\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9652\n",
      "Epoch 00029: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1159 - acc: 0.9652 - val_loss: 4.4460 - val_acc: 0.3890\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9670\n",
      "Epoch 00030: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1111 - acc: 0.9670 - val_loss: 4.5304 - val_acc: 0.4016\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9688\n",
      "Epoch 00031: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1048 - acc: 0.9688 - val_loss: 4.5335 - val_acc: 0.4016\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9674\n",
      "Epoch 00032: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1078 - acc: 0.9675 - val_loss: 4.5985 - val_acc: 0.4002\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9716\n",
      "Epoch 00033: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0978 - acc: 0.9716 - val_loss: 4.5507 - val_acc: 0.4083\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9699\n",
      "Epoch 00034: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1013 - acc: 0.9699 - val_loss: 4.6156 - val_acc: 0.3974\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9716\n",
      "Epoch 00035: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0937 - acc: 0.9716 - val_loss: 4.6115 - val_acc: 0.3990\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9714\n",
      "Epoch 00036: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0980 - acc: 0.9713 - val_loss: 4.6640 - val_acc: 0.4023\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9736\n",
      "Epoch 00037: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0886 - acc: 0.9736 - val_loss: 4.6908 - val_acc: 0.4007\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9743\n",
      "Epoch 00038: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0867 - acc: 0.9743 - val_loss: 4.7417 - val_acc: 0.3993\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9733\n",
      "Epoch 00039: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0917 - acc: 0.9733 - val_loss: 4.8032 - val_acc: 0.3962\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9749\n",
      "Epoch 00040: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0895 - acc: 0.9749 - val_loss: 4.8435 - val_acc: 0.4055\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9745\n",
      "Epoch 00041: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0854 - acc: 0.9745 - val_loss: 4.8306 - val_acc: 0.4072\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9765\n",
      "Epoch 00042: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0820 - acc: 0.9766 - val_loss: 4.7615 - val_acc: 0.4121\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9757\n",
      "Epoch 00043: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0845 - acc: 0.9757 - val_loss: 4.8394 - val_acc: 0.4072\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9762\n",
      "Epoch 00044: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0831 - acc: 0.9762 - val_loss: 4.9000 - val_acc: 0.4090\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9781\n",
      "Epoch 00045: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0772 - acc: 0.9781 - val_loss: 4.9651 - val_acc: 0.4062\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9764\n",
      "Epoch 00046: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0841 - acc: 0.9764 - val_loss: 4.9090 - val_acc: 0.4130\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9762\n",
      "Epoch 00047: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0828 - acc: 0.9763 - val_loss: 4.9855 - val_acc: 0.4055\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9798\n",
      "Epoch 00048: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0728 - acc: 0.9798 - val_loss: 4.9605 - val_acc: 0.4151\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9775\n",
      "Epoch 00049: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0773 - acc: 0.9775 - val_loss: 4.9410 - val_acc: 0.4177\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9792\n",
      "Epoch 00050: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0764 - acc: 0.9792 - val_loss: 5.0749 - val_acc: 0.4114\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9793\n",
      "Epoch 00051: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0742 - acc: 0.9793 - val_loss: 4.9661 - val_acc: 0.4153\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9786\n",
      "Epoch 00052: val_loss did not improve from 2.05232\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0777 - acc: 0.9786 - val_loss: 5.1019 - val_acc: 0.4083\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 789us/sample - loss: 2.0918 - acc: 0.3421\n",
      "Loss: 2.09183182830256 Accuracy: 0.34205607\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9764 - acc: 0.3709\n",
      "Epoch 00001: val_loss improved from inf to 1.61441, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv_checkpoint/001-1.6144.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.9764 - acc: 0.3709 - val_loss: 1.6144 - val_acc: 0.4936\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4233 - acc: 0.5567\n",
      "Epoch 00002: val_loss improved from 1.61441 to 1.51607, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv_checkpoint/002-1.5161.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.4234 - acc: 0.5567 - val_loss: 1.5161 - val_acc: 0.5276\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1772 - acc: 0.6362\n",
      "Epoch 00003: val_loss improved from 1.51607 to 1.48784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv_checkpoint/003-1.4878.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 1.1773 - acc: 0.6362 - val_loss: 1.4878 - val_acc: 0.5462\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9896 - acc: 0.6972\n",
      "Epoch 00004: val_loss improved from 1.48784 to 1.45825, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv_checkpoint/004-1.4583.hdf5\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.9896 - acc: 0.6972 - val_loss: 1.4583 - val_acc: 0.5553\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8383 - acc: 0.7434\n",
      "Epoch 00005: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.8382 - acc: 0.7434 - val_loss: 1.4836 - val_acc: 0.5649\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7874\n",
      "Epoch 00006: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.6984 - acc: 0.7875 - val_loss: 1.5138 - val_acc: 0.5635\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5877 - acc: 0.8242\n",
      "Epoch 00007: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.5876 - acc: 0.8242 - val_loss: 1.5222 - val_acc: 0.5786\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8546\n",
      "Epoch 00008: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4897 - acc: 0.8547 - val_loss: 1.5898 - val_acc: 0.5754\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8789\n",
      "Epoch 00009: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.4115 - acc: 0.8789 - val_loss: 1.6181 - val_acc: 0.5775\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8989\n",
      "Epoch 00010: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.3511 - acc: 0.8989 - val_loss: 1.6575 - val_acc: 0.5749\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.9191\n",
      "Epoch 00011: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2959 - acc: 0.9191 - val_loss: 1.6399 - val_acc: 0.5893\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9292\n",
      "Epoch 00012: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2565 - acc: 0.9292 - val_loss: 1.7425 - val_acc: 0.5844\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9379\n",
      "Epoch 00013: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2250 - acc: 0.9379 - val_loss: 1.7109 - val_acc: 0.5947\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9494\n",
      "Epoch 00014: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1956 - acc: 0.9494 - val_loss: 1.7752 - val_acc: 0.5961\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9551\n",
      "Epoch 00015: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1722 - acc: 0.9551 - val_loss: 1.8202 - val_acc: 0.5912\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9568\n",
      "Epoch 00016: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1599 - acc: 0.9568 - val_loss: 1.8562 - val_acc: 0.6007\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9620\n",
      "Epoch 00017: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1470 - acc: 0.9620 - val_loss: 1.8768 - val_acc: 0.5982\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9665\n",
      "Epoch 00018: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1307 - acc: 0.9666 - val_loss: 1.9179 - val_acc: 0.5921\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9711\n",
      "Epoch 00019: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1158 - acc: 0.9711 - val_loss: 1.9127 - val_acc: 0.6005\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9715\n",
      "Epoch 00020: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1105 - acc: 0.9715 - val_loss: 1.9039 - val_acc: 0.6019\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9744\n",
      "Epoch 00021: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1070 - acc: 0.9744 - val_loss: 1.9770 - val_acc: 0.5970\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9733\n",
      "Epoch 00022: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1044 - acc: 0.9733 - val_loss: 2.0228 - val_acc: 0.5996\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9776\n",
      "Epoch 00023: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0913 - acc: 0.9776 - val_loss: 2.0107 - val_acc: 0.6068\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9792\n",
      "Epoch 00024: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0871 - acc: 0.9792 - val_loss: 2.0701 - val_acc: 0.6089\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9787\n",
      "Epoch 00025: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0836 - acc: 0.9787 - val_loss: 2.0286 - val_acc: 0.6138\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9789\n",
      "Epoch 00026: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0833 - acc: 0.9789 - val_loss: 2.0828 - val_acc: 0.6080\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9812\n",
      "Epoch 00027: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0765 - acc: 0.9812 - val_loss: 2.1388 - val_acc: 0.6101\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9829\n",
      "Epoch 00028: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0715 - acc: 0.9829 - val_loss: 2.1167 - val_acc: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9823\n",
      "Epoch 00029: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0702 - acc: 0.9823 - val_loss: 2.1381 - val_acc: 0.6115\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9839\n",
      "Epoch 00030: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0652 - acc: 0.9839 - val_loss: 2.1402 - val_acc: 0.6166\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9832\n",
      "Epoch 00031: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0667 - acc: 0.9832 - val_loss: 2.1857 - val_acc: 0.6112\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9835\n",
      "Epoch 00032: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0657 - acc: 0.9835 - val_loss: 2.2296 - val_acc: 0.6131\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9837\n",
      "Epoch 00033: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0647 - acc: 0.9837 - val_loss: 2.1958 - val_acc: 0.6184\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9861\n",
      "Epoch 00034: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0568 - acc: 0.9861 - val_loss: 2.2416 - val_acc: 0.6136\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9835\n",
      "Epoch 00035: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0639 - acc: 0.9835 - val_loss: 2.2692 - val_acc: 0.6154\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9865\n",
      "Epoch 00036: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0537 - acc: 0.9865 - val_loss: 2.2685 - val_acc: 0.6173\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9865\n",
      "Epoch 00037: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0544 - acc: 0.9866 - val_loss: 2.2623 - val_acc: 0.6140\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9845\n",
      "Epoch 00038: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0606 - acc: 0.9845 - val_loss: 2.3010 - val_acc: 0.6180\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9834\n",
      "Epoch 00039: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0631 - acc: 0.9834 - val_loss: 2.3142 - val_acc: 0.6173\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9870\n",
      "Epoch 00040: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0553 - acc: 0.9870 - val_loss: 2.2912 - val_acc: 0.6254\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9861\n",
      "Epoch 00041: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0538 - acc: 0.9861 - val_loss: 2.3056 - val_acc: 0.6224\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9865\n",
      "Epoch 00042: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0532 - acc: 0.9865 - val_loss: 2.2922 - val_acc: 0.6233\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9884\n",
      "Epoch 00043: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0479 - acc: 0.9884 - val_loss: 2.3557 - val_acc: 0.6233\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9891\n",
      "Epoch 00044: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0468 - acc: 0.9891 - val_loss: 2.3960 - val_acc: 0.6194\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9871\n",
      "Epoch 00045: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0484 - acc: 0.9871 - val_loss: 2.4196 - val_acc: 0.6184\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9887\n",
      "Epoch 00046: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0467 - acc: 0.9887 - val_loss: 2.4572 - val_acc: 0.6224\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9882\n",
      "Epoch 00047: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0471 - acc: 0.9882 - val_loss: 2.4166 - val_acc: 0.6243\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9883\n",
      "Epoch 00048: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0467 - acc: 0.9883 - val_loss: 2.4338 - val_acc: 0.6191\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9891\n",
      "Epoch 00049: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.0454 - acc: 0.9891 - val_loss: 2.4361 - val_acc: 0.6196\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0444 - acc: 0.9887 - val_loss: 2.4530 - val_acc: 0.6257\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9891\n",
      "Epoch 00051: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0440 - acc: 0.9891 - val_loss: 2.4320 - val_acc: 0.6212\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9887\n",
      "Epoch 00052: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0440 - acc: 0.9887 - val_loss: 2.4412 - val_acc: 0.6219\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9895\n",
      "Epoch 00053: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0439 - acc: 0.9895 - val_loss: 2.4282 - val_acc: 0.6264\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9879\n",
      "Epoch 00054: val_loss did not improve from 1.45825\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0474 - acc: 0.9879 - val_loss: 2.4834 - val_acc: 0.6173\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 906us/sample - loss: 1.5243 - acc: 0.5333\n",
      "Loss: 1.524274631452709 Accuracy: 0.53333336\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8320 - acc: 0.4102\n",
      "Epoch 00001: val_loss improved from inf to 1.45530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/001-1.4553.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.8320 - acc: 0.4101 - val_loss: 1.4553 - val_acc: 0.5553\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3176 - acc: 0.5951\n",
      "Epoch 00002: val_loss improved from 1.45530 to 1.23744, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/002-1.2374.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.3176 - acc: 0.5951 - val_loss: 1.2374 - val_acc: 0.6233\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6554\n",
      "Epoch 00003: val_loss improved from 1.23744 to 1.16333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/003-1.1633.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1347 - acc: 0.6554 - val_loss: 1.1633 - val_acc: 0.6508\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0144 - acc: 0.6944\n",
      "Epoch 00004: val_loss improved from 1.16333 to 1.10993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/004-1.1099.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0144 - acc: 0.6944 - val_loss: 1.1099 - val_acc: 0.6730\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9114 - acc: 0.7290\n",
      "Epoch 00005: val_loss improved from 1.10993 to 1.07812, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/005-1.0781.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9114 - acc: 0.7290 - val_loss: 1.0781 - val_acc: 0.6776\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.7555\n",
      "Epoch 00006: val_loss improved from 1.07812 to 1.05432, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/006-1.0543.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8271 - acc: 0.7555 - val_loss: 1.0543 - val_acc: 0.6909\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7620 - acc: 0.7746\n",
      "Epoch 00007: val_loss improved from 1.05432 to 1.05099, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/007-1.0510.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7620 - acc: 0.7746 - val_loss: 1.0510 - val_acc: 0.6928\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.7977\n",
      "Epoch 00008: val_loss improved from 1.05099 to 1.03765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/008-1.0376.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6942 - acc: 0.7977 - val_loss: 1.0376 - val_acc: 0.6979\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6353 - acc: 0.8152\n",
      "Epoch 00009: val_loss did not improve from 1.03765\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6353 - acc: 0.8152 - val_loss: 1.0446 - val_acc: 0.7030\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5859 - acc: 0.8305\n",
      "Epoch 00010: val_loss improved from 1.03765 to 1.01536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv_checkpoint/010-1.0154.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5861 - acc: 0.8305 - val_loss: 1.0154 - val_acc: 0.7121\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8444\n",
      "Epoch 00011: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5379 - acc: 0.8444 - val_loss: 1.0226 - val_acc: 0.7107\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8582\n",
      "Epoch 00012: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4934 - acc: 0.8582 - val_loss: 1.0186 - val_acc: 0.7123\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8690\n",
      "Epoch 00013: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4550 - acc: 0.8690 - val_loss: 1.0555 - val_acc: 0.7130\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8815\n",
      "Epoch 00014: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4175 - acc: 0.8815 - val_loss: 1.0663 - val_acc: 0.7035\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8907\n",
      "Epoch 00015: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3859 - acc: 0.8907 - val_loss: 1.0409 - val_acc: 0.7147\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8993\n",
      "Epoch 00016: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3549 - acc: 0.8993 - val_loss: 1.0816 - val_acc: 0.7121\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.9076\n",
      "Epoch 00017: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3295 - acc: 0.9075 - val_loss: 1.0876 - val_acc: 0.7114\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9126\n",
      "Epoch 00018: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3097 - acc: 0.9126 - val_loss: 1.0618 - val_acc: 0.7179\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9204\n",
      "Epoch 00019: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2839 - acc: 0.9204 - val_loss: 1.0753 - val_acc: 0.7212\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9264\n",
      "Epoch 00020: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2639 - acc: 0.9264 - val_loss: 1.0686 - val_acc: 0.7198\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9325\n",
      "Epoch 00021: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2469 - acc: 0.9325 - val_loss: 1.0760 - val_acc: 0.7242\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9360\n",
      "Epoch 00022: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2304 - acc: 0.9360 - val_loss: 1.0938 - val_acc: 0.7214\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9404\n",
      "Epoch 00023: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2179 - acc: 0.9404 - val_loss: 1.1121 - val_acc: 0.7214\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9451\n",
      "Epoch 00024: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2011 - acc: 0.9451 - val_loss: 1.1046 - val_acc: 0.7233\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9497\n",
      "Epoch 00025: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1886 - acc: 0.9497 - val_loss: 1.1152 - val_acc: 0.7289\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9495\n",
      "Epoch 00026: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1808 - acc: 0.9495 - val_loss: 1.1257 - val_acc: 0.7249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9541\n",
      "Epoch 00027: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1712 - acc: 0.9541 - val_loss: 1.1160 - val_acc: 0.7314\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9576\n",
      "Epoch 00028: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1583 - acc: 0.9576 - val_loss: 1.1445 - val_acc: 0.7317\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9608\n",
      "Epoch 00029: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1495 - acc: 0.9608 - val_loss: 1.1455 - val_acc: 0.7361\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9609\n",
      "Epoch 00030: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1505 - acc: 0.9609 - val_loss: 1.1534 - val_acc: 0.7379\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9646\n",
      "Epoch 00031: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1365 - acc: 0.9647 - val_loss: 1.1655 - val_acc: 0.7386\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9665\n",
      "Epoch 00032: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1301 - acc: 0.9665 - val_loss: 1.1781 - val_acc: 0.7338\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9670\n",
      "Epoch 00033: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1265 - acc: 0.9670 - val_loss: 1.1915 - val_acc: 0.7375\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9666\n",
      "Epoch 00034: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1241 - acc: 0.9666 - val_loss: 1.1691 - val_acc: 0.7377\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9712\n",
      "Epoch 00035: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1102 - acc: 0.9712 - val_loss: 1.2114 - val_acc: 0.7382\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9729\n",
      "Epoch 00036: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1088 - acc: 0.9729 - val_loss: 1.2029 - val_acc: 0.7326\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9716\n",
      "Epoch 00037: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1075 - acc: 0.9716 - val_loss: 1.2438 - val_acc: 0.7296\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9732\n",
      "Epoch 00038: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1011 - acc: 0.9732 - val_loss: 1.2069 - val_acc: 0.7449\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9749\n",
      "Epoch 00039: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0999 - acc: 0.9749 - val_loss: 1.2211 - val_acc: 0.7368\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9743\n",
      "Epoch 00040: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0973 - acc: 0.9743 - val_loss: 1.2355 - val_acc: 0.7391\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9761\n",
      "Epoch 00041: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0920 - acc: 0.9761 - val_loss: 1.2409 - val_acc: 0.7389\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9762\n",
      "Epoch 00042: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0925 - acc: 0.9761 - val_loss: 1.2404 - val_acc: 0.7361\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9739\n",
      "Epoch 00043: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1007 - acc: 0.9739 - val_loss: 1.2479 - val_acc: 0.7391\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9791\n",
      "Epoch 00044: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0839 - acc: 0.9791 - val_loss: 1.2489 - val_acc: 0.7384\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9782\n",
      "Epoch 00045: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0827 - acc: 0.9782 - val_loss: 1.2810 - val_acc: 0.7372\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9775\n",
      "Epoch 00046: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0825 - acc: 0.9775 - val_loss: 1.2466 - val_acc: 0.7372\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9801\n",
      "Epoch 00047: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0788 - acc: 0.9801 - val_loss: 1.2452 - val_acc: 0.7396\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9796\n",
      "Epoch 00048: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0789 - acc: 0.9796 - val_loss: 1.2464 - val_acc: 0.7438\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9811\n",
      "Epoch 00049: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0756 - acc: 0.9811 - val_loss: 1.2640 - val_acc: 0.7410\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9810\n",
      "Epoch 00050: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0749 - acc: 0.9810 - val_loss: 1.2735 - val_acc: 0.7424\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9817\n",
      "Epoch 00051: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0713 - acc: 0.9817 - val_loss: 1.2961 - val_acc: 0.7445\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9808\n",
      "Epoch 00052: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0725 - acc: 0.9808 - val_loss: 1.2909 - val_acc: 0.7417\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9828\n",
      "Epoch 00053: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0672 - acc: 0.9828 - val_loss: 1.3157 - val_acc: 0.7389\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9814\n",
      "Epoch 00054: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0696 - acc: 0.9814 - val_loss: 1.2916 - val_acc: 0.7463\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9817\n",
      "Epoch 00055: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0681 - acc: 0.9817 - val_loss: 1.3376 - val_acc: 0.7447\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9807\n",
      "Epoch 00056: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0714 - acc: 0.9807 - val_loss: 1.2771 - val_acc: 0.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9840\n",
      "Epoch 00057: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0642 - acc: 0.9840 - val_loss: 1.2831 - val_acc: 0.7452\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9826\n",
      "Epoch 00058: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0658 - acc: 0.9826 - val_loss: 1.3115 - val_acc: 0.7491\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9842\n",
      "Epoch 00059: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0598 - acc: 0.9842 - val_loss: 1.3023 - val_acc: 0.7470\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9847\n",
      "Epoch 00060: val_loss did not improve from 1.01536\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0597 - acc: 0.9846 - val_loss: 1.3275 - val_acc: 0.7459\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 963us/sample - loss: 1.0789 - acc: 0.6908\n",
      "Loss: 1.0788706494764129 Accuracy: 0.69075805\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7890 - acc: 0.4242\n",
      "Epoch 00001: val_loss improved from inf to 1.42957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/001-1.4296.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.7891 - acc: 0.4242 - val_loss: 1.4296 - val_acc: 0.5658\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3650 - acc: 0.5788\n",
      "Epoch 00002: val_loss improved from 1.42957 to 1.24471, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/002-1.2447.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3650 - acc: 0.5788 - val_loss: 1.2447 - val_acc: 0.6282\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1713 - acc: 0.6445\n",
      "Epoch 00003: val_loss improved from 1.24471 to 1.08121, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/003-1.0812.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1714 - acc: 0.6445 - val_loss: 1.0812 - val_acc: 0.6748\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0552 - acc: 0.6853\n",
      "Epoch 00004: val_loss improved from 1.08121 to 0.99998, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/004-1.0000.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0551 - acc: 0.6853 - val_loss: 1.0000 - val_acc: 0.7042\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.7148\n",
      "Epoch 00005: val_loss improved from 0.99998 to 0.94981, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/005-0.9498.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9633 - acc: 0.7148 - val_loss: 0.9498 - val_acc: 0.7144\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8953 - acc: 0.7354\n",
      "Epoch 00006: val_loss improved from 0.94981 to 0.90493, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/006-0.9049.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8952 - acc: 0.7354 - val_loss: 0.9049 - val_acc: 0.7391\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8353 - acc: 0.7539\n",
      "Epoch 00007: val_loss improved from 0.90493 to 0.86916, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/007-0.8692.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8353 - acc: 0.7539 - val_loss: 0.8692 - val_acc: 0.7543\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7923 - acc: 0.7696\n",
      "Epoch 00008: val_loss improved from 0.86916 to 0.84680, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/008-0.8468.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7923 - acc: 0.7695 - val_loss: 0.8468 - val_acc: 0.7533\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7342 - acc: 0.7837\n",
      "Epoch 00009: val_loss improved from 0.84680 to 0.83261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/009-0.8326.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7342 - acc: 0.7837 - val_loss: 0.8326 - val_acc: 0.7580\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.7965\n",
      "Epoch 00010: val_loss improved from 0.83261 to 0.80963, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/010-0.8096.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6876 - acc: 0.7965 - val_loss: 0.8096 - val_acc: 0.7706\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.8096\n",
      "Epoch 00011: val_loss did not improve from 0.80963\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6525 - acc: 0.8096 - val_loss: 0.8107 - val_acc: 0.7680\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.8198\n",
      "Epoch 00012: val_loss improved from 0.80963 to 0.78106, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/012-0.7811.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6102 - acc: 0.8198 - val_loss: 0.7811 - val_acc: 0.7727\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5750 - acc: 0.8325\n",
      "Epoch 00013: val_loss did not improve from 0.78106\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5750 - acc: 0.8325 - val_loss: 0.8015 - val_acc: 0.7657\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5403 - acc: 0.8422\n",
      "Epoch 00014: val_loss improved from 0.78106 to 0.77695, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/014-0.7769.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5403 - acc: 0.8422 - val_loss: 0.7769 - val_acc: 0.7836\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8522\n",
      "Epoch 00015: val_loss did not improve from 0.77695\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5075 - acc: 0.8522 - val_loss: 0.7889 - val_acc: 0.7789\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.8599\n",
      "Epoch 00016: val_loss did not improve from 0.77695\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4782 - acc: 0.8599 - val_loss: 0.7961 - val_acc: 0.7843\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8664\n",
      "Epoch 00017: val_loss improved from 0.77695 to 0.75515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/017-0.7551.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4564 - acc: 0.8664 - val_loss: 0.7551 - val_acc: 0.7892\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.8770\n",
      "Epoch 00018: val_loss did not improve from 0.75515\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4256 - acc: 0.8770 - val_loss: 0.7922 - val_acc: 0.7743\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8822\n",
      "Epoch 00019: val_loss improved from 0.75515 to 0.75141, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/019-0.7514.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4022 - acc: 0.8822 - val_loss: 0.7514 - val_acc: 0.7845\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8891\n",
      "Epoch 00020: val_loss improved from 0.75141 to 0.74633, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv_checkpoint/020-0.7463.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3794 - acc: 0.8891 - val_loss: 0.7463 - val_acc: 0.7969\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8907\n",
      "Epoch 00021: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3866 - acc: 0.8906 - val_loss: 0.7562 - val_acc: 0.7920\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.9015\n",
      "Epoch 00022: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3403 - acc: 0.9015 - val_loss: 0.7611 - val_acc: 0.7906\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9094\n",
      "Epoch 00023: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3178 - acc: 0.9094 - val_loss: 0.7594 - val_acc: 0.7892\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9158\n",
      "Epoch 00024: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2991 - acc: 0.9157 - val_loss: 0.7538 - val_acc: 0.7929\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9169\n",
      "Epoch 00025: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2915 - acc: 0.9169 - val_loss: 0.7960 - val_acc: 0.7836\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9214\n",
      "Epoch 00026: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2723 - acc: 0.9214 - val_loss: 0.7781 - val_acc: 0.7950\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9267\n",
      "Epoch 00027: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2589 - acc: 0.9267 - val_loss: 0.7746 - val_acc: 0.7887\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9302\n",
      "Epoch 00028: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2435 - acc: 0.9302 - val_loss: 0.7775 - val_acc: 0.7952\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9356\n",
      "Epoch 00029: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2293 - acc: 0.9356 - val_loss: 0.7744 - val_acc: 0.7976\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9371\n",
      "Epoch 00030: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2184 - acc: 0.9371 - val_loss: 0.8001 - val_acc: 0.7859\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9401\n",
      "Epoch 00031: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2090 - acc: 0.9401 - val_loss: 0.7807 - val_acc: 0.7925\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9441\n",
      "Epoch 00032: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1971 - acc: 0.9441 - val_loss: 0.7966 - val_acc: 0.8008\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9461\n",
      "Epoch 00033: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1866 - acc: 0.9461 - val_loss: 0.7906 - val_acc: 0.7971\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9474\n",
      "Epoch 00034: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1834 - acc: 0.9474 - val_loss: 0.7834 - val_acc: 0.8013\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9519\n",
      "Epoch 00035: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1726 - acc: 0.9519 - val_loss: 0.7838 - val_acc: 0.8020\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9580\n",
      "Epoch 00036: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1591 - acc: 0.9580 - val_loss: 0.7923 - val_acc: 0.7997\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9567\n",
      "Epoch 00037: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1532 - acc: 0.9567 - val_loss: 0.8262 - val_acc: 0.7976\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9571\n",
      "Epoch 00038: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1508 - acc: 0.9571 - val_loss: 0.8272 - val_acc: 0.7983\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9579\n",
      "Epoch 00039: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1491 - acc: 0.9579 - val_loss: 0.8055 - val_acc: 0.7980\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9618\n",
      "Epoch 00040: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1375 - acc: 0.9618 - val_loss: 0.8556 - val_acc: 0.7939\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9632\n",
      "Epoch 00041: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1335 - acc: 0.9632 - val_loss: 0.8328 - val_acc: 0.7955\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9667\n",
      "Epoch 00042: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1233 - acc: 0.9667 - val_loss: 0.8530 - val_acc: 0.7918\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9663\n",
      "Epoch 00043: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1249 - acc: 0.9663 - val_loss: 0.8241 - val_acc: 0.8069\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9686\n",
      "Epoch 00044: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1182 - acc: 0.9686 - val_loss: 0.8463 - val_acc: 0.7959\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9676\n",
      "Epoch 00045: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1179 - acc: 0.9676 - val_loss: 0.8449 - val_acc: 0.7952\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9686\n",
      "Epoch 00046: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1155 - acc: 0.9686 - val_loss: 0.8499 - val_acc: 0.8032\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9703\n",
      "Epoch 00047: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1062 - acc: 0.9703 - val_loss: 0.8432 - val_acc: 0.8046\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9725\n",
      "Epoch 00048: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1018 - acc: 0.9725 - val_loss: 0.8570 - val_acc: 0.8029\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9726\n",
      "Epoch 00049: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1026 - acc: 0.9726 - val_loss: 0.8685 - val_acc: 0.7966\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9733\n",
      "Epoch 00050: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1001 - acc: 0.9733 - val_loss: 0.8681 - val_acc: 0.8006\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9736\n",
      "Epoch 00051: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0972 - acc: 0.9736 - val_loss: 0.8386 - val_acc: 0.8018\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9750\n",
      "Epoch 00052: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0930 - acc: 0.9750 - val_loss: 0.8889 - val_acc: 0.7962\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9751\n",
      "Epoch 00053: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0918 - acc: 0.9751 - val_loss: 0.8828 - val_acc: 0.8008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9750\n",
      "Epoch 00054: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0901 - acc: 0.9750 - val_loss: 0.9134 - val_acc: 0.7941\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9771\n",
      "Epoch 00055: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0856 - acc: 0.9771 - val_loss: 0.8820 - val_acc: 0.8095\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9751\n",
      "Epoch 00056: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0890 - acc: 0.9751 - val_loss: 0.8931 - val_acc: 0.7957\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9770\n",
      "Epoch 00057: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0837 - acc: 0.9770 - val_loss: 0.9245 - val_acc: 0.8025\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9766\n",
      "Epoch 00058: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0861 - acc: 0.9766 - val_loss: 0.9054 - val_acc: 0.8006\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9787\n",
      "Epoch 00059: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0785 - acc: 0.9787 - val_loss: 0.9354 - val_acc: 0.7948\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9785\n",
      "Epoch 00060: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0780 - acc: 0.9785 - val_loss: 0.9423 - val_acc: 0.7994\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9779\n",
      "Epoch 00061: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0814 - acc: 0.9779 - val_loss: 0.9531 - val_acc: 0.7913\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9805\n",
      "Epoch 00062: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0722 - acc: 0.9805 - val_loss: 0.9177 - val_acc: 0.8027\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9785\n",
      "Epoch 00063: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0779 - acc: 0.9785 - val_loss: 0.9032 - val_acc: 0.8032\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9796\n",
      "Epoch 00064: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0750 - acc: 0.9796 - val_loss: 0.9202 - val_acc: 0.8013\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9805\n",
      "Epoch 00065: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0733 - acc: 0.9805 - val_loss: 0.9248 - val_acc: 0.7994\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9814\n",
      "Epoch 00066: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0681 - acc: 0.9814 - val_loss: 0.9081 - val_acc: 0.8036\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9798\n",
      "Epoch 00067: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0720 - acc: 0.9798 - val_loss: 0.9061 - val_acc: 0.8083\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9813\n",
      "Epoch 00068: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0685 - acc: 0.9813 - val_loss: 0.9708 - val_acc: 0.8006\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9807\n",
      "Epoch 00069: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0690 - acc: 0.9807 - val_loss: 0.9501 - val_acc: 0.8036\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9833\n",
      "Epoch 00070: val_loss did not improve from 0.74633\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0635 - acc: 0.9833 - val_loss: 0.9186 - val_acc: 0.8046\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8261 - acc: 0.7659\n",
      "Loss: 0.8260591631985403 Accuracy: 0.7659398\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8535 - acc: 0.3998\n",
      "Epoch 00001: val_loss improved from inf to 1.45381, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/001-1.4538.hdf5\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 1.8534 - acc: 0.3998 - val_loss: 1.4538 - val_acc: 0.5595\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4256 - acc: 0.5573\n",
      "Epoch 00002: val_loss improved from 1.45381 to 1.19158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/002-1.1916.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4256 - acc: 0.5573 - val_loss: 1.1916 - val_acc: 0.6415\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2061 - acc: 0.6349\n",
      "Epoch 00003: val_loss improved from 1.19158 to 1.02672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/003-1.0267.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.2060 - acc: 0.6349 - val_loss: 1.0267 - val_acc: 0.7002\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0800 - acc: 0.6779\n",
      "Epoch 00004: val_loss improved from 1.02672 to 0.93084, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/004-0.9308.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0800 - acc: 0.6779 - val_loss: 0.9308 - val_acc: 0.7340\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9783 - acc: 0.7103\n",
      "Epoch 00005: val_loss improved from 0.93084 to 0.88693, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/005-0.8869.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9782 - acc: 0.7104 - val_loss: 0.8869 - val_acc: 0.7375\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9119 - acc: 0.7308\n",
      "Epoch 00006: val_loss improved from 0.88693 to 0.81156, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/006-0.8116.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9119 - acc: 0.7308 - val_loss: 0.8116 - val_acc: 0.7610\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8481 - acc: 0.7510\n",
      "Epoch 00007: val_loss improved from 0.81156 to 0.75720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/007-0.7572.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8482 - acc: 0.7510 - val_loss: 0.7572 - val_acc: 0.7803\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7659\n",
      "Epoch 00008: val_loss improved from 0.75720 to 0.71714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/008-0.7171.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7931 - acc: 0.7659 - val_loss: 0.7171 - val_acc: 0.7894\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7811\n",
      "Epoch 00009: val_loss improved from 0.71714 to 0.69277, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/009-0.6928.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7504 - acc: 0.7811 - val_loss: 0.6928 - val_acc: 0.8013\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7934\n",
      "Epoch 00010: val_loss improved from 0.69277 to 0.64252, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/010-0.6425.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7072 - acc: 0.7934 - val_loss: 0.6425 - val_acc: 0.8157\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.8041\n",
      "Epoch 00011: val_loss improved from 0.64252 to 0.63614, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/011-0.6361.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6756 - acc: 0.8040 - val_loss: 0.6361 - val_acc: 0.8220\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6484 - acc: 0.8108\n",
      "Epoch 00012: val_loss improved from 0.63614 to 0.63161, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/012-0.6316.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6484 - acc: 0.8108 - val_loss: 0.6316 - val_acc: 0.8239\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6178 - acc: 0.8204\n",
      "Epoch 00013: val_loss improved from 0.63161 to 0.59291, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/013-0.5929.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6180 - acc: 0.8204 - val_loss: 0.5929 - val_acc: 0.8314\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5938 - acc: 0.8284\n",
      "Epoch 00014: val_loss improved from 0.59291 to 0.58319, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/014-0.5832.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5937 - acc: 0.8284 - val_loss: 0.5832 - val_acc: 0.8348\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8343\n",
      "Epoch 00015: val_loss improved from 0.58319 to 0.56967, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/015-0.5697.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5680 - acc: 0.8343 - val_loss: 0.5697 - val_acc: 0.8442\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8423\n",
      "Epoch 00016: val_loss did not improve from 0.56967\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5450 - acc: 0.8423 - val_loss: 0.5832 - val_acc: 0.8381\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8459\n",
      "Epoch 00017: val_loss improved from 0.56967 to 0.53209, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/017-0.5321.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5278 - acc: 0.8459 - val_loss: 0.5321 - val_acc: 0.8530\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8552\n",
      "Epoch 00018: val_loss did not improve from 0.53209\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5059 - acc: 0.8552 - val_loss: 0.5372 - val_acc: 0.8467\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8577\n",
      "Epoch 00019: val_loss improved from 0.53209 to 0.51592, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/019-0.5159.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4905 - acc: 0.8577 - val_loss: 0.5159 - val_acc: 0.8556\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4690 - acc: 0.8627\n",
      "Epoch 00020: val_loss did not improve from 0.51592\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4689 - acc: 0.8627 - val_loss: 0.5446 - val_acc: 0.8451\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.8679\n",
      "Epoch 00021: val_loss improved from 0.51592 to 0.51531, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/021-0.5153.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4560 - acc: 0.8679 - val_loss: 0.5153 - val_acc: 0.8535\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8726\n",
      "Epoch 00022: val_loss improved from 0.51531 to 0.49231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/022-0.4923.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4404 - acc: 0.8726 - val_loss: 0.4923 - val_acc: 0.8670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4320 - acc: 0.8745\n",
      "Epoch 00023: val_loss did not improve from 0.49231\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4319 - acc: 0.8745 - val_loss: 0.5135 - val_acc: 0.8570\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8803\n",
      "Epoch 00024: val_loss improved from 0.49231 to 0.48090, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/024-0.4809.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4124 - acc: 0.8803 - val_loss: 0.4809 - val_acc: 0.8675\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8842\n",
      "Epoch 00025: val_loss improved from 0.48090 to 0.47704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/025-0.4770.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3999 - acc: 0.8842 - val_loss: 0.4770 - val_acc: 0.8693\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8872\n",
      "Epoch 00026: val_loss did not improve from 0.47704\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3838 - acc: 0.8872 - val_loss: 0.5071 - val_acc: 0.8640\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8905\n",
      "Epoch 00027: val_loss did not improve from 0.47704\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3784 - acc: 0.8904 - val_loss: 0.4910 - val_acc: 0.8598\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8936\n",
      "Epoch 00028: val_loss improved from 0.47704 to 0.46060, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/028-0.4606.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3653 - acc: 0.8936 - val_loss: 0.4606 - val_acc: 0.8703\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8954\n",
      "Epoch 00029: val_loss improved from 0.46060 to 0.45878, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/029-0.4588.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3559 - acc: 0.8954 - val_loss: 0.4588 - val_acc: 0.8742\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8944\n",
      "Epoch 00030: val_loss did not improve from 0.45878\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3604 - acc: 0.8944 - val_loss: 0.4602 - val_acc: 0.8768\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.9025\n",
      "Epoch 00031: val_loss did not improve from 0.45878\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3332 - acc: 0.9025 - val_loss: 0.4596 - val_acc: 0.8724\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.9026\n",
      "Epoch 00032: val_loss did not improve from 0.45878\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3278 - acc: 0.9026 - val_loss: 0.4735 - val_acc: 0.8714\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.9025\n",
      "Epoch 00033: val_loss improved from 0.45878 to 0.45142, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/033-0.4514.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3252 - acc: 0.9025 - val_loss: 0.4514 - val_acc: 0.8805\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9098\n",
      "Epoch 00034: val_loss improved from 0.45142 to 0.44798, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/034-0.4480.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3089 - acc: 0.9098 - val_loss: 0.4480 - val_acc: 0.8763\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3017 - acc: 0.9109\n",
      "Epoch 00035: val_loss improved from 0.44798 to 0.43908, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/035-0.4391.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3017 - acc: 0.9109 - val_loss: 0.4391 - val_acc: 0.8761\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9133\n",
      "Epoch 00036: val_loss did not improve from 0.43908\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2929 - acc: 0.9133 - val_loss: 0.4489 - val_acc: 0.8754\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9148\n",
      "Epoch 00037: val_loss did not improve from 0.43908\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2885 - acc: 0.9148 - val_loss: 0.4423 - val_acc: 0.8807\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9187\n",
      "Epoch 00038: val_loss did not improve from 0.43908\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2750 - acc: 0.9187 - val_loss: 0.4858 - val_acc: 0.8717\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9218\n",
      "Epoch 00039: val_loss did not improve from 0.43908\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2702 - acc: 0.9218 - val_loss: 0.4699 - val_acc: 0.8768\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9203\n",
      "Epoch 00040: val_loss improved from 0.43908 to 0.41328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/040-0.4133.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2685 - acc: 0.9203 - val_loss: 0.4133 - val_acc: 0.8835\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9245\n",
      "Epoch 00041: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2544 - acc: 0.9245 - val_loss: 0.4346 - val_acc: 0.8828\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9275\n",
      "Epoch 00042: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2471 - acc: 0.9275 - val_loss: 0.4210 - val_acc: 0.8856\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9296\n",
      "Epoch 00043: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2399 - acc: 0.9296 - val_loss: 0.4168 - val_acc: 0.8880\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9296\n",
      "Epoch 00044: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2379 - acc: 0.9297 - val_loss: 0.4206 - val_acc: 0.8884\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9296\n",
      "Epoch 00045: val_loss did not improve from 0.41328\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2341 - acc: 0.9297 - val_loss: 0.4338 - val_acc: 0.8887\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9321\n",
      "Epoch 00046: val_loss improved from 0.41328 to 0.40688, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/046-0.4069.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2260 - acc: 0.9320 - val_loss: 0.4069 - val_acc: 0.8926\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9355\n",
      "Epoch 00047: val_loss did not improve from 0.40688\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2172 - acc: 0.9354 - val_loss: 0.4090 - val_acc: 0.8894\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9379\n",
      "Epoch 00048: val_loss did not improve from 0.40688\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2086 - acc: 0.9379 - val_loss: 0.4315 - val_acc: 0.8877\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9400\n",
      "Epoch 00049: val_loss did not improve from 0.40688\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2066 - acc: 0.9400 - val_loss: 0.4192 - val_acc: 0.8910\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9398\n",
      "Epoch 00050: val_loss improved from 0.40688 to 0.40326, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/050-0.4033.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2013 - acc: 0.9398 - val_loss: 0.4033 - val_acc: 0.8947\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9395\n",
      "Epoch 00051: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2002 - acc: 0.9395 - val_loss: 0.4137 - val_acc: 0.8884\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9411\n",
      "Epoch 00052: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1947 - acc: 0.9411 - val_loss: 0.4163 - val_acc: 0.8901\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9433\n",
      "Epoch 00053: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1909 - acc: 0.9433 - val_loss: 0.4173 - val_acc: 0.8870\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9448\n",
      "Epoch 00054: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1824 - acc: 0.9448 - val_loss: 0.4113 - val_acc: 0.8901\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9476\n",
      "Epoch 00055: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1788 - acc: 0.9475 - val_loss: 0.4145 - val_acc: 0.8935\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9472\n",
      "Epoch 00056: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1772 - acc: 0.9472 - val_loss: 0.4351 - val_acc: 0.8912\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9480\n",
      "Epoch 00057: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1729 - acc: 0.9480 - val_loss: 0.4337 - val_acc: 0.8866\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9497\n",
      "Epoch 00058: val_loss did not improve from 0.40326\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1650 - acc: 0.9497 - val_loss: 0.4056 - val_acc: 0.8959\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9488\n",
      "Epoch 00059: val_loss improved from 0.40326 to 0.40107, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv_checkpoint/059-0.4011.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1658 - acc: 0.9488 - val_loss: 0.4011 - val_acc: 0.8945\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9496\n",
      "Epoch 00060: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1650 - acc: 0.9497 - val_loss: 0.4206 - val_acc: 0.8880\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9526\n",
      "Epoch 00061: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1577 - acc: 0.9526 - val_loss: 0.4101 - val_acc: 0.8968\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9574\n",
      "Epoch 00062: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1469 - acc: 0.9574 - val_loss: 0.4146 - val_acc: 0.8945\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9579\n",
      "Epoch 00063: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1448 - acc: 0.9579 - val_loss: 0.4184 - val_acc: 0.8924\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9572\n",
      "Epoch 00064: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1464 - acc: 0.9572 - val_loss: 0.4314 - val_acc: 0.8903\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9589\n",
      "Epoch 00065: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1383 - acc: 0.9589 - val_loss: 0.4342 - val_acc: 0.8915\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9592\n",
      "Epoch 00066: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1363 - acc: 0.9592 - val_loss: 0.4205 - val_acc: 0.8908\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9602\n",
      "Epoch 00067: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1348 - acc: 0.9602 - val_loss: 0.4285 - val_acc: 0.8933\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9591\n",
      "Epoch 00068: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1349 - acc: 0.9591 - val_loss: 0.4535 - val_acc: 0.8847\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9620\n",
      "Epoch 00069: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1270 - acc: 0.9620 - val_loss: 0.4367 - val_acc: 0.8849\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9618\n",
      "Epoch 00070: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1265 - acc: 0.9618 - val_loss: 0.4213 - val_acc: 0.8954\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9615\n",
      "Epoch 00071: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1251 - acc: 0.9614 - val_loss: 0.4204 - val_acc: 0.8938\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9590\n",
      "Epoch 00072: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1353 - acc: 0.9590 - val_loss: 0.4176 - val_acc: 0.8977\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9651\n",
      "Epoch 00073: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1168 - acc: 0.9651 - val_loss: 0.4234 - val_acc: 0.8952\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9651\n",
      "Epoch 00074: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1165 - acc: 0.9651 - val_loss: 0.4416 - val_acc: 0.8917\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9668\n",
      "Epoch 00075: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1136 - acc: 0.9669 - val_loss: 0.4419 - val_acc: 0.8912\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9666\n",
      "Epoch 00076: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1150 - acc: 0.9666 - val_loss: 0.4382 - val_acc: 0.8963\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9690\n",
      "Epoch 00077: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1082 - acc: 0.9690 - val_loss: 0.4269 - val_acc: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9660\n",
      "Epoch 00078: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1121 - acc: 0.9660 - val_loss: 0.4371 - val_acc: 0.8938\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9692\n",
      "Epoch 00079: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1047 - acc: 0.9692 - val_loss: 0.4291 - val_acc: 0.8987\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9678\n",
      "Epoch 00080: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1065 - acc: 0.9678 - val_loss: 0.4371 - val_acc: 0.8935\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9698\n",
      "Epoch 00081: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1021 - acc: 0.9698 - val_loss: 0.4405 - val_acc: 0.8973\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9727\n",
      "Epoch 00082: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0972 - acc: 0.9727 - val_loss: 0.4332 - val_acc: 0.8956\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9705\n",
      "Epoch 00083: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1029 - acc: 0.9705 - val_loss: 0.4139 - val_acc: 0.9019\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9703\n",
      "Epoch 00084: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1017 - acc: 0.9703 - val_loss: 0.4279 - val_acc: 0.8991\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9718\n",
      "Epoch 00085: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0954 - acc: 0.9718 - val_loss: 0.4393 - val_acc: 0.8980\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9724\n",
      "Epoch 00086: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0924 - acc: 0.9724 - val_loss: 0.4965 - val_acc: 0.8901\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9727\n",
      "Epoch 00087: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0925 - acc: 0.9727 - val_loss: 0.4651 - val_acc: 0.8875\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9727\n",
      "Epoch 00088: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0912 - acc: 0.9727 - val_loss: 0.4280 - val_acc: 0.8982\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9738\n",
      "Epoch 00089: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0898 - acc: 0.9738 - val_loss: 0.4526 - val_acc: 0.8954\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9740\n",
      "Epoch 00090: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0879 - acc: 0.9740 - val_loss: 0.4450 - val_acc: 0.8959\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9748\n",
      "Epoch 00091: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0861 - acc: 0.9748 - val_loss: 0.4444 - val_acc: 0.8938\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9745\n",
      "Epoch 00092: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0870 - acc: 0.9745 - val_loss: 0.4396 - val_acc: 0.9005\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9765\n",
      "Epoch 00093: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0821 - acc: 0.9765 - val_loss: 0.4518 - val_acc: 0.8942\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9752\n",
      "Epoch 00094: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0825 - acc: 0.9752 - val_loss: 0.4611 - val_acc: 0.8984\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9769\n",
      "Epoch 00095: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0795 - acc: 0.9769 - val_loss: 0.4350 - val_acc: 0.8961\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9762\n",
      "Epoch 00096: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0803 - acc: 0.9763 - val_loss: 0.4500 - val_acc: 0.8959\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9762\n",
      "Epoch 00097: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0801 - acc: 0.9761 - val_loss: 0.4561 - val_acc: 0.8921\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9748\n",
      "Epoch 00098: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0834 - acc: 0.9747 - val_loss: 0.4882 - val_acc: 0.8901\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9749\n",
      "Epoch 00099: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0829 - acc: 0.9749 - val_loss: 0.4681 - val_acc: 0.8931\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9788\n",
      "Epoch 00100: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0747 - acc: 0.9788 - val_loss: 0.4518 - val_acc: 0.8996\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9795\n",
      "Epoch 00101: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0716 - acc: 0.9795 - val_loss: 0.4682 - val_acc: 0.8908\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9778\n",
      "Epoch 00102: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0741 - acc: 0.9778 - val_loss: 0.4707 - val_acc: 0.8938\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9789\n",
      "Epoch 00103: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0706 - acc: 0.9789 - val_loss: 0.4456 - val_acc: 0.8987\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9769\n",
      "Epoch 00104: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0784 - acc: 0.9769 - val_loss: 0.4854 - val_acc: 0.8931\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9809\n",
      "Epoch 00105: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0675 - acc: 0.9809 - val_loss: 0.4503 - val_acc: 0.8987\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9767\n",
      "Epoch 00106: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0799 - acc: 0.9767 - val_loss: 0.4722 - val_acc: 0.8994\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9815\n",
      "Epoch 00107: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0645 - acc: 0.9815 - val_loss: 0.4396 - val_acc: 0.9001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9812\n",
      "Epoch 00108: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0674 - acc: 0.9812 - val_loss: 0.4372 - val_acc: 0.8987\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9812\n",
      "Epoch 00109: val_loss did not improve from 0.40107\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0661 - acc: 0.9812 - val_loss: 0.4549 - val_acc: 0.9001\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4952 - acc: 0.8627\n",
      "Loss: 0.4951543708085271 Accuracy: 0.86272067\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9570 - acc: 0.3617\n",
      "Epoch 00001: val_loss improved from inf to 1.48582, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/001-1.4858.hdf5\n",
      "36805/36805 [==============================] - 98s 3ms/sample - loss: 1.9569 - acc: 0.3617 - val_loss: 1.4858 - val_acc: 0.5344\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4683 - acc: 0.5340\n",
      "Epoch 00002: val_loss improved from 1.48582 to 1.18410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/002-1.1841.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4682 - acc: 0.5340 - val_loss: 1.1841 - val_acc: 0.6622\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1960 - acc: 0.6363\n",
      "Epoch 00003: val_loss improved from 1.18410 to 0.99234, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/003-0.9923.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1960 - acc: 0.6362 - val_loss: 0.9923 - val_acc: 0.7109\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0443 - acc: 0.6865\n",
      "Epoch 00004: val_loss improved from 0.99234 to 0.86807, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/004-0.8681.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0444 - acc: 0.6865 - val_loss: 0.8681 - val_acc: 0.7440\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.7225\n",
      "Epoch 00005: val_loss improved from 0.86807 to 0.78244, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/005-0.7824.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9322 - acc: 0.7224 - val_loss: 0.7824 - val_acc: 0.7734\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7509\n",
      "Epoch 00006: val_loss improved from 0.78244 to 0.68614, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/006-0.6861.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8389 - acc: 0.7510 - val_loss: 0.6861 - val_acc: 0.8067\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7764\n",
      "Epoch 00007: val_loss improved from 0.68614 to 0.61915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/007-0.6192.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7526 - acc: 0.7764 - val_loss: 0.6192 - val_acc: 0.8290\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6898 - acc: 0.7994\n",
      "Epoch 00008: val_loss improved from 0.61915 to 0.57649, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/008-0.5765.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6898 - acc: 0.7994 - val_loss: 0.5765 - val_acc: 0.8409\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6312 - acc: 0.8180\n",
      "Epoch 00009: val_loss improved from 0.57649 to 0.52645, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/009-0.5265.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6312 - acc: 0.8180 - val_loss: 0.5265 - val_acc: 0.8516\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.8293\n",
      "Epoch 00010: val_loss improved from 0.52645 to 0.49328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/010-0.4933.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5936 - acc: 0.8293 - val_loss: 0.4933 - val_acc: 0.8637\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5463 - acc: 0.8424\n",
      "Epoch 00011: val_loss improved from 0.49328 to 0.46915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/011-0.4692.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5463 - acc: 0.8424 - val_loss: 0.4692 - val_acc: 0.8635\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.8521\n",
      "Epoch 00012: val_loss improved from 0.46915 to 0.43082, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/012-0.4308.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5103 - acc: 0.8521 - val_loss: 0.4308 - val_acc: 0.8814\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.8619\n",
      "Epoch 00013: val_loss did not improve from 0.43082\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4818 - acc: 0.8619 - val_loss: 0.4364 - val_acc: 0.8840\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8679\n",
      "Epoch 00014: val_loss improved from 0.43082 to 0.38019, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/014-0.3802.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4608 - acc: 0.8679 - val_loss: 0.3802 - val_acc: 0.8989\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8739\n",
      "Epoch 00015: val_loss did not improve from 0.38019\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4368 - acc: 0.8739 - val_loss: 0.3863 - val_acc: 0.8949\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8833\n",
      "Epoch 00016: val_loss improved from 0.38019 to 0.35112, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/016-0.3511.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4087 - acc: 0.8833 - val_loss: 0.3511 - val_acc: 0.9073\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8879\n",
      "Epoch 00017: val_loss did not improve from 0.35112\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3909 - acc: 0.8878 - val_loss: 0.3558 - val_acc: 0.8991\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8928\n",
      "Epoch 00018: val_loss improved from 0.35112 to 0.33456, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/018-0.3346.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3718 - acc: 0.8927 - val_loss: 0.3346 - val_acc: 0.9085\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8959\n",
      "Epoch 00019: val_loss did not improve from 0.33456\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3606 - acc: 0.8959 - val_loss: 0.3405 - val_acc: 0.9080\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8986\n",
      "Epoch 00020: val_loss improved from 0.33456 to 0.31594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/020-0.3159.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3504 - acc: 0.8986 - val_loss: 0.3159 - val_acc: 0.9101\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.9061\n",
      "Epoch 00021: val_loss improved from 0.31594 to 0.31533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/021-0.3153.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3314 - acc: 0.9061 - val_loss: 0.3153 - val_acc: 0.9073\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9085\n",
      "Epoch 00022: val_loss improved from 0.31533 to 0.30554, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/022-0.3055.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3189 - acc: 0.9085 - val_loss: 0.3055 - val_acc: 0.9166\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9116\n",
      "Epoch 00023: val_loss improved from 0.30554 to 0.28991, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/023-0.2899.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3069 - acc: 0.9116 - val_loss: 0.2899 - val_acc: 0.9189\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9154\n",
      "Epoch 00024: val_loss did not improve from 0.28991\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2962 - acc: 0.9154 - val_loss: 0.3057 - val_acc: 0.9131\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9145\n",
      "Epoch 00025: val_loss improved from 0.28991 to 0.28044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/025-0.2804.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2927 - acc: 0.9145 - val_loss: 0.2804 - val_acc: 0.9238\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9197\n",
      "Epoch 00026: val_loss improved from 0.28044 to 0.27959, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/026-0.2796.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2812 - acc: 0.9197 - val_loss: 0.2796 - val_acc: 0.9243\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9212\n",
      "Epoch 00027: val_loss improved from 0.27959 to 0.27657, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/027-0.2766.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2716 - acc: 0.9212 - val_loss: 0.2766 - val_acc: 0.9287\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9258\n",
      "Epoch 00028: val_loss did not improve from 0.27657\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2563 - acc: 0.9258 - val_loss: 0.2912 - val_acc: 0.9245\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9249\n",
      "Epoch 00029: val_loss improved from 0.27657 to 0.24840, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/029-0.2484.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2560 - acc: 0.9250 - val_loss: 0.2484 - val_acc: 0.9336\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9277\n",
      "Epoch 00030: val_loss did not improve from 0.24840\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2454 - acc: 0.9277 - val_loss: 0.2684 - val_acc: 0.9276\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9302\n",
      "Epoch 00031: val_loss did not improve from 0.24840\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2355 - acc: 0.9302 - val_loss: 0.2565 - val_acc: 0.9299\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9322\n",
      "Epoch 00032: val_loss did not improve from 0.24840\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2322 - acc: 0.9322 - val_loss: 0.2775 - val_acc: 0.9241\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9326\n",
      "Epoch 00033: val_loss improved from 0.24840 to 0.23597, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/033-0.2360.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2319 - acc: 0.9326 - val_loss: 0.2360 - val_acc: 0.9383\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9335\n",
      "Epoch 00034: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2214 - acc: 0.9335 - val_loss: 0.2443 - val_acc: 0.9341\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9372\n",
      "Epoch 00035: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2118 - acc: 0.9372 - val_loss: 0.2551 - val_acc: 0.9304\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9387\n",
      "Epoch 00036: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2066 - acc: 0.9387 - val_loss: 0.2418 - val_acc: 0.9357\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9392\n",
      "Epoch 00037: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2044 - acc: 0.9391 - val_loss: 0.2651 - val_acc: 0.9357\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9407\n",
      "Epoch 00038: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2007 - acc: 0.9406 - val_loss: 0.2396 - val_acc: 0.9364\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9427\n",
      "Epoch 00039: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1914 - acc: 0.9427 - val_loss: 0.2504 - val_acc: 0.9320\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9441\n",
      "Epoch 00040: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1885 - acc: 0.9441 - val_loss: 0.2642 - val_acc: 0.9308\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9451\n",
      "Epoch 00041: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1847 - acc: 0.9451 - val_loss: 0.2377 - val_acc: 0.9378\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9481\n",
      "Epoch 00042: val_loss did not improve from 0.23597\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1740 - acc: 0.9481 - val_loss: 0.2616 - val_acc: 0.9327\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9455\n",
      "Epoch 00043: val_loss improved from 0.23597 to 0.23546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/043-0.2355.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1803 - acc: 0.9455 - val_loss: 0.2355 - val_acc: 0.9357\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9474\n",
      "Epoch 00044: val_loss improved from 0.23546 to 0.22881, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/044-0.2288.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1764 - acc: 0.9475 - val_loss: 0.2288 - val_acc: 0.9427\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9500\n",
      "Epoch 00045: val_loss did not improve from 0.22881\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1672 - acc: 0.9500 - val_loss: 0.2495 - val_acc: 0.9397\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9499\n",
      "Epoch 00046: val_loss did not improve from 0.22881\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1649 - acc: 0.9499 - val_loss: 0.2390 - val_acc: 0.9397\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9522\n",
      "Epoch 00047: val_loss did not improve from 0.22881\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1598 - acc: 0.9522 - val_loss: 0.2404 - val_acc: 0.9399\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9538\n",
      "Epoch 00048: val_loss did not improve from 0.22881\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1542 - acc: 0.9538 - val_loss: 0.2304 - val_acc: 0.9429\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9551\n",
      "Epoch 00049: val_loss did not improve from 0.22881\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1512 - acc: 0.9551 - val_loss: 0.2289 - val_acc: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9556\n",
      "Epoch 00050: val_loss improved from 0.22881 to 0.22030, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/050-0.2203.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1483 - acc: 0.9556 - val_loss: 0.2203 - val_acc: 0.9439\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9558\n",
      "Epoch 00051: val_loss improved from 0.22030 to 0.21515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv_checkpoint/051-0.2151.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1470 - acc: 0.9558 - val_loss: 0.2151 - val_acc: 0.9411\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9569\n",
      "Epoch 00052: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1425 - acc: 0.9569 - val_loss: 0.2512 - val_acc: 0.9392\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9596\n",
      "Epoch 00053: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1342 - acc: 0.9596 - val_loss: 0.2286 - val_acc: 0.9420\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9603\n",
      "Epoch 00054: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1317 - acc: 0.9603 - val_loss: 0.2339 - val_acc: 0.9411\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9588\n",
      "Epoch 00055: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1351 - acc: 0.9588 - val_loss: 0.2562 - val_acc: 0.9392\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9607\n",
      "Epoch 00056: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1279 - acc: 0.9607 - val_loss: 0.2554 - val_acc: 0.9397\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9606\n",
      "Epoch 00057: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1320 - acc: 0.9606 - val_loss: 0.2165 - val_acc: 0.9474\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9641\n",
      "Epoch 00058: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1188 - acc: 0.9641 - val_loss: 0.2444 - val_acc: 0.9408\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9621\n",
      "Epoch 00059: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1223 - acc: 0.9621 - val_loss: 0.2477 - val_acc: 0.9392\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9655\n",
      "Epoch 00060: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1154 - acc: 0.9655 - val_loss: 0.2504 - val_acc: 0.9401\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9658\n",
      "Epoch 00061: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1147 - acc: 0.9658 - val_loss: 0.2201 - val_acc: 0.9460\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9655\n",
      "Epoch 00062: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1142 - acc: 0.9655 - val_loss: 0.2452 - val_acc: 0.9385\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9667\n",
      "Epoch 00063: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1108 - acc: 0.9667 - val_loss: 0.2435 - val_acc: 0.9401\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9677\n",
      "Epoch 00064: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1078 - acc: 0.9677 - val_loss: 0.2263 - val_acc: 0.9432\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9664\n",
      "Epoch 00065: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1080 - acc: 0.9663 - val_loss: 0.2574 - val_acc: 0.9343\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9667\n",
      "Epoch 00066: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1097 - acc: 0.9667 - val_loss: 0.2281 - val_acc: 0.9455\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9698\n",
      "Epoch 00067: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0990 - acc: 0.9698 - val_loss: 0.2260 - val_acc: 0.9481\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9712\n",
      "Epoch 00068: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0962 - acc: 0.9712 - val_loss: 0.2263 - val_acc: 0.9434\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9706\n",
      "Epoch 00069: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0950 - acc: 0.9705 - val_loss: 0.2883 - val_acc: 0.9380\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9688\n",
      "Epoch 00070: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1022 - acc: 0.9688 - val_loss: 0.2688 - val_acc: 0.9415\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9737\n",
      "Epoch 00071: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0858 - acc: 0.9738 - val_loss: 0.2377 - val_acc: 0.9450\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9739\n",
      "Epoch 00072: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0865 - acc: 0.9739 - val_loss: 0.2403 - val_acc: 0.9408\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9720\n",
      "Epoch 00073: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0902 - acc: 0.9720 - val_loss: 0.2568 - val_acc: 0.9383\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9747\n",
      "Epoch 00074: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0841 - acc: 0.9747 - val_loss: 0.2212 - val_acc: 0.9485\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9736\n",
      "Epoch 00075: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0849 - acc: 0.9736 - val_loss: 0.2492 - val_acc: 0.9404\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9758\n",
      "Epoch 00076: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0794 - acc: 0.9758 - val_loss: 0.2253 - val_acc: 0.9481\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9759\n",
      "Epoch 00077: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0794 - acc: 0.9759 - val_loss: 0.2456 - val_acc: 0.9422\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9777\n",
      "Epoch 00078: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0748 - acc: 0.9777 - val_loss: 0.2448 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9772\n",
      "Epoch 00079: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0801 - acc: 0.9772 - val_loss: 0.2356 - val_acc: 0.9457\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9780\n",
      "Epoch 00080: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0747 - acc: 0.9780 - val_loss: 0.2340 - val_acc: 0.9460\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9784\n",
      "Epoch 00081: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0726 - acc: 0.9784 - val_loss: 0.2421 - val_acc: 0.9434\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9792\n",
      "Epoch 00082: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0700 - acc: 0.9792 - val_loss: 0.2383 - val_acc: 0.9443\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9794\n",
      "Epoch 00083: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0693 - acc: 0.9794 - val_loss: 0.2535 - val_acc: 0.9441\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9800\n",
      "Epoch 00084: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0683 - acc: 0.9800 - val_loss: 0.2564 - val_acc: 0.9429\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9793\n",
      "Epoch 00085: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0678 - acc: 0.9794 - val_loss: 0.2416 - val_acc: 0.9467\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9797\n",
      "Epoch 00086: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0659 - acc: 0.9797 - val_loss: 0.2329 - val_acc: 0.9462\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9800\n",
      "Epoch 00087: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0640 - acc: 0.9800 - val_loss: 0.2422 - val_acc: 0.9434\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9821\n",
      "Epoch 00088: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0601 - acc: 0.9821 - val_loss: 0.2478 - val_acc: 0.9434\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9819\n",
      "Epoch 00089: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0621 - acc: 0.9819 - val_loss: 0.2245 - val_acc: 0.9462\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9821\n",
      "Epoch 00090: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0588 - acc: 0.9821 - val_loss: 0.2254 - val_acc: 0.9457\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9820\n",
      "Epoch 00091: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0597 - acc: 0.9820 - val_loss: 0.2467 - val_acc: 0.9443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9835\n",
      "Epoch 00092: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0546 - acc: 0.9835 - val_loss: 0.2298 - val_acc: 0.9469\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9824\n",
      "Epoch 00093: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0580 - acc: 0.9824 - val_loss: 0.2677 - val_acc: 0.9471\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9838\n",
      "Epoch 00094: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0542 - acc: 0.9838 - val_loss: 0.2516 - val_acc: 0.9464\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9839\n",
      "Epoch 00095: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0537 - acc: 0.9839 - val_loss: 0.2729 - val_acc: 0.9378\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9830\n",
      "Epoch 00096: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0561 - acc: 0.9830 - val_loss: 0.2787 - val_acc: 0.9406\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9815\n",
      "Epoch 00097: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0602 - acc: 0.9815 - val_loss: 0.2438 - val_acc: 0.9434\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9867\n",
      "Epoch 00098: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0455 - acc: 0.9867 - val_loss: 0.2382 - val_acc: 0.9446\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9862\n",
      "Epoch 00099: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0473 - acc: 0.9862 - val_loss: 0.2592 - val_acc: 0.9448\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9846\n",
      "Epoch 00100: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0510 - acc: 0.9845 - val_loss: 0.2594 - val_acc: 0.9427\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9848\n",
      "Epoch 00101: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0506 - acc: 0.9848 - val_loss: 0.2668 - val_acc: 0.9439\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmSWZTBaSEEJC2Pd9R2kRULGIorgLVlu1rX5t1Uptbam11WqrVm1rsS4/tFZt3ReqVBQ3KIigLIKgoOxZIQlkX2d5fn+cSTKBBAJkSIDn/XrdVzJ3fWYyOc8995x7rhERlFJKqUNxtHUASimljg+aMJRSSrWIJgyllFItoglDKaVUi2jCUEop1SKaMJRSSrWIJgyllFItoglDKaVUi2jCUEop1SKutg6gNaWkpEjPnj3bOgyllDpurFmzplBEOrVk3RMqYfTs2ZPVq1e3dRhKKXXcMMbsaum6EbskZYzpZoxZbIz5yhjzpTHmlibWMcaYucaYrcaYL4wxo8OWXW2M2RKaro5UnEoppVomkjUMP/BzEVlrjIkH1hhj3heRr8LWOQfoF5pOBR4HTjXGJAN3AmMBCW37logURTBepZRSBxGxGoaI5InI2tDvZcAmIGO/1S4AnhNrJZBojEkHzgbeF5F9oSTxPjAtUrEqpZQ6tGPShmGM6QmMAj7db1EGkBX2Ojs0r7n5h83n85GdnU11dfWRbH7S83g8dO3aFbfb3dahKKXaWMQThjEmDngdmC0ipRHY//XA9QDdu3c/YHl2djbx8fH07NkTY0xrH/6EJiLs3buX7OxsevXq1dbhKKXaWETvwzDGuLHJ4nkReaOJVXKAbmGvu4bmNTf/ACIyT0TGisjYTp0O7BlWXV1Nx44dNVkcAWMMHTt21NqZUgqIbC8pA/wD2CQif2lmtbeA74d6S40HSkQkD1gETDXGJBljkoCpoXlHGsuRbnrS089OKVUnkpekJgDfAzYYY9aF5t0OdAcQkSeAhcC5wFagErg2tGyfMeYeYFVou7tFZF+kAq2pycXpjMXl6hCpQyil1HEvYglDRD4GDnp6KvaB4jc2s+xp4OkIhHaA2trduN2dIpIwiouLeeGFF/jJT35y2Nuee+65vPDCCyQmJrZo/bvuuou4uDh+8YtfHPaxlFLqUHQsKcAYFyKBiOy7uLiYxx57rMllfr//oNsuXLiwxclCKaUiTRMGYIwTe59h65szZw7btm1j5MiR3HbbbSxZsoSJEycyY8YMBg8eDMCFF17ImDFjGDJkCPPmzavftmfPnhQWFrJz504GDRrEddddx5AhQ5g6dSpVVVUHPe66desYP348w4cP56KLLqKoyN7zOHfuXAYPHszw4cOZNWsWAP/73/8YOXIkI0eOZNSoUZSVlUXks1BKHd9OqLGkDmXLltmUl687YH4wWAmAw+E97H3GxY2kX7+Hm11+//33s3HjRtats8ddsmQJa9euZePGjfVdVZ9++mmSk5Opqqpi3LhxXHLJJXTs2HG/2Lfw4osv8uSTT3L55Zfz+uuvc9VVVzV73O9///s88sgjTJ48md/97nf8/ve/5+GHH+b+++9nx44dREdHU1xcDMBDDz3Eo48+yoQJEygvL8fj8Rz256CUOvFpDQM4RFNLqzvllFMa3dcwd+5cRowYwfjx48nKymLLli0HbNOrVy9GjhwJwJgxY9i5c2ez+y8pKaG4uJjJkycDcPXVV7N06VIAhg8fzpVXXsm///1vXC57vjBhwgRuvfVW5s6dS3Fxcf18pZQKd1KVDM3VBKqqthMIVBAXN+yYxBEbG1v/+5IlS/jggw9YsWIFXq+X008/vcn7HqKjo+t/dzqdh7wk1Zy3336bpUuXsmDBAv74xz+yYcMG5syZw/Tp01m4cCETJkxg0aJFDBw48Ij2r5Q6cWkNA9uGEalG7/j4+IO2CZSUlJCUlITX62Xz5s2sXLnyqI/ZoUMHkpKSWLZsGQD/+te/mDx5MsFgkKysLM444wz+9Kc/UVJSQnl5Odu2bWPYsGH86le/Yty4cWzevPmoY1BKnXhOqhpGc2yjdwARafUb1Tp27MiECRMYOnQo55xzDtOnT2+0fNq0aTzxxBMMGjSIAQMGMH78+FY57rPPPssNN9xAZWUlvXv35p///CeBQICrrrqKkpISRISf/vSnJCYm8tvf/pbFixfjcDgYMmQI55xzTqvEoJQ6sRh7K8SJYezYsbL/A5Q2bdrEoEGDDrpdTU0etbU5xMWNCiUPFa4ln6FS6vhkjFkjImNbsq5eksLehwFE7LKUUkqdCDRhQH2tQhOGUko1TxMGmjCUUqolNGEAUNduoQlDKaWaowkDrWEopVRLaMJAE4ZSSrWEJgzaX8KIi4s7rPlKKXUsaMIA7MdgiNSItUopdSLQhEHdY0gjMzzInDlzePTRR+tf33XXXTz00EOUl5czZcoURo8ezbBhw3jzzTdbvE8R4bbbbmPo0KEMGzaMl19+GYC8vDwmTZrEyJEjGTp0KMuWLSMQCHDNNdfUr/vXv/611d+jUurkELGhQYwxTwPnAfkiMrSJ5bcBV4bFMQjoFHo8606gDNttyd/SuxAPafZsWHfg8OYA3kAFGCc4DnNo75Ej4eHmhzefOXMms2fP5sYb7YMFX3nlFRYtWoTH42H+/PkkJCRQWFjI+PHjmTFjRouGJnnjjTdYt24d69evp7CwkHHjxjFp0iReeOEFzj77bH7zm98QCASorKxk3bp15OTksHHjRoD6Ic2VUupwRXIsqWeAvwPPNbVQRB4EHgQwxpwP/Gy/53afISKFEYxvPwZo/WFSRo0aRX5+Prm5uRQUFJCUlES3bt3w+XzcfvvtLF26FIfDQU5ODnv27CEtLe2Q+/z444+54oorcDqddO7cmcmTJ7Nq1SrGjRvHD37wA3w+HxdeeCEjR46kd+/ebN++nZtvvpnp06czderUVn+PSqmTQySf6b3UGNOzhatfAbwYqVjqHaQmUFP5NSJCbGzrD+t92WWX8dprr7F7925mzpwJwPPPP09BQQFr1qzB7XbTs2fPJoc1PxyTJk1i6dKlvP3221xzzTXceuutfP/732f9+vUsWrSIJ554gldeeYWnnz4mj0pXSp1g2rwNwxjjBaYBr4fNFuA9Y8waY8z1xyYOO2JtJMycOZOXXnqJ1157jcsuuwyww5qnpqbidrtZvHgxu3btavH+Jk6cyMsvv0wgEKCgoIClS5dyyimnsGvXLjp37sx1113Hj370I9auXUthYSHBYJBLLrmEP/zhD6xduzYi71EpdeJrD8Obnw8s3+9y1GkikmOMSQXeN8ZsFpGlTW0cSijXA3Tv3v0owojcMzGGDBlCWVkZGRkZpKenA3DllVdy/vnnM2zYMMaOHXtYDyy66KKLWLFiBSNGjMAYwwMPPEBaWhrPPvssDz74IG63m7i4OJ577jlycnK49tprCQaDANx3330ReY9KqRNfRIc3D12S+m9Tjd5h68wHXhWRF5pZfhdQLiIPHep4Rzq8OUB1dRY+XwHx8aMPue7JRoc3V+rEddwMb26M6QBMBt4MmxdrjImv+x2YCmyMfCxOIMiJ9HwQpZRqTZHsVvsicDqQYozJBu4E3AAi8kRotYuA90SkImzTzsD8UPdSF/CCiLwbqTgb4m2427vu+RhKKaUaRLKX1BUtWOcZbPfb8HnbgRGRiepgwkes1YShlFL7a/NeUu1FextPSiml2htNGCGaMJRS6uA0YYRowlBKqYPThFGvrg2jdUesLS4u5rHHHjuibc8991wd+0kp1W5owgiJVA3jYAnD7z94clq4cCGJiYmtGo9SSh0pTRghkUoYc+bMYdu2bYwcOZLbbruNJUuWMHHiRGbMmMHgwYMBuPDCCxkzZgxDhgxh3rx59dv27NmTwsJCdu7cyaBBg7juuusYMmQIU6dOpaqq6oBjLViwgFNPPZVRo0Zx1llnsWfPHgDKy8u59tprGTZsGMOHD+f11+0oLO+++y6jR49mxIgRTJkypVXft1LqxHNS9R89yOjmgINAYADGuHEcRho9xOjm3H///WzcuJF1oQMvWbKEtWvXsnHjRnr16gXA008/TXJyMlVVVYwbN45LLrmEjh07NtrPli1bePHFF3nyySe5/PLLef3117nqqqsarXPaaaexcuVKjDE89dRTPPDAA/z5z3/mnnvuoUOHDmzYsAGAoqIiCgoKuO6661i6dCm9evVi3759KKXUwZxUCePQDv0sitZwyimn1CcLgLlz5zJ//nwAsrKy2LJlywEJo1evXowcORKAMWPGsHPnzgP2m52dzcyZM8nLy6O2trb+GB988AEvvfRS/XpJSUksWLCASZMm1a+TnJzcqu9RKXXiOakSxsFqAgAVFTtxOGKIiekT0ThiY2Prf1+yZAkffPABK1aswOv1cvrppzc5zHl0dHT9706ns8lLUjfffDO33norM2bMYMmSJdx1110RiV8pdXLSNoxGWn/E2vj4eMrKyppdXlJSQlJSEl6vl82bN7Ny5cojPlZJSQkZGRkAPPvss/Xzv/Od7zR6TGxRURHjx49n6dKl7NixA0AvSSmlDkkTRhhjWj9hdOzYkQkTJjB06FBuu+22A5ZPmzYNv9/PoEGDmDNnDuPHjz/iY911111cdtlljBkzhpSUlPr5d9xxB0VFRQwdOpQRI0awePFiOnXqxLx587j44osZMWJE/YOdlFKqOREd3vxYO5rhzQGqqrYRCFQRF9fsaOwnJR3eXKkT13EzvHl7Y0epbd0b95RS6kShCaORyD11TymljneaMMLYm/cEkWBbh6KUUu2OJowwOgChUko1TxNGGE0YSinVvIglDGPM08aYfGNMk8/jNsacbowpMcasC02/C1s2zRjztTFmqzFmTqRiPFD4U/eUUkqFi2QN4xlg2iHWWSYiI0PT3QDGnuY/CpwDDAauMMYMjmCc9dpLDSMuLq5Nj6+UUk2JWMIQkaXAkdw+fAqwVUS2i0gt8BJwQasG14yGhKFda5VSan9t3YbxLWPMemPMO8aYIaF5GUBW2DrZoXlNMsZcb4xZbYxZXVBQcFTB2PswWreGMWfOnEbDctx111089NBDlJeXM2XKFEaPHs2wYcN48803D7mv5oZBb2qY8uaGNFdKqSPVloMPrgV6iEi5MeZc4D9Av8PdiYjMA+aBvdP7YOvOfnc263Y3Mb55ZSW4XBDlJhAox+GIxpioFh1/ZNpIHp7W/KiGM2fOZPbs2dx4440AvPLKKyxatAiPx8P8+fNJSEigsLCQ8ePHM2PGDIxpfsTcpoZBDwaDTQ5T3tSQ5kopdTTaLGGISGnY7wuNMY8ZY1KAHKBb2KpdQ/MiJxi0U2h4cxHhIOX2YRk1ahT5+fnk5uZSUFBAUlIS3bp1w+fzcfvtt7N06VIcDgc5OTns2bOHtLS0ZvfV1DDoBQUFTQ5T3tSQ5kopdTTaLGEYY9KAPSIixphTsJfH9gLFQD9jTC9sopgFfLc1jtlsTeDLLyEqCvr1o6zsc9zujng83VvjkABcdtllvPbaa+zevbt+kL/nn3+egoIC1qxZg9vtpmfPnk0Oa16npcOgK6VUpESyW+2LwApggDEm2xjzQ2PMDcaYG0KrXApsNMasB+YCs8TyAzcBi4BNwCsi8mWk4gTA7QafLxS3GxFfq+5+5syZvPTSS7z22mtcdtllgB2KPDU1FbfbzeLFi9m1a9dB99HcMOjNDVPe1JDmSil1NCJWwxCRKw6x/O/A35tZthBYGIm4muR2Q+iBRA6Hm2CwtlV3P2TIEMrKysjIyCA9PR2AK6+8kvPPP59hw4YxduxYBg4ceNB9TJs2jSeeeIJBgwYxYMCA+mHQw4cpDwaDpKam8v7773PHHXdw4403MnToUJxOJ3feeScXX3xxq74vpdTJRYc3B8jOht27YcwYqqp3EgiUERc3PIKRHl90eHOlTlw6vPnhcrvtT78fhyMKkVpOpESqlFKtQRMGNCQMnw9j7O+t3Y6hlFLHu5MiYRyythAVuufC56u//0IThqU1LaVUnRM+YXg8Hvbu3Xvwgi+shuFw2N9bu+H7eCQi7N27F4/H09ahKKXagba80/uY6Nq1K9nZ2Rx02BARKCwEvx9JiKOmphCXK4DLlXDsAm2nPB4PXbt2beswlFLtwAmfMNxud/1d0Af17W/D976HzP0bS5eOoWvXn9Gnz/2RD1AppY4TJ/wlqRZLT4e8PIxxEB3dhZqa7LaOSCml2hVNGHVCCQMgOrqrJgyllNqPJow6+yWM2trIjneolFLHG00YdeoShghRURnU1GRrl1KllAqjCaNOejpUV0NJCdHRXQkGq/H7j+SBgUopdWLShFEnNCggeXlER9tupDU1ellKKaXqaMKo0yhh2CfCasO3Uko10IRRp8kahiYMpZSqowmjTljCiIpKAxx6SUoppcJowqiTkAAxMZCXh8PhJiqqs9YwlFIqTCQf0fq0MSbfGLOxmeVXGmO+MMZsMMZ8YowZEbZsZ2j+OmPM6qa2j0DAevOeUkodRCRrGM8A0w6yfAcwWUSGAfcA8/ZbfoaIjGzpk6BaxQEJQy9JKaVUnYglDBFZCjR7I4OIfCIiRaGXK4G2HxK1UcLI0BqGUkqFaS9tGD8E3gl7LcB7xpg1xpjrD7ahMeZ6Y8xqY8zqgw5h3hL71TACgRL8/vKj26dSSp0g2jxhGGPOwCaMX4XNPk1ERgPnADcaYyY1t72IzBORsSIytlOnTkcXTHo6lJZCZWV911odU0oppaw2TRjGmOHAU8AFIrK3br6I5IR+5gPzgVOOSUCNutbqzXtKKRWuzRKGMaY78AbwPRH5Jmx+rDEmvu53YCrQZE+rVqc37ymlVLMi9sQ9Y8yLwOlAijEmG7gTcAOIyBPA74COwGPGGAB/qEdUZ2B+aJ4LeEFE3o1UnI00ShhjAB1PSiml6kQsYYjIFYdY/iPgR03M3w6MOHCLYyAsYTidMbhcydTUZLVJKEop1d60eaN3u9KxI7hc9T2lvN7+VFRsauOglFKqfdCEEc7hgK5dYdcuAOLiRlNe/jkiwTYOTCml2p4mjP0NGABffw1AXNwoAoFSqqq2t3FQSinV9jRh7G/AAPjmGxAhPn40AOXla9s4KKWUanuaMPY3YACUl0NuLrGxQzDGTXn5520dlVJKtTlNGPsbOND+3LwZhyOa2NghlJVpDUMppTRh7G/AAPuzvh1jNOXlaxGRNgxKKaXaniaM/XXpAnFx9QkjPn40Pl+h3vGtlDrpacLYnzG2lrF5M2B7SgHajqGUOulpwmhKo661IwCj7RhKqZOeJoymDBgAmZlQVYXTGYvXO1C71iqlTnqaMJoyYACIwJYtQMMd30opdTLThNGUsK61APHxo6ipyaa2Nr8Ng1JKqbbVooRhjLnFGJNgrH8YY9YaY6ZGOrg206+f/RnWtRa04VspdXJraQ3jByJSin2YURLwPeD+iEXV1rxe6N690ZhSgDZ8K6VOai1NGCb081zgXyLyZdi8E9PAgfUJw+1OxOPpTVnZqjYOSiml2k5LE8YaY8x72ISxKPQI1RN7zO+6ezFCd3h36DCR4uL/6VDnSqmTVksTxg+BOcA4EanEPmr12kNtZIx52hiTb4xp8pncoTaRucaYrcaYL4wxo8OWXW2M2RKarm5hnK2nbhDC0MOUkpLOwu/fR3n5umMeilJKtQctTRjfAr4WkWJjzFXAHUBJC7Z7Bph2kOXnAP1C0/XA4wDGmGTsM8BPBU4B7jTGJLUw1tax35hSSUlnAlBU9OExDUMppdqLliaMx4FKY8wI4OfANuC5Q20kIkuBfQdZ5QLgObFWAonGmHTgbOB9EdknIkXA+xw88bS+/brWRkd3wesdpAlDKXXScrVwPb+IiDHmAuDvIvIPY8wPW+H4GUBW2Ovs0Lzm5h/AGHM9tnZC9+7dWyGkusgyIDa2PmEAJCVNIS/vaYLBWhyOqNY7llJK7UcE9uyBvXvB7QaXC2JioEMH+9O0QbejliaMMmPMr7HdaScaYxzYdow2JyLzgHkAY8eObb0xyI2B4cNh9er6WYmJU8jJ+TulpStJTJzUaodSSjUIBKCqqr6/CTU1UFxsp+pqW3C6QiVXba1dXltrtwsEwOdrPD8YtFPdMr/f/u5yNRTEdcsDgYbf/X57zH37oKjIxmMMOBwN+wwG7by6qaoKSktt86fTaXvox8TYOMrL7eTzNbw3lwuio8HjsesmJEB8PBQWwoYNUFDQ9GdUt11dDKmpdjSjSGtpwpgJfBd7P8ZuY0x34MFWOH4O0C3sddfQvBzg9P3mL2mF4x2eiRPhr3+134KYGBITTwccFBV9qAlDtXsiDYWcMfb32lqoqIDKSlv4VlfbeTExtkIdG2vX8/ttwbZnD+Tm2qmuICwvt4Wm2w1RUfZpAB062MIuJwe+/BK++squX1egxcRAUhIkJtrj7dljp9rahkLV77cFZXFxW39yDbxeSE62cTudDe/H4bCv687yg0H7ucXE2AI/Lc3Oq6qyn3VUFHTrZj+rqNDFCWPse677O1RU2ASxbZv9PGfMsOesaWkNia6y0n6upaV2G6fTxpKQcGw+jxYljFCSeB4YZ4w5D/hMRA7ZhtECbwE3GWNewjZwl4hInjFmEXBvWEP3VODXrXC8w3PaafDAA7BqFUyahNudSHz8GIqKPqRXr98f83DU8SsYtP/gVVX2bLWuwCwpaTgbNsb+4yck2EKguNiuW1lp92GM/T0zE3btsssSE21BHBVlC+vMTLtfv7/hLLZuW2g870gYYwtRaDiT31/nzjBkiL33NfzMu7gYtm+3iaZzZ1sYRkc3FKouF3TsCCkpNnHVbRsVZd9nYqI9E687+xex20dF2X263bYArTv7jo5umOdwNCQ5t9v+XpcU/f6GBFA3ORwNNRDVoEUJwxhzObZGsQR7w94jxpjbROS1Q2z3IramkGKMycb2fHIDiMgTwELsvR1bgUpCXXVFZJ8x5h6g7k65u0XkYI3nkTFhgv25bBlMsjWKpKSzyMp6EL+/DJcr/piHpI6OiL3EkJ9vC+bUVFsoiDQU5vn5dioosIVJndJSez25sNAW9HVnemVl9uywosIW/HUFUd3ljWDQvj5qJoiJqqRrahw9ekDPnjaO7dttMuraFaZMgfT0hkLRmIYY6s6AY2MbzupjYuy61dW25lBR0VB4ulyQ0ilIfGoR0Yl76ZeeRufEhEbXzkXsNiUldurc2Rb6LZFXlocgdInv0mh+cXUxVb4q0uPTW+FDOzRfwIcxBqdxYlqpYaDaX02Nv4agBAlKEKfDSZQziihnFC5HSy/stD8tjfw32Hsw8gGMMZ2AD4CDJgwRueIQywW4sZllTwNPtzC+yEhOtqdKH39cPyspaQqZmfdRUrKUjh2nt2FwJ5baQC17yvfQJb4LToezfr6IPTMtK7NTZaUtkP1+e3ZbN7+01CaCuuvNdZdOKipCZ7BVQpl/H/m7kqitadw5MD5BqI7eiS/9Y8j4DPwxUNbFTn5PXSRgBEyQuPgAnuS9uJJyIT2XaKeHeNOFHo50TFQF5e7tlLm2Y4wQRzqxpJHg6ERCdCKJ0Yl4vH6CsbupjcoDdxXeKA+xUR48zliiJRF3IIkoE0NMrB+P109+TSaLd37I/zIXU1RdRGzKQPp2Hc+otFEkeZLo4OlAjCuGmkAN1f5qCisLWbd7HWvy1pBZksnYLmM5o+cZjO86nkAwQGlNKcXVxeSW5ZJblktRdRHje4/nvP7n0b1DdzYXbuafn/+TV756hawvsghIoP6z6hLfhYEpAxmWOozhnYczLHUYtYFadpXsIrMkk13bd5FZmklmSSZxUXH0S+5H3+S+dEvoRmpsKp1iO7E2by0vbnyRZbuWIQhDOg3h7D5n43a6+XDHh6zNW0tQgqTFpTE6fTR9k/oS7YomyhlFRW0FW4u2snXfVnaX7yYQDBCUIC6Hi9TYVFJjU+ng6UBtoJZqfzW1gVocxoHDOHAaJ163F6/bizGGrJIsdhTvoLCysP79uR1uUmNTSYtLIzU2FbfTVjECwQAFlQXsLt9NfkU+IoLL4cLtdON1e4mLisPr9lJWU0Z+RT5ltWXNftf7d+zP9H7Tmd5vOqmxqewu301eeR67y3fXT0XVRVT6Kqn0VVIbqK1PPFHOKJJjkkmOSSbaGU2Vv4qK2griouJ47fKDFsetwrTkWdXGmA0iMizstQNYHz6vPRg7dqysDmukbhU//jG88IItiZxOAoEqPv44iYyMn9C3719a91jHmaAEyS7NxuVwkR6XjjEGf9DPx5kfs+DrBVT4KhiTPoaRqeNwVqWxLa+QHXsKqSiNIqFqOJXFcRSVV/K54ylWRT1AhSMHh0QRV9ub6Mo++At7UZbVE3+lF1K/hM7rIWkHEPrOmiA4a+0EUNQbV8kAYqr7EGMSiHZ6iYoOUpn0KcUJH1PlzsEtsaS7htDdO4Cy2jIKa3IpCu6i0rEHgGgTRxAfPqk55Pt3GAepsanU+Gsoqi6qn+91e+mV2Aunw8nu8t0UVBQgNP1/FuWMojbQxHWd/XTv0J0pvabQM7Enq3JXsSJrBXur9ja7fqInkdHpo+mW0I2V2Sv5eu/XTa6XEJ1ArDuWvPK8+uNklmTiNE6m9Z3GiM4j6BzXmeSYZLJLs9lcuJmvCr7iy4IvqfRVHrC/jjEd6d6hO906dKOspoyt+7aSVZp1wHoDUwZyxdAr8Lq9LNq2iGW7lhGQAOO7jmdKrykkxySzNm9tfdLzBXzUBGrwuDz0Te5L3+S+ZMRn4HK4cBgHtYFa8ivy2VOxh9KaUjwuDx6XB7fDjSAEJYg/6KfKV0WlrxJ/0E/XhK70SuxFRkIGBvvdrfZXN0oMdcnSYOgU28kmEm8qTocTX8CHL+ij0ldJeW05Fb4K4qPi6xNXjCsGh3FgjCEQDOAL+qjyVbEiewVLdi6hJnDgdyzWHUt6fDqJnkRi3bHERsXidrhxOpw4jINqfzVFVUXsq9pHtb8ar9tLbFQsaXFpvDnrzUN+j5pijFkjImNbtG4LE8aDwHDgxdCsmcAXIvKrI4owQiKSMF5Q0tgZAAAgAElEQVR4Aa68EtauhVF2EML166dSXZ3JqaduPsTGx4+gBCmpLmFv1V7S49KJjYoFQET4JOsTXtr4EjllOdT6/RSV+sgtzyGnags+qQbAGfTiLutLrSebYPQ+TCAaE4ghGNVMC6YYKBwI3r0Qm48zexKebZfhSMwmmLSFYIdt+OJ24HeWAuAx8fTwDKebtx8upwtjwOkweKOjiPVE4XD5ya3azpair9lRtKPRWXFGfAYTe0xkdNposkuz2ViwkW/2fkOH6A5kJGTQJb4LY9PHMrHHRIamDsVg2Fe1j7zyvEaFed2ZqsHQ0duR1NjU+ssLVb4q8srz8Lq9dI7t3OjShi/go7i6mKLqIoqqinA5XI3OYIMSpDZQS3ltOSXVJRRXF1Ppq8TtdONyuOgY05GeiT0b7VNEKKgsoLSmlNKaUip9lfWFZEJ0At0SujVaP7csl3W71xHjiiEhOoGE6ATS49OJi4pDRPh679cs+HoBy7OWc1r307hq+FWkxaU1+30JBANsL9rOxvyNxLhj6NGhB906dCMuKu6Adat8VfUFcH5FPt07dGd45+GN4qvyVSEIXre32WPWve/WumzUlipqK1i8c3H9pbe0uDQ6x3YmPvrYX+Zu9YQR2uklQOiiPstEZP4RxhcxEUkYmZnQowfMnQs33wxATs6jbNlyE+PGbSI2dmDrHq8VlNaUEggGcDvdOIyDkuoS9lXto7i6mERPImlxacRHx7Ns1zLe2DSf/37zNjllDZcejDhI8A3AvW84ZQmrqPFuxxHw4i7rQ02VC4JOKE+Dvf3t5AiQ0GsL0elbiXMk07n4AjoUTsXrjiUmYyu1KatwxxeTkdSJ7h1TcMWWs6NqLV8Vr8HtcnLr+FuZ2GNik++lqKqIstoyuiZ0xWFadp+piFATqKk/k+zk7XRCFDJKRUJEEsbxICIJA2zCOPVUeOUVAKqrs1m5shu9et1Ljx6R77xVXltOrDu2vtBblbOKR1c9ynvb3mNsl7Gc3edsRqePZsnOJczfPJ9VuYcxqq4vBrZNhfwhUNkJqpIgeQcxvdYSTF1PbHV/OuV+j/jsi8joFM+wYTBsmG1gjY+33QTT0mzjqVLq+HM4CeOgjd7GmDJo8uKrwbZZH6Pev21s4kT46KP6Tu0eT1fi48dRWDg/oglj6a6l3LnkTpbsXEJ8VDx9kvsgIqzfs55YdyzT+k7j892fs+CbBfXb9POOY2rU76kuTqCoxE9xaYCKfR0o25OMrzwBoktwJ+0mo38h/eJGMzLubNJP8dKli725vW6K0hvZlVL7OWjCEBHtNwr2foznn7f9F/v0ASAl5SJ27Lid6upsPJ6uR32IspoyFu9czJ7yPeRX5LN452I+3PEhaXFp3DHxDkpqSthetJ2SmhLmTnuEIf7v8/GHCbg3gSd7K9sqPqdm27fYUtqVLdgz/65doX+G7WbZeaitCZx6Kpxyiu2jrpRSh+P47RB8LE0MXV9ftuyAhLF375tkZDTZM7hFRITXvnqN2Ytmk1uWWz8/Iz6Dv0z9CzeMvQGPK4YdO2DNGli6Ah74HWRn2z72PXvCgAF9+c6Yvgy+wfYCHjTI9ghWSqnWpAmjJQYNsrfT/u9/cM01AMTGDsTrHUhBwfwWJ4y6Hkcrs1dijMFhHLyz9R3e2/Yeo9JG8cwFzzAwZSBRvlSWL41m9QK44E6bKPaFblv0eGDaNLj3XjjvPBuWUkodC5owWsLhsKX0ggX2lt3QeAEpKReSmfkgPt8+3O7mT+lLa0p5/ovneXz142zI39BoWUJ0AnOnzWVW3x/z9gIXD78K779vD+Ny2Qbmiy+GceNg7FgYOlTbF5RSbUMTRkvNmgUvvggffmiTB/ayVGbm/ezd+1/S0r7faHV/0M97297jX1/8i/9s/g/V/mpGpY3iyfOf5KKBF+FyuCguDbLov17+8/tofv6BTRLdu8NPfwoXXQRjxtgahVJKtQeaMFrq7LPtEJIvvVSfMOLjxxIVlUFh4fz6hJFZkslTa5/iH5//g9yyXJJjkvnByB9w9cirGddlHMYYdu2CRx6BJ5+0Q1r06AG33AKXXmobpPWWAaVUe6QJo6Wio+21oddfhyeeAI8HYxx06nQxubnzKK7M5db37+DZ9c8iIpzT7xwePfdRzu13LlHOKPx+ePtt+Mc/7JUtgMsvt/cCjh+vSUIp1f619BGtCuxlqdJSePfd+lmpqbPYUV7DKU+O45l1zzD71Nlsv2U7b3/3bS4ceCEOieLPf7Zj4Z9/PnzyCfz857Bjhx115Fvf0mShlDo+aA3jcJx5ph2s/6WX4MILAXgvO5sfrzV43YW8/733mdJ7Sv3qq1fDddfBunUwdaqtmJx7ro6xr5Q6PmkN43C4XHDZZfaaUkUFi7Yu4orXv8vg5HTmjQ4yqdtowA69ffvt9ia5PXvgjTdg0SK44AJNFkqp45cmjMM1axZUVrL21blc+uqlDE0dypuXP0/HKD+FhfPJz7e1ifvug2uvhU2bbI8npZQ63kU0YRhjphljvjbGbDXGzGli+V+NMetC0zfGmOKwZYGwZW9FMs7Dctpp7BzQmelbf0+SJ4mFVy4kI2UyHk9vPvxwHWPGwIoV8Mwz8NRTtmOVUkqdCCLWhmGMcQKPAt8BsoFVxpi3ROSrunVE5Gdh698MjArbRZWIjIxUfEcqt2I3Uy+rptpfwweXzq9/vOSaNfdw660X0aVLgE8+cdY9OkMppU4YkaxhnAJsFZHtIlILvARccJD1r6DhAU3tUk5pDqc/czp5UbW8/TwMWZ+HCDz4INx003fp128tb775L00WSqkTUiQTRgYQ/mzG7NC8AxhjegC9gI/CZnuMMauNMSuNMRdGLsyWySnN4Yxnz2B3+W4WXfkO3y6OJ/jmAm6+GX75S7j8cuHxx3+C3/9UW4eqlFIR0V4avWcBr4mEPVcTeoQe6vFd4GFjTJ+mNjTGXB9KLKsLCgoiElxhZSFnPnemTRZXLeLbvScTOPtcrnt+Mo8+au+rePFFQ8+eV1NaupySkpURiUMppdpSJBNGDtAt7HXX0LymzGK/y1EikhP6uR1YQuP2jfD15onIWBEZ26lTp6ON+QCVvkrOf/F8dhXvYuGVC/lWt2/h98PVuffxdNV3ufOH2Tz4oB2fMD39OlyuJLKy/tTqcSilVFuLZMJYBfQzxvQyxkRhk8IBvZ2MMQOBJGBF2LwkY0x06PcU7LPEv9p/20jzB/1c8foVfJr9KS9c8gKndT8NEbj6anj+k17ca37DXZ0fr79T2+WKJyPjZgoL/0NFxaZjHa5SSkVUxBKGiPiBm4BFwCbgFRH50hhztzFmRtiqs4CXpPHDxQcBq40x64HFwP3hvauOlZ+9+zPe+vot5p4zl4sHXQzAH/9oh/T44x/h16evgLca58CMjJtxOGLIynrgWIerlFIRZRqX08e3sWPHyurVq1tlX0t3LWXyM5O55dRbeHjawwC8+aYdEeSqq+C558D87WH42c9g2zbo3bt+2y1bbiE39zFOPXU7Hk+35g6hlFJtzhizJtRefEjtpdG7XfEFfPzk7Z/Qo0MP7p1yLwAbNthEMW4czJsXGjDw/PPtBvvVMrp1uxWArKw/H8uwlVIqojRhNGHup3P5suBL/jbtb3jdXmpr7RBS8fEwfz7ExIRW7NPHPkT71Vcbbe/x9CA19bvk5c2jtnbPsX8DSikVAZow9pNTmsNd/7uL6f2mM2OAbWr529/g66/tUB8Z+99J8oMf2DHL165tNLtHj98QDNaQmfngMYpcKaUiSxPGfm57/zb8QT9zz5mLMYa8PLj7bjjvPDs0+QF++EOIi4O//rXRbK+3P507X0Vu7mPU1Ow+NsErpVQEacIIU+mr5NWvXuXHY39M7yTbiD1nDtTWHpAPGnToYGsZL78MubmNFvXocQfBYC1ZWVrLUEod/zRhhPks5zP8QT9n9T4LsKPOPvcc3Hor9O17kA1/+lP7EIzHHms02+vtF6plPK61DKXUcU8TRpjlmcsBGN91PAC33QZdusBvfnOIDfv0gRkz7CP1qqoaLWqoZejd30qp45smjDDLs5YzuNNgkmOSyc2F5cvh5pttE8UhzZ4Ne/fCv//daLbX25e0tGvIyfk7xcUfRyZwpZQ6BjRhhAQlyIrsFUzoNgGAd9+186dPb+EOJk+GkSPto/bKyxst6tv3z3g8vfjqq8uoqclrxaiVUurY0YQR8lXBVxRXF9cnjIULbRfaoUNbuANjbP/bnTvttawwLlcHhgx5Hb+/lK++upxg0Ne6wSul1DGgCSOkrv1iQvcJ+Hzw/vu2G23dwIItMmmSHev8iSfgnXcaLYqLG8aAAU9RUvIx27f/shUjV0qpY0MTRsjyrOWkxqbSJ6kPn3wCpaXN3HdxKPfcY6slP/yhbdMI07nzFWRk3ER29sMUFy9rncCVUuoY0YQR8knWJ0zoNgFjDAsXgtsNU6YcwY48HvjXv6CwEG666YDFvXvfT3R0D7755v8IBmuPPnCllDpGNGEAe8r3sK1oW6P2i4kT7dhRR2TkSNsX96WX4MMPGy1yOmPp1+/vVFZuIivroaOMXCmljh1NGNjLUWDbL7KyYOPGI7wcFe5Xv4JeveCWW8DXuJE7JeU8UlIuYdeue6iq2naUB1JKqWNDEwa2wTvaGc2otFH1bdXnnHOUO/V47HgiX355wB3gAP36/Q1j3HzzzY8RCR7lwZRSKvI0YWBrGOMyxhHtimbhQujRAwYNaoUdz5gBZ58Nd94J+fmNFkVHZ9C79wMUFb3Pzp13t8LBlFIqsiKaMIwx04wxXxtjthpj5jSx/BpjTIExZl1o+lHYsquNMVtC09WRirHGX8Pnuz9nQrcJiMCSJTB16mF2p21O3b0ZFRX2EtV+unT5P9LSrmHXrt+Tn/9aKxxQKaUixxWpHRtjnMCjwHeAbGCVMeatJp7N/bKI3LTftsnAncBYQIA1oW2LWjvOaFc0ubfm4g/6yc6GkhIYNaoVDzBgAPzyl3DvvTB6tB1rJMQYQ//+T1BZuZnNm68mJqYv8fEjW/HgSinVeiJZwzgF2Coi20WkFngJuKCF254NvC8i+0JJ4n1gWoTipKO3I53jOrNxo33d4ru7W+ruu+3DwG+5BV5/vdEihyOaIUPewOVKYuPGGVRV7WjlgyulVOuIZMLIALLCXmeH5u3vEmPMF8aY14wx3Q5z21ZVlzCGDGnlHTud8MIL8K1vwZVXwseNByGMjk5n2LAFBAJlrFs3mcrKra0cgFJKHb22bvReAPQUkeHYWsSzh7sDY8z1xpjVxpjVBQUFRxXMxo12OPPk5KPaTdNiYuCtt6BnT9tn9+23Gy2Ojx/FiBGLCQQqWbduEhUVmyMQhFJKHblIJowcoFvY666hefVEZK+I1IRePgWMaem2YfuYJyJjRWRsp06djirgL7+MQO0iXMeO8MEH0K8fnH8+PPAAiNQvjo8fyciRSxAJsm7dRPbufTeCwSil1OGJZMJYBfQzxvQyxkQBs4C3wlcwxqSHvZwBbAr9vgiYaoxJMsYkAVND8yImEICvvopA+8X+unaFZcvgsstsz6lrr4Vgw30YcXFDGTVqKVFRaWzYcA5bttxCIFAd4aCUUurQItZLSkT8xpibsAW9E3haRL40xtwNrBaRt4CfGmNmAH5gH3BNaNt9xph7sEkH4G4R2RepWAF27LAPy4t4wgDweu2wIQMH2gbx/v3h9tvDFvdn9OjP2L59Djk5cykuXsyQIW/g9R7sObFKKRVZRsIuiRzvxo4dK6tXrz6ibf/zH7joIli5Ek49tZUDa46IbQR/+WU7nvqZZx6wyt6977Bp01WAMHjwKyQnn3WMglNKnQyMMWtEZGxL1m3rRu9248sv7c/Bg4/hQY2BefPsvRpXXAG5uTaJZGbCZtvo3bHjOYwZs4ro6Ay++OJssrIe5kRK8kqp44cmjJCNG20HpiMeofZIxcXZezMqKuCUUyAlxY5NMmQIbNgAQExMb0aN+oSUlBls2/Yztm+fo0lDKXXMacII2bjxGLVfNGXQIPsMjR494JJL4JFHbCL57W/rV3G54hky5HW6dPkxWVkPsGXLzTpooVLqmIpYo/fxxOeDr7+G6dPbMIiLLrJTneJimzA+/bS+UcUYB/36PYrTGUtW1kMEgxX07/8EDkd0GwWtlDqZaA0D2LLFJo02q2E0ZfZs6NTJPogpjDGG3r0foGfP37N79zOsWjWUffsi2uNYKaUATRgAkRtD6mjExdlk8eGHDU/tEwG/H2MMPXv+juHD3wMcfPHFNDZuvJSamtw2DVkpdWLThIFNGA6HvS2iXfm//4Nu3eDGG+2d4enp9m7xjz4CIDn5O4wb9wW9ev2Rffve5rPPBpOb+5Q2iCulIkITBrZLbd++9iF57YrHY4dF/+Yb2LrVPqgjPR0uvdReR8OOdtujx+2MHbuB+PhRfPPNdaxfP4Wiog+1UVwp1ar0xj3sbRBDhx4w8nj74feDK9Q/Yft22/22Y0d7l2FSUv1qIkHy8v7B9u2/wu8vIjq6B2lpV9O581V4vf3aKHilVHumN+4dBp8P9u5tZ+0X+3OFdWbr3Rvmz7djmUyfDj/8IXz729C/P+Zf/6ZL+o/41rdyGTToRbze/uzadQ+ffdaf1avHkpX1Z/z+0rZ7H0qp45rWMLBtyT4fREVFIKhIefZZ28aRmGjv4ygvh9Wr4bvfhcceg4QE2L6d2m9WsadfJvklr1JWtpro6K706/c4KSnntfU7UEq1A4dTw9CEcTwLv1QVCMD998Odd9rLVbW19l4OgAkT4I03KPXs4Ouvf0RFxUZSU2fRu/f9eDw92i5+pVSb04RxMluxwjaUd+kCY8bY6tPPfgapqfDWWwSHDiQz83527foDIgFSUmaQkXEziYlnYIxp6+iVUseYJgzV2OrVcMEFtsYxdChERRFwC6VDYce4jZR2K8HrHUiXLjfQufP3cbuTDr1PpdQJQROGOlBuLsyZA3v22Aab0lJYuxZE8PXrQt6l0ew4cwcmykNKysV0KZ5MhyeWYb4zFb73vbaOXikVIZowVMvs3g1vvAHPPQeffkqwTzfyf9QXPl5O53dqATBB2PfjcVT8aiaJyWcSHz/q4PuLjm7U1Vcp1b5pt1rVMmlp8JOf2HaP//4XhzeRtF8vpvP7UHXdeXzz0XnkX5BI8uOriL72F2xcOJp17w8jd/Of8dUUNuxn5064/nro3t128a2qarO3pI6h6mo47TTbW6+kpK2jUceCiERsAqYBXwNbgTlNLL8V+Ar4AvgQ6BG2LACsC01vteR4Y8aMEXUUAgGRDz4Q2bmzYV4wKME//UnENp/XTwEXUpPukZoRPSTocolERYlcfrldPnv2oY9VUyNyxx0iP/iBiN8fufekIueJJ+zf2xiRrl1F3nmnrSM6vqxeLXLPPSIXXijSs6fIDTfY/8FjDPvI7JaV6S1d8XAn7HO8twG9gShgPTB4v3XOALyh338MvBy2rPxwj6kJI4I++UTkyScl+MgjUvWHW2Tf/42X/HPiZO8YJPuSKMn85Ofi85WK3HST/Vp99JHdrqBA5JprRKZMEfnXv0SqqkQ2bxYZPbohAd133+HHs22byF//avfXnLw8kWeeEamoOHDZ7t0iweDhH1dZPp9I794ip5wisnKlyKBB9m95xx1tHVn7UlMj8r3vifzqVyJFRXZeba3Ib35jEy2I9OsncuaZ9vff/rbl+y4oEHntNZEbbxSZNeuIQ2wvCeNbwKKw178Gfn2Q9UcBy8Nea8Jo54LBoJSWrpYNGy6SxYuRjz9OkU1rrpDqnglSmx4r+X+bKcHOKSJutz2DApHkZBGv1/584w2Ryy4TcblEVq068AB+v8h774k8/rhIYWHD/IULRRIT7f6+/W37jxNu61Z7thYd3bDO3r12WSAgcvvtdv6119p/3mPJ57PTwfznP7bmdcMNIrfcIvLss4d3jCNNhNnZtnBbvfrQ6z7/vP0M58+3r6uqbMwgcu+9TW+zb59NKP/+95HFdzSKiuxx//QnWwP+8Y9F1q6N/HFvvLGhFpaUZD+b8ePtvB/8oOF7GQw2fH5NfT75+SIXXyzSoYNIQoJIfHzDCVdsrMi55x5xTb29JIxLgafCXn8P+PtB1v87cEfYaz+wGlgJXNiSY2rCaDslJZ/J+vXnyiefdJcvnuwkQaf9Mpf1QTa9NFpysh6X2oWv2ctWs2aJ5OTYDffutZcz+vcXKS+3X/pPPxX55S9FunRp+Kfweu0/3x132H++ESNE/v53mxT69hVZt84WYmefLeJw2Etk118v8thj9vfBg0U2bhQ577yGJAIi06aJlJUd+IY++khkzBiR888XefllkcrKo/+QKitFTj1VpFcvkTVrDlxeUyNy880NibVTJ1sYNFWIFBSIPPKIyC9+ITJzpshpp9nPIT5eJCVFZNmyxuvv22drZH/7m611LVzYOFnm5zfUEjp0EPnss+bfRzAoMmyY/UzDL6EEAiJXXmn38cgjjd/Xww/b9wT27/Puu42XX3utPXmoOwuv89FH9mz87rttYf/vfzc+eWjqM9yxw/4UsX/be+9tOMEAkbg4+30Cezno88+bfo8bNthkPXu2yEUXiSxe3Pxxi4pE5swRefPNhoT973/bY/z85/b7efbZDZ/vyy83Hfvkyfb7+uqr9m8mYj+rzp3td/266+xJxOzZIn/8o8jy5Ud90nPcJQzgqlBiiA6blxH62RvYCfRpZtvrQ4lldffu3Y/qg1Ot6J//lNq7b5Od39wln346UBYvRhYvdsq6dVMlM/Mh2bPnVSkpWSmVlVul+p0XJGiMBIcPE+nY0X4tXS5bWL/6qi1cr73W1lRA5KqrGi4zLV9uC8i6wqBHD5tUcnMbYlm82J6V1e330UftP/WTT4o4nTYxvPWWLYSrq20hbIwt2OuSVlycSLdu9iwxKspu88ADjdt7DiYYFLniCrvfun/+//f/7PyyMpGPP7aXd+ragOoKPJ/PJoPYWJFNm+y8XbvsZQxoSJinn24T8S232OQbHy+yYoVdf8sWkQEDGj6jumnIENtmVVQkMmqUiMdjk27v3vbz+vTTpt/LggV2++eeO3CZz2cLYbDJsU+fhsL5rLPs32v4cFtobtpk/47TpjX8bfr1s/PLy0V+8hOpPzsPj9vptAXrXXeJzJ1rE+DDD4tMn96QYB0O+/eq+z5Nn24/j7qTg+Jikd//3sYBNln+4hf2Es9PfyrSvXvD8WJibPJ2OkX+8pcDa3DLl9vvXd36Z51lv7cxMSKTJjWuUX7yiUhWVvPfk8LChr8t2PdQ97f64otDfcuOSHtJGC26JAWcBWwCUg+yr2eASw91TK1htE/BYFDKytbJtm23y4oVfULJo/G042qkOgXZd15XKX78p+LfnXngjnJz7SWq/f9ht261//xLljTfaLh+vS009j9L/O9/bTKo+wetOxO94YaGGs+HH9pLGFdfbdtofvELkXHjGrZJTrYFxrBhttB+7rkDL5Pdf7/UX64pKBCZOtW+Dq9FJSSIvP76gbFnZ9ukOGSIPRvu1s0WdP/7X9OXn7KzbUHdoYNtmO7Y0ca4eLEtkLZtswVa7972uGlpNhnXNVpnZjYkjZkz7Xu//XZ7ff03v7E1ix49mj+zra62Z8JnnmmT5OzZdt91se7cKZKaahPdpEk2ITz5pMjSpbZgTkiwy0DkZz+zl7t8Pvv3+OwzG8PQoQcmwL59bZJ54gkb61VX2fg/+aTpOEVssnz4YVvI152QeDwiM2aI/OMfNnn5/SKlpfaSENh9PvusTfg//7lNJL162Vrd3/5mTyrqPtfwE5eWKisTWbTIfmdmzRL59a9bp4bbjPaSMFzAdqBXWKP3kP3WGRVqGO+33/ykutoGkAJs2b/BvKlJE0b7FwwGpbZ2r5SVrZOCggWSl/eM5Ob+U3Jz/yFbttwqy5eny+LFyNKl8fLll7Nkz55Xxe8vj2xQFRW28L3vPntJZcGClm23bZutZdx4o8j3vy9ywQW29lB3Vjx0qC0wf/5z+3rWrIZC0++3286cKfKHP9gaTn5+88datMjuwxhbqDZ1GSXcrl0N7Ub9+9taxv6qquxlja5d7Zl1uMxMe+bfv79NVk6nPbbTaWs1zzzTss+oOcuX25qa0ynywguN4x4zxp7h13WcaE5NjU2+27bZeI9Waakt9Ju6RCli/3b33WdrL+GJ6oorbI2lTmGhrf00ddmxHTqchBHRG/eMMecCD2N7TD0tIn80xtwdCvAtY8wHwDAgL7RJpojMMMZ8G/h/QBB7r8jDIvKPQx1Pb9w7/okEKC5eQn7+yxQW/gefrwBjXMTEDCAubjhe7yDc7k643cm43al4PD2Iju6Kw+Fu69CtYNDeQf/227BqFWzYAJmZcOqp9kmJXu+R7/tPf7KjFL/xRsseD7lzJzz9tH0+fHLykR83Uv73P3A67b0c4YJBO4UP69+e7N4NlZX2AWderx0x+jimd3qrE0Iw6KekZBlFRR9QUfEF5eUbqKnZ1cSaDjyeXnTseA4pKRfRocMkHI52VNiUlUFsrH0OsFLtjCYMdcIKBKrx+/fh8+3D59tDdfUuqqt3UF6+nqKi9wkGq3E6E/B4ehAV1YXo6C54PD3weHoRE9OH+PhT2k9tRKl24HASRjs6DVPq0JxOD06nTQTQ+DGJgUAF+/a9R1HRB9TU5FBbm0tFxQZqa/MAe2LkdnciNfUKOnW6lNraPEpLP6OiYj0uV3J9UklMPF0faatUE7SGoU54wWAN1dWZVFRsID//RQoL30IkNLiiiSY2diiBQCnV1TsR8QHg9Q4mJeUCvN7BuN0puN0pxMT00aHf1QlHaxhKhXE4ovF6++H19qNTp4vx+YopLv4Ij6cHsbHDcDjss3lFAlRVbWffvncoLPwPmZkPYGIA+bwAAAx8SURBVIc0axAVlUFs7FDi4kYQFzeSuLhRxMT00ctc6qSgNQylmuH3l1FbuxufrxCfL5/Kyq+pqNhIRcUGKiq+rK+NgMHt7kR0dJf6dpOoqC643ck4HLE4nbFERXXG6x1AVFQXfbKhale0hqFUK3C54nG54oED2zOCwVoqKzdRXr6O6uqd1NTkhtpN8igvX0tt7R7q2k3COZ1xxMWNIjl5GsnJ04iLG4kxjXtPBQJVQBCnMzYyb0ypI6Q1DKUiIBj0EwiUEghUEAiUU1OTQ1XVN1RWbqakZDnl5WsBMCaKqKhU3O7OGGOors7E58sHHMTHj6ZDh0nExPSmqmoH1dXb/n979x5b91nfcfz9+Z27L7XjOLclreM0WWnaQVsa1I2OVXR/tAzRSutGN8oQAvWfTsC0iVG0i4Y0aUxoHdM6Bips7ShQ1pUu2x8DFlAp0noJFNqSUhFI1jhrbCdx7Ph2rt/98Tx2T92EnCY+sfM735dkxb/L+fl58tjne547AJs23cnAwE1eU3HLwmsYzq2wJMmSJAPkcmHCXHf35YRVcIJKZZTjx7/BzMzzVKtjVCqjmDUYHLyGYvESGo15Tpx4nMOH78WsTJIUKRa3UatNcPToo3R17WT9+tupVo9RLr9EvT5Nb+8u+vqup7f3WjKZHpKk8Jrai3PnwmsYzq1iC/NO8vmNSAmNRoWxsYc4dOhTzMw8S5J0UywOkSQFpqefZWknPWSQskhZkqRAobCZQuESisWLyec3UyhsirWbDNDArE69fpJabZJ6fZb+/rfR27vLazMp5jUM51JiYd7JgiTJs3Hje9mw4Q7q9ZNkMr2Lb+b1+gxTU08yPf0sjcY8ZmUajTJmdczqNBpzlMsjlMsvMTX1P9Rqx1tKQ6l0GRs23EFPz1UUCpvI5zeSyfSRyXQBYnb2RSYnv8PU1BP09u5i06YP+qixlPIahnMdqtEoU6kcaeqgT5ASMpkeMpk+pISjR3czOvoAk5OPn/IZUm5xtFgmcxH1+hSl0mVceulfMzDwDswqNBoVqtXR2A9zkEymh4su2kWptMObzFYBXxrEObesKpVR5ucPxgBzhFotdOg3GnOUSjvo7/81SqXtHDv2H/z0px9lbu7FMz4zk+kjn9+4GFSy2f7F+S3F4iVN9/XS1fUGisUhIKFcfonp6eeo1yfp7r6Srq7LF+fSuNfPm6Scc8sqn99APr/hjPcNDr6LgYGbGR19kHJ5hCTJIeXI5dZTKg1TKAxRq01w8uRTTE09Ta12giTJI+WpVseYnHycsbEvnfLZUoEkKVCvTy05n6NU2k4uN0g2O0Aut3Zx5FkutzZ2/udIkjxJUopfxXguh5Qnm+0jm+2LfTnudLyG4ZxbVarV41QqRwDF42PMzb3I7OyPqddn6e7+JXp63kg228f09HPMzPyQ2dmfNC1KOU61Ot40sbJ1mcxFZLNryOXWkM2uiQtXblscuVatHqdenyKbXUuhsIVCYQuSaDRCf1GhsIWursuaVg9oUK2OkyTdZLM9P/dnNxpl5uYOMDe3n2JxKz09V/7c+5eL1zCccxessNfJq/fv6O+//pT3dndfAdz+mvNmRq02QbV6DLMqjUYlDgKYb/qqxmtl6vVJqtUJarUJarUTi689fvybVCqHlzw9w2tHo70i7N/yizQaZcrlQ4vrluVygxSLw2Sz/Uih5tVozFCpjFGtjscg+coH+MHBWxka+jN6e69ezFO1eoz5+YPMzx+gVpuII+AyJEk369ffdsb/23PlAcM5lzqSThl4zka9Pk+5PEImUyKbHSBJinGxykNUKv8HaLHZa37+QFw+Zh+ZTIlC4TcpFLZQr08zPx86/ev1k4vBKpPppljcSm/vLgqFLZRK2ymVhpmY2MPIyD0cPfoohcLQ4lDn0wWqfH7jeQkY3iTlnHOrUK02yeHD9zIzs2+xjyWXWxeX4R8mm11LmDtTA6BU2nZWP2fVNElJugn4NKEOd5+Z/dWS6wXgAeDNwDHg3WZ2MF67G/gAIaR+yMy+3s60OufcapLN9jE09PGVTsartG0QtMJwg3uBm4GdwO9I2rnktg8AE2a2HbgH+GR87U5Cw+QVwE3AP8iHLzjn3Ipq56yZtwD7zexnFnp9vgLcsuSeW4D74/cPAzcqTFu9BfiKmZXN7ACwPz7POefcCmlnwNgMHGo6HonnTnmPhYa4SWBti691zjl3Hl3w8/Il3Slpr6S94+PjK50c55xLrXYGjMPAxU3HW+K5U94jKQv0ETq/W3ktAGb2OTO71syuXbdu3TIl3Tnn3FLtDBhPAzskDUvKEzqxdy+5Zzfwvvj9bcC3LIzz3Q3cLqkgaZiw5dlTbUyrc865M2jbsFozq0n6feDrhGG1XzCzH0n6BLDXzHYDnwf+RdJ+4Dhxyma876vAPqAG3GVmp59a6Zxzru184p5zznWwjl3eXNI48L9n+fJB4OgyJudC4HlOv07LL3ieX68hM2upAzhVAeNcSNrbapRNC89z+nVafsHz3E4X/LBa55xz54cHDOeccy3xgPGKz610AlaA5zn9Oi2/4HluG+/DcM451xKvYTjnnGtJxwcMSTdJelHSfkkfW+n0tIOkiyV9W9I+ST+S9OF4fkDSNyX9JP67ZqXTutwkZSQ9I+k/4/GwpCdjeT8UVyFIDUn9kh6W9GNJL0j65bSXs6Q/iL/Xz0v6sqRi2spZ0hckjUl6vuncKctVwd/FvD8r6ZrlSkdHB4wW9+xIgxrwh2a2E7gOuCvm82PAHjPbAeyJx2nzYeCFpuNPAvfEPVgmCHuypMmngf8yszcAbyLkPbXlLGkz8CHgWjO7krCqxO2kr5z/mbA3ULPTlevNhOWUdgB3Ap9ZrkR0dMCgtT07Lnhm9rKZfT9+f5LwJrKZV+9Hcj9w68qksD0kbQF+A7gvHgt4O2HvFUhZniX1AW8jLLmDmVXM7AQpL2fCEkeluIBpF/AyKStnM/sOYfmkZqcr11uAByx4AuiXtGk50tHpAaPj9t2QtBW4GngS2GBmL8dLR4ANK5Ssdvlb4KNAIx6vBU7YwibI6SvvYWAc+KfYDHefpG5SXM5mdhj4FPASIVBMAt8j3eW84HTl2rb3tU4PGB1FUg/wb8BHzGyq+VpcJTg1Q+YkvRMYM7PvrXRazqMscA3wGTO7GphhSfNTCst5DeET9TDwC0A3r226Sb3zVa6dHjBa3nfjQicpRwgWD5rZI/H06EJVNf47tlLpa4O3Au+SdJDQ1Ph2Qvt+f2y6gPSV9wgwYmZPxuOHCQEkzeX868ABMxs3syrwCKHs01zOC05Xrm17X+v0gNHKnh0XvNh2/3ngBTP7m6ZLzfuRvA/49/OdtnYxs7vNbIuZbSWU67fM7D3Atwl7r0D68nwEOCTpsnjqRsIWAaktZ0JT1HWSuuLv+UKeU1vOTU5XrruB34ujpa4DJpuars5Jx0/ck/QOQlv3wp4df7nCSVp2kq4HHgee45X2/I8T+jG+ClxCWOX3t81sacfaBU/SDcAfmdk7JW0j1DgGgGeAO8ysvJLpW06SriJ08ueBnwHvJ3wwTG05S/oL4N2E0YDPAB8ktNmnppwlfRm4gbAq7Sjw58CjnKJcY+D8e0LT3CzwfjNbln0fOj5gOOeca02nN0k555xrkQcM55xzLfGA4ZxzriUeMJxzzrXEA4ZzzrmWeMBwbhWQdMPCirrOrVYeMJxzzrXEA4Zzr4OkOyQ9JekHkj4b99uYlnRP3JNhj6R18d6rJD0R9yT4WtN+Bdsl/bekH0r6vqRL4+N7mvayeDBOwHJu1fCA4VyLJF1OmFH8VjO7CqgD7yEseLfXzK4AHiPMwgV4APhjM3sjYZb9wvkHgXvN7E3ArxBWWYWwivBHCHuzbCOsieTcqpE98y3OuehG4M3A0/HDf4mw4FsDeCje80Xgkbg3Rb+ZPRbP3w/8q6ReYLOZfQ3AzOYB4vOeMrORePwDYCvw3fZny7nWeMBwrnUC7jezu191UvrTJfed7Xo7zWsd1fG/T7fKeJOUc63bA9wmaT0s7qk8RPg7WlgZ9XeB75rZJDAh6Vfj+fcCj8UdD0ck3RqfUZDUdV5z4dxZ8k8wzrXIzPZJ+hPgG5ISoArcRdio6C3x2hihnwPCktP/GAPCwsqxEILHZyV9Ij7jt85jNpw7a75arXPnSNK0mfWsdDqcazdvknLOOdcSr2E455xridcwnHPOtcQDhnPOuZZ4wHDOOdcSDxjOOeda4gHDOedcSzxgOOeca8n/AwhUbL9eTi9bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2559 - acc: 0.9246\n",
      "Loss: 0.255949091651358 Accuracy: 0.9246106\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9407 - acc: 0.3754\n",
      "Epoch 00001: val_loss improved from inf to 1.44203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/001-1.4420.hdf5\n",
      "36805/36805 [==============================] - 101s 3ms/sample - loss: 1.9405 - acc: 0.3754 - val_loss: 1.4420 - val_acc: 0.5549\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3642 - acc: 0.5751\n",
      "Epoch 00002: val_loss improved from 1.44203 to 1.00884, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/002-1.0088.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.3643 - acc: 0.5750 - val_loss: 1.0088 - val_acc: 0.7058\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0525 - acc: 0.6819\n",
      "Epoch 00003: val_loss improved from 1.00884 to 0.77729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/003-0.7773.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0524 - acc: 0.6819 - val_loss: 0.7773 - val_acc: 0.7796\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8311 - acc: 0.7537\n",
      "Epoch 00004: val_loss improved from 0.77729 to 0.59419, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/004-0.5942.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8311 - acc: 0.7537 - val_loss: 0.5942 - val_acc: 0.8432\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.8016\n",
      "Epoch 00005: val_loss improved from 0.59419 to 0.48984, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/005-0.4898.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6796 - acc: 0.8017 - val_loss: 0.4898 - val_acc: 0.8623\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8315\n",
      "Epoch 00006: val_loss improved from 0.48984 to 0.42100, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/006-0.4210.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5740 - acc: 0.8316 - val_loss: 0.4210 - val_acc: 0.8880\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8552\n",
      "Epoch 00007: val_loss improved from 0.42100 to 0.40283, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/007-0.4028.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5008 - acc: 0.8552 - val_loss: 0.4028 - val_acc: 0.8891\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8698\n",
      "Epoch 00008: val_loss improved from 0.40283 to 0.33247, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/008-0.3325.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4523 - acc: 0.8697 - val_loss: 0.3325 - val_acc: 0.9103\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8858\n",
      "Epoch 00009: val_loss improved from 0.33247 to 0.28974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/009-0.2897.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3978 - acc: 0.8858 - val_loss: 0.2897 - val_acc: 0.9236\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8953\n",
      "Epoch 00010: val_loss improved from 0.28974 to 0.28536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/010-0.2854.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3649 - acc: 0.8953 - val_loss: 0.2854 - val_acc: 0.9196\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.9018\n",
      "Epoch 00011: val_loss improved from 0.28536 to 0.25133, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/011-0.2513.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3361 - acc: 0.9018 - val_loss: 0.2513 - val_acc: 0.9322\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9086\n",
      "Epoch 00012: val_loss improved from 0.25133 to 0.24324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/012-0.2432.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3143 - acc: 0.9085 - val_loss: 0.2432 - val_acc: 0.9327\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9144\n",
      "Epoch 00013: val_loss improved from 0.24324 to 0.22037, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/013-0.2204.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2945 - acc: 0.9144 - val_loss: 0.2204 - val_acc: 0.9392\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9202\n",
      "Epoch 00014: val_loss improved from 0.22037 to 0.21418, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/014-0.2142.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2739 - acc: 0.9202 - val_loss: 0.2142 - val_acc: 0.9371\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9234\n",
      "Epoch 00015: val_loss improved from 0.21418 to 0.19184, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/015-0.1918.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2590 - acc: 0.9234 - val_loss: 0.1918 - val_acc: 0.9490\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9275\n",
      "Epoch 00016: val_loss did not improve from 0.19184\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2456 - acc: 0.9275 - val_loss: 0.2100 - val_acc: 0.9397\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9304\n",
      "Epoch 00017: val_loss did not improve from 0.19184\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2356 - acc: 0.9304 - val_loss: 0.1936 - val_acc: 0.9471\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9346\n",
      "Epoch 00018: val_loss improved from 0.19184 to 0.18846, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/018-0.1885.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2217 - acc: 0.9346 - val_loss: 0.1885 - val_acc: 0.9481\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9388\n",
      "Epoch 00019: val_loss improved from 0.18846 to 0.17360, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/019-0.1736.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2091 - acc: 0.9388 - val_loss: 0.1736 - val_acc: 0.9536\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9414\n",
      "Epoch 00020: val_loss did not improve from 0.17360\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2018 - acc: 0.9414 - val_loss: 0.1801 - val_acc: 0.9464\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9416\n",
      "Epoch 00021: val_loss improved from 0.17360 to 0.16294, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/021-0.1629.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1946 - acc: 0.9416 - val_loss: 0.1629 - val_acc: 0.9536\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9460\n",
      "Epoch 00022: val_loss did not improve from 0.16294\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1846 - acc: 0.9460 - val_loss: 0.1720 - val_acc: 0.9474\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9457\n",
      "Epoch 00023: val_loss did not improve from 0.16294\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1799 - acc: 0.9457 - val_loss: 0.1659 - val_acc: 0.9525\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9484\n",
      "Epoch 00024: val_loss did not improve from 0.16294\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1724 - acc: 0.9484 - val_loss: 0.1747 - val_acc: 0.9499\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9511\n",
      "Epoch 00025: val_loss did not improve from 0.16294\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1680 - acc: 0.9511 - val_loss: 0.1666 - val_acc: 0.9546\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9523\n",
      "Epoch 00026: val_loss improved from 0.16294 to 0.14459, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/026-0.1446.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1599 - acc: 0.9523 - val_loss: 0.1446 - val_acc: 0.9555\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9541\n",
      "Epoch 00027: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1516 - acc: 0.9541 - val_loss: 0.1632 - val_acc: 0.9555\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9563\n",
      "Epoch 00028: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1457 - acc: 0.9563 - val_loss: 0.1543 - val_acc: 0.9557\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9581\n",
      "Epoch 00029: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1420 - acc: 0.9581 - val_loss: 0.1577 - val_acc: 0.9557\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9591\n",
      "Epoch 00030: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1376 - acc: 0.9591 - val_loss: 0.1566 - val_acc: 0.9564\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9589\n",
      "Epoch 00031: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1366 - acc: 0.9589 - val_loss: 0.1805 - val_acc: 0.9504\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9606\n",
      "Epoch 00032: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1293 - acc: 0.9606 - val_loss: 0.1460 - val_acc: 0.9560\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9632\n",
      "Epoch 00033: val_loss did not improve from 0.14459\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1238 - acc: 0.9631 - val_loss: 0.1555 - val_acc: 0.9543\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9636\n",
      "Epoch 00034: val_loss improved from 0.14459 to 0.13936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/034-0.1394.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1204 - acc: 0.9636 - val_loss: 0.1394 - val_acc: 0.9606\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9646\n",
      "Epoch 00035: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1162 - acc: 0.9646 - val_loss: 0.1628 - val_acc: 0.9532\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9666\n",
      "Epoch 00036: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1112 - acc: 0.9666 - val_loss: 0.1440 - val_acc: 0.9574\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9678\n",
      "Epoch 00037: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1077 - acc: 0.9678 - val_loss: 0.1621 - val_acc: 0.9536\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9685\n",
      "Epoch 00038: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1027 - acc: 0.9685 - val_loss: 0.1639 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9668\n",
      "Epoch 00039: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1106 - acc: 0.9668 - val_loss: 0.1546 - val_acc: 0.9543\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9711\n",
      "Epoch 00040: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0937 - acc: 0.9711 - val_loss: 0.1475 - val_acc: 0.9550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9700\n",
      "Epoch 00041: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0939 - acc: 0.9700 - val_loss: 0.1578 - val_acc: 0.9562\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9739\n",
      "Epoch 00042: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0876 - acc: 0.9739 - val_loss: 0.1404 - val_acc: 0.9592\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9744\n",
      "Epoch 00043: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0839 - acc: 0.9744 - val_loss: 0.1506 - val_acc: 0.9550\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9769\n",
      "Epoch 00044: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0777 - acc: 0.9769 - val_loss: 0.1438 - val_acc: 0.9585\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9765\n",
      "Epoch 00045: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0776 - acc: 0.9765 - val_loss: 0.1581 - val_acc: 0.9562\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9750\n",
      "Epoch 00046: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0799 - acc: 0.9750 - val_loss: 0.1622 - val_acc: 0.9543\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9777\n",
      "Epoch 00047: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0732 - acc: 0.9777 - val_loss: 0.1887 - val_acc: 0.9483\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9778\n",
      "Epoch 00048: val_loss did not improve from 0.13936\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0732 - acc: 0.9778 - val_loss: 0.1485 - val_acc: 0.9562\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9789\n",
      "Epoch 00049: val_loss improved from 0.13936 to 0.13625, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv_checkpoint/049-0.1363.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0692 - acc: 0.9789 - val_loss: 0.1363 - val_acc: 0.9578\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9811\n",
      "Epoch 00050: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0619 - acc: 0.9811 - val_loss: 0.1640 - val_acc: 0.9520\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9832\n",
      "Epoch 00051: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0560 - acc: 0.9832 - val_loss: 0.1475 - val_acc: 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9817\n",
      "Epoch 00052: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0592 - acc: 0.9817 - val_loss: 0.1560 - val_acc: 0.9550\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9848\n",
      "Epoch 00053: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0527 - acc: 0.9848 - val_loss: 0.1458 - val_acc: 0.9546\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9840\n",
      "Epoch 00054: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0519 - acc: 0.9841 - val_loss: 0.1629 - val_acc: 0.9525\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9830\n",
      "Epoch 00055: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0558 - acc: 0.9830 - val_loss: 0.1583 - val_acc: 0.9553\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9847\n",
      "Epoch 00056: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0496 - acc: 0.9847 - val_loss: 0.1788 - val_acc: 0.9497\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9858\n",
      "Epoch 00057: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0474 - acc: 0.9858 - val_loss: 0.1635 - val_acc: 0.9564\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9873\n",
      "Epoch 00058: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0448 - acc: 0.9873 - val_loss: 0.1521 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9886\n",
      "Epoch 00059: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0398 - acc: 0.9886 - val_loss: 0.1742 - val_acc: 0.9527\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9860\n",
      "Epoch 00060: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0463 - acc: 0.9860 - val_loss: 0.1689 - val_acc: 0.9555\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9895\n",
      "Epoch 00061: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0364 - acc: 0.9895 - val_loss: 0.1528 - val_acc: 0.9597\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9869\n",
      "Epoch 00062: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0424 - acc: 0.9869 - val_loss: 0.1681 - val_acc: 0.9567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9888\n",
      "Epoch 00063: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0370 - acc: 0.9888 - val_loss: 0.1647 - val_acc: 0.9583\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9900\n",
      "Epoch 00064: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0323 - acc: 0.9900 - val_loss: 0.1647 - val_acc: 0.9590\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9893\n",
      "Epoch 00065: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0368 - acc: 0.9892 - val_loss: 0.2189 - val_acc: 0.9464\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9877\n",
      "Epoch 00066: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0428 - acc: 0.9877 - val_loss: 0.1620 - val_acc: 0.9571\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9916\n",
      "Epoch 00067: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0292 - acc: 0.9916 - val_loss: 0.1771 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9916\n",
      "Epoch 00068: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0303 - acc: 0.9916 - val_loss: 0.1688 - val_acc: 0.9553\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9926\n",
      "Epoch 00069: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0281 - acc: 0.9926 - val_loss: 0.1826 - val_acc: 0.9525\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9905\n",
      "Epoch 00070: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0335 - acc: 0.9905 - val_loss: 0.1729 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9923\n",
      "Epoch 00071: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0263 - acc: 0.9923 - val_loss: 0.2556 - val_acc: 0.9387\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9935\n",
      "Epoch 00072: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0241 - acc: 0.9935 - val_loss: 0.1752 - val_acc: 0.9571\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9919\n",
      "Epoch 00073: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0282 - acc: 0.9919 - val_loss: 0.1968 - val_acc: 0.9515\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9926\n",
      "Epoch 00074: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0246 - acc: 0.9926 - val_loss: 0.1657 - val_acc: 0.9583\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9925\n",
      "Epoch 00075: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0265 - acc: 0.9925 - val_loss: 0.1724 - val_acc: 0.9574\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9927\n",
      "Epoch 00076: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0241 - acc: 0.9927 - val_loss: 0.1923 - val_acc: 0.9529\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9939\n",
      "Epoch 00077: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0209 - acc: 0.9939 - val_loss: 0.1963 - val_acc: 0.9541\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9935\n",
      "Epoch 00078: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0227 - acc: 0.9935 - val_loss: 0.1899 - val_acc: 0.9560\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9929\n",
      "Epoch 00079: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9929 - val_loss: 0.1887 - val_acc: 0.9562\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9937\n",
      "Epoch 00080: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0233 - acc: 0.9937 - val_loss: 0.1841 - val_acc: 0.9562\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9965\n",
      "Epoch 00081: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0151 - acc: 0.9965 - val_loss: 0.1830 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9945\n",
      "Epoch 00082: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0200 - acc: 0.9945 - val_loss: 0.2216 - val_acc: 0.9502\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9923\n",
      "Epoch 00083: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0267 - acc: 0.9923 - val_loss: 0.1861 - val_acc: 0.9557\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9959\n",
      "Epoch 00084: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0162 - acc: 0.9959 - val_loss: 0.1819 - val_acc: 0.9567\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9932\n",
      "Epoch 00085: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0216 - acc: 0.9932 - val_loss: 0.2027 - val_acc: 0.9557\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00086: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.1673 - val_acc: 0.9585\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9966\n",
      "Epoch 00087: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0133 - acc: 0.9966 - val_loss: 0.2056 - val_acc: 0.9515\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9933\n",
      "Epoch 00088: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0219 - acc: 0.9933 - val_loss: 0.1770 - val_acc: 0.9543\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9943\n",
      "Epoch 00089: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.1695 - val_acc: 0.9597\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9939\n",
      "Epoch 00090: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0209 - acc: 0.9939 - val_loss: 0.2159 - val_acc: 0.9529\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9957\n",
      "Epoch 00091: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.1837 - val_acc: 0.9581\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9972\n",
      "Epoch 00092: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0118 - acc: 0.9972 - val_loss: 0.1736 - val_acc: 0.9602\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9971\n",
      "Epoch 00093: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0122 - acc: 0.9971 - val_loss: 0.2017 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9937\n",
      "Epoch 00094: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0215 - acc: 0.9938 - val_loss: 0.2109 - val_acc: 0.9539\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9956\n",
      "Epoch 00095: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0150 - acc: 0.9956 - val_loss: 0.2078 - val_acc: 0.9520\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9929\n",
      "Epoch 00096: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0221 - acc: 0.9929 - val_loss: 0.1983 - val_acc: 0.9532\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00097: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.1890 - val_acc: 0.9574\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9977\n",
      "Epoch 00098: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0097 - acc: 0.9977 - val_loss: 0.1904 - val_acc: 0.9560\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 00099: val_loss did not improve from 0.13625\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0226 - acc: 0.9927 - val_loss: 0.2092 - val_acc: 0.9529\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1810 - acc: 0.9487\n",
      "Loss: 0.18097060811884802 Accuracy: 0.948702\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5759 - acc: 0.5020\n",
      "Epoch 00001: val_loss improved from inf to 0.89477, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/001-0.8948.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 1.5759 - acc: 0.5020 - val_loss: 0.8948 - val_acc: 0.7421\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.7448\n",
      "Epoch 00002: val_loss improved from 0.89477 to 0.51750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/002-0.5175.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8338 - acc: 0.7449 - val_loss: 0.5175 - val_acc: 0.8526\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5584 - acc: 0.8336\n",
      "Epoch 00003: val_loss improved from 0.51750 to 0.36491, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/003-0.3649.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5583 - acc: 0.8336 - val_loss: 0.3649 - val_acc: 0.8970\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8746\n",
      "Epoch 00004: val_loss improved from 0.36491 to 0.28305, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/004-0.2830.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4231 - acc: 0.8746 - val_loss: 0.2830 - val_acc: 0.9222\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8989\n",
      "Epoch 00005: val_loss improved from 0.28305 to 0.25475, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/005-0.2548.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3445 - acc: 0.8989 - val_loss: 0.2548 - val_acc: 0.9271\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.9122\n",
      "Epoch 00006: val_loss improved from 0.25475 to 0.21490, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/006-0.2149.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2942 - acc: 0.9122 - val_loss: 0.2149 - val_acc: 0.9383\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.9200\n",
      "Epoch 00007: val_loss improved from 0.21490 to 0.19734, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/007-0.1973.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2690 - acc: 0.9200 - val_loss: 0.1973 - val_acc: 0.9429\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9321\n",
      "Epoch 00008: val_loss improved from 0.19734 to 0.17612, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/008-0.1761.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2316 - acc: 0.9322 - val_loss: 0.1761 - val_acc: 0.9511\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9369\n",
      "Epoch 00009: val_loss did not improve from 0.17612\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2105 - acc: 0.9369 - val_loss: 0.1848 - val_acc: 0.9467\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9409\n",
      "Epoch 00010: val_loss improved from 0.17612 to 0.16705, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/010-0.1671.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1960 - acc: 0.9409 - val_loss: 0.1671 - val_acc: 0.9492\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9468\n",
      "Epoch 00011: val_loss improved from 0.16705 to 0.15637, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/011-0.1564.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1781 - acc: 0.9468 - val_loss: 0.1564 - val_acc: 0.9553\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9494\n",
      "Epoch 00012: val_loss improved from 0.15637 to 0.15093, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/012-0.1509.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1678 - acc: 0.9494 - val_loss: 0.1509 - val_acc: 0.9557\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9535\n",
      "Epoch 00013: val_loss improved from 0.15093 to 0.14454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/013-0.1445.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1562 - acc: 0.9535 - val_loss: 0.1445 - val_acc: 0.9576\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9567\n",
      "Epoch 00014: val_loss did not improve from 0.14454\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1469 - acc: 0.9566 - val_loss: 0.1737 - val_acc: 0.9471\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9568\n",
      "Epoch 00015: val_loss improved from 0.14454 to 0.13179, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/015-0.1318.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1415 - acc: 0.9569 - val_loss: 0.1318 - val_acc: 0.9627\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9609\n",
      "Epoch 00016: val_loss did not improve from 0.13179\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1296 - acc: 0.9609 - val_loss: 0.1368 - val_acc: 0.9599\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9643\n",
      "Epoch 00017: val_loss improved from 0.13179 to 0.12949, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/017-0.1295.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1208 - acc: 0.9643 - val_loss: 0.1295 - val_acc: 0.9616\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9660\n",
      "Epoch 00018: val_loss improved from 0.12949 to 0.12740, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/018-0.1274.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1123 - acc: 0.9660 - val_loss: 0.1274 - val_acc: 0.9634\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9681\n",
      "Epoch 00019: val_loss improved from 0.12740 to 0.12337, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/019-0.1234.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1064 - acc: 0.9681 - val_loss: 0.1234 - val_acc: 0.9641\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9697\n",
      "Epoch 00020: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0995 - acc: 0.9697 - val_loss: 0.1311 - val_acc: 0.9595\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9707\n",
      "Epoch 00021: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0949 - acc: 0.9707 - val_loss: 0.1266 - val_acc: 0.9632\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9735\n",
      "Epoch 00022: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0870 - acc: 0.9735 - val_loss: 0.1301 - val_acc: 0.9623\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9730\n",
      "Epoch 00023: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0861 - acc: 0.9730 - val_loss: 0.1345 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9753\n",
      "Epoch 00024: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0788 - acc: 0.9753 - val_loss: 0.1293 - val_acc: 0.9627\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9782\n",
      "Epoch 00025: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0714 - acc: 0.9782 - val_loss: 0.1486 - val_acc: 0.9581\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9787\n",
      "Epoch 00026: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0713 - acc: 0.9787 - val_loss: 0.1307 - val_acc: 0.9604\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9805\n",
      "Epoch 00027: val_loss did not improve from 0.12337\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0641 - acc: 0.9805 - val_loss: 0.1285 - val_acc: 0.9616\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9807\n",
      "Epoch 00028: val_loss improved from 0.12337 to 0.12306, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/028-0.1231.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0628 - acc: 0.9807 - val_loss: 0.1231 - val_acc: 0.9616\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9825\n",
      "Epoch 00029: val_loss improved from 0.12306 to 0.12275, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv_checkpoint/029-0.1227.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0581 - acc: 0.9825 - val_loss: 0.1227 - val_acc: 0.9665\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9836\n",
      "Epoch 00030: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0543 - acc: 0.9836 - val_loss: 0.1393 - val_acc: 0.9595\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9847\n",
      "Epoch 00031: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0507 - acc: 0.9847 - val_loss: 0.1351 - val_acc: 0.9595\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9854\n",
      "Epoch 00032: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0470 - acc: 0.9854 - val_loss: 0.1313 - val_acc: 0.9644\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9856\n",
      "Epoch 00033: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0468 - acc: 0.9856 - val_loss: 0.1399 - val_acc: 0.9632\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9862\n",
      "Epoch 00034: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0430 - acc: 0.9862 - val_loss: 0.1394 - val_acc: 0.9609\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9881\n",
      "Epoch 00035: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0390 - acc: 0.9881 - val_loss: 0.1324 - val_acc: 0.9651\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9886\n",
      "Epoch 00036: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0380 - acc: 0.9886 - val_loss: 0.1417 - val_acc: 0.9604\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9902\n",
      "Epoch 00037: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0333 - acc: 0.9902 - val_loss: 0.1470 - val_acc: 0.9609\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9904\n",
      "Epoch 00038: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0325 - acc: 0.9904 - val_loss: 0.1305 - val_acc: 0.9660\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9911\n",
      "Epoch 00039: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0296 - acc: 0.9911 - val_loss: 0.1498 - val_acc: 0.9625\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9901\n",
      "Epoch 00040: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0312 - acc: 0.9901 - val_loss: 0.1401 - val_acc: 0.9616\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9916\n",
      "Epoch 00041: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0270 - acc: 0.9916 - val_loss: 0.1538 - val_acc: 0.9618\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 00042: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0231 - acc: 0.9934 - val_loss: 0.1509 - val_acc: 0.9604\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9920\n",
      "Epoch 00043: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0270 - acc: 0.9920 - val_loss: 0.1415 - val_acc: 0.9632\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9931\n",
      "Epoch 00044: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0233 - acc: 0.9931 - val_loss: 0.1478 - val_acc: 0.9611\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 00045: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0240 - acc: 0.9928 - val_loss: 0.1528 - val_acc: 0.9627\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9954\n",
      "Epoch 00046: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0173 - acc: 0.9954 - val_loss: 0.1547 - val_acc: 0.9602\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00047: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0164 - acc: 0.9954 - val_loss: 0.1302 - val_acc: 0.9679\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9931\n",
      "Epoch 00048: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0221 - acc: 0.9931 - val_loss: 0.1572 - val_acc: 0.9637\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9937\n",
      "Epoch 00049: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0203 - acc: 0.9938 - val_loss: 0.1339 - val_acc: 0.9660\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9958\n",
      "Epoch 00050: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0159 - acc: 0.9958 - val_loss: 0.1519 - val_acc: 0.9651\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0178 - acc: 0.9946 - val_loss: 0.1584 - val_acc: 0.9611\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9945\n",
      "Epoch 00052: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0182 - acc: 0.9945 - val_loss: 0.1683 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9958\n",
      "Epoch 00053: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0146 - acc: 0.9958 - val_loss: 0.1568 - val_acc: 0.9646\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0146 - acc: 0.9955 - val_loss: 0.1630 - val_acc: 0.9632\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9957\n",
      "Epoch 00055: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0155 - acc: 0.9957 - val_loss: 0.1881 - val_acc: 0.9574\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9936\n",
      "Epoch 00056: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0203 - acc: 0.9936 - val_loss: 0.1670 - val_acc: 0.9616\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00057: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0093 - acc: 0.9975 - val_loss: 0.1573 - val_acc: 0.9646\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0171 - acc: 0.9948 - val_loss: 0.1481 - val_acc: 0.9672\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0124 - acc: 0.9964 - val_loss: 0.1805 - val_acc: 0.9583\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9949\n",
      "Epoch 00060: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0171 - acc: 0.9949 - val_loss: 0.1913 - val_acc: 0.9597\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9952\n",
      "Epoch 00061: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0153 - acc: 0.9952 - val_loss: 0.1698 - val_acc: 0.9658\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9974\n",
      "Epoch 00062: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0098 - acc: 0.9974 - val_loss: 0.1564 - val_acc: 0.9667\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9977\n",
      "Epoch 00063: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0078 - acc: 0.9977 - val_loss: 0.1809 - val_acc: 0.9613\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9944\n",
      "Epoch 00064: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0182 - acc: 0.9944 - val_loss: 0.1854 - val_acc: 0.9609\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9968\n",
      "Epoch 00065: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0112 - acc: 0.9968 - val_loss: 0.1641 - val_acc: 0.9646\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 00066: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.1644 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0125 - acc: 0.9962 - val_loss: 0.1854 - val_acc: 0.9585\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9971\n",
      "Epoch 00068: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0104 - acc: 0.9971 - val_loss: 0.1729 - val_acc: 0.9625\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9958\n",
      "Epoch 00069: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0137 - acc: 0.9958 - val_loss: 0.1499 - val_acc: 0.9651\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9962\n",
      "Epoch 00070: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0131 - acc: 0.9962 - val_loss: 0.1885 - val_acc: 0.9590\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00071: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0112 - acc: 0.9965 - val_loss: 0.1715 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00072: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1757 - val_acc: 0.9623\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 00073: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0165 - acc: 0.9947 - val_loss: 0.1637 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9979\n",
      "Epoch 00074: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0077 - acc: 0.9979 - val_loss: 0.1735 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00075: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.1607 - val_acc: 0.9653\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9962\n",
      "Epoch 00076: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0124 - acc: 0.9963 - val_loss: 0.1774 - val_acc: 0.9653\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9964\n",
      "Epoch 00077: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0123 - acc: 0.9964 - val_loss: 0.1599 - val_acc: 0.9662\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980\n",
      "Epoch 00078: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0071 - acc: 0.9980 - val_loss: 0.1611 - val_acc: 0.9660\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 0.12275\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0137 - acc: 0.9954 - val_loss: 0.2027 - val_acc: 0.9571\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1803 - acc: 0.9516\n",
      "Loss: 0.18033145975131978 Accuracy: 0.95160955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_conv_3_VGG_tanh_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 584us/sample - loss: 2.7204 - acc: 0.1001\n",
      "Loss: 2.7204171821954715 Accuracy: 0.10010384\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 918us/sample - loss: 2.0918 - acc: 0.3421\n",
      "Loss: 2.09183182830256 Accuracy: 0.34205607\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.5243 - acc: 0.5333\n",
      "Loss: 1.524274631452709 Accuracy: 0.53333336\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.0789 - acc: 0.6908\n",
      "Loss: 1.0788706494764129 Accuracy: 0.69075805\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8261 - acc: 0.7659\n",
      "Loss: 0.8260591631985403 Accuracy: 0.7659398\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4952 - acc: 0.8627\n",
      "Loss: 0.4951543708085271 Accuracy: 0.86272067\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2559 - acc: 0.9246\n",
      "Loss: 0.255949091651358 Accuracy: 0.9246106\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1810 - acc: 0.9487\n",
      "Loss: 0.18097060811884802 Accuracy: 0.948702\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1803 - acc: 0.9516\n",
      "Loss: 0.18033145975131978 Accuracy: 0.95160955\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 710us/sample - loss: 11.6227 - acc: 0.0964\n",
      "Loss: 11.622703189394308 Accuracy: 0.09636553\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 5.4567 - acc: 0.3786\n",
      "Loss: 5.456749533319523 Accuracy: 0.37860852\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.8522 - acc: 0.5823\n",
      "Loss: 2.852198066592588 Accuracy: 0.58234686\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.4097 - acc: 0.7161\n",
      "Loss: 1.4096859444215166 Accuracy: 0.7160955\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.0627 - acc: 0.7680\n",
      "Loss: 1.0627176352379106 Accuracy: 0.76801664\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5414 - acc: 0.8733\n",
      "Loss: 0.5414140965275056 Accuracy: 0.8733126\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2781 - acc: 0.9315\n",
      "Loss: 0.2780605069472844 Accuracy: 0.9314642\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2546 - acc: 0.9450\n",
      "Loss: 0.2545633427661419 Accuracy: 0.94496363\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_tanh_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.2882 - acc: 0.9427\n",
      "Loss: 0.2882134547712415 Accuracy: 0.9426791\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
