{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,248\n",
      "Trainable params: 8,195,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,105,456\n",
      "Trainable params: 4,105,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,063,664\n",
      "Trainable params: 2,063,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,045,872\n",
      "Trainable params: 1,045,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,064,432\n",
      "Trainable params: 1,064,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 577,136\n",
      "Trainable params: 577,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 345,840\n",
      "Trainable params: 345,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 242,544\n",
      "Trainable params: 242,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 315,504\n",
      "Trainable params: 315,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 350,576\n",
      "Trainable params: 350,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 416,368\n",
      "Trainable params: 416,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 498,544\n",
      "Trainable params: 498,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 791,920\n",
      "Trainable params: 791,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3515 - acc: 0.2442\n",
      "Epoch 00001: val_loss improved from inf to 2.08062, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv_checkpoint/001-2.0806.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.3513 - acc: 0.2443 - val_loss: 2.0806 - val_acc: 0.3625\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9812 - acc: 0.3823\n",
      "Epoch 00002: val_loss improved from 2.08062 to 1.86970, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv_checkpoint/002-1.8697.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.9811 - acc: 0.3823 - val_loss: 1.8697 - val_acc: 0.4223\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7388 - acc: 0.4727\n",
      "Epoch 00003: val_loss improved from 1.86970 to 1.66780, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv_checkpoint/003-1.6678.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.7388 - acc: 0.4727 - val_loss: 1.6678 - val_acc: 0.4889\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5350 - acc: 0.5384\n",
      "Epoch 00004: val_loss improved from 1.66780 to 1.59180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv_checkpoint/004-1.5918.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.5350 - acc: 0.5384 - val_loss: 1.5918 - val_acc: 0.5078\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3628 - acc: 0.5964\n",
      "Epoch 00005: val_loss improved from 1.59180 to 1.55987, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv_checkpoint/005-1.5599.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.3627 - acc: 0.5965 - val_loss: 1.5599 - val_acc: 0.5295\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1872 - acc: 0.6494\n",
      "Epoch 00006: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1872 - acc: 0.6493 - val_loss: 1.5738 - val_acc: 0.5292\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0185 - acc: 0.6991\n",
      "Epoch 00007: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0185 - acc: 0.6991 - val_loss: 1.6346 - val_acc: 0.5115\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7396\n",
      "Epoch 00008: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8715 - acc: 0.7396 - val_loss: 1.7041 - val_acc: 0.4999\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7738\n",
      "Epoch 00009: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7532 - acc: 0.7738 - val_loss: 1.7734 - val_acc: 0.5083\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7998\n",
      "Epoch 00010: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6571 - acc: 0.7998 - val_loss: 1.8952 - val_acc: 0.5008\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5646 - acc: 0.8284\n",
      "Epoch 00011: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5645 - acc: 0.8284 - val_loss: 1.9627 - val_acc: 0.5015\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8438\n",
      "Epoch 00012: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5053 - acc: 0.8438 - val_loss: 2.0270 - val_acc: 0.5073\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4423 - acc: 0.8628\n",
      "Epoch 00013: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4423 - acc: 0.8628 - val_loss: 2.0612 - val_acc: 0.5104\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8773\n",
      "Epoch 00014: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3928 - acc: 0.8773 - val_loss: 2.2143 - val_acc: 0.4978\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8900\n",
      "Epoch 00015: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3501 - acc: 0.8900 - val_loss: 2.2557 - val_acc: 0.5069\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.9024\n",
      "Epoch 00016: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3165 - acc: 0.9024 - val_loss: 2.3075 - val_acc: 0.5087\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9136\n",
      "Epoch 00017: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2819 - acc: 0.9136 - val_loss: 2.3839 - val_acc: 0.5090\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9218\n",
      "Epoch 00018: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2533 - acc: 0.9218 - val_loss: 2.4841 - val_acc: 0.5113\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9268\n",
      "Epoch 00019: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2351 - acc: 0.9268 - val_loss: 2.5417 - val_acc: 0.5090\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9343\n",
      "Epoch 00020: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2153 - acc: 0.9343 - val_loss: 2.6537 - val_acc: 0.5090\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9385\n",
      "Epoch 00021: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2028 - acc: 0.9385 - val_loss: 2.6171 - val_acc: 0.5122\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9456\n",
      "Epoch 00022: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1840 - acc: 0.9456 - val_loss: 2.6553 - val_acc: 0.5225\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9455\n",
      "Epoch 00023: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1804 - acc: 0.9455 - val_loss: 2.6789 - val_acc: 0.5239\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9520\n",
      "Epoch 00024: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1601 - acc: 0.9520 - val_loss: 2.7943 - val_acc: 0.5220\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9554\n",
      "Epoch 00025: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1493 - acc: 0.9554 - val_loss: 2.8523 - val_acc: 0.5274\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9576\n",
      "Epoch 00026: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1450 - acc: 0.9576 - val_loss: 2.8757 - val_acc: 0.5213\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9573\n",
      "Epoch 00027: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1449 - acc: 0.9573 - val_loss: 2.9062 - val_acc: 0.5155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9610\n",
      "Epoch 00028: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1333 - acc: 0.9610 - val_loss: 2.8791 - val_acc: 0.5260\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9624\n",
      "Epoch 00029: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1271 - acc: 0.9625 - val_loss: 2.8930 - val_acc: 0.5262\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9638\n",
      "Epoch 00030: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1232 - acc: 0.9638 - val_loss: 2.9088 - val_acc: 0.5353\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9656\n",
      "Epoch 00031: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1176 - acc: 0.9656 - val_loss: 2.9649 - val_acc: 0.5318\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9680\n",
      "Epoch 00032: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1143 - acc: 0.9680 - val_loss: 2.9569 - val_acc: 0.5306\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9682\n",
      "Epoch 00033: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1110 - acc: 0.9682 - val_loss: 2.9204 - val_acc: 0.5327\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9708\n",
      "Epoch 00034: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1067 - acc: 0.9708 - val_loss: 2.9410 - val_acc: 0.5344\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9729\n",
      "Epoch 00035: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0983 - acc: 0.9729 - val_loss: 2.9441 - val_acc: 0.5416\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9724\n",
      "Epoch 00036: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0975 - acc: 0.9724 - val_loss: 2.9834 - val_acc: 0.5311\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9739\n",
      "Epoch 00037: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0940 - acc: 0.9739 - val_loss: 3.0705 - val_acc: 0.5376\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9746\n",
      "Epoch 00038: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0930 - acc: 0.9746 - val_loss: 3.0993 - val_acc: 0.5260\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9745\n",
      "Epoch 00039: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0929 - acc: 0.9745 - val_loss: 3.0007 - val_acc: 0.5439\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9757\n",
      "Epoch 00040: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0886 - acc: 0.9757 - val_loss: 2.9901 - val_acc: 0.5367\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9775\n",
      "Epoch 00041: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0837 - acc: 0.9775 - val_loss: 3.1209 - val_acc: 0.5353\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9775\n",
      "Epoch 00042: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0817 - acc: 0.9775 - val_loss: 3.0728 - val_acc: 0.5395\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9777\n",
      "Epoch 00043: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0820 - acc: 0.9777 - val_loss: 3.2023 - val_acc: 0.5376\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9789\n",
      "Epoch 00044: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0799 - acc: 0.9789 - val_loss: 3.0801 - val_acc: 0.5416\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9780\n",
      "Epoch 00045: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0835 - acc: 0.9780 - val_loss: 3.0521 - val_acc: 0.5339\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9802\n",
      "Epoch 00046: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0768 - acc: 0.9802 - val_loss: 3.1498 - val_acc: 0.5386\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9791\n",
      "Epoch 00047: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0771 - acc: 0.9791 - val_loss: 3.0594 - val_acc: 0.5516\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9812\n",
      "Epoch 00048: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0750 - acc: 0.9812 - val_loss: 3.0932 - val_acc: 0.5453\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9814\n",
      "Epoch 00049: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0711 - acc: 0.9814 - val_loss: 3.1480 - val_acc: 0.5413\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9819\n",
      "Epoch 00050: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0689 - acc: 0.9819 - val_loss: 3.2076 - val_acc: 0.5460\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9820\n",
      "Epoch 00051: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0708 - acc: 0.9820 - val_loss: 3.2013 - val_acc: 0.5416\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9821\n",
      "Epoch 00052: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0717 - acc: 0.9821 - val_loss: 3.0789 - val_acc: 0.5432\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9821\n",
      "Epoch 00053: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0677 - acc: 0.9821 - val_loss: 3.1331 - val_acc: 0.5399\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9834\n",
      "Epoch 00054: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0651 - acc: 0.9834 - val_loss: 3.0818 - val_acc: 0.5439\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9846\n",
      "Epoch 00055: val_loss did not improve from 1.55987\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0632 - acc: 0.9846 - val_loss: 3.1906 - val_acc: 0.5511\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 563us/sample - loss: 1.6396 - acc: 0.4926\n",
      "Loss: 1.639625862281023 Accuracy: 0.4926272\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3635 - acc: 0.2445\n",
      "Epoch 00001: val_loss improved from inf to 1.89961, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv_checkpoint/001-1.8996.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.3634 - acc: 0.2445 - val_loss: 1.8996 - val_acc: 0.4286\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6938 - acc: 0.4775\n",
      "Epoch 00002: val_loss improved from 1.89961 to 1.56494, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv_checkpoint/002-1.5649.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.6939 - acc: 0.4775 - val_loss: 1.5649 - val_acc: 0.5160\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4170 - acc: 0.5660\n",
      "Epoch 00003: val_loss improved from 1.56494 to 1.46193, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv_checkpoint/003-1.4619.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4169 - acc: 0.5660 - val_loss: 1.4619 - val_acc: 0.5395\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2432 - acc: 0.6155\n",
      "Epoch 00004: val_loss improved from 1.46193 to 1.38117, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv_checkpoint/004-1.3812.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2432 - acc: 0.6155 - val_loss: 1.3812 - val_acc: 0.5730\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1112 - acc: 0.6560\n",
      "Epoch 00005: val_loss did not improve from 1.38117\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1113 - acc: 0.6560 - val_loss: 1.3866 - val_acc: 0.5688\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0014 - acc: 0.6918\n",
      "Epoch 00006: val_loss improved from 1.38117 to 1.38056, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv_checkpoint/006-1.3806.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0014 - acc: 0.6918 - val_loss: 1.3806 - val_acc: 0.5793\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7237\n",
      "Epoch 00007: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8967 - acc: 0.7237 - val_loss: 1.5223 - val_acc: 0.5542\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8147 - acc: 0.7463\n",
      "Epoch 00008: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8147 - acc: 0.7463 - val_loss: 1.4177 - val_acc: 0.5851\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7713\n",
      "Epoch 00009: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7297 - acc: 0.7713 - val_loss: 1.4320 - val_acc: 0.5905\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6593 - acc: 0.7908\n",
      "Epoch 00010: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6593 - acc: 0.7908 - val_loss: 1.4655 - val_acc: 0.5889\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5920 - acc: 0.8133\n",
      "Epoch 00011: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5919 - acc: 0.8133 - val_loss: 1.5138 - val_acc: 0.5942\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.8305\n",
      "Epoch 00012: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5319 - acc: 0.8305 - val_loss: 1.5626 - val_acc: 0.5868\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8455\n",
      "Epoch 00013: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4862 - acc: 0.8455 - val_loss: 1.5739 - val_acc: 0.6035\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8560\n",
      "Epoch 00014: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4463 - acc: 0.8559 - val_loss: 1.6292 - val_acc: 0.5961\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8700\n",
      "Epoch 00015: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4065 - acc: 0.8700 - val_loss: 1.6673 - val_acc: 0.5975\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8816\n",
      "Epoch 00016: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3650 - acc: 0.8816 - val_loss: 1.6859 - val_acc: 0.6075\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8908\n",
      "Epoch 00017: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3351 - acc: 0.8908 - val_loss: 1.7411 - val_acc: 0.5940\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9006\n",
      "Epoch 00018: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3121 - acc: 0.9006 - val_loss: 1.7610 - val_acc: 0.6075\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9067\n",
      "Epoch 00019: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2868 - acc: 0.9067 - val_loss: 1.9529 - val_acc: 0.5886\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9120\n",
      "Epoch 00020: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2705 - acc: 0.9121 - val_loss: 1.8670 - val_acc: 0.6068\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9205\n",
      "Epoch 00021: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2464 - acc: 0.9204 - val_loss: 1.8918 - val_acc: 0.6052\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9244\n",
      "Epoch 00022: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2372 - acc: 0.9244 - val_loss: 1.9210 - val_acc: 0.6010\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9314\n",
      "Epoch 00023: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2165 - acc: 0.9313 - val_loss: 1.9541 - val_acc: 0.6047\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9352\n",
      "Epoch 00024: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2058 - acc: 0.9352 - val_loss: 1.9367 - val_acc: 0.6201\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9364\n",
      "Epoch 00025: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1993 - acc: 0.9364 - val_loss: 1.9280 - val_acc: 0.6229\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9428\n",
      "Epoch 00026: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1844 - acc: 0.9428 - val_loss: 2.1081 - val_acc: 0.6091\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9455\n",
      "Epoch 00027: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1740 - acc: 0.9455 - val_loss: 2.0007 - val_acc: 0.6313\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9478\n",
      "Epoch 00028: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1649 - acc: 0.9478 - val_loss: 2.1600 - val_acc: 0.6087\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9503\n",
      "Epoch 00029: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1606 - acc: 0.9503 - val_loss: 2.0616 - val_acc: 0.6219\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9507\n",
      "Epoch 00030: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1556 - acc: 0.9507 - val_loss: 2.0904 - val_acc: 0.6280\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9534\n",
      "Epoch 00031: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1488 - acc: 0.9534 - val_loss: 2.0772 - val_acc: 0.6261\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9565\n",
      "Epoch 00032: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1419 - acc: 0.9565 - val_loss: 2.0628 - val_acc: 0.6285\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9576\n",
      "Epoch 00033: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1362 - acc: 0.9576 - val_loss: 2.0948 - val_acc: 0.6369\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9598\n",
      "Epoch 00034: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1310 - acc: 0.9598 - val_loss: 2.1898 - val_acc: 0.6331\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9608\n",
      "Epoch 00035: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1314 - acc: 0.9608 - val_loss: 2.2174 - val_acc: 0.6296\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9618\n",
      "Epoch 00036: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1267 - acc: 0.9618 - val_loss: 2.1953 - val_acc: 0.6254\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9618\n",
      "Epoch 00037: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1235 - acc: 0.9618 - val_loss: 2.1982 - val_acc: 0.6315\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9633\n",
      "Epoch 00038: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1183 - acc: 0.9633 - val_loss: 2.2363 - val_acc: 0.6268\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9657\n",
      "Epoch 00039: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1137 - acc: 0.9657 - val_loss: 2.1944 - val_acc: 0.6387\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9672\n",
      "Epoch 00040: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1111 - acc: 0.9672 - val_loss: 2.1582 - val_acc: 0.6511\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9670\n",
      "Epoch 00041: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1103 - acc: 0.9670 - val_loss: 2.1508 - val_acc: 0.6389\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9682\n",
      "Epoch 00042: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1064 - acc: 0.9682 - val_loss: 2.1491 - val_acc: 0.6359\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9712\n",
      "Epoch 00043: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1007 - acc: 0.9713 - val_loss: 2.1526 - val_acc: 0.6352\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9689\n",
      "Epoch 00044: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1046 - acc: 0.9689 - val_loss: 2.1300 - val_acc: 0.6557\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9694\n",
      "Epoch 00045: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1049 - acc: 0.9694 - val_loss: 2.1790 - val_acc: 0.6452\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9712\n",
      "Epoch 00046: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0993 - acc: 0.9713 - val_loss: 2.2114 - val_acc: 0.6399\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9714\n",
      "Epoch 00047: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0969 - acc: 0.9714 - val_loss: 2.1481 - val_acc: 0.6506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9723\n",
      "Epoch 00048: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0952 - acc: 0.9723 - val_loss: 2.2525 - val_acc: 0.6448\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9724\n",
      "Epoch 00049: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0955 - acc: 0.9724 - val_loss: 2.1956 - val_acc: 0.6473\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9732\n",
      "Epoch 00050: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0892 - acc: 0.9731 - val_loss: 2.2103 - val_acc: 0.6508\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9736\n",
      "Epoch 00051: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0915 - acc: 0.9736 - val_loss: 2.1959 - val_acc: 0.6471\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9752\n",
      "Epoch 00052: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0876 - acc: 0.9752 - val_loss: 2.1964 - val_acc: 0.6382\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9763\n",
      "Epoch 00053: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0836 - acc: 0.9763 - val_loss: 2.2098 - val_acc: 0.6539\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9755\n",
      "Epoch 00054: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0835 - acc: 0.9755 - val_loss: 2.2658 - val_acc: 0.6408\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9769\n",
      "Epoch 00055: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0822 - acc: 0.9769 - val_loss: 2.2032 - val_acc: 0.6576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9766\n",
      "Epoch 00056: val_loss did not improve from 1.38056\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0795 - acc: 0.9766 - val_loss: 2.2748 - val_acc: 0.6338\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 604us/sample - loss: 1.4313 - acc: 0.5512\n",
      "Loss: 1.4313198433486845 Accuracy: 0.5511942\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4439 - acc: 0.2075\n",
      "Epoch 00001: val_loss improved from inf to 2.09493, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv_checkpoint/001-2.0949.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 2.4438 - acc: 0.2076 - val_loss: 2.0949 - val_acc: 0.3326\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9541 - acc: 0.3813\n",
      "Epoch 00002: val_loss improved from 2.09493 to 1.73149, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv_checkpoint/002-1.7315.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.9541 - acc: 0.3813 - val_loss: 1.7315 - val_acc: 0.4659\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5777 - acc: 0.5102\n",
      "Epoch 00003: val_loss improved from 1.73149 to 1.55480, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv_checkpoint/003-1.5548.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.5776 - acc: 0.5103 - val_loss: 1.5548 - val_acc: 0.4992\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3696 - acc: 0.5796\n",
      "Epoch 00004: val_loss improved from 1.55480 to 1.50478, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv_checkpoint/004-1.5048.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.3696 - acc: 0.5797 - val_loss: 1.5048 - val_acc: 0.5148\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2278 - acc: 0.6243\n",
      "Epoch 00005: val_loss improved from 1.50478 to 1.37296, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv_checkpoint/005-1.3730.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.2278 - acc: 0.6243 - val_loss: 1.3730 - val_acc: 0.5728\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0967 - acc: 0.6670\n",
      "Epoch 00006: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0969 - acc: 0.6670 - val_loss: 1.4161 - val_acc: 0.5560\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.7044\n",
      "Epoch 00007: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9718 - acc: 0.7044 - val_loss: 1.3731 - val_acc: 0.5628\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8568 - acc: 0.7398\n",
      "Epoch 00008: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8570 - acc: 0.7398 - val_loss: 1.3771 - val_acc: 0.5770\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7681\n",
      "Epoch 00009: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7538 - acc: 0.7680 - val_loss: 1.4461 - val_acc: 0.5765\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.7971\n",
      "Epoch 00010: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6531 - acc: 0.7971 - val_loss: 1.4793 - val_acc: 0.5691\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.8205\n",
      "Epoch 00011: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5758 - acc: 0.8205 - val_loss: 1.5224 - val_acc: 0.5765\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8396\n",
      "Epoch 00012: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5108 - acc: 0.8397 - val_loss: 1.5517 - val_acc: 0.5758\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8614\n",
      "Epoch 00013: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4393 - acc: 0.8614 - val_loss: 1.6399 - val_acc: 0.5784\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8738\n",
      "Epoch 00014: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3975 - acc: 0.8738 - val_loss: 1.7399 - val_acc: 0.5660\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8883\n",
      "Epoch 00015: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3503 - acc: 0.8883 - val_loss: 1.7133 - val_acc: 0.5879\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.8999\n",
      "Epoch 00016: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3117 - acc: 0.8999 - val_loss: 1.7156 - val_acc: 0.5849\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2909 - acc: 0.9076\n",
      "Epoch 00017: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2909 - acc: 0.9076 - val_loss: 1.7956 - val_acc: 0.5807\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9127\n",
      "Epoch 00018: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2715 - acc: 0.9127 - val_loss: 1.8993 - val_acc: 0.5842\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9211\n",
      "Epoch 00019: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2480 - acc: 0.9210 - val_loss: 1.8181 - val_acc: 0.6000\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9269\n",
      "Epoch 00020: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2331 - acc: 0.9269 - val_loss: 1.9345 - val_acc: 0.5868\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9322\n",
      "Epoch 00021: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2114 - acc: 0.9322 - val_loss: 1.8931 - val_acc: 0.6007\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9369\n",
      "Epoch 00022: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1988 - acc: 0.9369 - val_loss: 1.9690 - val_acc: 0.5842\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9387\n",
      "Epoch 00023: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1902 - acc: 0.9387 - val_loss: 2.0902 - val_acc: 0.5851\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9426\n",
      "Epoch 00024: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1763 - acc: 0.9426 - val_loss: 1.9738 - val_acc: 0.6112\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9447\n",
      "Epoch 00025: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1723 - acc: 0.9447 - val_loss: 2.0792 - val_acc: 0.5996\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9478\n",
      "Epoch 00026: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1628 - acc: 0.9478 - val_loss: 2.0241 - val_acc: 0.6033\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9523\n",
      "Epoch 00027: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1530 - acc: 0.9523 - val_loss: 2.0565 - val_acc: 0.6042\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9530\n",
      "Epoch 00028: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1489 - acc: 0.9530 - val_loss: 2.1645 - val_acc: 0.6047\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9541\n",
      "Epoch 00029: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1445 - acc: 0.9541 - val_loss: 2.0627 - val_acc: 0.6084\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9560\n",
      "Epoch 00030: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1380 - acc: 0.9560 - val_loss: 2.1733 - val_acc: 0.6010\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9592\n",
      "Epoch 00031: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1303 - acc: 0.9592 - val_loss: 2.0974 - val_acc: 0.6161\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9607\n",
      "Epoch 00032: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1236 - acc: 0.9607 - val_loss: 2.2243 - val_acc: 0.6040\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9625\n",
      "Epoch 00033: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1231 - acc: 0.9625 - val_loss: 2.1300 - val_acc: 0.6173\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9628\n",
      "Epoch 00034: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1168 - acc: 0.9628 - val_loss: 2.2683 - val_acc: 0.6098\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9639\n",
      "Epoch 00035: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1192 - acc: 0.9639 - val_loss: 2.2123 - val_acc: 0.6082\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9666\n",
      "Epoch 00036: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1074 - acc: 0.9666 - val_loss: 2.2641 - val_acc: 0.6038\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9666\n",
      "Epoch 00037: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1101 - acc: 0.9666 - val_loss: 2.1993 - val_acc: 0.6129\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9674\n",
      "Epoch 00038: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1062 - acc: 0.9675 - val_loss: 2.2117 - val_acc: 0.6154\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9645\n",
      "Epoch 00039: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1122 - acc: 0.9645 - val_loss: 2.1958 - val_acc: 0.6091\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9683\n",
      "Epoch 00040: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1051 - acc: 0.9683 - val_loss: 2.2391 - val_acc: 0.6080\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9688\n",
      "Epoch 00041: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1029 - acc: 0.9688 - val_loss: 2.2111 - val_acc: 0.6212\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9707\n",
      "Epoch 00042: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0982 - acc: 0.9707 - val_loss: 2.1657 - val_acc: 0.6208\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9715\n",
      "Epoch 00043: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0938 - acc: 0.9716 - val_loss: 2.2615 - val_acc: 0.6124\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9735\n",
      "Epoch 00044: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0874 - acc: 0.9735 - val_loss: 2.2707 - val_acc: 0.6252\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9730\n",
      "Epoch 00045: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0930 - acc: 0.9730 - val_loss: 2.2854 - val_acc: 0.6212\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9718\n",
      "Epoch 00046: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0931 - acc: 0.9719 - val_loss: 2.1849 - val_acc: 0.6226\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9738\n",
      "Epoch 00047: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0882 - acc: 0.9738 - val_loss: 2.2506 - val_acc: 0.6259\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9742\n",
      "Epoch 00048: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0845 - acc: 0.9742 - val_loss: 2.2426 - val_acc: 0.6264\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9755\n",
      "Epoch 00049: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0842 - acc: 0.9755 - val_loss: 2.2784 - val_acc: 0.6145\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9740\n",
      "Epoch 00050: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0889 - acc: 0.9741 - val_loss: 2.2853 - val_acc: 0.6182\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9755\n",
      "Epoch 00051: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0817 - acc: 0.9755 - val_loss: 2.2455 - val_acc: 0.6250\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9769\n",
      "Epoch 00052: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0801 - acc: 0.9769 - val_loss: 2.2847 - val_acc: 0.6264\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9769\n",
      "Epoch 00053: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0793 - acc: 0.9769 - val_loss: 2.2052 - val_acc: 0.6250\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9774\n",
      "Epoch 00054: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0788 - acc: 0.9774 - val_loss: 2.2794 - val_acc: 0.6278\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9762\n",
      "Epoch 00055: val_loss did not improve from 1.37296\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0820 - acc: 0.9762 - val_loss: 2.2611 - val_acc: 0.6348\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 667us/sample - loss: 1.4565 - acc: 0.5335\n",
      "Loss: 1.4564784374068707 Accuracy: 0.533541\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5348 - acc: 0.1719\n",
      "Epoch 00001: val_loss improved from inf to 2.11840, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/001-2.1184.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.5347 - acc: 0.1719 - val_loss: 2.1184 - val_acc: 0.3354\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8848 - acc: 0.4049\n",
      "Epoch 00002: val_loss improved from 2.11840 to 1.60849, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/002-1.6085.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.8847 - acc: 0.4049 - val_loss: 1.6085 - val_acc: 0.4845\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5885 - acc: 0.4968\n",
      "Epoch 00003: val_loss improved from 1.60849 to 1.45851, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/003-1.4585.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.5884 - acc: 0.4968 - val_loss: 1.4585 - val_acc: 0.5402\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4196 - acc: 0.5556\n",
      "Epoch 00004: val_loss improved from 1.45851 to 1.34933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/004-1.3493.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.4197 - acc: 0.5556 - val_loss: 1.3493 - val_acc: 0.5782\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2939 - acc: 0.6002\n",
      "Epoch 00005: val_loss improved from 1.34933 to 1.25453, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/005-1.2545.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.2938 - acc: 0.6002 - val_loss: 1.2545 - val_acc: 0.6189\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1888 - acc: 0.6369\n",
      "Epoch 00006: val_loss improved from 1.25453 to 1.22511, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/006-1.2251.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1888 - acc: 0.6369 - val_loss: 1.2251 - val_acc: 0.6143\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1027 - acc: 0.6660\n",
      "Epoch 00007: val_loss improved from 1.22511 to 1.17151, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/007-1.1715.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1030 - acc: 0.6660 - val_loss: 1.1715 - val_acc: 0.6322\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0318 - acc: 0.6897\n",
      "Epoch 00008: val_loss improved from 1.17151 to 1.10658, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/008-1.1066.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.0317 - acc: 0.6898 - val_loss: 1.1066 - val_acc: 0.6539\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9604 - acc: 0.7101\n",
      "Epoch 00009: val_loss improved from 1.10658 to 1.04930, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/009-1.0493.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.9605 - acc: 0.7100 - val_loss: 1.0493 - val_acc: 0.6860\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8930 - acc: 0.7308\n",
      "Epoch 00010: val_loss improved from 1.04930 to 1.04592, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/010-1.0459.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8930 - acc: 0.7308 - val_loss: 1.0459 - val_acc: 0.6841\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8275 - acc: 0.7493\n",
      "Epoch 00011: val_loss improved from 1.04592 to 1.01601, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/011-1.0160.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8274 - acc: 0.7494 - val_loss: 1.0160 - val_acc: 0.6909\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7644 - acc: 0.7706\n",
      "Epoch 00012: val_loss did not improve from 1.01601\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7644 - acc: 0.7706 - val_loss: 1.0908 - val_acc: 0.6709\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7149 - acc: 0.7793\n",
      "Epoch 00013: val_loss did not improve from 1.01601\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7148 - acc: 0.7794 - val_loss: 1.0297 - val_acc: 0.6979\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.8007\n",
      "Epoch 00014: val_loss did not improve from 1.01601\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6568 - acc: 0.8007 - val_loss: 1.0323 - val_acc: 0.7042\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8124\n",
      "Epoch 00015: val_loss improved from 1.01601 to 0.99251, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/015-0.9925.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.6175 - acc: 0.8124 - val_loss: 0.9925 - val_acc: 0.7167\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8270\n",
      "Epoch 00016: val_loss improved from 0.99251 to 0.98862, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv_checkpoint/016-0.9886.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5664 - acc: 0.8270 - val_loss: 0.9886 - val_acc: 0.7167\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5187 - acc: 0.8418\n",
      "Epoch 00017: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5187 - acc: 0.8418 - val_loss: 0.9895 - val_acc: 0.7151\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8524\n",
      "Epoch 00018: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4828 - acc: 0.8524 - val_loss: 0.9901 - val_acc: 0.7149\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4536 - acc: 0.8605\n",
      "Epoch 00019: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4536 - acc: 0.8605 - val_loss: 1.0306 - val_acc: 0.7119\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8708\n",
      "Epoch 00020: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4089 - acc: 0.8708 - val_loss: 1.0471 - val_acc: 0.7181\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8799\n",
      "Epoch 00021: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3824 - acc: 0.8799 - val_loss: 1.1272 - val_acc: 0.6972\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8871\n",
      "Epoch 00022: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3612 - acc: 0.8871 - val_loss: 1.0648 - val_acc: 0.7158\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8930\n",
      "Epoch 00023: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3382 - acc: 0.8930 - val_loss: 1.0665 - val_acc: 0.7249\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9024\n",
      "Epoch 00024: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3082 - acc: 0.9024 - val_loss: 1.1490 - val_acc: 0.7177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9041\n",
      "Epoch 00025: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2998 - acc: 0.9041 - val_loss: 1.1486 - val_acc: 0.7116\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9097\n",
      "Epoch 00026: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2840 - acc: 0.9097 - val_loss: 1.1267 - val_acc: 0.7279\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9143\n",
      "Epoch 00027: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2666 - acc: 0.9144 - val_loss: 1.2860 - val_acc: 0.6953\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9179\n",
      "Epoch 00028: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2521 - acc: 0.9179 - val_loss: 1.1896 - val_acc: 0.7170\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9242\n",
      "Epoch 00029: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2369 - acc: 0.9242 - val_loss: 1.2087 - val_acc: 0.7230\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9270\n",
      "Epoch 00030: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2274 - acc: 0.9270 - val_loss: 1.3346 - val_acc: 0.6956\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9304\n",
      "Epoch 00031: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2166 - acc: 0.9304 - val_loss: 1.2328 - val_acc: 0.7216\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9356\n",
      "Epoch 00032: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2020 - acc: 0.9356 - val_loss: 1.2248 - val_acc: 0.7317\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9362\n",
      "Epoch 00033: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1981 - acc: 0.9363 - val_loss: 1.2454 - val_acc: 0.7247\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9409\n",
      "Epoch 00034: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1862 - acc: 0.9409 - val_loss: 1.3527 - val_acc: 0.7338\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9374\n",
      "Epoch 00035: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1922 - acc: 0.9374 - val_loss: 1.2214 - val_acc: 0.7368\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9427\n",
      "Epoch 00036: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1745 - acc: 0.9427 - val_loss: 1.2940 - val_acc: 0.7384\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9456\n",
      "Epoch 00037: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1702 - acc: 0.9456 - val_loss: 1.2808 - val_acc: 0.7291\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9487\n",
      "Epoch 00038: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1588 - acc: 0.9487 - val_loss: 1.2468 - val_acc: 0.7391\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9490\n",
      "Epoch 00039: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1603 - acc: 0.9490 - val_loss: 1.3118 - val_acc: 0.7398\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9499\n",
      "Epoch 00040: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1545 - acc: 0.9499 - val_loss: 1.4655 - val_acc: 0.7142\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9489\n",
      "Epoch 00041: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1598 - acc: 0.9489 - val_loss: 1.3890 - val_acc: 0.7310\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9542\n",
      "Epoch 00042: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1453 - acc: 0.9542 - val_loss: 1.3202 - val_acc: 0.7386\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9547\n",
      "Epoch 00043: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1428 - acc: 0.9547 - val_loss: 1.3567 - val_acc: 0.7365\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9559\n",
      "Epoch 00044: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1407 - acc: 0.9559 - val_loss: 1.3148 - val_acc: 0.7361\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9568\n",
      "Epoch 00045: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1370 - acc: 0.9568 - val_loss: 1.4197 - val_acc: 0.7338\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9565\n",
      "Epoch 00046: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1408 - acc: 0.9565 - val_loss: 1.4582 - val_acc: 0.7184\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9595\n",
      "Epoch 00047: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1252 - acc: 0.9595 - val_loss: 1.3334 - val_acc: 0.7354\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9597\n",
      "Epoch 00048: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1280 - acc: 0.9597 - val_loss: 1.3147 - val_acc: 0.7421\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9614\n",
      "Epoch 00049: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1248 - acc: 0.9614 - val_loss: 1.4044 - val_acc: 0.7382\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9617\n",
      "Epoch 00050: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1230 - acc: 0.9617 - val_loss: 1.3816 - val_acc: 0.7335\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9620\n",
      "Epoch 00051: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1234 - acc: 0.9620 - val_loss: 1.3539 - val_acc: 0.7461\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9635\n",
      "Epoch 00052: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1173 - acc: 0.9635 - val_loss: 1.4565 - val_acc: 0.7403\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9647\n",
      "Epoch 00053: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1155 - acc: 0.9647 - val_loss: 1.3615 - val_acc: 0.7461\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9622\n",
      "Epoch 00054: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1195 - acc: 0.9622 - val_loss: 1.3836 - val_acc: 0.7419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9676\n",
      "Epoch 00055: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1062 - acc: 0.9676 - val_loss: 1.3281 - val_acc: 0.7517\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9653\n",
      "Epoch 00056: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1131 - acc: 0.9653 - val_loss: 1.4861 - val_acc: 0.7256\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9643\n",
      "Epoch 00057: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1140 - acc: 0.9643 - val_loss: 1.3298 - val_acc: 0.7480\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9666\n",
      "Epoch 00058: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1068 - acc: 0.9666 - val_loss: 1.3707 - val_acc: 0.7452\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9685\n",
      "Epoch 00059: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1035 - acc: 0.9684 - val_loss: 1.3363 - val_acc: 0.7487\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9674\n",
      "Epoch 00060: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1080 - acc: 0.9674 - val_loss: 1.4491 - val_acc: 0.7400\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9708\n",
      "Epoch 00061: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0970 - acc: 0.9708 - val_loss: 1.4473 - val_acc: 0.7468\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9699\n",
      "Epoch 00062: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0984 - acc: 0.9699 - val_loss: 1.4135 - val_acc: 0.7412\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9698\n",
      "Epoch 00063: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0991 - acc: 0.9698 - val_loss: 1.3633 - val_acc: 0.7545\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9706\n",
      "Epoch 00064: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0974 - acc: 0.9706 - val_loss: 1.4105 - val_acc: 0.7494\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9705\n",
      "Epoch 00065: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0950 - acc: 0.9705 - val_loss: 1.3944 - val_acc: 0.7473\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9706\n",
      "Epoch 00066: val_loss did not improve from 0.98862\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0944 - acc: 0.9706 - val_loss: 1.5119 - val_acc: 0.7345\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 703us/sample - loss: 1.0802 - acc: 0.6827\n",
      "Loss: 1.080234902385363 Accuracy: 0.6826584\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4353 - acc: 0.1951\n",
      "Epoch 00001: val_loss improved from inf to 1.83527, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/001-1.8353.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 2.4353 - acc: 0.1951 - val_loss: 1.8353 - val_acc: 0.4032\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6826 - acc: 0.4600\n",
      "Epoch 00002: val_loss improved from 1.83527 to 1.40936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/002-1.4094.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.6827 - acc: 0.4600 - val_loss: 1.4094 - val_acc: 0.5558\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4285 - acc: 0.5459\n",
      "Epoch 00003: val_loss improved from 1.40936 to 1.31215, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/003-1.3122.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.4286 - acc: 0.5459 - val_loss: 1.3122 - val_acc: 0.5886\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3229 - acc: 0.5847\n",
      "Epoch 00004: val_loss improved from 1.31215 to 1.21200, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/004-1.2120.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.3229 - acc: 0.5847 - val_loss: 1.2120 - val_acc: 0.6315\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2515 - acc: 0.6096\n",
      "Epoch 00005: val_loss did not improve from 1.21200\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.2515 - acc: 0.6096 - val_loss: 1.2406 - val_acc: 0.6187\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1744 - acc: 0.6347\n",
      "Epoch 00006: val_loss improved from 1.21200 to 1.09751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/006-1.0975.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.1744 - acc: 0.6347 - val_loss: 1.0975 - val_acc: 0.6692\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6635\n",
      "Epoch 00007: val_loss improved from 1.09751 to 1.03213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/007-1.0321.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.1023 - acc: 0.6636 - val_loss: 1.0321 - val_acc: 0.6981\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0197 - acc: 0.6886\n",
      "Epoch 00008: val_loss improved from 1.03213 to 1.01442, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/008-1.0144.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.0197 - acc: 0.6886 - val_loss: 1.0144 - val_acc: 0.6921\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9424 - acc: 0.7132\n",
      "Epoch 00009: val_loss improved from 1.01442 to 0.88937, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/009-0.8894.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.9424 - acc: 0.7132 - val_loss: 0.8894 - val_acc: 0.7384\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.7365\n",
      "Epoch 00010: val_loss improved from 0.88937 to 0.84772, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/010-0.8477.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.8707 - acc: 0.7365 - val_loss: 0.8477 - val_acc: 0.7549\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7563\n",
      "Epoch 00011: val_loss improved from 0.84772 to 0.78876, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/011-0.7888.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.8146 - acc: 0.7563 - val_loss: 0.7888 - val_acc: 0.7629\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7515 - acc: 0.7754\n",
      "Epoch 00012: val_loss improved from 0.78876 to 0.75585, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/012-0.7559.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.7515 - acc: 0.7754 - val_loss: 0.7559 - val_acc: 0.7768\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7100 - acc: 0.7873\n",
      "Epoch 00013: val_loss improved from 0.75585 to 0.74949, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/013-0.7495.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.7100 - acc: 0.7873 - val_loss: 0.7495 - val_acc: 0.7841\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.8015\n",
      "Epoch 00014: val_loss improved from 0.74949 to 0.72006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/014-0.7201.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.6674 - acc: 0.8016 - val_loss: 0.7201 - val_acc: 0.7876\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.8126\n",
      "Epoch 00015: val_loss improved from 0.72006 to 0.67568, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/015-0.6757.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.6309 - acc: 0.8126 - val_loss: 0.6757 - val_acc: 0.8053\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8220\n",
      "Epoch 00016: val_loss improved from 0.67568 to 0.65788, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/016-0.6579.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5960 - acc: 0.8220 - val_loss: 0.6579 - val_acc: 0.8088\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8324\n",
      "Epoch 00017: val_loss did not improve from 0.65788\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5590 - acc: 0.8324 - val_loss: 0.6584 - val_acc: 0.8181\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8418\n",
      "Epoch 00018: val_loss improved from 0.65788 to 0.62972, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv_checkpoint/018-0.6297.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5297 - acc: 0.8419 - val_loss: 0.6297 - val_acc: 0.8220\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.8532\n",
      "Epoch 00019: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4980 - acc: 0.8531 - val_loss: 0.6979 - val_acc: 0.8074\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.8564\n",
      "Epoch 00020: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4680 - acc: 0.8564 - val_loss: 0.6996 - val_acc: 0.8076\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8655\n",
      "Epoch 00021: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4437 - acc: 0.8655 - val_loss: 0.6476 - val_acc: 0.8167\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8733\n",
      "Epoch 00022: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4200 - acc: 0.8733 - val_loss: 0.6475 - val_acc: 0.8209\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8779\n",
      "Epoch 00023: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4015 - acc: 0.8778 - val_loss: 0.6532 - val_acc: 0.8150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8841\n",
      "Epoch 00024: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3823 - acc: 0.8841 - val_loss: 0.6359 - val_acc: 0.8274\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8918\n",
      "Epoch 00025: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3520 - acc: 0.8918 - val_loss: 0.6500 - val_acc: 0.8337\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8981\n",
      "Epoch 00026: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3306 - acc: 0.8981 - val_loss: 0.6445 - val_acc: 0.8390\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9033\n",
      "Epoch 00027: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3152 - acc: 0.9034 - val_loss: 0.7054 - val_acc: 0.8220\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9041\n",
      "Epoch 00028: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3042 - acc: 0.9041 - val_loss: 0.6622 - val_acc: 0.8255\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9083\n",
      "Epoch 00029: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2906 - acc: 0.9083 - val_loss: 0.6564 - val_acc: 0.8402\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9161\n",
      "Epoch 00030: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2688 - acc: 0.9161 - val_loss: 0.6679 - val_acc: 0.8344\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9191\n",
      "Epoch 00031: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2588 - acc: 0.9191 - val_loss: 0.6786 - val_acc: 0.8388\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9215\n",
      "Epoch 00032: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2429 - acc: 0.9215 - val_loss: 0.6807 - val_acc: 0.8402\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9261\n",
      "Epoch 00033: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2359 - acc: 0.9260 - val_loss: 0.6564 - val_acc: 0.8404\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9279\n",
      "Epoch 00034: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2241 - acc: 0.9279 - val_loss: 0.6496 - val_acc: 0.8460\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9290\n",
      "Epoch 00035: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2195 - acc: 0.9290 - val_loss: 0.6706 - val_acc: 0.8421\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9354\n",
      "Epoch 00036: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2009 - acc: 0.9353 - val_loss: 0.6810 - val_acc: 0.8425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9358\n",
      "Epoch 00037: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2011 - acc: 0.9358 - val_loss: 0.6886 - val_acc: 0.8414\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9407\n",
      "Epoch 00038: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1868 - acc: 0.9407 - val_loss: 0.6953 - val_acc: 0.8425\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9410\n",
      "Epoch 00039: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1854 - acc: 0.9410 - val_loss: 0.7123 - val_acc: 0.8353\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9418\n",
      "Epoch 00040: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1769 - acc: 0.9419 - val_loss: 0.8422 - val_acc: 0.8258\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9440\n",
      "Epoch 00041: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1739 - acc: 0.9441 - val_loss: 0.7440 - val_acc: 0.8362\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9461\n",
      "Epoch 00042: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1702 - acc: 0.9461 - val_loss: 0.7474 - val_acc: 0.8358\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9496\n",
      "Epoch 00043: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1594 - acc: 0.9497 - val_loss: 0.7022 - val_acc: 0.8442\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9489\n",
      "Epoch 00044: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1567 - acc: 0.9489 - val_loss: 0.7128 - val_acc: 0.8425\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9530\n",
      "Epoch 00045: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1521 - acc: 0.9530 - val_loss: 0.7082 - val_acc: 0.8458\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9531\n",
      "Epoch 00046: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1470 - acc: 0.9531 - val_loss: 0.6884 - val_acc: 0.8493\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9534\n",
      "Epoch 00047: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1433 - acc: 0.9534 - val_loss: 0.7402 - val_acc: 0.8532\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9554\n",
      "Epoch 00048: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1420 - acc: 0.9554 - val_loss: 0.7582 - val_acc: 0.8353\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9565\n",
      "Epoch 00049: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1359 - acc: 0.9566 - val_loss: 0.7987 - val_acc: 0.8430\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9566\n",
      "Epoch 00050: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1335 - acc: 0.9566 - val_loss: 0.7462 - val_acc: 0.8435\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9555\n",
      "Epoch 00051: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1399 - acc: 0.9555 - val_loss: 0.7299 - val_acc: 0.8460\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9588\n",
      "Epoch 00052: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1308 - acc: 0.9588 - val_loss: 0.7662 - val_acc: 0.8439\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9598\n",
      "Epoch 00053: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1254 - acc: 0.9598 - val_loss: 0.7297 - val_acc: 0.8537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9605\n",
      "Epoch 00054: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1229 - acc: 0.9605 - val_loss: 0.7474 - val_acc: 0.8456\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9616\n",
      "Epoch 00055: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1223 - acc: 0.9616 - val_loss: 0.7855 - val_acc: 0.8500\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9589\n",
      "Epoch 00056: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1292 - acc: 0.9589 - val_loss: 0.7298 - val_acc: 0.8491\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9635\n",
      "Epoch 00057: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1156 - acc: 0.9635 - val_loss: 0.7809 - val_acc: 0.8535\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9631\n",
      "Epoch 00058: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1173 - acc: 0.9631 - val_loss: 0.7777 - val_acc: 0.8526\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9668\n",
      "Epoch 00059: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1099 - acc: 0.9669 - val_loss: 0.7422 - val_acc: 0.8514\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9661\n",
      "Epoch 00060: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1087 - acc: 0.9661 - val_loss: 0.7216 - val_acc: 0.8507\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9654\n",
      "Epoch 00061: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1147 - acc: 0.9654 - val_loss: 0.7250 - val_acc: 0.8544\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9664\n",
      "Epoch 00062: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1071 - acc: 0.9664 - val_loss: 0.8126 - val_acc: 0.8411\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9650\n",
      "Epoch 00063: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1105 - acc: 0.9650 - val_loss: 0.7514 - val_acc: 0.8553\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9675\n",
      "Epoch 00064: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1049 - acc: 0.9675 - val_loss: 0.7563 - val_acc: 0.8512\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9689\n",
      "Epoch 00065: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1004 - acc: 0.9689 - val_loss: 0.7577 - val_acc: 0.8530\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9670\n",
      "Epoch 00066: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1081 - acc: 0.9669 - val_loss: 0.7540 - val_acc: 0.8621\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9643\n",
      "Epoch 00067: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1157 - acc: 0.9643 - val_loss: 0.7319 - val_acc: 0.8558\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9699\n",
      "Epoch 00068: val_loss did not improve from 0.62972\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0990 - acc: 0.9699 - val_loss: 0.7922 - val_acc: 0.8491\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 706us/sample - loss: 0.7538 - acc: 0.7873\n",
      "Loss: 0.7537895601238912 Accuracy: 0.7873313\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5190 - acc: 0.1605\n",
      "Epoch 00001: val_loss improved from inf to 2.00545, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/001-2.0054.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 2.5190 - acc: 0.1605 - val_loss: 2.0054 - val_acc: 0.3613\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7545 - acc: 0.4258\n",
      "Epoch 00002: val_loss improved from 2.00545 to 1.41750, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/002-1.4175.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.7545 - acc: 0.4258 - val_loss: 1.4175 - val_acc: 0.5469\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4411 - acc: 0.5324\n",
      "Epoch 00003: val_loss improved from 1.41750 to 1.28762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/003-1.2876.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.4411 - acc: 0.5325 - val_loss: 1.2876 - val_acc: 0.6063\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3207 - acc: 0.5753\n",
      "Epoch 00004: val_loss improved from 1.28762 to 1.21757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/004-1.2176.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.3208 - acc: 0.5752 - val_loss: 1.2176 - val_acc: 0.6110\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2454 - acc: 0.6036\n",
      "Epoch 00005: val_loss improved from 1.21757 to 1.20448, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/005-1.2045.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 1.2454 - acc: 0.6036 - val_loss: 1.2045 - val_acc: 0.6399\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1779 - acc: 0.6276\n",
      "Epoch 00006: val_loss improved from 1.20448 to 1.05721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/006-1.0572.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.1778 - acc: 0.6277 - val_loss: 1.0572 - val_acc: 0.6704\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.6556\n",
      "Epoch 00007: val_loss improved from 1.05721 to 1.02453, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/007-1.0245.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.1012 - acc: 0.6556 - val_loss: 1.0245 - val_acc: 0.6883\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0234 - acc: 0.6818\n",
      "Epoch 00008: val_loss improved from 1.02453 to 0.94613, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/008-0.9461.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.0233 - acc: 0.6819 - val_loss: 0.9461 - val_acc: 0.7147\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9630 - acc: 0.7025\n",
      "Epoch 00009: val_loss improved from 0.94613 to 0.87681, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/009-0.8768.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.9629 - acc: 0.7026 - val_loss: 0.8768 - val_acc: 0.7382\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.7301\n",
      "Epoch 00010: val_loss improved from 0.87681 to 0.83720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/010-0.8372.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8875 - acc: 0.7301 - val_loss: 0.8372 - val_acc: 0.7594\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8195 - acc: 0.7497\n",
      "Epoch 00011: val_loss improved from 0.83720 to 0.75325, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/011-0.7533.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.8195 - acc: 0.7497 - val_loss: 0.7533 - val_acc: 0.7829\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.7704\n",
      "Epoch 00012: val_loss improved from 0.75325 to 0.74765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/012-0.7477.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.7624 - acc: 0.7704 - val_loss: 0.7477 - val_acc: 0.7794\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.7876\n",
      "Epoch 00013: val_loss improved from 0.74765 to 0.68160, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/013-0.6816.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.7076 - acc: 0.7876 - val_loss: 0.6816 - val_acc: 0.8048\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.8017\n",
      "Epoch 00014: val_loss improved from 0.68160 to 0.65487, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/014-0.6549.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.6563 - acc: 0.8018 - val_loss: 0.6549 - val_acc: 0.8123\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.8088\n",
      "Epoch 00015: val_loss improved from 0.65487 to 0.61535, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/015-0.6153.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.6308 - acc: 0.8088 - val_loss: 0.6153 - val_acc: 0.8253\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.8218\n",
      "Epoch 00016: val_loss did not improve from 0.61535\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5949 - acc: 0.8218 - val_loss: 0.6187 - val_acc: 0.8176\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.8310\n",
      "Epoch 00017: val_loss improved from 0.61535 to 0.55973, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/017-0.5597.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.5640 - acc: 0.8310 - val_loss: 0.5597 - val_acc: 0.8451\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5274 - acc: 0.8416\n",
      "Epoch 00018: val_loss did not improve from 0.55973\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5275 - acc: 0.8416 - val_loss: 0.5720 - val_acc: 0.8437\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8475\n",
      "Epoch 00019: val_loss improved from 0.55973 to 0.50134, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/019-0.5013.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.5076 - acc: 0.8475 - val_loss: 0.5013 - val_acc: 0.8630\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8549\n",
      "Epoch 00020: val_loss improved from 0.50134 to 0.48920, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/020-0.4892.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4785 - acc: 0.8550 - val_loss: 0.4892 - val_acc: 0.8665\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8649\n",
      "Epoch 00021: val_loss did not improve from 0.48920\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4465 - acc: 0.8649 - val_loss: 0.5747 - val_acc: 0.8395\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8658\n",
      "Epoch 00022: val_loss did not improve from 0.48920\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.4384 - acc: 0.8658 - val_loss: 0.5205 - val_acc: 0.8507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8730\n",
      "Epoch 00023: val_loss did not improve from 0.48920\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.4215 - acc: 0.8731 - val_loss: 0.5347 - val_acc: 0.8523\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4004 - acc: 0.8781\n",
      "Epoch 00024: val_loss improved from 0.48920 to 0.47601, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/024-0.4760.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.4004 - acc: 0.8781 - val_loss: 0.4760 - val_acc: 0.8679\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8826\n",
      "Epoch 00025: val_loss did not improve from 0.47601\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3827 - acc: 0.8826 - val_loss: 0.4818 - val_acc: 0.8686\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8885\n",
      "Epoch 00026: val_loss improved from 0.47601 to 0.45455, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/026-0.4546.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3684 - acc: 0.8885 - val_loss: 0.4546 - val_acc: 0.8784\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8915\n",
      "Epoch 00027: val_loss improved from 0.45455 to 0.43757, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/027-0.4376.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3531 - acc: 0.8915 - val_loss: 0.4376 - val_acc: 0.8779\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8919\n",
      "Epoch 00028: val_loss did not improve from 0.43757\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3498 - acc: 0.8919 - val_loss: 0.4425 - val_acc: 0.8824\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.9012\n",
      "Epoch 00029: val_loss did not improve from 0.43757\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3242 - acc: 0.9012 - val_loss: 0.4443 - val_acc: 0.8835\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.9032\n",
      "Epoch 00030: val_loss improved from 0.43757 to 0.43702, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/030-0.4370.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3156 - acc: 0.9032 - val_loss: 0.4370 - val_acc: 0.8826\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9062\n",
      "Epoch 00031: val_loss improved from 0.43702 to 0.42163, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/031-0.4216.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.3069 - acc: 0.9062 - val_loss: 0.4216 - val_acc: 0.8884\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9089\n",
      "Epoch 00032: val_loss did not improve from 0.42163\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2962 - acc: 0.9089 - val_loss: 0.4218 - val_acc: 0.8915\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9122\n",
      "Epoch 00033: val_loss improved from 0.42163 to 0.41654, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/033-0.4165.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2830 - acc: 0.9122 - val_loss: 0.4165 - val_acc: 0.8833\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9158\n",
      "Epoch 00034: val_loss did not improve from 0.41654\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2711 - acc: 0.9158 - val_loss: 0.4186 - val_acc: 0.8877\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9165\n",
      "Epoch 00035: val_loss improved from 0.41654 to 0.40208, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/035-0.4021.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2659 - acc: 0.9165 - val_loss: 0.4021 - val_acc: 0.8954\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9200\n",
      "Epoch 00036: val_loss did not improve from 0.40208\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2540 - acc: 0.9200 - val_loss: 0.4632 - val_acc: 0.8803\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9228\n",
      "Epoch 00037: val_loss did not improve from 0.40208\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2477 - acc: 0.9228 - val_loss: 0.4226 - val_acc: 0.8849\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9232\n",
      "Epoch 00038: val_loss did not improve from 0.40208\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2387 - acc: 0.9232 - val_loss: 0.4547 - val_acc: 0.8784\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9243\n",
      "Epoch 00039: val_loss improved from 0.40208 to 0.40138, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv_checkpoint/039-0.4014.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.2366 - acc: 0.9243 - val_loss: 0.4014 - val_acc: 0.8998\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9293\n",
      "Epoch 00040: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2270 - acc: 0.9294 - val_loss: 0.4147 - val_acc: 0.8947\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9292\n",
      "Epoch 00041: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2219 - acc: 0.9291 - val_loss: 0.4708 - val_acc: 0.8852\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9298\n",
      "Epoch 00042: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2208 - acc: 0.9298 - val_loss: 0.4199 - val_acc: 0.8959\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9339\n",
      "Epoch 00043: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2087 - acc: 0.9339 - val_loss: 0.4178 - val_acc: 0.8942\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9346\n",
      "Epoch 00044: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2077 - acc: 0.9346 - val_loss: 0.4425 - val_acc: 0.8935\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9387\n",
      "Epoch 00045: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1914 - acc: 0.9387 - val_loss: 0.4125 - val_acc: 0.8977\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9399\n",
      "Epoch 00046: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1885 - acc: 0.9399 - val_loss: 0.4337 - val_acc: 0.8977\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9383\n",
      "Epoch 00047: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1925 - acc: 0.9383 - val_loss: 0.4208 - val_acc: 0.9029\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9395\n",
      "Epoch 00048: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1840 - acc: 0.9394 - val_loss: 0.4131 - val_acc: 0.9012\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9429\n",
      "Epoch 00049: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1807 - acc: 0.9429 - val_loss: 0.4036 - val_acc: 0.9038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9426\n",
      "Epoch 00050: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1753 - acc: 0.9426 - val_loss: 0.4247 - val_acc: 0.9038\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9439\n",
      "Epoch 00051: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1737 - acc: 0.9439 - val_loss: 0.4153 - val_acc: 0.9017\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9478\n",
      "Epoch 00052: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1615 - acc: 0.9478 - val_loss: 0.4304 - val_acc: 0.9012\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9475\n",
      "Epoch 00053: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1621 - acc: 0.9475 - val_loss: 0.4202 - val_acc: 0.8989\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9479\n",
      "Epoch 00054: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1610 - acc: 0.9479 - val_loss: 0.4403 - val_acc: 0.9017\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9480\n",
      "Epoch 00055: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1582 - acc: 0.9481 - val_loss: 0.4543 - val_acc: 0.8915\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9514\n",
      "Epoch 00056: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1532 - acc: 0.9514 - val_loss: 0.4130 - val_acc: 0.9129\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9501\n",
      "Epoch 00057: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1533 - acc: 0.9501 - val_loss: 0.4106 - val_acc: 0.9036\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9512\n",
      "Epoch 00058: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1494 - acc: 0.9512 - val_loss: 0.4350 - val_acc: 0.8996\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9521\n",
      "Epoch 00059: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1445 - acc: 0.9522 - val_loss: 0.4610 - val_acc: 0.9043\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9536\n",
      "Epoch 00060: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1389 - acc: 0.9536 - val_loss: 0.4243 - val_acc: 0.9033\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9562\n",
      "Epoch 00061: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1373 - acc: 0.9562 - val_loss: 0.4379 - val_acc: 0.9043\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9551\n",
      "Epoch 00062: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1348 - acc: 0.9551 - val_loss: 0.4647 - val_acc: 0.8942\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9572\n",
      "Epoch 00063: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1297 - acc: 0.9572 - val_loss: 0.4087 - val_acc: 0.9094\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9567\n",
      "Epoch 00064: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1312 - acc: 0.9567 - val_loss: 0.4537 - val_acc: 0.9022\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9571\n",
      "Epoch 00065: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1330 - acc: 0.9571 - val_loss: 0.4363 - val_acc: 0.9099\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9587\n",
      "Epoch 00066: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1262 - acc: 0.9586 - val_loss: 0.4148 - val_acc: 0.9108\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9583\n",
      "Epoch 00067: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.1268 - acc: 0.9583 - val_loss: 0.4101 - val_acc: 0.9131\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9612\n",
      "Epoch 00068: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1204 - acc: 0.9612 - val_loss: 0.4323 - val_acc: 0.9124\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9617\n",
      "Epoch 00069: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1177 - acc: 0.9617 - val_loss: 0.4361 - val_acc: 0.9124\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9627\n",
      "Epoch 00070: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1160 - acc: 0.9627 - val_loss: 0.4384 - val_acc: 0.9089\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9614\n",
      "Epoch 00071: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1164 - acc: 0.9614 - val_loss: 0.4332 - val_acc: 0.9122\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9629\n",
      "Epoch 00072: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1134 - acc: 0.9629 - val_loss: 0.4324 - val_acc: 0.9119\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9640\n",
      "Epoch 00073: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1084 - acc: 0.9640 - val_loss: 0.4546 - val_acc: 0.9087\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9650\n",
      "Epoch 00074: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1088 - acc: 0.9650 - val_loss: 0.4479 - val_acc: 0.9103\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9623\n",
      "Epoch 00075: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1104 - acc: 0.9623 - val_loss: 0.4659 - val_acc: 0.9038\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9655\n",
      "Epoch 00076: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1056 - acc: 0.9655 - val_loss: 0.4565 - val_acc: 0.8998\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9651\n",
      "Epoch 00077: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1045 - acc: 0.9651 - val_loss: 0.4623 - val_acc: 0.9068\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9638\n",
      "Epoch 00078: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1078 - acc: 0.9638 - val_loss: 0.4508 - val_acc: 0.9124\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9665\n",
      "Epoch 00079: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1038 - acc: 0.9665 - val_loss: 0.4448 - val_acc: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9670\n",
      "Epoch 00080: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1011 - acc: 0.9670 - val_loss: 0.4313 - val_acc: 0.9115\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9668\n",
      "Epoch 00081: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1026 - acc: 0.9668 - val_loss: 0.4859 - val_acc: 0.9057\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9660\n",
      "Epoch 00082: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1010 - acc: 0.9660 - val_loss: 0.4358 - val_acc: 0.9189\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9672\n",
      "Epoch 00083: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1015 - acc: 0.9672 - val_loss: 0.4440 - val_acc: 0.9101\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9688\n",
      "Epoch 00084: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0974 - acc: 0.9688 - val_loss: 0.4437 - val_acc: 0.9143\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9690\n",
      "Epoch 00085: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0926 - acc: 0.9690 - val_loss: 0.4409 - val_acc: 0.9150\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9684\n",
      "Epoch 00086: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0970 - acc: 0.9684 - val_loss: 0.4573 - val_acc: 0.9047\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9685\n",
      "Epoch 00087: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0936 - acc: 0.9685 - val_loss: 0.4411 - val_acc: 0.9136\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9672\n",
      "Epoch 00088: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0997 - acc: 0.9672 - val_loss: 0.4517 - val_acc: 0.9113\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9698\n",
      "Epoch 00089: val_loss did not improve from 0.40138\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0942 - acc: 0.9698 - val_loss: 0.4524 - val_acc: 0.9087\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 751us/sample - loss: 0.4481 - acc: 0.8731\n",
      "Loss: 0.44814825140922365 Accuracy: 0.8731049\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5595 - acc: 0.1525\n",
      "Epoch 00001: val_loss improved from inf to 2.23847, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/001-2.2385.hdf5\n",
      "36805/36805 [==============================] - 75s 2ms/sample - loss: 2.5594 - acc: 0.1525 - val_loss: 2.2385 - val_acc: 0.2690\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9424 - acc: 0.3604\n",
      "Epoch 00002: val_loss improved from 2.23847 to 1.53353, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/002-1.5335.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.9424 - acc: 0.3604 - val_loss: 1.5335 - val_acc: 0.5034\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4691 - acc: 0.5156\n",
      "Epoch 00003: val_loss improved from 1.53353 to 1.25591, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/003-1.2559.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4691 - acc: 0.5156 - val_loss: 1.2559 - val_acc: 0.5954\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2530 - acc: 0.5954\n",
      "Epoch 00004: val_loss improved from 1.25591 to 1.04004, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/004-1.0400.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2530 - acc: 0.5954 - val_loss: 1.0400 - val_acc: 0.6685\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1172 - acc: 0.6421\n",
      "Epoch 00005: val_loss improved from 1.04004 to 0.93786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/005-0.9379.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1171 - acc: 0.6421 - val_loss: 0.9379 - val_acc: 0.7063\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0139 - acc: 0.6743\n",
      "Epoch 00006: val_loss improved from 0.93786 to 0.85183, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/006-0.8518.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0140 - acc: 0.6743 - val_loss: 0.8518 - val_acc: 0.7345\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7022\n",
      "Epoch 00007: val_loss improved from 0.85183 to 0.80295, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/007-0.8029.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9360 - acc: 0.7022 - val_loss: 0.8029 - val_acc: 0.7524\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8569 - acc: 0.7268\n",
      "Epoch 00008: val_loss did not improve from 0.80295\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8570 - acc: 0.7268 - val_loss: 0.8157 - val_acc: 0.7603\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8039 - acc: 0.7450\n",
      "Epoch 00009: val_loss improved from 0.80295 to 0.69518, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/009-0.6952.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8038 - acc: 0.7450 - val_loss: 0.6952 - val_acc: 0.7855\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7679\n",
      "Epoch 00010: val_loss did not improve from 0.69518\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7316 - acc: 0.7678 - val_loss: 0.7244 - val_acc: 0.7794\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6787 - acc: 0.7875\n",
      "Epoch 00011: val_loss improved from 0.69518 to 0.65020, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/011-0.6502.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6787 - acc: 0.7875 - val_loss: 0.6502 - val_acc: 0.8074\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.7985\n",
      "Epoch 00012: val_loss improved from 0.65020 to 0.57257, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/012-0.5726.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6370 - acc: 0.7985 - val_loss: 0.5726 - val_acc: 0.8337\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5826 - acc: 0.8167\n",
      "Epoch 00013: val_loss improved from 0.57257 to 0.52388, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/013-0.5239.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5825 - acc: 0.8167 - val_loss: 0.5239 - val_acc: 0.8493\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.8282\n",
      "Epoch 00014: val_loss improved from 0.52388 to 0.49765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/014-0.4977.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5458 - acc: 0.8282 - val_loss: 0.4977 - val_acc: 0.8553\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5135 - acc: 0.8391\n",
      "Epoch 00015: val_loss did not improve from 0.49765\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5135 - acc: 0.8391 - val_loss: 0.5153 - val_acc: 0.8521\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8509\n",
      "Epoch 00016: val_loss did not improve from 0.49765\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4786 - acc: 0.8509 - val_loss: 0.5099 - val_acc: 0.8463\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8598\n",
      "Epoch 00017: val_loss improved from 0.49765 to 0.49683, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/017-0.4968.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4509 - acc: 0.8598 - val_loss: 0.4968 - val_acc: 0.8523\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4250 - acc: 0.8688\n",
      "Epoch 00018: val_loss improved from 0.49683 to 0.47054, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/018-0.4705.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4250 - acc: 0.8688 - val_loss: 0.4705 - val_acc: 0.8593\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8737\n",
      "Epoch 00019: val_loss improved from 0.47054 to 0.42190, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/019-0.4219.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4050 - acc: 0.8737 - val_loss: 0.4219 - val_acc: 0.8779\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8800\n",
      "Epoch 00020: val_loss did not improve from 0.42190\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3839 - acc: 0.8800 - val_loss: 0.4302 - val_acc: 0.8693\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8879\n",
      "Epoch 00021: val_loss improved from 0.42190 to 0.37458, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/021-0.3746.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3560 - acc: 0.8879 - val_loss: 0.3746 - val_acc: 0.8924\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8930\n",
      "Epoch 00022: val_loss did not improve from 0.37458\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3402 - acc: 0.8930 - val_loss: 0.3804 - val_acc: 0.8898\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8996\n",
      "Epoch 00023: val_loss did not improve from 0.37458\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3195 - acc: 0.8997 - val_loss: 0.3938 - val_acc: 0.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9040\n",
      "Epoch 00024: val_loss improved from 0.37458 to 0.35372, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/024-0.3537.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3043 - acc: 0.9040 - val_loss: 0.3537 - val_acc: 0.9092\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9109\n",
      "Epoch 00025: val_loss did not improve from 0.35372\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2845 - acc: 0.9109 - val_loss: 0.3688 - val_acc: 0.9015\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9119\n",
      "Epoch 00026: val_loss did not improve from 0.35372\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2766 - acc: 0.9119 - val_loss: 0.3736 - val_acc: 0.8947\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9176\n",
      "Epoch 00027: val_loss did not improve from 0.35372\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2636 - acc: 0.9176 - val_loss: 0.3748 - val_acc: 0.8961\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2575 - acc: 0.9181\n",
      "Epoch 00028: val_loss improved from 0.35372 to 0.35361, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/028-0.3536.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2575 - acc: 0.9181 - val_loss: 0.3536 - val_acc: 0.9068\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9239\n",
      "Epoch 00029: val_loss improved from 0.35361 to 0.31175, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/029-0.3117.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2411 - acc: 0.9239 - val_loss: 0.3117 - val_acc: 0.9143\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9273\n",
      "Epoch 00030: val_loss improved from 0.31175 to 0.29502, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/030-0.2950.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2287 - acc: 0.9273 - val_loss: 0.2950 - val_acc: 0.9199\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9308\n",
      "Epoch 00031: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2158 - acc: 0.9308 - val_loss: 0.3145 - val_acc: 0.9150\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9283\n",
      "Epoch 00032: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2192 - acc: 0.9284 - val_loss: 0.3095 - val_acc: 0.9215\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9344\n",
      "Epoch 00033: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2038 - acc: 0.9344 - val_loss: 0.3098 - val_acc: 0.9154\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9358\n",
      "Epoch 00034: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.2005 - acc: 0.9358 - val_loss: 0.2982 - val_acc: 0.9185\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9389\n",
      "Epoch 00035: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1882 - acc: 0.9388 - val_loss: 0.3012 - val_acc: 0.9203\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9429\n",
      "Epoch 00036: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1802 - acc: 0.9429 - val_loss: 0.3186 - val_acc: 0.9178\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9439\n",
      "Epoch 00037: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1716 - acc: 0.9439 - val_loss: 0.3042 - val_acc: 0.9243\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9454\n",
      "Epoch 00038: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1733 - acc: 0.9454 - val_loss: 0.3110 - val_acc: 0.9189\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.29502\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1655 - acc: 0.9468 - val_loss: 0.3019 - val_acc: 0.9234\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9490\n",
      "Epoch 00040: val_loss improved from 0.29502 to 0.29210, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/040-0.2921.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1559 - acc: 0.9491 - val_loss: 0.2921 - val_acc: 0.9229\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9493\n",
      "Epoch 00041: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1553 - acc: 0.9493 - val_loss: 0.3038 - val_acc: 0.9250\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9507\n",
      "Epoch 00042: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1510 - acc: 0.9507 - val_loss: 0.2933 - val_acc: 0.9285\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9542\n",
      "Epoch 00043: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1413 - acc: 0.9542 - val_loss: 0.2976 - val_acc: 0.9278\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9544\n",
      "Epoch 00044: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1371 - acc: 0.9544 - val_loss: 0.3307 - val_acc: 0.9238\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9533\n",
      "Epoch 00045: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1434 - acc: 0.9533 - val_loss: 0.2966 - val_acc: 0.9250\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9574\n",
      "Epoch 00046: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1331 - acc: 0.9574 - val_loss: 0.3168 - val_acc: 0.9264\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9585\n",
      "Epoch 00047: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1279 - acc: 0.9585 - val_loss: 0.3340 - val_acc: 0.9213\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9586\n",
      "Epoch 00048: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1259 - acc: 0.9586 - val_loss: 0.3119 - val_acc: 0.9255\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9608\n",
      "Epoch 00049: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1198 - acc: 0.9608 - val_loss: 0.3140 - val_acc: 0.9250\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9609\n",
      "Epoch 00050: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1176 - acc: 0.9609 - val_loss: 0.2960 - val_acc: 0.9313\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9615\n",
      "Epoch 00051: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1152 - acc: 0.9615 - val_loss: 0.3329 - val_acc: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9625\n",
      "Epoch 00052: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1115 - acc: 0.9625 - val_loss: 0.3182 - val_acc: 0.9248\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9627\n",
      "Epoch 00053: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1139 - acc: 0.9627 - val_loss: 0.3190 - val_acc: 0.9241\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9647\n",
      "Epoch 00054: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1055 - acc: 0.9647 - val_loss: 0.3081 - val_acc: 0.9290\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9666\n",
      "Epoch 00055: val_loss did not improve from 0.29210\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1009 - acc: 0.9666 - val_loss: 0.3319 - val_acc: 0.9210\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9648\n",
      "Epoch 00056: val_loss improved from 0.29210 to 0.29028, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/056-0.2903.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1057 - acc: 0.9648 - val_loss: 0.2903 - val_acc: 0.9324\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9680- ETA: 1s - loss: 0\n",
      "Epoch 00057: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0964 - acc: 0.9680 - val_loss: 0.3109 - val_acc: 0.9269\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9684\n",
      "Epoch 00058: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0969 - acc: 0.9684 - val_loss: 0.3023 - val_acc: 0.9283\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9698\n",
      "Epoch 00059: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0938 - acc: 0.9698 - val_loss: 0.3163 - val_acc: 0.9299\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9690\n",
      "Epoch 00060: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0920 - acc: 0.9689 - val_loss: 0.3397 - val_acc: 0.9304\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9591\n",
      "Epoch 00061: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.1303 - acc: 0.9590 - val_loss: 0.3160 - val_acc: 0.9294\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9683\n",
      "Epoch 00062: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0970 - acc: 0.9683 - val_loss: 0.3170 - val_acc: 0.9336\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9726\n",
      "Epoch 00063: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0866 - acc: 0.9726 - val_loss: 0.3092 - val_acc: 0.9322\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9714- ETA: 0s - loss: 0.0859 - acc: \n",
      "Epoch 00064: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0859 - acc: 0.9714 - val_loss: 0.2904 - val_acc: 0.9304\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9740\n",
      "Epoch 00065: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0787 - acc: 0.9740 - val_loss: 0.3276 - val_acc: 0.9294\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9727\n",
      "Epoch 00066: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0818 - acc: 0.9727 - val_loss: 0.3330 - val_acc: 0.9238\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9733\n",
      "Epoch 00067: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0805 - acc: 0.9733 - val_loss: 0.2982 - val_acc: 0.9364\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9741\n",
      "Epoch 00068: val_loss did not improve from 0.29028\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0774 - acc: 0.9741 - val_loss: 0.3204 - val_acc: 0.9371\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9745\n",
      "Epoch 00069: val_loss improved from 0.29028 to 0.28066, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv_checkpoint/069-0.2807.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0766 - acc: 0.9745 - val_loss: 0.2807 - val_acc: 0.9345\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9740\n",
      "Epoch 00070: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0766 - acc: 0.9740 - val_loss: 0.2991 - val_acc: 0.9331\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9752\n",
      "Epoch 00071: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0762 - acc: 0.9752 - val_loss: 0.3277 - val_acc: 0.9327\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9736\n",
      "Epoch 00072: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0817 - acc: 0.9736 - val_loss: 0.3059 - val_acc: 0.9336\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9768\n",
      "Epoch 00073: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0749 - acc: 0.9768 - val_loss: 0.3443 - val_acc: 0.9266\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9762\n",
      "Epoch 00074: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0723 - acc: 0.9763 - val_loss: 0.3050 - val_acc: 0.9343\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9784\n",
      "Epoch 00075: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0671 - acc: 0.9784 - val_loss: 0.3301 - val_acc: 0.9283\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9759\n",
      "Epoch 00076: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0746 - acc: 0.9759 - val_loss: 0.3085 - val_acc: 0.9359\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9774\n",
      "Epoch 00077: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0673 - acc: 0.9774 - val_loss: 0.3225 - val_acc: 0.9317\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9784\n",
      "Epoch 00078: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0645 - acc: 0.9784 - val_loss: 0.3286 - val_acc: 0.9322\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9794\n",
      "Epoch 00079: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0647 - acc: 0.9794 - val_loss: 0.3016 - val_acc: 0.9348\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9779\n",
      "Epoch 00080: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0670 - acc: 0.9779 - val_loss: 0.3282 - val_acc: 0.9329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9789\n",
      "Epoch 00081: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0651 - acc: 0.9789 - val_loss: 0.3154 - val_acc: 0.9359\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9779\n",
      "Epoch 00082: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0666 - acc: 0.9779 - val_loss: 0.3208 - val_acc: 0.9280\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9792\n",
      "Epoch 00083: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0640 - acc: 0.9792 - val_loss: 0.3610 - val_acc: 0.9299\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9797\n",
      "Epoch 00084: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0631 - acc: 0.9797 - val_loss: 0.3468 - val_acc: 0.9287\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9802\n",
      "Epoch 00085: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0585 - acc: 0.9802 - val_loss: 0.3070 - val_acc: 0.9338\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9802\n",
      "Epoch 00086: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0598 - acc: 0.9802 - val_loss: 0.3314 - val_acc: 0.9378\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9798\n",
      "Epoch 00087: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0616 - acc: 0.9798 - val_loss: 0.3173 - val_acc: 0.9364\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9818\n",
      "Epoch 00088: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0560 - acc: 0.9818 - val_loss: 0.3202 - val_acc: 0.9383\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9807\n",
      "Epoch 00089: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0583 - acc: 0.9807 - val_loss: 0.3223 - val_acc: 0.9322\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9807\n",
      "Epoch 00090: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0605 - acc: 0.9807 - val_loss: 0.3358 - val_acc: 0.9311\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9808\n",
      "Epoch 00091: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0573 - acc: 0.9808 - val_loss: 0.3078 - val_acc: 0.9387\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9820\n",
      "Epoch 00092: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0548 - acc: 0.9820 - val_loss: 0.3410 - val_acc: 0.9355\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9810\n",
      "Epoch 00093: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0602 - acc: 0.9810 - val_loss: 0.3006 - val_acc: 0.9383\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9823\n",
      "Epoch 00094: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0525 - acc: 0.9823 - val_loss: 0.3462 - val_acc: 0.9338\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9825\n",
      "Epoch 00095: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0529 - acc: 0.9825 - val_loss: 0.3426 - val_acc: 0.9280\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9831\n",
      "Epoch 00096: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0540 - acc: 0.9831 - val_loss: 0.3182 - val_acc: 0.9359\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9827\n",
      "Epoch 00097: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0533 - acc: 0.9827 - val_loss: 0.3094 - val_acc: 0.9383\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9840\n",
      "Epoch 00098: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0521 - acc: 0.9840 - val_loss: 0.3233 - val_acc: 0.9371\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9830\n",
      "Epoch 00099: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0499 - acc: 0.9830 - val_loss: 0.3472 - val_acc: 0.9341\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9840\n",
      "Epoch 00100: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0507 - acc: 0.9840 - val_loss: 0.3419 - val_acc: 0.9343\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9830\n",
      "Epoch 00101: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0524 - acc: 0.9830 - val_loss: 0.3109 - val_acc: 0.9429\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9830\n",
      "Epoch 00102: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0523 - acc: 0.9830 - val_loss: 0.3161 - val_acc: 0.9369\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9846\n",
      "Epoch 00103: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0475 - acc: 0.9846 - val_loss: 0.3433 - val_acc: 0.9376\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9844\n",
      "Epoch 00104: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0498 - acc: 0.9843 - val_loss: 0.3082 - val_acc: 0.9357\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9841\n",
      "Epoch 00105: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0487 - acc: 0.9841 - val_loss: 0.3355 - val_acc: 0.9411\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9849\n",
      "Epoch 00106: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0491 - acc: 0.9849 - val_loss: 0.3483 - val_acc: 0.9373\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9857\n",
      "Epoch 00107: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0439 - acc: 0.9857 - val_loss: 0.3415 - val_acc: 0.9357\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9830\n",
      "Epoch 00108: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0528 - acc: 0.9830 - val_loss: 0.2956 - val_acc: 0.9404\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9853\n",
      "Epoch 00109: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0451 - acc: 0.9853 - val_loss: 0.3314 - val_acc: 0.9408\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9848\n",
      "Epoch 00110: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0478 - acc: 0.9848 - val_loss: 0.3286 - val_acc: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9867\n",
      "Epoch 00111: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0427 - acc: 0.9867 - val_loss: 0.3431 - val_acc: 0.9378\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9846\n",
      "Epoch 00112: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0474 - acc: 0.9846 - val_loss: 0.3110 - val_acc: 0.9383\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9870\n",
      "Epoch 00113: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0411 - acc: 0.9870 - val_loss: 0.3054 - val_acc: 0.9394\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9862\n",
      "Epoch 00114: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0427 - acc: 0.9862 - val_loss: 0.3230 - val_acc: 0.9415\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9857\n",
      "Epoch 00115: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0454 - acc: 0.9857 - val_loss: 0.3336 - val_acc: 0.9429\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9883\n",
      "Epoch 00116: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0399 - acc: 0.9883 - val_loss: 0.3187 - val_acc: 0.9427\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9863\n",
      "Epoch 00117: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0449 - acc: 0.9863 - val_loss: 0.3248 - val_acc: 0.9390\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9872\n",
      "Epoch 00118: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0415 - acc: 0.9872 - val_loss: 0.3235 - val_acc: 0.9399\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9855\n",
      "Epoch 00119: val_loss did not improve from 0.28066\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.0469 - acc: 0.9855 - val_loss: 0.3100 - val_acc: 0.9357\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 810us/sample - loss: 0.2989 - acc: 0.9248\n",
      "Loss: 0.29892948167717714 Accuracy: 0.9248183\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6271 - acc: 0.1196\n",
      "Epoch 00001: val_loss improved from inf to 2.28136, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/001-2.2814.hdf5\n",
      "36805/36805 [==============================] - 79s 2ms/sample - loss: 2.6269 - acc: 0.1197 - val_loss: 2.2814 - val_acc: 0.2916\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8824 - acc: 0.3779\n",
      "Epoch 00002: val_loss improved from 2.28136 to 1.57365, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/002-1.5736.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.8824 - acc: 0.3779 - val_loss: 1.5736 - val_acc: 0.4973\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3858 - acc: 0.5448\n",
      "Epoch 00003: val_loss improved from 1.57365 to 1.07061, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/003-1.0706.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.3858 - acc: 0.5448 - val_loss: 1.0706 - val_acc: 0.6704\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1111 - acc: 0.6414\n",
      "Epoch 00004: val_loss improved from 1.07061 to 0.89661, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/004-0.8966.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.1111 - acc: 0.6414 - val_loss: 0.8966 - val_acc: 0.7235\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9509 - acc: 0.6935\n",
      "Epoch 00005: val_loss improved from 0.89661 to 0.75635, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/005-0.7564.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.9509 - acc: 0.6935 - val_loss: 0.7564 - val_acc: 0.7626\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.7283\n",
      "Epoch 00006: val_loss improved from 0.75635 to 0.65689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/006-0.6569.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.8444 - acc: 0.7282 - val_loss: 0.6569 - val_acc: 0.7904\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7582\n",
      "Epoch 00007: val_loss improved from 0.65689 to 0.59160, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/007-0.5916.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.7488 - acc: 0.7582 - val_loss: 0.5916 - val_acc: 0.8143\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6763 - acc: 0.7815\n",
      "Epoch 00008: val_loss improved from 0.59160 to 0.53810, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/008-0.5381.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.6762 - acc: 0.7815 - val_loss: 0.5381 - val_acc: 0.8339\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.8049\n",
      "Epoch 00009: val_loss improved from 0.53810 to 0.47515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/009-0.4751.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6083 - acc: 0.8049 - val_loss: 0.4751 - val_acc: 0.8542\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8244\n",
      "Epoch 00010: val_loss improved from 0.47515 to 0.43558, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/010-0.4356.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.5473 - acc: 0.8245 - val_loss: 0.4356 - val_acc: 0.8689\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8391\n",
      "Epoch 00011: val_loss improved from 0.43558 to 0.41143, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/011-0.4114.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5042 - acc: 0.8391 - val_loss: 0.4114 - val_acc: 0.8700\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8498\n",
      "Epoch 00012: val_loss improved from 0.41143 to 0.36070, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/012-0.3607.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4635 - acc: 0.8497 - val_loss: 0.3607 - val_acc: 0.8919\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8653\n",
      "Epoch 00013: val_loss improved from 0.36070 to 0.34345, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/013-0.3434.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4244 - acc: 0.8653 - val_loss: 0.3434 - val_acc: 0.8931\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8752\n",
      "Epoch 00014: val_loss improved from 0.34345 to 0.34163, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/014-0.3416.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3907 - acc: 0.8752 - val_loss: 0.3416 - val_acc: 0.8947\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8861\n",
      "Epoch 00015: val_loss improved from 0.34163 to 0.28493, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/015-0.2849.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3601 - acc: 0.8861 - val_loss: 0.2849 - val_acc: 0.9154\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8896\n",
      "Epoch 00016: val_loss improved from 0.28493 to 0.27012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/016-0.2701.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3431 - acc: 0.8896 - val_loss: 0.2701 - val_acc: 0.9182\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8971\n",
      "Epoch 00017: val_loss did not improve from 0.27012\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.3225 - acc: 0.8971 - val_loss: 0.2934 - val_acc: 0.9136\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.8996\n",
      "Epoch 00018: val_loss improved from 0.27012 to 0.25389, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/018-0.2539.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3063 - acc: 0.8997 - val_loss: 0.2539 - val_acc: 0.9264\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9077\n",
      "Epoch 00019: val_loss did not improve from 0.25389\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2918 - acc: 0.9077 - val_loss: 0.3128 - val_acc: 0.9103\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9114\n",
      "Epoch 00020: val_loss improved from 0.25389 to 0.24261, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/020-0.2426.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2713 - acc: 0.9114 - val_loss: 0.2426 - val_acc: 0.9331\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9141\n",
      "Epoch 00021: val_loss did not improve from 0.24261\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2611 - acc: 0.9141 - val_loss: 0.2525 - val_acc: 0.9292\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9198\n",
      "Epoch 00022: val_loss did not improve from 0.24261\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2471 - acc: 0.9198 - val_loss: 0.2783 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9239\n",
      "Epoch 00023: val_loss did not improve from 0.24261\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2373 - acc: 0.9239 - val_loss: 0.2646 - val_acc: 0.9241\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9241\n",
      "Epoch 00024: val_loss improved from 0.24261 to 0.22974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/024-0.2297.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2355 - acc: 0.9241 - val_loss: 0.2297 - val_acc: 0.9359\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9275\n",
      "Epoch 00025: val_loss improved from 0.22974 to 0.22833, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/025-0.2283.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2217 - acc: 0.9275 - val_loss: 0.2283 - val_acc: 0.9317\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9327\n",
      "Epoch 00026: val_loss did not improve from 0.22833\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2097 - acc: 0.9327 - val_loss: 0.2472 - val_acc: 0.9271\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9341\n",
      "Epoch 00027: val_loss improved from 0.22833 to 0.22349, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/027-0.2235.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2006 - acc: 0.9341 - val_loss: 0.2235 - val_acc: 0.9366\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9350\n",
      "Epoch 00028: val_loss did not improve from 0.22349\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1950 - acc: 0.9350 - val_loss: 0.2454 - val_acc: 0.9336\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9406\n",
      "Epoch 00029: val_loss improved from 0.22349 to 0.21302, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/029-0.2130.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.1815 - acc: 0.9406 - val_loss: 0.2130 - val_acc: 0.9401\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9405\n",
      "Epoch 00030: val_loss improved from 0.21302 to 0.20857, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/030-0.2086.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1806 - acc: 0.9405 - val_loss: 0.2086 - val_acc: 0.9450\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9438\n",
      "Epoch 00031: val_loss did not improve from 0.20857\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1707 - acc: 0.9438 - val_loss: 0.2415 - val_acc: 0.9387\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9446\n",
      "Epoch 00032: val_loss did not improve from 0.20857\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1711 - acc: 0.9445 - val_loss: 0.2371 - val_acc: 0.9336\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9424\n",
      "Epoch 00033: val_loss improved from 0.20857 to 0.20508, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/033-0.2051.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1785 - acc: 0.9424 - val_loss: 0.2051 - val_acc: 0.9457\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9478\n",
      "Epoch 00034: val_loss did not improve from 0.20508\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.1554 - acc: 0.9478 - val_loss: 0.2163 - val_acc: 0.9406\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9512\n",
      "Epoch 00035: val_loss did not improve from 0.20508\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1479 - acc: 0.9512 - val_loss: 0.2133 - val_acc: 0.9390\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9516\n",
      "Epoch 00036: val_loss did not improve from 0.20508\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1466 - acc: 0.9516 - val_loss: 0.2069 - val_acc: 0.9408\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9526\n",
      "Epoch 00037: val_loss did not improve from 0.20508\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1405 - acc: 0.9526 - val_loss: 0.2073 - val_acc: 0.9425\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9551\n",
      "Epoch 00038: val_loss improved from 0.20508 to 0.20068, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/038-0.2007.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1347 - acc: 0.9551 - val_loss: 0.2007 - val_acc: 0.9443\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9559\n",
      "Epoch 00039: val_loss did not improve from 0.20068\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1354 - acc: 0.9559 - val_loss: 0.2200 - val_acc: 0.9415\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9595\n",
      "Epoch 00040: val_loss did not improve from 0.20068\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1226 - acc: 0.9595 - val_loss: 0.2045 - val_acc: 0.9478\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9585\n",
      "Epoch 00041: val_loss did not improve from 0.20068\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1278 - acc: 0.9584 - val_loss: 0.2092 - val_acc: 0.9443\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9611\n",
      "Epoch 00042: val_loss did not improve from 0.20068\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1195 - acc: 0.9611 - val_loss: 0.2180 - val_acc: 0.9429\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9598\n",
      "Epoch 00043: val_loss improved from 0.20068 to 0.19929, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/043-0.1993.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1179 - acc: 0.9598 - val_loss: 0.1993 - val_acc: 0.9483\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9626\n",
      "Epoch 00044: val_loss did not improve from 0.19929\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1110 - acc: 0.9626 - val_loss: 0.2065 - val_acc: 0.9474\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9622\n",
      "Epoch 00045: val_loss did not improve from 0.19929\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1158 - acc: 0.9622 - val_loss: 0.2126 - val_acc: 0.9457\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9648\n",
      "Epoch 00046: val_loss did not improve from 0.19929\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.1094 - acc: 0.9647 - val_loss: 0.2075 - val_acc: 0.9448\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9631\n",
      "Epoch 00047: val_loss improved from 0.19929 to 0.18365, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/047-0.1836.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.1081 - acc: 0.9631 - val_loss: 0.1836 - val_acc: 0.9506\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9664\n",
      "Epoch 00048: val_loss did not improve from 0.18365\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0992 - acc: 0.9664 - val_loss: 0.2178 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9661\n",
      "Epoch 00049: val_loss improved from 0.18365 to 0.18266, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/049-0.1827.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0987 - acc: 0.9661 - val_loss: 0.1827 - val_acc: 0.9546\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9675\n",
      "Epoch 00050: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0965 - acc: 0.9675 - val_loss: 0.2241 - val_acc: 0.9434\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9678\n",
      "Epoch 00051: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0959 - acc: 0.9678 - val_loss: 0.1982 - val_acc: 0.9509\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9700\n",
      "Epoch 00052: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0904 - acc: 0.9700 - val_loss: 0.1960 - val_acc: 0.9522\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9696\n",
      "Epoch 00053: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0881 - acc: 0.9696 - val_loss: 0.2157 - val_acc: 0.9490\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9706\n",
      "Epoch 00054: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0896 - acc: 0.9706 - val_loss: 0.2002 - val_acc: 0.9550\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9716\n",
      "Epoch 00055: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0843 - acc: 0.9716 - val_loss: 0.2116 - val_acc: 0.9527\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9718\n",
      "Epoch 00056: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0828 - acc: 0.9718 - val_loss: 0.2539 - val_acc: 0.9453\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9737\n",
      "Epoch 00057: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0789 - acc: 0.9738 - val_loss: 0.2147 - val_acc: 0.9506\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9726\n",
      "Epoch 00058: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0809 - acc: 0.9726 - val_loss: 0.1950 - val_acc: 0.9567\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9737\n",
      "Epoch 00059: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0790 - acc: 0.9737 - val_loss: 0.2002 - val_acc: 0.9513\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9736\n",
      "Epoch 00060: val_loss did not improve from 0.18266\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0794 - acc: 0.9736 - val_loss: 0.2058 - val_acc: 0.9499\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9741\n",
      "Epoch 00061: val_loss improved from 0.18266 to 0.18199, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv_checkpoint/061-0.1820.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0761 - acc: 0.9741 - val_loss: 0.1820 - val_acc: 0.9560\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9748\n",
      "Epoch 00062: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0742 - acc: 0.9748 - val_loss: 0.1967 - val_acc: 0.9502\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9748\n",
      "Epoch 00063: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0742 - acc: 0.9748 - val_loss: 0.2031 - val_acc: 0.9513\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9771\n",
      "Epoch 00064: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0703 - acc: 0.9771 - val_loss: 0.2005 - val_acc: 0.9550\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9768\n",
      "Epoch 00065: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0692 - acc: 0.9769 - val_loss: 0.2062 - val_acc: 0.9518\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9779\n",
      "Epoch 00066: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0653 - acc: 0.9779 - val_loss: 0.2363 - val_acc: 0.9476\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9764\n",
      "Epoch 00067: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0696 - acc: 0.9764 - val_loss: 0.2016 - val_acc: 0.9506\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9782\n",
      "Epoch 00068: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0651 - acc: 0.9782 - val_loss: 0.2206 - val_acc: 0.9443\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9782\n",
      "Epoch 00069: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0622 - acc: 0.9782 - val_loss: 0.2054 - val_acc: 0.9557\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9789\n",
      "Epoch 00070: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0635 - acc: 0.9789 - val_loss: 0.2119 - val_acc: 0.9518\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9787\n",
      "Epoch 00071: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0629 - acc: 0.9788 - val_loss: 0.2112 - val_acc: 0.9529\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9818\n",
      "Epoch 00072: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0568 - acc: 0.9818 - val_loss: 0.2048 - val_acc: 0.9560\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9804\n",
      "Epoch 00073: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0579 - acc: 0.9804 - val_loss: 0.1922 - val_acc: 0.9592\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9807\n",
      "Epoch 00074: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0592 - acc: 0.9807 - val_loss: 0.1827 - val_acc: 0.9583\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9803\n",
      "Epoch 00075: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0575 - acc: 0.9803 - val_loss: 0.1970 - val_acc: 0.9553\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9812\n",
      "Epoch 00076: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0571 - acc: 0.9812 - val_loss: 0.2328 - val_acc: 0.9497\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9824\n",
      "Epoch 00077: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0535 - acc: 0.9824 - val_loss: 0.2091 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9820\n",
      "Epoch 00078: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0546 - acc: 0.9820 - val_loss: 0.2087 - val_acc: 0.9560\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9827\n",
      "Epoch 00079: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0523 - acc: 0.9827 - val_loss: 0.2217 - val_acc: 0.9532\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9818\n",
      "Epoch 00080: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0516 - acc: 0.9818 - val_loss: 0.2067 - val_acc: 0.9536\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9832\n",
      "Epoch 00081: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0489 - acc: 0.9832 - val_loss: 0.2270 - val_acc: 0.9550\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9822\n",
      "Epoch 00082: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0518 - acc: 0.9822 - val_loss: 0.2133 - val_acc: 0.9529\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9826\n",
      "Epoch 00083: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0482 - acc: 0.9826 - val_loss: 0.2051 - val_acc: 0.9546\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00084: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0532 - acc: 0.9815 - val_loss: 0.1996 - val_acc: 0.9520\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9846\n",
      "Epoch 00085: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0475 - acc: 0.9846 - val_loss: 0.2124 - val_acc: 0.9546\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9841- ETA:\n",
      "Epoch 00086: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0474 - acc: 0.9841 - val_loss: 0.2439 - val_acc: 0.9555\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9837\n",
      "Epoch 00087: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0488 - acc: 0.9838 - val_loss: 0.1948 - val_acc: 0.9532\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9838\n",
      "Epoch 00088: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0479 - acc: 0.9838 - val_loss: 0.2098 - val_acc: 0.9550\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9851\n",
      "Epoch 00089: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0429 - acc: 0.9851 - val_loss: 0.2192 - val_acc: 0.9555\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9845\n",
      "Epoch 00090: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0464 - acc: 0.9845 - val_loss: 0.2400 - val_acc: 0.9529\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9851\n",
      "Epoch 00091: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0437 - acc: 0.9851 - val_loss: 0.2277 - val_acc: 0.9513\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9848\n",
      "Epoch 00092: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0453 - acc: 0.9848 - val_loss: 0.2091 - val_acc: 0.9525\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9846\n",
      "Epoch 00093: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0444 - acc: 0.9846 - val_loss: 0.2090 - val_acc: 0.9553\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9862\n",
      "Epoch 00094: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0425 - acc: 0.9862 - val_loss: 0.2163 - val_acc: 0.9529\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9863\n",
      "Epoch 00095: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0410 - acc: 0.9863 - val_loss: 0.2217 - val_acc: 0.9557\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9868\n",
      "Epoch 00096: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0406 - acc: 0.9868 - val_loss: 0.2373 - val_acc: 0.9564\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9870\n",
      "Epoch 00097: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0389 - acc: 0.9870 - val_loss: 0.2305 - val_acc: 0.9583\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9855\n",
      "Epoch 00098: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0440 - acc: 0.9855 - val_loss: 0.2085 - val_acc: 0.9576\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9857\n",
      "Epoch 00099: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0424 - acc: 0.9857 - val_loss: 0.2421 - val_acc: 0.9534\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9862\n",
      "Epoch 00100: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0404 - acc: 0.9863 - val_loss: 0.2082 - val_acc: 0.9557\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9862\n",
      "Epoch 00101: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0403 - acc: 0.9862 - val_loss: 0.2320 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9872\n",
      "Epoch 00102: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0389 - acc: 0.9872 - val_loss: 0.2062 - val_acc: 0.9557\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9865\n",
      "Epoch 00103: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0392 - acc: 0.9865 - val_loss: 0.2011 - val_acc: 0.9571\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9879\n",
      "Epoch 00104: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0367 - acc: 0.9879 - val_loss: 0.2249 - val_acc: 0.9564\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9871\n",
      "Epoch 00105: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0380 - acc: 0.9871 - val_loss: 0.2032 - val_acc: 0.9616\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9876\n",
      "Epoch 00106: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0368 - acc: 0.9876 - val_loss: 0.2200 - val_acc: 0.9567\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9880\n",
      "Epoch 00107: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0362 - acc: 0.9880 - val_loss: 0.2376 - val_acc: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9881\n",
      "Epoch 00108: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0346 - acc: 0.9881 - val_loss: 0.2299 - val_acc: 0.9515\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9861\n",
      "Epoch 00109: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0423 - acc: 0.9861 - val_loss: 0.2311 - val_acc: 0.9560\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9878\n",
      "Epoch 00110: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0351 - acc: 0.9878 - val_loss: 0.2322 - val_acc: 0.9557\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9881\n",
      "Epoch 00111: val_loss did not improve from 0.18199\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.0363 - acc: 0.9881 - val_loss: 0.2182 - val_acc: 0.9564\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 824us/sample - loss: 0.2158 - acc: 0.9398\n",
      "Loss: 0.2158330811177954 Accuracy: 0.93977153\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6387 - acc: 0.1130\n",
      "Epoch 00001: val_loss improved from inf to 2.31623, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/001-2.3162.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.6387 - acc: 0.1129 - val_loss: 2.3162 - val_acc: 0.2427\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9077 - acc: 0.3650\n",
      "Epoch 00002: val_loss improved from 2.31623 to 1.24552, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/002-1.2455.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.9076 - acc: 0.3650 - val_loss: 1.2455 - val_acc: 0.5912\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2859 - acc: 0.5672\n",
      "Epoch 00003: val_loss improved from 1.24552 to 0.99656, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/003-0.9966.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2859 - acc: 0.5672 - val_loss: 0.9966 - val_acc: 0.6706\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0527 - acc: 0.6525\n",
      "Epoch 00004: val_loss improved from 0.99656 to 0.75288, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/004-0.7529.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0527 - acc: 0.6525 - val_loss: 0.7529 - val_acc: 0.7661\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9095 - acc: 0.7006\n",
      "Epoch 00005: val_loss improved from 0.75288 to 0.67733, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/005-0.6773.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9096 - acc: 0.7006 - val_loss: 0.6773 - val_acc: 0.7873\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7986 - acc: 0.7365\n",
      "Epoch 00006: val_loss improved from 0.67733 to 0.59536, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/006-0.5954.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7985 - acc: 0.7365 - val_loss: 0.5954 - val_acc: 0.8036\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7164 - acc: 0.7616\n",
      "Epoch 00007: val_loss improved from 0.59536 to 0.55716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/007-0.5572.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7165 - acc: 0.7616 - val_loss: 0.5572 - val_acc: 0.8218\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.7798\n",
      "Epoch 00008: val_loss improved from 0.55716 to 0.49375, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/008-0.4938.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6655 - acc: 0.7798 - val_loss: 0.4938 - val_acc: 0.8453\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.7994\n",
      "Epoch 00009: val_loss did not improve from 0.49375\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6066 - acc: 0.7994 - val_loss: 0.5478 - val_acc: 0.8276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.8180\n",
      "Epoch 00010: val_loss improved from 0.49375 to 0.39721, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/010-0.3972.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5460 - acc: 0.8180 - val_loss: 0.3972 - val_acc: 0.8777\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8370\n",
      "Epoch 00011: val_loss did not improve from 0.39721\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5008 - acc: 0.8370 - val_loss: 0.4010 - val_acc: 0.8812\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.8536\n",
      "Epoch 00012: val_loss improved from 0.39721 to 0.34318, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/012-0.3432.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4525 - acc: 0.8536 - val_loss: 0.3432 - val_acc: 0.8987\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8654\n",
      "Epoch 00013: val_loss improved from 0.34318 to 0.30328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/013-0.3033.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4235 - acc: 0.8654 - val_loss: 0.3033 - val_acc: 0.9087\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8737\n",
      "Epoch 00014: val_loss improved from 0.30328 to 0.30226, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/014-0.3023.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3931 - acc: 0.8738 - val_loss: 0.3023 - val_acc: 0.9103\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8809\n",
      "Epoch 00015: val_loss improved from 0.30226 to 0.28365, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/015-0.2836.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3713 - acc: 0.8809 - val_loss: 0.2836 - val_acc: 0.9154\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8877\n",
      "Epoch 00016: val_loss did not improve from 0.28365\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3497 - acc: 0.8877 - val_loss: 0.2918 - val_acc: 0.9133\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8967\n",
      "Epoch 00017: val_loss did not improve from 0.28365\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3217 - acc: 0.8966 - val_loss: 0.2857 - val_acc: 0.9157\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9030\n",
      "Epoch 00018: val_loss improved from 0.28365 to 0.23438, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/018-0.2344.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3036 - acc: 0.9029 - val_loss: 0.2344 - val_acc: 0.9320\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9025\n",
      "Epoch 00019: val_loss did not improve from 0.23438\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2984 - acc: 0.9025 - val_loss: 0.2480 - val_acc: 0.9287\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9118\n",
      "Epoch 00020: val_loss improved from 0.23438 to 0.22991, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/020-0.2299.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2698 - acc: 0.9118 - val_loss: 0.2299 - val_acc: 0.9292\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9127\n",
      "Epoch 00021: val_loss improved from 0.22991 to 0.20303, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/021-0.2030.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2690 - acc: 0.9127 - val_loss: 0.2030 - val_acc: 0.9446\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9199\n",
      "Epoch 00022: val_loss did not improve from 0.20303\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2486 - acc: 0.9200 - val_loss: 0.2146 - val_acc: 0.9397\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9241\n",
      "Epoch 00023: val_loss did not improve from 0.20303\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2374 - acc: 0.9241 - val_loss: 0.2108 - val_acc: 0.9415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9251\n",
      "Epoch 00024: val_loss did not improve from 0.20303\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2296 - acc: 0.9251 - val_loss: 0.2065 - val_acc: 0.9443\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9279\n",
      "Epoch 00025: val_loss did not improve from 0.20303\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2219 - acc: 0.9279 - val_loss: 0.2058 - val_acc: 0.9436\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9314\n",
      "Epoch 00026: val_loss did not improve from 0.20303\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2123 - acc: 0.9314 - val_loss: 0.2354 - val_acc: 0.9294\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9337\n",
      "Epoch 00027: val_loss improved from 0.20303 to 0.18399, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/027-0.1840.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2074 - acc: 0.9337 - val_loss: 0.1840 - val_acc: 0.9488\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9338\n",
      "Epoch 00028: val_loss improved from 0.18399 to 0.16931, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/028-0.1693.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1992 - acc: 0.9338 - val_loss: 0.1693 - val_acc: 0.9520\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9379\n",
      "Epoch 00029: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1898 - acc: 0.9379 - val_loss: 0.1707 - val_acc: 0.9534\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9398\n",
      "Epoch 00030: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1853 - acc: 0.9398 - val_loss: 0.2194 - val_acc: 0.9352\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9386\n",
      "Epoch 00031: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1875 - acc: 0.9386 - val_loss: 0.1944 - val_acc: 0.9469\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9449\n",
      "Epoch 00032: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1664 - acc: 0.9449 - val_loss: 0.1903 - val_acc: 0.9453\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9446\n",
      "Epoch 00033: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1675 - acc: 0.9446 - val_loss: 0.1761 - val_acc: 0.9497\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9470\n",
      "Epoch 00034: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1621 - acc: 0.9470 - val_loss: 0.1831 - val_acc: 0.9481\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9489\n",
      "Epoch 00035: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1531 - acc: 0.9489 - val_loss: 0.1866 - val_acc: 0.9476\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9501\n",
      "Epoch 00036: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1523 - acc: 0.9501 - val_loss: 0.1739 - val_acc: 0.9511\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9513\n",
      "Epoch 00037: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1487 - acc: 0.9513 - val_loss: 0.1743 - val_acc: 0.9511\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9532\n",
      "Epoch 00038: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1392 - acc: 0.9532 - val_loss: 0.1699 - val_acc: 0.9536\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9536\n",
      "Epoch 00039: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1375 - acc: 0.9536 - val_loss: 0.1942 - val_acc: 0.9462\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9554\n",
      "Epoch 00040: val_loss did not improve from 0.16931\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1317 - acc: 0.9554 - val_loss: 0.1930 - val_acc: 0.9471\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9559\n",
      "Epoch 00041: val_loss improved from 0.16931 to 0.16343, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/041-0.1634.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1305 - acc: 0.9559 - val_loss: 0.1634 - val_acc: 0.9569\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9581\n",
      "Epoch 00042: val_loss improved from 0.16343 to 0.16097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/042-0.1610.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1234 - acc: 0.9581 - val_loss: 0.1610 - val_acc: 0.9567\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9599\n",
      "Epoch 00043: val_loss did not improve from 0.16097\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1228 - acc: 0.9599 - val_loss: 0.1666 - val_acc: 0.9515\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9606\n",
      "Epoch 00044: val_loss did not improve from 0.16097\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1164 - acc: 0.9606 - val_loss: 0.1749 - val_acc: 0.9546\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9611\n",
      "Epoch 00045: val_loss improved from 0.16097 to 0.15775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/045-0.1577.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1197 - acc: 0.9611 - val_loss: 0.1577 - val_acc: 0.9562\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9626\n",
      "Epoch 00046: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1116 - acc: 0.9626 - val_loss: 0.1594 - val_acc: 0.9546\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9641\n",
      "Epoch 00047: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1082 - acc: 0.9641 - val_loss: 0.1643 - val_acc: 0.9518\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9650\n",
      "Epoch 00048: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1043 - acc: 0.9650 - val_loss: 0.1668 - val_acc: 0.9555\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9652\n",
      "Epoch 00049: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1032 - acc: 0.9652 - val_loss: 0.2088 - val_acc: 0.9450\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9651\n",
      "Epoch 00050: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1038 - acc: 0.9651 - val_loss: 0.1873 - val_acc: 0.9522\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9668\n",
      "Epoch 00051: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1003 - acc: 0.9667 - val_loss: 0.1695 - val_acc: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9673\n",
      "Epoch 00052: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0974 - acc: 0.9673 - val_loss: 0.2065 - val_acc: 0.9474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9679\n",
      "Epoch 00053: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0945 - acc: 0.9679 - val_loss: 0.1630 - val_acc: 0.9571\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9690\n",
      "Epoch 00054: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0900 - acc: 0.9690 - val_loss: 0.1595 - val_acc: 0.9569\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9696\n",
      "Epoch 00055: val_loss did not improve from 0.15775\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0931 - acc: 0.9696 - val_loss: 0.1802 - val_acc: 0.9560\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9712\n",
      "Epoch 00056: val_loss improved from 0.15775 to 0.15510, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/056-0.1551.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0869 - acc: 0.9712 - val_loss: 0.1551 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9705\n",
      "Epoch 00057: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0874 - acc: 0.9705 - val_loss: 0.1582 - val_acc: 0.9623\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9713\n",
      "Epoch 00058: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0841 - acc: 0.9713 - val_loss: 0.1804 - val_acc: 0.9534\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9722\n",
      "Epoch 00059: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0814 - acc: 0.9722 - val_loss: 0.1751 - val_acc: 0.9578\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9717\n",
      "Epoch 00060: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0835 - acc: 0.9717 - val_loss: 0.1678 - val_acc: 0.9564\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9726\n",
      "Epoch 00061: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0786 - acc: 0.9726 - val_loss: 0.1875 - val_acc: 0.9553\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9737\n",
      "Epoch 00062: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0787 - acc: 0.9737 - val_loss: 0.1737 - val_acc: 0.9574\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9751\n",
      "Epoch 00063: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0722 - acc: 0.9751 - val_loss: 0.1740 - val_acc: 0.9588\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9750\n",
      "Epoch 00064: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0746 - acc: 0.9750 - val_loss: 0.1674 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9757\n",
      "Epoch 00065: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0702 - acc: 0.9757 - val_loss: 0.1782 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9755\n",
      "Epoch 00066: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0714 - acc: 0.9755 - val_loss: 0.1738 - val_acc: 0.9620\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9761\n",
      "Epoch 00067: val_loss did not improve from 0.15510\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0697 - acc: 0.9761 - val_loss: 0.1848 - val_acc: 0.9562\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9786\n",
      "Epoch 00068: val_loss improved from 0.15510 to 0.15475, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv_checkpoint/068-0.1548.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0639 - acc: 0.9786 - val_loss: 0.1548 - val_acc: 0.9625\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9756\n",
      "Epoch 00069: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0715 - acc: 0.9756 - val_loss: 0.1748 - val_acc: 0.9632\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9768\n",
      "Epoch 00070: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0671 - acc: 0.9769 - val_loss: 0.1706 - val_acc: 0.9597\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9787\n",
      "Epoch 00071: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0610 - acc: 0.9788 - val_loss: 0.1741 - val_acc: 0.9597\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9794\n",
      "Epoch 00072: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0595 - acc: 0.9794 - val_loss: 0.1742 - val_acc: 0.9634\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9804\n",
      "Epoch 00073: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0575 - acc: 0.9804 - val_loss: 0.1741 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9805\n",
      "Epoch 00074: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0588 - acc: 0.9805 - val_loss: 0.1778 - val_acc: 0.9571\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9795\n",
      "Epoch 00075: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0606 - acc: 0.9795 - val_loss: 0.1810 - val_acc: 0.9592\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9806\n",
      "Epoch 00076: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0576 - acc: 0.9806 - val_loss: 0.1877 - val_acc: 0.9576\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9798\n",
      "Epoch 00077: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0596 - acc: 0.9798 - val_loss: 0.1850 - val_acc: 0.9571\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9803\n",
      "Epoch 00078: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0566 - acc: 0.9803 - val_loss: 0.1706 - val_acc: 0.9604\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9804\n",
      "Epoch 00079: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0553 - acc: 0.9804 - val_loss: 0.1617 - val_acc: 0.9618\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00080: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0533 - acc: 0.9814 - val_loss: 0.1601 - val_acc: 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9784\n",
      "Epoch 00081: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0676 - acc: 0.9784 - val_loss: 0.1686 - val_acc: 0.9613\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9834\n",
      "Epoch 00082: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0479 - acc: 0.9834 - val_loss: 0.1597 - val_acc: 0.9602\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9829\n",
      "Epoch 00083: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0498 - acc: 0.9829 - val_loss: 0.1913 - val_acc: 0.9590\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9828\n",
      "Epoch 00084: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0502 - acc: 0.9828 - val_loss: 0.1852 - val_acc: 0.9590\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9830\n",
      "Epoch 00085: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0507 - acc: 0.9830 - val_loss: 0.1832 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9833\n",
      "Epoch 00086: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0483 - acc: 0.9833 - val_loss: 0.1804 - val_acc: 0.9574\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9850\n",
      "Epoch 00087: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0450 - acc: 0.9850 - val_loss: 0.1882 - val_acc: 0.9599\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9845\n",
      "Epoch 00088: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0476 - acc: 0.9845 - val_loss: 0.1668 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9853\n",
      "Epoch 00089: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0436 - acc: 0.9853 - val_loss: 0.1787 - val_acc: 0.9602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9838\n",
      "Epoch 00090: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0472 - acc: 0.9838 - val_loss: 0.2269 - val_acc: 0.9583\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9827\n",
      "Epoch 00091: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0517 - acc: 0.9827 - val_loss: 0.1851 - val_acc: 0.9599\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9850\n",
      "Epoch 00092: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0459 - acc: 0.9850 - val_loss: 0.1636 - val_acc: 0.9639\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9851\n",
      "Epoch 00093: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0452 - acc: 0.9851 - val_loss: 0.1856 - val_acc: 0.9588\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9864\n",
      "Epoch 00094: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0410 - acc: 0.9864 - val_loss: 0.1843 - val_acc: 0.9590\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9857\n",
      "Epoch 00095: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0429 - acc: 0.9857 - val_loss: 0.1574 - val_acc: 0.9627\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9858\n",
      "Epoch 00096: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0418 - acc: 0.9858 - val_loss: 0.1861 - val_acc: 0.9578\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9868\n",
      "Epoch 00097: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0409 - acc: 0.9868 - val_loss: 0.1973 - val_acc: 0.9606\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9857\n",
      "Epoch 00098: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0423 - acc: 0.9857 - val_loss: 0.1949 - val_acc: 0.9602\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9861\n",
      "Epoch 00099: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0422 - acc: 0.9861 - val_loss: 0.1709 - val_acc: 0.9611\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9869\n",
      "Epoch 00100: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0389 - acc: 0.9869 - val_loss: 0.1893 - val_acc: 0.9597\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9866\n",
      "Epoch 00101: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0403 - acc: 0.9866 - val_loss: 0.1676 - val_acc: 0.9634\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9887\n",
      "Epoch 00102: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0338 - acc: 0.9887 - val_loss: 0.1767 - val_acc: 0.9623\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9865\n",
      "Epoch 00103: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0387 - acc: 0.9865 - val_loss: 0.2206 - val_acc: 0.9546\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9865\n",
      "Epoch 00104: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0398 - acc: 0.9865 - val_loss: 0.1799 - val_acc: 0.9574\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9877\n",
      "Epoch 00105: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0378 - acc: 0.9877 - val_loss: 0.1768 - val_acc: 0.9623\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9871\n",
      "Epoch 00106: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0373 - acc: 0.9871 - val_loss: 0.1927 - val_acc: 0.9599\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9881\n",
      "Epoch 00107: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0372 - acc: 0.9881 - val_loss: 0.1791 - val_acc: 0.9625\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9880\n",
      "Epoch 00108: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.0355 - acc: 0.9880 - val_loss: 0.1806 - val_acc: 0.9606\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9888\n",
      "Epoch 00109: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0341 - acc: 0.9888 - val_loss: 0.1999 - val_acc: 0.9574\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9883\n",
      "Epoch 00110: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0355 - acc: 0.9883 - val_loss: 0.1880 - val_acc: 0.9602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9889\n",
      "Epoch 00111: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0341 - acc: 0.9889 - val_loss: 0.2004 - val_acc: 0.9583\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9901\n",
      "Epoch 00112: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0315 - acc: 0.9901 - val_loss: 0.1924 - val_acc: 0.9583\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9886\n",
      "Epoch 00113: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0341 - acc: 0.9886 - val_loss: 0.1955 - val_acc: 0.9609\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888\n",
      "Epoch 00114: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0350 - acc: 0.9888 - val_loss: 0.1900 - val_acc: 0.9599\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9893\n",
      "Epoch 00115: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0320 - acc: 0.9893 - val_loss: 0.1942 - val_acc: 0.9627\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9884\n",
      "Epoch 00116: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0350 - acc: 0.9884 - val_loss: 0.1811 - val_acc: 0.9620\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9911\n",
      "Epoch 00117: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0278 - acc: 0.9911 - val_loss: 0.1936 - val_acc: 0.9585\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9889\n",
      "Epoch 00118: val_loss did not improve from 0.15475\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0324 - acc: 0.9889 - val_loss: 0.2138 - val_acc: 0.9637\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 896us/sample - loss: 0.2224 - acc: 0.9454\n",
      "Loss: 0.22243543011586056 Accuracy: 0.945379\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6495 - acc: 0.1140\n",
      "Epoch 00001: val_loss improved from inf to 2.20679, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/001-2.2068.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.6495 - acc: 0.1141 - val_loss: 2.2068 - val_acc: 0.3086\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9369 - acc: 0.3621\n",
      "Epoch 00002: val_loss improved from 2.20679 to 1.44663, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/002-1.4466.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.9368 - acc: 0.3621 - val_loss: 1.4466 - val_acc: 0.5367\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3758 - acc: 0.5427\n",
      "Epoch 00003: val_loss improved from 1.44663 to 1.06821, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/003-1.0682.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.3758 - acc: 0.5427 - val_loss: 1.0682 - val_acc: 0.6497\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0275 - acc: 0.6612\n",
      "Epoch 00004: val_loss improved from 1.06821 to 0.73943, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/004-0.7394.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0274 - acc: 0.6612 - val_loss: 0.7394 - val_acc: 0.7678\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8405 - acc: 0.7243\n",
      "Epoch 00005: val_loss improved from 0.73943 to 0.61418, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/005-0.6142.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.8405 - acc: 0.7243 - val_loss: 0.6142 - val_acc: 0.8004\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7314 - acc: 0.7589\n",
      "Epoch 00006: val_loss improved from 0.61418 to 0.55246, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/006-0.5525.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7314 - acc: 0.7589 - val_loss: 0.5525 - val_acc: 0.8171\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.7846\n",
      "Epoch 00007: val_loss improved from 0.55246 to 0.53044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/007-0.5304.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6612 - acc: 0.7846 - val_loss: 0.5304 - val_acc: 0.8251\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5944 - acc: 0.8083\n",
      "Epoch 00008: val_loss improved from 0.53044 to 0.50432, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/008-0.5043.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5944 - acc: 0.8083 - val_loss: 0.5043 - val_acc: 0.8379\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.8199\n",
      "Epoch 00009: val_loss improved from 0.50432 to 0.48482, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/009-0.4848.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5478 - acc: 0.8199 - val_loss: 0.4848 - val_acc: 0.8488\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8352\n",
      "Epoch 00010: val_loss improved from 0.48482 to 0.46323, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/010-0.4632.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5036 - acc: 0.8352 - val_loss: 0.4632 - val_acc: 0.8500\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4604 - acc: 0.8489\n",
      "Epoch 00011: val_loss improved from 0.46323 to 0.35072, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/011-0.3507.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4603 - acc: 0.8489 - val_loss: 0.3507 - val_acc: 0.8912\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4360 - acc: 0.8575\n",
      "Epoch 00012: val_loss improved from 0.35072 to 0.33397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/012-0.3340.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4360 - acc: 0.8575 - val_loss: 0.3340 - val_acc: 0.8935\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8709\n",
      "Epoch 00013: val_loss improved from 0.33397 to 0.30700, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/013-0.3070.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3968 - acc: 0.8709 - val_loss: 0.3070 - val_acc: 0.9045\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8781\n",
      "Epoch 00014: val_loss did not improve from 0.30700\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3744 - acc: 0.8781 - val_loss: 0.3407 - val_acc: 0.8908\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.8869\n",
      "Epoch 00015: val_loss improved from 0.30700 to 0.27259, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/015-0.2726.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3475 - acc: 0.8869 - val_loss: 0.2726 - val_acc: 0.9220\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8956\n",
      "Epoch 00016: val_loss improved from 0.27259 to 0.25323, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/016-0.2532.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3209 - acc: 0.8956 - val_loss: 0.2532 - val_acc: 0.9189\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9026\n",
      "Epoch 00017: val_loss improved from 0.25323 to 0.23811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/017-0.2381.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3035 - acc: 0.9026 - val_loss: 0.2381 - val_acc: 0.9248\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9058\n",
      "Epoch 00018: val_loss improved from 0.23811 to 0.23211, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/018-0.2321.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2907 - acc: 0.9058 - val_loss: 0.2321 - val_acc: 0.9297\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2680 - acc: 0.9126\n",
      "Epoch 00019: val_loss improved from 0.23211 to 0.22097, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/019-0.2210.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2680 - acc: 0.9126 - val_loss: 0.2210 - val_acc: 0.9311\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9168\n",
      "Epoch 00020: val_loss did not improve from 0.22097\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2549 - acc: 0.9168 - val_loss: 0.3356 - val_acc: 0.9001\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9193\n",
      "Epoch 00021: val_loss improved from 0.22097 to 0.19309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/021-0.1931.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2455 - acc: 0.9193 - val_loss: 0.1931 - val_acc: 0.9404\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9252\n",
      "Epoch 00022: val_loss improved from 0.19309 to 0.19238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/022-0.1924.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2283 - acc: 0.9253 - val_loss: 0.1924 - val_acc: 0.9397\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9301\n",
      "Epoch 00023: val_loss improved from 0.19238 to 0.18496, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/023-0.1850.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2144 - acc: 0.9301 - val_loss: 0.1850 - val_acc: 0.9446\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9319\n",
      "Epoch 00024: val_loss did not improve from 0.18496\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2131 - acc: 0.9319 - val_loss: 0.1888 - val_acc: 0.9420\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9360\n",
      "Epoch 00025: val_loss did not improve from 0.18496\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1971 - acc: 0.9360 - val_loss: 0.1926 - val_acc: 0.9392\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9386\n",
      "Epoch 00026: val_loss improved from 0.18496 to 0.17456, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/026-0.1746.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1873 - acc: 0.9386 - val_loss: 0.1746 - val_acc: 0.9457\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9395\n",
      "Epoch 00027: val_loss improved from 0.17456 to 0.17016, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/027-0.1702.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1844 - acc: 0.9394 - val_loss: 0.1702 - val_acc: 0.9490\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9437\n",
      "Epoch 00028: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1722 - acc: 0.9437 - val_loss: 0.1870 - val_acc: 0.9469\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9449\n",
      "Epoch 00029: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1651 - acc: 0.9450 - val_loss: 0.1869 - val_acc: 0.9441\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9473\n",
      "Epoch 00030: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1616 - acc: 0.9473 - val_loss: 0.1760 - val_acc: 0.9476\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9490\n",
      "Epoch 00031: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1548 - acc: 0.9491 - val_loss: 0.1718 - val_acc: 0.9488\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9511\n",
      "Epoch 00032: val_loss did not improve from 0.17016\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1477 - acc: 0.9511 - val_loss: 0.1703 - val_acc: 0.9511\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9550\n",
      "Epoch 00033: val_loss improved from 0.17016 to 0.15356, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/033-0.1536.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1386 - acc: 0.9550 - val_loss: 0.1536 - val_acc: 0.9560\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9549\n",
      "Epoch 00034: val_loss did not improve from 0.15356\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1360 - acc: 0.9549 - val_loss: 0.1578 - val_acc: 0.9564\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9564\n",
      "Epoch 00035: val_loss did not improve from 0.15356\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1290 - acc: 0.9564 - val_loss: 0.1576 - val_acc: 0.9564\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9586\n",
      "Epoch 00036: val_loss did not improve from 0.15356\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1232 - acc: 0.9585 - val_loss: 0.1619 - val_acc: 0.9543\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9576\n",
      "Epoch 00037: val_loss improved from 0.15356 to 0.15230, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/037-0.1523.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1270 - acc: 0.9576 - val_loss: 0.1523 - val_acc: 0.9569\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9606\n",
      "Epoch 00038: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1173 - acc: 0.9606 - val_loss: 0.1879 - val_acc: 0.9434\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9629\n",
      "Epoch 00039: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1108 - acc: 0.9629 - val_loss: 0.1565 - val_acc: 0.9574\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9647\n",
      "Epoch 00040: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1056 - acc: 0.9647 - val_loss: 0.1564 - val_acc: 0.9585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9652\n",
      "Epoch 00041: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1039 - acc: 0.9652 - val_loss: 0.1856 - val_acc: 0.9518\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9654\n",
      "Epoch 00042: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0994 - acc: 0.9654 - val_loss: 0.1564 - val_acc: 0.9590\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9659\n",
      "Epoch 00043: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1001 - acc: 0.9659 - val_loss: 0.1667 - val_acc: 0.9606\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9690\n",
      "Epoch 00044: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0931 - acc: 0.9690 - val_loss: 0.1553 - val_acc: 0.9604\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9692\n",
      "Epoch 00045: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0906 - acc: 0.9692 - val_loss: 0.1535 - val_acc: 0.9599\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9708\n",
      "Epoch 00046: val_loss did not improve from 0.15230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0883 - acc: 0.9708 - val_loss: 0.1614 - val_acc: 0.9562\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9709\n",
      "Epoch 00047: val_loss improved from 0.15230 to 0.15126, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/047-0.1513.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0869 - acc: 0.9709 - val_loss: 0.1513 - val_acc: 0.9618\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9730\n",
      "Epoch 00048: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0809 - acc: 0.9730 - val_loss: 0.1615 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9738\n",
      "Epoch 00049: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0788 - acc: 0.9738 - val_loss: 0.1653 - val_acc: 0.9557\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9722\n",
      "Epoch 00050: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0815 - acc: 0.9722 - val_loss: 0.1599 - val_acc: 0.9571\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9737\n",
      "Epoch 00051: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0763 - acc: 0.9736 - val_loss: 0.1705 - val_acc: 0.9543\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9758\n",
      "Epoch 00052: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0734 - acc: 0.9758 - val_loss: 0.1711 - val_acc: 0.9592\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9752\n",
      "Epoch 00053: val_loss did not improve from 0.15126\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0749 - acc: 0.9752 - val_loss: 0.1657 - val_acc: 0.9555\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9764\n",
      "Epoch 00054: val_loss improved from 0.15126 to 0.14901, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv_checkpoint/054-0.1490.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0698 - acc: 0.9764 - val_loss: 0.1490 - val_acc: 0.9609\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9760\n",
      "Epoch 00055: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0691 - acc: 0.9760 - val_loss: 0.1828 - val_acc: 0.9574\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9765\n",
      "Epoch 00056: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0680 - acc: 0.9766 - val_loss: 0.2127 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9776\n",
      "Epoch 00057: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0656 - acc: 0.9776 - val_loss: 0.1612 - val_acc: 0.9627\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9773\n",
      "Epoch 00058: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0663 - acc: 0.9773 - val_loss: 0.1803 - val_acc: 0.9555\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9793\n",
      "Epoch 00059: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0593 - acc: 0.9793 - val_loss: 0.1628 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9799\n",
      "Epoch 00060: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0589 - acc: 0.9799 - val_loss: 0.1500 - val_acc: 0.9632\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9813\n",
      "Epoch 00061: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0562 - acc: 0.9813 - val_loss: 0.1705 - val_acc: 0.9609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9805\n",
      "Epoch 00062: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0573 - acc: 0.9805 - val_loss: 0.1759 - val_acc: 0.9599\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9804\n",
      "Epoch 00063: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0600 - acc: 0.9804 - val_loss: 0.1895 - val_acc: 0.9567\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9811\n",
      "Epoch 00064: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0556 - acc: 0.9810 - val_loss: 0.1888 - val_acc: 0.9550\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9807\n",
      "Epoch 00065: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0564 - acc: 0.9807 - val_loss: 0.1568 - val_acc: 0.9609\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9818\n",
      "Epoch 00066: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0537 - acc: 0.9818 - val_loss: 0.1664 - val_acc: 0.9639\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9817\n",
      "Epoch 00067: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0529 - acc: 0.9816 - val_loss: 0.1624 - val_acc: 0.9627\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9825\n",
      "Epoch 00068: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0504 - acc: 0.9825 - val_loss: 0.1699 - val_acc: 0.9630\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9839\n",
      "Epoch 00069: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0477 - acc: 0.9839 - val_loss: 0.1697 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9832\n",
      "Epoch 00070: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0490 - acc: 0.9832 - val_loss: 0.1754 - val_acc: 0.9616\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9846\n",
      "Epoch 00071: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0467 - acc: 0.9846 - val_loss: 0.1882 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9844\n",
      "Epoch 00072: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0471 - acc: 0.9844 - val_loss: 0.1815 - val_acc: 0.9623\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9847\n",
      "Epoch 00073: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0473 - acc: 0.9847 - val_loss: 0.1680 - val_acc: 0.9623\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9855\n",
      "Epoch 00074: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0424 - acc: 0.9855 - val_loss: 0.1919 - val_acc: 0.9611\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9845\n",
      "Epoch 00075: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0436 - acc: 0.9845 - val_loss: 0.1910 - val_acc: 0.9602\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9863\n",
      "Epoch 00076: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0430 - acc: 0.9863 - val_loss: 0.1879 - val_acc: 0.9627\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9872\n",
      "Epoch 00077: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0390 - acc: 0.9872 - val_loss: 0.1783 - val_acc: 0.9616\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9860\n",
      "Epoch 00078: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0412 - acc: 0.9860 - val_loss: 0.1636 - val_acc: 0.9632\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9846\n",
      "Epoch 00079: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0437 - acc: 0.9846 - val_loss: 0.1727 - val_acc: 0.9609\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9875\n",
      "Epoch 00080: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0375 - acc: 0.9875 - val_loss: 0.1781 - val_acc: 0.9630\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9869\n",
      "Epoch 00081: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0401 - acc: 0.9869 - val_loss: 0.2051 - val_acc: 0.9550\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9873\n",
      "Epoch 00082: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0390 - acc: 0.9873 - val_loss: 0.1789 - val_acc: 0.9634\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9874\n",
      "Epoch 00083: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0377 - acc: 0.9874 - val_loss: 0.1773 - val_acc: 0.9634\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9870\n",
      "Epoch 00084: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0383 - acc: 0.9870 - val_loss: 0.2030 - val_acc: 0.9609\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9884\n",
      "Epoch 00085: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0357 - acc: 0.9884 - val_loss: 0.1666 - val_acc: 0.9606\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9877\n",
      "Epoch 00086: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0350 - acc: 0.9877 - val_loss: 0.1697 - val_acc: 0.9620\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9886\n",
      "Epoch 00087: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0338 - acc: 0.9886 - val_loss: 0.2156 - val_acc: 0.9597\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9888\n",
      "Epoch 00088: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0338 - acc: 0.9888 - val_loss: 0.2004 - val_acc: 0.9590\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9881\n",
      "Epoch 00089: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0351 - acc: 0.9880 - val_loss: 0.2131 - val_acc: 0.9602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9877\n",
      "Epoch 00090: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0381 - acc: 0.9877 - val_loss: 0.1824 - val_acc: 0.9648\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9885\n",
      "Epoch 00091: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0344 - acc: 0.9885 - val_loss: 0.1640 - val_acc: 0.9651\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9904\n",
      "Epoch 00092: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0293 - acc: 0.9904 - val_loss: 0.1957 - val_acc: 0.9630\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9887\n",
      "Epoch 00093: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0325 - acc: 0.9887 - val_loss: 0.2222 - val_acc: 0.9613\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9889\n",
      "Epoch 00094: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0312 - acc: 0.9889 - val_loss: 0.1798 - val_acc: 0.9616\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9897\n",
      "Epoch 00095: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0310 - acc: 0.9897 - val_loss: 0.1932 - val_acc: 0.9618\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9888\n",
      "Epoch 00096: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0339 - acc: 0.9888 - val_loss: 0.1969 - val_acc: 0.9618\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9908\n",
      "Epoch 00097: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0293 - acc: 0.9908 - val_loss: 0.2078 - val_acc: 0.9627\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9905\n",
      "Epoch 00098: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0278 - acc: 0.9905 - val_loss: 0.2279 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9912\n",
      "Epoch 00099: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0280 - acc: 0.9912 - val_loss: 0.2176 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9907\n",
      "Epoch 00100: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0293 - acc: 0.9907 - val_loss: 0.1830 - val_acc: 0.9630\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9894\n",
      "Epoch 00101: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0320 - acc: 0.9894 - val_loss: 0.1930 - val_acc: 0.9620\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9897\n",
      "Epoch 00102: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0316 - acc: 0.9897 - val_loss: 0.2035 - val_acc: 0.9613\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9909\n",
      "Epoch 00103: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0276 - acc: 0.9909 - val_loss: 0.1897 - val_acc: 0.9644\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9904\n",
      "Epoch 00104: val_loss did not improve from 0.14901\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0273 - acc: 0.9904 - val_loss: 0.2121 - val_acc: 0.9651\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 889us/sample - loss: 0.2326 - acc: 0.9435\n",
      "Loss: 0.2325756860438537 Accuracy: 0.9435099\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4847 - acc: 0.1740\n",
      "Epoch 00001: val_loss improved from inf to 1.89926, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/001-1.8993.hdf5\n",
      "36805/36805 [==============================] - 101s 3ms/sample - loss: 2.4846 - acc: 0.1741 - val_loss: 1.8993 - val_acc: 0.3781\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5465 - acc: 0.4927\n",
      "Epoch 00002: val_loss improved from 1.89926 to 0.97407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/002-0.9741.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.5465 - acc: 0.4927 - val_loss: 0.9741 - val_acc: 0.6781\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9817 - acc: 0.6766\n",
      "Epoch 00003: val_loss improved from 0.97407 to 0.77763, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/003-0.7776.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.9817 - acc: 0.6766 - val_loss: 0.7776 - val_acc: 0.7342\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8067 - acc: 0.7372\n",
      "Epoch 00004: val_loss improved from 0.77763 to 0.63320, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/004-0.6332.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.8068 - acc: 0.7371 - val_loss: 0.6332 - val_acc: 0.7929\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7038 - acc: 0.7707\n",
      "Epoch 00005: val_loss improved from 0.63320 to 0.62590, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/005-0.6259.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7038 - acc: 0.7707 - val_loss: 0.6259 - val_acc: 0.7957\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6214 - acc: 0.7959\n",
      "Epoch 00006: val_loss improved from 0.62590 to 0.51014, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/006-0.5101.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.6214 - acc: 0.7959 - val_loss: 0.5101 - val_acc: 0.8286\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.8154\n",
      "Epoch 00007: val_loss improved from 0.51014 to 0.45578, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/007-0.4558.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5639 - acc: 0.8154 - val_loss: 0.4558 - val_acc: 0.8449\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8329\n",
      "Epoch 00008: val_loss improved from 0.45578 to 0.41096, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/008-0.4110.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5099 - acc: 0.8329 - val_loss: 0.4110 - val_acc: 0.8619\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8473\n",
      "Epoch 00009: val_loss did not improve from 0.41096\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4692 - acc: 0.8473 - val_loss: 0.4272 - val_acc: 0.8658\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.8611\n",
      "Epoch 00010: val_loss improved from 0.41096 to 0.35511, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/010-0.3551.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4256 - acc: 0.8611 - val_loss: 0.3551 - val_acc: 0.8898\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8721\n",
      "Epoch 00011: val_loss improved from 0.35511 to 0.34881, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/011-0.3488.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3893 - acc: 0.8721 - val_loss: 0.3488 - val_acc: 0.8877\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8838\n",
      "Epoch 00012: val_loss improved from 0.34881 to 0.32227, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/012-0.3223.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3614 - acc: 0.8838 - val_loss: 0.3223 - val_acc: 0.8991\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.8896\n",
      "Epoch 00013: val_loss improved from 0.32227 to 0.31683, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/013-0.3168.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3369 - acc: 0.8896 - val_loss: 0.3168 - val_acc: 0.8994\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8980\n",
      "Epoch 00014: val_loss did not improve from 0.31683\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3137 - acc: 0.8980 - val_loss: 0.3241 - val_acc: 0.9015\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.9022\n",
      "Epoch 00015: val_loss improved from 0.31683 to 0.28012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/015-0.2801.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2994 - acc: 0.9021 - val_loss: 0.2801 - val_acc: 0.9101\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9079\n",
      "Epoch 00016: val_loss improved from 0.28012 to 0.26993, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/016-0.2699.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2813 - acc: 0.9079 - val_loss: 0.2699 - val_acc: 0.9189\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9157\n",
      "Epoch 00017: val_loss improved from 0.26993 to 0.25793, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/017-0.2579.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2631 - acc: 0.9157 - val_loss: 0.2579 - val_acc: 0.9229\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9192\n",
      "Epoch 00018: val_loss improved from 0.25793 to 0.24609, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/018-0.2461.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2485 - acc: 0.9191 - val_loss: 0.2461 - val_acc: 0.9229\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9223\n",
      "Epoch 00019: val_loss improved from 0.24609 to 0.23983, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/019-0.2398.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2324 - acc: 0.9223 - val_loss: 0.2398 - val_acc: 0.9255\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9260\n",
      "Epoch 00020: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2211 - acc: 0.9260 - val_loss: 0.2446 - val_acc: 0.9248\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9292\n",
      "Epoch 00021: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2149 - acc: 0.9292 - val_loss: 0.2421 - val_acc: 0.9227\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9345\n",
      "Epoch 00022: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1992 - acc: 0.9345 - val_loss: 0.2406 - val_acc: 0.9271\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9386\n",
      "Epoch 00023: val_loss improved from 0.23983 to 0.22725, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/023-0.2272.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1839 - acc: 0.9386 - val_loss: 0.2272 - val_acc: 0.9278\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9404\n",
      "Epoch 00024: val_loss improved from 0.22725 to 0.22714, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/024-0.2271.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1782 - acc: 0.9404 - val_loss: 0.2271 - val_acc: 0.9366\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9440\n",
      "Epoch 00025: val_loss did not improve from 0.22714\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1689 - acc: 0.9440 - val_loss: 0.2565 - val_acc: 0.9266\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9468\n",
      "Epoch 00026: val_loss improved from 0.22714 to 0.21326, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/026-0.2133.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1576 - acc: 0.9468 - val_loss: 0.2133 - val_acc: 0.9397\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9502\n",
      "Epoch 00027: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1470 - acc: 0.9502 - val_loss: 0.2219 - val_acc: 0.9364\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9521\n",
      "Epoch 00028: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1427 - acc: 0.9522 - val_loss: 0.2323 - val_acc: 0.9311\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9574\n",
      "Epoch 00029: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1299 - acc: 0.9574 - val_loss: 0.2419 - val_acc: 0.9317\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9565\n",
      "Epoch 00030: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1319 - acc: 0.9565 - val_loss: 0.2166 - val_acc: 0.9408\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9597\n",
      "Epoch 00031: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1198 - acc: 0.9597 - val_loss: 0.2543 - val_acc: 0.9350\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9627\n",
      "Epoch 00032: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1099 - acc: 0.9627 - val_loss: 0.2650 - val_acc: 0.9297\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9618\n",
      "Epoch 00033: val_loss did not improve from 0.21326\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1113 - acc: 0.9619 - val_loss: 0.2682 - val_acc: 0.9329\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9655\n",
      "Epoch 00034: val_loss improved from 0.21326 to 0.20834, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/034-0.2083.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1030 - acc: 0.9655 - val_loss: 0.2083 - val_acc: 0.9446\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9651\n",
      "Epoch 00035: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0998 - acc: 0.9651 - val_loss: 0.2236 - val_acc: 0.9420\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9691\n",
      "Epoch 00036: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0909 - acc: 0.9691 - val_loss: 0.2210 - val_acc: 0.9392\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9689\n",
      "Epoch 00037: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0902 - acc: 0.9689 - val_loss: 0.2383 - val_acc: 0.9432\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9705\n",
      "Epoch 00038: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0845 - acc: 0.9705 - val_loss: 0.2235 - val_acc: 0.9455\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9718\n",
      "Epoch 00039: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0850 - acc: 0.9718 - val_loss: 0.2697 - val_acc: 0.9378\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9731\n",
      "Epoch 00040: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0783 - acc: 0.9731 - val_loss: 0.2383 - val_acc: 0.9418\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9740\n",
      "Epoch 00041: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0757 - acc: 0.9740 - val_loss: 0.2129 - val_acc: 0.9446\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9757\n",
      "Epoch 00042: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0708 - acc: 0.9757 - val_loss: 0.2440 - val_acc: 0.9455\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9729\n",
      "Epoch 00043: val_loss did not improve from 0.20834\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0804 - acc: 0.9729 - val_loss: 0.2517 - val_acc: 0.9453\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9760\n",
      "Epoch 00044: val_loss improved from 0.20834 to 0.19607, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv_checkpoint/044-0.1961.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0705 - acc: 0.9760 - val_loss: 0.1961 - val_acc: 0.9490\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9792\n",
      "Epoch 00045: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0592 - acc: 0.9792 - val_loss: 0.2400 - val_acc: 0.9469\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9797\n",
      "Epoch 00046: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0601 - acc: 0.9797 - val_loss: 0.2470 - val_acc: 0.9453\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9788\n",
      "Epoch 00047: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0625 - acc: 0.9788 - val_loss: 0.2698 - val_acc: 0.9453\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9789\n",
      "Epoch 00048: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0629 - acc: 0.9789 - val_loss: 0.2394 - val_acc: 0.9406\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9806\n",
      "Epoch 00049: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0574 - acc: 0.9806 - val_loss: 0.2771 - val_acc: 0.9425\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9818\n",
      "Epoch 00050: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0548 - acc: 0.9818 - val_loss: 0.2994 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9743\n",
      "Epoch 00051: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0778 - acc: 0.9744 - val_loss: 0.2451 - val_acc: 0.9467\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9835\n",
      "Epoch 00052: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0504 - acc: 0.9835 - val_loss: 0.2550 - val_acc: 0.9476\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9838\n",
      "Epoch 00053: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0487 - acc: 0.9838 - val_loss: 0.2948 - val_acc: 0.9436\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9829\n",
      "Epoch 00054: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0519 - acc: 0.9829 - val_loss: 0.2252 - val_acc: 0.9478\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9856\n",
      "Epoch 00055: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0434 - acc: 0.9856 - val_loss: 0.2685 - val_acc: 0.9462\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9831\n",
      "Epoch 00056: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0508 - acc: 0.9831 - val_loss: 0.2256 - val_acc: 0.9476\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9867\n",
      "Epoch 00057: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0391 - acc: 0.9867 - val_loss: 0.2412 - val_acc: 0.9450\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9847\n",
      "Epoch 00058: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0478 - acc: 0.9847 - val_loss: 0.2668 - val_acc: 0.9483\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9826\n",
      "Epoch 00059: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0541 - acc: 0.9826 - val_loss: 0.2404 - val_acc: 0.9457\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9862\n",
      "Epoch 00060: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0434 - acc: 0.9863 - val_loss: 0.2427 - val_acc: 0.9522\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9878\n",
      "Epoch 00061: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0373 - acc: 0.9878 - val_loss: 0.2557 - val_acc: 0.9495\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9874\n",
      "Epoch 00062: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0390 - acc: 0.9874 - val_loss: 0.2338 - val_acc: 0.9509\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9874\n",
      "Epoch 00063: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0383 - acc: 0.9874 - val_loss: 0.2407 - val_acc: 0.9478\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9884\n",
      "Epoch 00064: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0347 - acc: 0.9884 - val_loss: 0.2885 - val_acc: 0.9460\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9868\n",
      "Epoch 00065: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0385 - acc: 0.9868 - val_loss: 0.2717 - val_acc: 0.9504\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9864\n",
      "Epoch 00066: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0404 - acc: 0.9864 - val_loss: 0.2708 - val_acc: 0.9483\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9895\n",
      "Epoch 00067: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0329 - acc: 0.9895 - val_loss: 0.2681 - val_acc: 0.9522\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9892\n",
      "Epoch 00068: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0331 - acc: 0.9892 - val_loss: 0.2550 - val_acc: 0.9550\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9882\n",
      "Epoch 00069: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0356 - acc: 0.9882 - val_loss: 0.2803 - val_acc: 0.9420\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9879\n",
      "Epoch 00070: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0355 - acc: 0.9879 - val_loss: 0.2773 - val_acc: 0.9448\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9884\n",
      "Epoch 00071: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0375 - acc: 0.9884 - val_loss: 0.2915 - val_acc: 0.9450\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9907\n",
      "Epoch 00072: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0280 - acc: 0.9907 - val_loss: 0.2801 - val_acc: 0.9506\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9883\n",
      "Epoch 00073: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0379 - acc: 0.9883 - val_loss: 0.2893 - val_acc: 0.9401\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9914\n",
      "Epoch 00074: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0261 - acc: 0.9914 - val_loss: 0.3508 - val_acc: 0.9460\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9901\n",
      "Epoch 00075: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0333 - acc: 0.9900 - val_loss: 0.3499 - val_acc: 0.9394\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9871\n",
      "Epoch 00076: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0437 - acc: 0.9871 - val_loss: 0.2833 - val_acc: 0.9492\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 00077: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0272 - acc: 0.9911 - val_loss: 0.2843 - val_acc: 0.9441\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9898\n",
      "Epoch 00078: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0320 - acc: 0.9898 - val_loss: 0.2887 - val_acc: 0.9495\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9912\n",
      "Epoch 00079: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0266 - acc: 0.9912 - val_loss: 0.2560 - val_acc: 0.9495\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9916\n",
      "Epoch 00080: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0257 - acc: 0.9916 - val_loss: 0.2681 - val_acc: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9895\n",
      "Epoch 00081: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0358 - acc: 0.9895 - val_loss: 0.2771 - val_acc: 0.9471\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9909\n",
      "Epoch 00082: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0297 - acc: 0.9909 - val_loss: 0.3575 - val_acc: 0.9336\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9913\n",
      "Epoch 00083: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0266 - acc: 0.9913 - val_loss: 0.2802 - val_acc: 0.9532\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9917\n",
      "Epoch 00084: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0247 - acc: 0.9917 - val_loss: 0.2823 - val_acc: 0.9515\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00085: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.2971 - val_acc: 0.9522\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9911\n",
      "Epoch 00086: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0270 - acc: 0.9911 - val_loss: 0.3172 - val_acc: 0.9429\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9916\n",
      "Epoch 00087: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0276 - acc: 0.9916 - val_loss: 0.3104 - val_acc: 0.9481\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9926\n",
      "Epoch 00088: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0227 - acc: 0.9926 - val_loss: 0.3028 - val_acc: 0.9476\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9924\n",
      "Epoch 00089: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0234 - acc: 0.9924 - val_loss: 0.2667 - val_acc: 0.9499\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9931\n",
      "Epoch 00090: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0238 - acc: 0.9931 - val_loss: 0.2910 - val_acc: 0.9488\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9926\n",
      "Epoch 00091: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0229 - acc: 0.9926 - val_loss: 0.2844 - val_acc: 0.9534\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9923\n",
      "Epoch 00092: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0231 - acc: 0.9923 - val_loss: 0.2526 - val_acc: 0.9536\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9925\n",
      "Epoch 00093: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0227 - acc: 0.9925 - val_loss: 0.3076 - val_acc: 0.9467\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9923\n",
      "Epoch 00094: val_loss did not improve from 0.19607\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0254 - acc: 0.9923 - val_loss: 0.2865 - val_acc: 0.9511\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 981us/sample - loss: 0.2804 - acc: 0.9321\n",
      "Loss: 0.28042409526085554 Accuracy: 0.93208724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO'\n",
    "    \n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,063,664\n",
      "Trainable params: 2,063,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 799us/sample - loss: 1.6396 - acc: 0.4926\n",
      "Loss: 1.639625862281023 Accuracy: 0.4926272\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,045,872\n",
      "Trainable params: 1,045,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 863us/sample - loss: 1.4313 - acc: 0.5512\n",
      "Loss: 1.4313198433486845 Accuracy: 0.5511942\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,064,432\n",
      "Trainable params: 1,064,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 912us/sample - loss: 1.4565 - acc: 0.5335\n",
      "Loss: 1.4564784374068707 Accuracy: 0.533541\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 577,136\n",
      "Trainable params: 577,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 984us/sample - loss: 1.0802 - acc: 0.6827\n",
      "Loss: 1.080234902385363 Accuracy: 0.6826584\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 345,840\n",
      "Trainable params: 345,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 988us/sample - loss: 0.7538 - acc: 0.7873\n",
      "Loss: 0.7537895601238912 Accuracy: 0.7873313\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 242,544\n",
      "Trainable params: 242,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4481 - acc: 0.8731\n",
      "Loss: 0.44814825140922365 Accuracy: 0.8731049\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 315,504\n",
      "Trainable params: 315,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2989 - acc: 0.9248\n",
      "Loss: 0.29892948167717714 Accuracy: 0.9248183\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 350,576\n",
      "Trainable params: 350,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2158 - acc: 0.9398\n",
      "Loss: 0.2158330811177954 Accuracy: 0.93977153\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 416,368\n",
      "Trainable params: 416,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2224 - acc: 0.9454\n",
      "Loss: 0.22243543011586056 Accuracy: 0.945379\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 498,544\n",
      "Trainable params: 498,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2326 - acc: 0.9435\n",
      "Loss: 0.2325756860438537 Accuracy: 0.9435099\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 791,920\n",
      "Trainable params: 791,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2804 - acc: 0.9321\n",
      "Loss: 0.28042409526085554 Accuracy: 0.93208724\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,063,664\n",
      "Trainable params: 2,063,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 3.4627 - acc: 0.5178\n",
      "Loss: 3.4627304140033504 Accuracy: 0.517757\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,045,872\n",
      "Trainable params: 1,045,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 2.4891 - acc: 0.5913\n",
      "Loss: 2.4891143224939998 Accuracy: 0.59127724\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,064,432\n",
      "Trainable params: 1,064,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.5647 - acc: 0.5985\n",
      "Loss: 2.5647225386380903 Accuracy: 0.5985462\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 577,136\n",
      "Trainable params: 577,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.6355 - acc: 0.6972\n",
      "Loss: 1.6355090392093916 Accuracy: 0.69719625\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 345,840\n",
      "Trainable params: 345,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9524 - acc: 0.8233\n",
      "Loss: 0.9523630419871765 Accuracy: 0.82326066\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 242,544\n",
      "Trainable params: 242,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5105 - acc: 0.8897\n",
      "Loss: 0.510548832656686 Accuracy: 0.8897196\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 315,504\n",
      "Trainable params: 315,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3444 - acc: 0.9248\n",
      "Loss: 0.3444039510479845 Accuracy: 0.9248183\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 350,576\n",
      "Trainable params: 350,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2553 - acc: 0.9452\n",
      "Loss: 0.25528948166226795 Accuracy: 0.94517136\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 416,368\n",
      "Trainable params: 416,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2687 - acc: 0.9481\n",
      "Loss: 0.2687434640010917 Accuracy: 0.94807893\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 498,544\n",
      "Trainable params: 498,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2958 - acc: 0.9485\n",
      "Loss: 0.2957992545655886 Accuracy: 0.9484943\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 791,920\n",
      "Trainable params: 791,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.4002 - acc: 0.9364\n",
      "Loss: 0.4001683564795721 Accuracy: 0.9364486\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
