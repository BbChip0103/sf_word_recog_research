{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_128_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=128, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=128*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 227456)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                3639312   \n",
      "=================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75776)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1212432   \n",
      "=================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50432)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                806928    \n",
      "=================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16640)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                266256    \n",
      "=================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                86032     \n",
      "=================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1792)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                28688     \n",
      "=================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 128)        768       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 128)        82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16000, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 128)         82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 256)          164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 256)          327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 256)           327936    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 512)            655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 7, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_128_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.3588 - acc: 0.1067\n",
      "Epoch 00001: val_loss improved from inf to 14.36688, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/001-14.3669.hdf5\n",
      "36805/36805 [==============================] - 153s 4ms/sample - loss: 14.3590 - acc: 0.1067 - val_loss: 14.3669 - val_acc: 0.1083\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.1034 - acc: 0.1242\n",
      "Epoch 00002: val_loss did not improve from 14.36688\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 14.1032 - acc: 0.1242 - val_loss: 14.4339 - val_acc: 0.1039\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.0179 - acc: 0.1299\n",
      "Epoch 00003: val_loss improved from 14.36688 to 13.89631, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/003-13.8963.hdf5\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 14.0182 - acc: 0.1299 - val_loss: 13.8963 - val_acc: 0.1377\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9633 - acc: 0.1333\n",
      "Epoch 00004: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.9636 - acc: 0.1333 - val_loss: 14.5782 - val_acc: 0.0955\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8785 - acc: 0.1386\n",
      "Epoch 00005: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8784 - acc: 0.1386 - val_loss: 13.9477 - val_acc: 0.1342\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.0328 - acc: 0.1289\n",
      "Epoch 00006: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 14.0331 - acc: 0.1289 - val_loss: 14.0195 - val_acc: 0.1293\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.0604 - acc: 0.1273\n",
      "Epoch 00007: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 14.0606 - acc: 0.1273 - val_loss: 14.7702 - val_acc: 0.0836\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.0423 - acc: 0.1285\n",
      "Epoch 00008: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 14.0421 - acc: 0.1285 - val_loss: 14.0056 - val_acc: 0.1309\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9087 - acc: 0.1368\n",
      "Epoch 00009: val_loss did not improve from 13.89631\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.9086 - acc: 0.1369 - val_loss: 14.0012 - val_acc: 0.1311\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8581 - acc: 0.1399\n",
      "Epoch 00010: val_loss improved from 13.89631 to 13.82072, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/010-13.8207.hdf5\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8580 - acc: 0.1399 - val_loss: 13.8207 - val_acc: 0.1423\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9211 - acc: 0.1361\n",
      "Epoch 00011: val_loss did not improve from 13.82072\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.9210 - acc: 0.1361 - val_loss: 14.0892 - val_acc: 0.1251\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8672 - acc: 0.1395\n",
      "Epoch 00012: val_loss did not improve from 13.82072\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8670 - acc: 0.1395 - val_loss: 13.8728 - val_acc: 0.1393\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8310 - acc: 0.1417\n",
      "Epoch 00013: val_loss did not improve from 13.82072\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.8309 - acc: 0.1417 - val_loss: 14.3603 - val_acc: 0.1090\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9123 - acc: 0.1367\n",
      "Epoch 00014: val_loss did not improve from 13.82072\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.9122 - acc: 0.1367 - val_loss: 13.8694 - val_acc: 0.1393\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7807 - acc: 0.1449\n",
      "Epoch 00015: val_loss did not improve from 13.82072\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7811 - acc: 0.1448 - val_loss: 14.0361 - val_acc: 0.1286\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7538 - acc: 0.1466\n",
      "Epoch 00016: val_loss improved from 13.82072 to 13.77528, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/016-13.7753.hdf5\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7537 - acc: 0.1466 - val_loss: 13.7753 - val_acc: 0.1454\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8050 - acc: 0.1432\n",
      "Epoch 00017: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8053 - acc: 0.1431 - val_loss: 13.9545 - val_acc: 0.1339\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8494 - acc: 0.1406\n",
      "Epoch 00018: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8493 - acc: 0.1406 - val_loss: 14.3197 - val_acc: 0.1116\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8630 - acc: 0.1397\n",
      "Epoch 00019: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.8624 - acc: 0.1397 - val_loss: 13.8603 - val_acc: 0.1400\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7863 - acc: 0.1446\n",
      "Epoch 00020: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7857 - acc: 0.1446 - val_loss: 13.8363 - val_acc: 0.1414\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7970 - acc: 0.1439\n",
      "Epoch 00021: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7968 - acc: 0.1439 - val_loss: 14.0249 - val_acc: 0.1295\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8044 - acc: 0.1435\n",
      "Epoch 00022: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8043 - acc: 0.1435 - val_loss: 13.7759 - val_acc: 0.1451\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7898 - acc: 0.1443\n",
      "Epoch 00023: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7902 - acc: 0.1443 - val_loss: 13.8158 - val_acc: 0.1428\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7765 - acc: 0.1451\n",
      "Epoch 00024: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7768 - acc: 0.1451 - val_loss: 13.9593 - val_acc: 0.1339\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8248 - acc: 0.1421\n",
      "Epoch 00025: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.8251 - acc: 0.1421 - val_loss: 13.8605 - val_acc: 0.1400\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8083 - acc: 0.1432\n",
      "Epoch 00026: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8081 - acc: 0.1432 - val_loss: 13.7836 - val_acc: 0.1447\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7410 - acc: 0.1474\n",
      "Epoch 00027: val_loss did not improve from 13.77528\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.7409 - acc: 0.1474 - val_loss: 13.9941 - val_acc: 0.1316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7451 - acc: 0.1471\n",
      "Epoch 00028: val_loss improved from 13.77528 to 13.75852, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/028-13.7585.hdf5\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7454 - acc: 0.1471 - val_loss: 13.7585 - val_acc: 0.1463\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7408 - acc: 0.1474\n",
      "Epoch 00029: val_loss did not improve from 13.75852\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7407 - acc: 0.1474 - val_loss: 13.8620 - val_acc: 0.1398\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7882 - acc: 0.1445\n",
      "Epoch 00030: val_loss did not improve from 13.75852\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7885 - acc: 0.1445 - val_loss: 13.8480 - val_acc: 0.1405\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7683 - acc: 0.1456\n",
      "Epoch 00031: val_loss did not improve from 13.75852\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7686 - acc: 0.1456 - val_loss: 13.7978 - val_acc: 0.1440\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7999 - acc: 0.1436\n",
      "Epoch 00032: val_loss did not improve from 13.75852\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.7998 - acc: 0.1436 - val_loss: 13.8354 - val_acc: 0.1416\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7164 - acc: 0.1490\n",
      "Epoch 00033: val_loss did not improve from 13.75852\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7163 - acc: 0.1490 - val_loss: 13.7640 - val_acc: 0.1461\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7787 - acc: 0.1450\n",
      "Epoch 00034: val_loss improved from 13.75852 to 13.75652, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/034-13.7565.hdf5\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7790 - acc: 0.1450 - val_loss: 13.7565 - val_acc: 0.1465\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7434 - acc: 0.1473\n",
      "Epoch 00035: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7433 - acc: 0.1473 - val_loss: 13.8166 - val_acc: 0.1428\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7701 - acc: 0.1455\n",
      "Epoch 00036: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7704 - acc: 0.1454 - val_loss: 14.1390 - val_acc: 0.1228\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7398 - acc: 0.1473\n",
      "Epoch 00037: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7393 - acc: 0.1473 - val_loss: 13.7813 - val_acc: 0.1449\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7181 - acc: 0.1488\n",
      "Epoch 00038: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7184 - acc: 0.1488 - val_loss: 13.8088 - val_acc: 0.1430\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7367 - acc: 0.1476\n",
      "Epoch 00039: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7371 - acc: 0.1476 - val_loss: 13.8542 - val_acc: 0.1405\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7337 - acc: 0.1479\n",
      "Epoch 00040: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7340 - acc: 0.1478 - val_loss: 13.7640 - val_acc: 0.1461\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7366 - acc: 0.1476\n",
      "Epoch 00041: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7357 - acc: 0.1477 - val_loss: 13.8053 - val_acc: 0.1435\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7366 - acc: 0.1476\n",
      "Epoch 00042: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7365 - acc: 0.1476 - val_loss: 13.8279 - val_acc: 0.1421\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7994 - acc: 0.1437\n",
      "Epoch 00043: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7998 - acc: 0.1437 - val_loss: 13.7715 - val_acc: 0.1456\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7894 - acc: 0.1443\n",
      "Epoch 00044: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7893 - acc: 0.1444 - val_loss: 13.8231 - val_acc: 0.1423\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7529 - acc: 0.1467\n",
      "Epoch 00045: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7523 - acc: 0.1467 - val_loss: 13.8006 - val_acc: 0.1437\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7038 - acc: 0.1497\n",
      "Epoch 00046: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7041 - acc: 0.1497 - val_loss: 13.7735 - val_acc: 0.1454\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7313 - acc: 0.1480\n",
      "Epoch 00047: val_loss did not improve from 13.75652\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7316 - acc: 0.1479 - val_loss: 13.8392 - val_acc: 0.1414\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7320 - acc: 0.1479\n",
      "Epoch 00048: val_loss improved from 13.75652 to 13.75276, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/048-13.7528.hdf5\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7324 - acc: 0.1479 - val_loss: 13.7528 - val_acc: 0.1468\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7060 - acc: 0.1496\n",
      "Epoch 00049: val_loss did not improve from 13.75276\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7058 - acc: 0.1496 - val_loss: 13.7593 - val_acc: 0.1463\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7171 - acc: 0.1489\n",
      "Epoch 00050: val_loss did not improve from 13.75276\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7174 - acc: 0.1489 - val_loss: 13.8166 - val_acc: 0.1428\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7001 - acc: 0.1500\n",
      "Epoch 00051: val_loss did not improve from 13.75276\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7004 - acc: 0.1500 - val_loss: 13.7790 - val_acc: 0.1451\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7382 - acc: 0.1476\n",
      "Epoch 00052: val_loss improved from 13.75276 to 13.72548, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/052-13.7255.hdf5\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7381 - acc: 0.1476 - val_loss: 13.7255 - val_acc: 0.1484\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7188 - acc: 0.1488\n",
      "Epoch 00053: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7191 - acc: 0.1488 - val_loss: 13.8100 - val_acc: 0.1430\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7209 - acc: 0.1486\n",
      "Epoch 00054: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7212 - acc: 0.1486 - val_loss: 13.7790 - val_acc: 0.1451\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8890 - acc: 0.1382\n",
      "Epoch 00055: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.8893 - acc: 0.1381 - val_loss: 13.7494 - val_acc: 0.1468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7019 - acc: 0.1498\n",
      "Epoch 00056: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7018 - acc: 0.1498 - val_loss: 13.7560 - val_acc: 0.1465\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7179 - acc: 0.1488\n",
      "Epoch 00057: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7182 - acc: 0.1488 - val_loss: 13.7317 - val_acc: 0.1479\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7978 - acc: 0.1439\n",
      "Epoch 00058: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7977 - acc: 0.1439 - val_loss: 13.7766 - val_acc: 0.1451\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7193 - acc: 0.1487\n",
      "Epoch 00059: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7192 - acc: 0.1487 - val_loss: 13.7378 - val_acc: 0.1477\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6962 - acc: 0.1501\n",
      "Epoch 00060: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6965 - acc: 0.1501 - val_loss: 13.7798 - val_acc: 0.1449\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8139 - acc: 0.1429\n",
      "Epoch 00061: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8137 - acc: 0.1429 - val_loss: 13.7727 - val_acc: 0.1454\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7392 - acc: 0.1476\n",
      "Epoch 00062: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7382 - acc: 0.1476 - val_loss: 13.7455 - val_acc: 0.1470\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7425 - acc: 0.1473\n",
      "Epoch 00063: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7428 - acc: 0.1473 - val_loss: 13.7508 - val_acc: 0.1468\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7111 - acc: 0.1493\n",
      "Epoch 00064: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7101 - acc: 0.1493 - val_loss: 13.7434 - val_acc: 0.1472\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6849 - acc: 0.1509\n",
      "Epoch 00065: val_loss did not improve from 13.72548\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6848 - acc: 0.1509 - val_loss: 13.7302 - val_acc: 0.1481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6896 - acc: 0.1505\n",
      "Epoch 00066: val_loss improved from 13.72548 to 13.70770, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/066-13.7077.hdf5\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6890 - acc: 0.1506 - val_loss: 13.7077 - val_acc: 0.1495\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6818 - acc: 0.1510\n",
      "Epoch 00067: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6817 - acc: 0.1510 - val_loss: 13.8278 - val_acc: 0.1421\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7628 - acc: 0.1460\n",
      "Epoch 00068: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7627 - acc: 0.1460 - val_loss: 13.7393 - val_acc: 0.1474\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7007 - acc: 0.1499\n",
      "Epoch 00069: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7011 - acc: 0.1498 - val_loss: 13.8146 - val_acc: 0.1428\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7338 - acc: 0.1478\n",
      "Epoch 00070: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7328 - acc: 0.1479 - val_loss: 13.7753 - val_acc: 0.1454\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7386 - acc: 0.1476\n",
      "Epoch 00071: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7380 - acc: 0.1476 - val_loss: 13.7315 - val_acc: 0.1479\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7015 - acc: 0.1499\n",
      "Epoch 00072: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7014 - acc: 0.1499 - val_loss: 13.7876 - val_acc: 0.1444\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7389 - acc: 0.1475\n",
      "Epoch 00073: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7392 - acc: 0.1475 - val_loss: 13.8053 - val_acc: 0.1435\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6900 - acc: 0.1506\n",
      "Epoch 00074: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6903 - acc: 0.1506 - val_loss: 13.7791 - val_acc: 0.1451\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6796 - acc: 0.1512\n",
      "Epoch 00075: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6799 - acc: 0.1512 - val_loss: 13.7385 - val_acc: 0.1474\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6932 - acc: 0.1504\n",
      "Epoch 00076: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6926 - acc: 0.1504 - val_loss: 13.8786 - val_acc: 0.1388\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7072 - acc: 0.1495\n",
      "Epoch 00077: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7071 - acc: 0.1495 - val_loss: 13.8128 - val_acc: 0.1430\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7086 - acc: 0.1494\n",
      "Epoch 00078: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7089 - acc: 0.1494 - val_loss: 13.7528 - val_acc: 0.1468\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7041 - acc: 0.1498\n",
      "Epoch 00079: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7044 - acc: 0.1497 - val_loss: 13.7452 - val_acc: 0.1472\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6998 - acc: 0.1499\n",
      "Epoch 00080: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7002 - acc: 0.1499 - val_loss: 13.9747 - val_acc: 0.1328\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9372 - acc: 0.1352\n",
      "Epoch 00081: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.9371 - acc: 0.1352 - val_loss: 14.0609 - val_acc: 0.1274\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8539 - acc: 0.1404\n",
      "Epoch 00082: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8537 - acc: 0.1404 - val_loss: 14.0021 - val_acc: 0.1309\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7094 - acc: 0.1494\n",
      "Epoch 00083: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.7098 - acc: 0.1494 - val_loss: 13.7304 - val_acc: 0.1481\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6747 - acc: 0.1516\n",
      "Epoch 00084: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6746 - acc: 0.1516 - val_loss: 13.7377 - val_acc: 0.1477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6976 - acc: 0.1501\n",
      "Epoch 00085: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6971 - acc: 0.1501 - val_loss: 13.7715 - val_acc: 0.1454\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7363 - acc: 0.1477\n",
      "Epoch 00086: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7366 - acc: 0.1477 - val_loss: 13.7882 - val_acc: 0.1444\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6932 - acc: 0.1503\n",
      "Epoch 00087: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6927 - acc: 0.1504 - val_loss: 13.7864 - val_acc: 0.1447\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6844 - acc: 0.1509\n",
      "Epoch 00088: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6843 - acc: 0.1509 - val_loss: 13.7607 - val_acc: 0.1458\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7280 - acc: 0.1483\n",
      "Epoch 00089: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7284 - acc: 0.1482 - val_loss: 13.8836 - val_acc: 0.1381\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.8525 - acc: 0.1405\n",
      "Epoch 00090: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.8524 - acc: 0.1405 - val_loss: 13.8880 - val_acc: 0.1384\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7721 - acc: 0.1455\n",
      "Epoch 00091: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7716 - acc: 0.1455 - val_loss: 13.8101 - val_acc: 0.1430\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7515 - acc: 0.1468\n",
      "Epoch 00092: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7518 - acc: 0.1467 - val_loss: 13.7867 - val_acc: 0.1447\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7167 - acc: 0.1489\n",
      "Epoch 00093: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7165 - acc: 0.1489 - val_loss: 13.7978 - val_acc: 0.1440\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7539 - acc: 0.1466\n",
      "Epoch 00094: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7538 - acc: 0.1466 - val_loss: 13.7227 - val_acc: 0.1486\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7108 - acc: 0.1493\n",
      "Epoch 00095: val_loss did not improve from 13.70770\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7111 - acc: 0.1493 - val_loss: 13.8354 - val_acc: 0.1416\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7324 - acc: 0.1479\n",
      "Epoch 00096: val_loss improved from 13.70770 to 13.67767, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/096-13.6777.hdf5\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7319 - acc: 0.1479 - val_loss: 13.6777 - val_acc: 0.1514\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6567 - acc: 0.1526\n",
      "Epoch 00097: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6566 - acc: 0.1526 - val_loss: 13.6964 - val_acc: 0.1502\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6741 - acc: 0.1515\n",
      "Epoch 00098: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6740 - acc: 0.1515 - val_loss: 13.7491 - val_acc: 0.1470\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6577 - acc: 0.1526\n",
      "Epoch 00099: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6581 - acc: 0.1526 - val_loss: 13.9480 - val_acc: 0.1346\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7029 - acc: 0.1498\n",
      "Epoch 00100: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7023 - acc: 0.1499 - val_loss: 13.6889 - val_acc: 0.1507\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6583 - acc: 0.1525\n",
      "Epoch 00101: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6586 - acc: 0.1525 - val_loss: 13.6782 - val_acc: 0.1512\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6460 - acc: 0.1533\n",
      "Epoch 00102: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6463 - acc: 0.1533 - val_loss: 13.6901 - val_acc: 0.1505\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6367 - acc: 0.1539\n",
      "Epoch 00103: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6370 - acc: 0.1539 - val_loss: 13.6811 - val_acc: 0.1512\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6441 - acc: 0.1534\n",
      "Epoch 00104: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6436 - acc: 0.1535 - val_loss: 13.7115 - val_acc: 0.1493\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6774 - acc: 0.1514\n",
      "Epoch 00105: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6777 - acc: 0.1514 - val_loss: 13.7115 - val_acc: 0.1493\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7075 - acc: 0.1495\n",
      "Epoch 00106: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7079 - acc: 0.1495 - val_loss: 13.6964 - val_acc: 0.1502\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6784 - acc: 0.1514\n",
      "Epoch 00107: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6787 - acc: 0.1513 - val_loss: 13.7689 - val_acc: 0.1456\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6903 - acc: 0.1506\n",
      "Epoch 00108: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6902 - acc: 0.1506 - val_loss: 13.7139 - val_acc: 0.1488\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6636 - acc: 0.1522\n",
      "Epoch 00109: val_loss did not improve from 13.67767\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6640 - acc: 0.1522 - val_loss: 13.7175 - val_acc: 0.1488\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6308 - acc: 0.1543\n",
      "Epoch 00110: val_loss improved from 13.67767 to 13.65947, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv_checkpoint/110-13.6595.hdf5\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6311 - acc: 0.1543 - val_loss: 13.6595 - val_acc: 0.1523\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6844 - acc: 0.1509\n",
      "Epoch 00111: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6843 - acc: 0.1509 - val_loss: 13.6964 - val_acc: 0.1502\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6767 - acc: 0.1514\n",
      "Epoch 00112: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6766 - acc: 0.1514 - val_loss: 13.7753 - val_acc: 0.1454\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7004 - acc: 0.1499\n",
      "Epoch 00113: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7007 - acc: 0.1499 - val_loss: 13.7715 - val_acc: 0.1456\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7355 - acc: 0.1477\n",
      "Epoch 00114: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7358 - acc: 0.1477 - val_loss: 13.8391 - val_acc: 0.1414\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7486 - acc: 0.1469\n",
      "Epoch 00115: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7490 - acc: 0.1469 - val_loss: 13.7640 - val_acc: 0.1461\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6889 - acc: 0.1507\n",
      "Epoch 00116: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6888 - acc: 0.1507 - val_loss: 13.7528 - val_acc: 0.1468\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6865 - acc: 0.1508\n",
      "Epoch 00117: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6868 - acc: 0.1508 - val_loss: 13.8091 - val_acc: 0.1433\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6985 - acc: 0.1501\n",
      "Epoch 00118: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6984 - acc: 0.1501 - val_loss: 13.7715 - val_acc: 0.1456\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6736 - acc: 0.1516\n",
      "Epoch 00119: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6740 - acc: 0.1516 - val_loss: 13.7892 - val_acc: 0.1444\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7139 - acc: 0.1490\n",
      "Epoch 00120: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.7134 - acc: 0.1491 - val_loss: 13.7598 - val_acc: 0.1463\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6648 - acc: 0.1522\n",
      "Epoch 00121: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6651 - acc: 0.1522 - val_loss: 13.7415 - val_acc: 0.1474\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6559 - acc: 0.1527\n",
      "Epoch 00122: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6558 - acc: 0.1528 - val_loss: 13.7703 - val_acc: 0.1454\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7137 - acc: 0.1490\n",
      "Epoch 00123: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.7140 - acc: 0.1490 - val_loss: 13.7753 - val_acc: 0.1454\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6483 - acc: 0.1532\n",
      "Epoch 00124: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6482 - acc: 0.1532 - val_loss: 13.7077 - val_acc: 0.1495\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6659 - acc: 0.1521\n",
      "Epoch 00125: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6662 - acc: 0.1521 - val_loss: 13.7578 - val_acc: 0.1463\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7112 - acc: 0.1492\n",
      "Epoch 00126: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7115 - acc: 0.1492 - val_loss: 13.7565 - val_acc: 0.1465\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6494 - acc: 0.1531\n",
      "Epoch 00127: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6498 - acc: 0.1531 - val_loss: 13.7148 - val_acc: 0.1491\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6427 - acc: 0.1535\n",
      "Epoch 00128: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6426 - acc: 0.1535 - val_loss: 13.7363 - val_acc: 0.1477\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6497 - acc: 0.1531\n",
      "Epoch 00129: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6492 - acc: 0.1531 - val_loss: 13.7227 - val_acc: 0.1486\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6659 - acc: 0.1521\n",
      "Epoch 00130: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6658 - acc: 0.1521 - val_loss: 13.7182 - val_acc: 0.1488\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6524 - acc: 0.1529\n",
      "Epoch 00131: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6523 - acc: 0.1529 - val_loss: 13.7565 - val_acc: 0.1465\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6750 - acc: 0.1515\n",
      "Epoch 00132: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6749 - acc: 0.1515 - val_loss: 13.8212 - val_acc: 0.1423\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7011 - acc: 0.1498\n",
      "Epoch 00133: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7010 - acc: 0.1498 - val_loss: 13.7753 - val_acc: 0.1454\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6686 - acc: 0.1519\n",
      "Epoch 00134: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6689 - acc: 0.1519 - val_loss: 13.7190 - val_acc: 0.1488\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6225 - acc: 0.1548\n",
      "Epoch 00135: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6228 - acc: 0.1548 - val_loss: 13.6927 - val_acc: 0.1505\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6448 - acc: 0.1534\n",
      "Epoch 00136: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6452 - acc: 0.1534 - val_loss: 13.7603 - val_acc: 0.1463\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6596 - acc: 0.1525\n",
      "Epoch 00137: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6595 - acc: 0.1525 - val_loss: 13.7613 - val_acc: 0.1461\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6635 - acc: 0.1522\n",
      "Epoch 00138: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6638 - acc: 0.1522 - val_loss: 13.7516 - val_acc: 0.1468\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6167 - acc: 0.1551\n",
      "Epoch 00139: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6166 - acc: 0.1551 - val_loss: 13.6678 - val_acc: 0.1519\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6242 - acc: 0.1547\n",
      "Epoch 00140: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6241 - acc: 0.1547 - val_loss: 13.6927 - val_acc: 0.1505\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6532 - acc: 0.1529\n",
      "Epoch 00141: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6536 - acc: 0.1528 - val_loss: 13.7077 - val_acc: 0.1495\n",
      "Epoch 142/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6128 - acc: 0.1554\n",
      "Epoch 00142: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6131 - acc: 0.1554 - val_loss: 13.6867 - val_acc: 0.1507\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6170 - acc: 0.1552\n",
      "Epoch 00143: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6169 - acc: 0.1552 - val_loss: 13.6764 - val_acc: 0.1514\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6211 - acc: 0.1549\n",
      "Epoch 00144: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6210 - acc: 0.1549 - val_loss: 13.7086 - val_acc: 0.1493\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6361 - acc: 0.1539\n",
      "Epoch 00145: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6364 - acc: 0.1539 - val_loss: 13.8541 - val_acc: 0.1405\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6497 - acc: 0.1531\n",
      "Epoch 00146: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6492 - acc: 0.1531 - val_loss: 13.6980 - val_acc: 0.1500\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6300 - acc: 0.1543\n",
      "Epoch 00147: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6303 - acc: 0.1543 - val_loss: 13.6852 - val_acc: 0.1509\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6631 - acc: 0.1522\n",
      "Epoch 00148: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6630 - acc: 0.1522 - val_loss: 13.7377 - val_acc: 0.1477\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6658 - acc: 0.1521\n",
      "Epoch 00149: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6657 - acc: 0.1521 - val_loss: 13.6900 - val_acc: 0.1505\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6275 - acc: 0.1545\n",
      "Epoch 00150: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6274 - acc: 0.1545 - val_loss: 13.7828 - val_acc: 0.1449\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6615 - acc: 0.1524\n",
      "Epoch 00151: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6618 - acc: 0.1523 - val_loss: 13.6852 - val_acc: 0.1509\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7228 - acc: 0.1485\n",
      "Epoch 00152: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.7231 - acc: 0.1485 - val_loss: 13.7882 - val_acc: 0.1444\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7396 - acc: 0.1475\n",
      "Epoch 00153: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.7395 - acc: 0.1475 - val_loss: 13.7603 - val_acc: 0.1463\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6978 - acc: 0.1501\n",
      "Epoch 00154: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.6968 - acc: 0.1502 - val_loss: 13.7415 - val_acc: 0.1474\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6929 - acc: 0.1504\n",
      "Epoch 00155: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.6932 - acc: 0.1504 - val_loss: 13.7372 - val_acc: 0.1477\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.9307 - acc: 0.1357\n",
      "Epoch 00156: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 147s 4ms/sample - loss: 13.9310 - acc: 0.1356 - val_loss: 13.9337 - val_acc: 0.1353\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.7621 - acc: 0.1461\n",
      "Epoch 00157: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 146s 4ms/sample - loss: 13.7624 - acc: 0.1461 - val_loss: 13.7415 - val_acc: 0.1474\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6888 - acc: 0.1507\n",
      "Epoch 00158: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 145s 4ms/sample - loss: 13.6883 - acc: 0.1507 - val_loss: 13.7729 - val_acc: 0.1454\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6678 - acc: 0.1520\n",
      "Epoch 00159: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6681 - acc: 0.1520 - val_loss: 13.6923 - val_acc: 0.1502\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6266 - acc: 0.1545\n",
      "Epoch 00160: val_loss did not improve from 13.65947\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 13.6265 - acc: 0.1545 - val_loss: 13.6882 - val_acc: 0.1507\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 13.6140 - acc: 0.1553\n",
      "Loss: 13.613969903274489 Accuracy: 0.15534787\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 14.0240 - acc: 0.1262\n",
      "Epoch 00001: val_loss improved from inf to 14.64002, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv_checkpoint/001-14.6400.hdf5\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 14.0243 - acc: 0.1262 - val_loss: 14.6400 - val_acc: 0.0899\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.8656 - acc: 0.1890\n",
      "Epoch 00002: val_loss improved from 14.64002 to 12.60628, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv_checkpoint/002-12.6063.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 12.8660 - acc: 0.1890 - val_loss: 12.6063 - val_acc: 0.1975\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 5.8098 - acc: 0.3909\n",
      "Epoch 00003: val_loss improved from 12.60628 to 1.19762, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv_checkpoint/003-1.1976.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 5.8093 - acc: 0.3909 - val_loss: 1.1976 - val_acc: 0.6308\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0663 - acc: 0.6795\n",
      "Epoch 00004: val_loss did not improve from 1.19762\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 1.0663 - acc: 0.6796 - val_loss: 1.2091 - val_acc: 0.6564\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7525\n",
      "Epoch 00005: val_loss improved from 1.19762 to 0.82203, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv_checkpoint/005-0.8220.hdf5\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.8228 - acc: 0.7525 - val_loss: 0.8220 - val_acc: 0.7575\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6763 - acc: 0.7981\n",
      "Epoch 00006: val_loss improved from 0.82203 to 0.77932, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv_checkpoint/006-0.7793.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.6763 - acc: 0.7981 - val_loss: 0.7793 - val_acc: 0.7713\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.8310\n",
      "Epoch 00007: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5639 - acc: 0.8310 - val_loss: 0.8237 - val_acc: 0.7659\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8572\n",
      "Epoch 00008: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.4767 - acc: 0.8572 - val_loss: 1.0906 - val_acc: 0.6921\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8779\n",
      "Epoch 00009: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.4031 - acc: 0.8779 - val_loss: 0.8770 - val_acc: 0.7671\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8967\n",
      "Epoch 00010: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.3378 - acc: 0.8968 - val_loss: 0.9221 - val_acc: 0.7529\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9151\n",
      "Epoch 00011: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2729 - acc: 0.9150 - val_loss: 1.1432 - val_acc: 0.7279\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9178\n",
      "Epoch 00012: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2662 - acc: 0.9178 - val_loss: 1.2664 - val_acc: 0.7156\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9314\n",
      "Epoch 00013: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2165 - acc: 0.9314 - val_loss: 0.9832 - val_acc: 0.7757\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9460\n",
      "Epoch 00014: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1697 - acc: 0.9460 - val_loss: 0.9472 - val_acc: 0.7654\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9538\n",
      "Epoch 00015: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1473 - acc: 0.9538 - val_loss: 1.2245 - val_acc: 0.7307\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9577\n",
      "Epoch 00016: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.1376 - acc: 0.9577 - val_loss: 1.4838 - val_acc: 0.6995\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9640\n",
      "Epoch 00017: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1198 - acc: 0.9640 - val_loss: 2.6819 - val_acc: 0.5982\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9276\n",
      "Epoch 00018: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.2383 - acc: 0.9276 - val_loss: 1.0130 - val_acc: 0.7803\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9668\n",
      "Epoch 00019: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.1086 - acc: 0.9668 - val_loss: 1.2202 - val_acc: 0.7575\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9689\n",
      "Epoch 00020: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.1048 - acc: 0.9689 - val_loss: 1.0785 - val_acc: 0.7752\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9759\n",
      "Epoch 00021: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0805 - acc: 0.9759 - val_loss: 1.1531 - val_acc: 0.7750\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9806\n",
      "Epoch 00022: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0700 - acc: 0.9806 - val_loss: 1.2153 - val_acc: 0.7671\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9777\n",
      "Epoch 00023: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0743 - acc: 0.9777 - val_loss: 1.1990 - val_acc: 0.7694\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9828\n",
      "Epoch 00024: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0628 - acc: 0.9828 - val_loss: 1.0843 - val_acc: 0.7890\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9825\n",
      "Epoch 00025: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0617 - acc: 0.9825 - val_loss: 1.2630 - val_acc: 0.7584\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9836\n",
      "Epoch 00026: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0566 - acc: 0.9836 - val_loss: 1.2280 - val_acc: 0.7678\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9849\n",
      "Epoch 00027: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0535 - acc: 0.9849 - val_loss: 1.1711 - val_acc: 0.7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9862\n",
      "Epoch 00028: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0498 - acc: 0.9862 - val_loss: 1.1398 - val_acc: 0.7813\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9888\n",
      "Epoch 00029: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0437 - acc: 0.9888 - val_loss: 1.2037 - val_acc: 0.7838\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9879\n",
      "Epoch 00030: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0443 - acc: 0.9879 - val_loss: 1.2163 - val_acc: 0.7843\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9882\n",
      "Epoch 00031: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0437 - acc: 0.9882 - val_loss: 1.2707 - val_acc: 0.7806\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9892\n",
      "Epoch 00032: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0396 - acc: 0.9892 - val_loss: 1.8997 - val_acc: 0.7170\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9821\n",
      "Epoch 00033: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0638 - acc: 0.9821 - val_loss: 1.2177 - val_acc: 0.7768\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9887\n",
      "Epoch 00034: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0420 - acc: 0.9887 - val_loss: 1.2504 - val_acc: 0.7796\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9899\n",
      "Epoch 00035: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0391 - acc: 0.9899 - val_loss: 1.1942 - val_acc: 0.7866\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9897\n",
      "Epoch 00036: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0391 - acc: 0.9897 - val_loss: 1.2180 - val_acc: 0.7862\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9914\n",
      "Epoch 00037: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0340 - acc: 0.9914 - val_loss: 1.2364 - val_acc: 0.7892\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9925\n",
      "Epoch 00038: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0318 - acc: 0.9925 - val_loss: 1.2124 - val_acc: 0.7862\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9929\n",
      "Epoch 00039: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0297 - acc: 0.9929 - val_loss: 1.2398 - val_acc: 0.7952\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9919\n",
      "Epoch 00040: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.0323 - acc: 0.9919 - val_loss: 1.2593 - val_acc: 0.7901\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9921\n",
      "Epoch 00041: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0313 - acc: 0.9921 - val_loss: 1.2598 - val_acc: 0.7855\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9929\n",
      "Epoch 00042: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0283 - acc: 0.9929 - val_loss: 1.2695 - val_acc: 0.7908\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9927\n",
      "Epoch 00043: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0284 - acc: 0.9927 - val_loss: 1.5716 - val_acc: 0.7694\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9873\n",
      "Epoch 00044: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0516 - acc: 0.9873 - val_loss: 1.2613 - val_acc: 0.7894\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9931\n",
      "Epoch 00045: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0287 - acc: 0.9931 - val_loss: 1.2773 - val_acc: 0.7932\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9939\n",
      "Epoch 00046: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0270 - acc: 0.9939 - val_loss: 1.2421 - val_acc: 0.7955\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9938\n",
      "Epoch 00047: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0262 - acc: 0.9938 - val_loss: 1.4598 - val_acc: 0.7761\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9933\n",
      "Epoch 00048: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0285 - acc: 0.9933 - val_loss: 1.3583 - val_acc: 0.7796\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9942\n",
      "Epoch 00049: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0262 - acc: 0.9942 - val_loss: 1.2979 - val_acc: 0.7862\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9937\n",
      "Epoch 00050: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0264 - acc: 0.9937 - val_loss: 1.2266 - val_acc: 0.8008\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9927\n",
      "Epoch 00051: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0278 - acc: 0.9927 - val_loss: 1.2735 - val_acc: 0.7906\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9942\n",
      "Epoch 00052: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0244 - acc: 0.9942 - val_loss: 1.2943 - val_acc: 0.7901\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9950\n",
      "Epoch 00053: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 156s 4ms/sample - loss: 0.0228 - acc: 0.9950 - val_loss: 1.2569 - val_acc: 0.7945\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9959\n",
      "Epoch 00054: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0219 - acc: 0.9959 - val_loss: 1.2762 - val_acc: 0.8022\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9952\n",
      "Epoch 00055: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 155s 4ms/sample - loss: 0.0220 - acc: 0.9952 - val_loss: 1.2371 - val_acc: 0.8001\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9954\n",
      "Epoch 00056: val_loss did not improve from 0.77932\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 0.0212 - acc: 0.9954 - val_loss: 1.3158 - val_acc: 0.7994\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.8486 - acc: 0.7362\n",
      "Loss: 0.8485809605193411 Accuracy: 0.7362409\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.6992 - acc: 0.1434\n",
      "Epoch 00001: val_loss improved from inf to 13.42317, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/001-13.4232.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 13.6990 - acc: 0.1434 - val_loss: 13.4232 - val_acc: 0.1642\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.3680 - acc: 0.1668\n",
      "Epoch 00002: val_loss did not improve from 13.42317\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 13.3683 - acc: 0.1668 - val_loss: 13.8444 - val_acc: 0.1391\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 13.0219 - acc: 0.1863\n",
      "Epoch 00003: val_loss improved from 13.42317 to 13.14129, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/003-13.1413.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 13.0223 - acc: 0.1863 - val_loss: 13.1413 - val_acc: 0.1624\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.5683 - acc: 0.2090\n",
      "Epoch 00004: val_loss improved from 13.14129 to 12.61721, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/004-12.6172.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 12.5688 - acc: 0.2090 - val_loss: 12.6172 - val_acc: 0.1966\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 12.5001 - acc: 0.2131\n",
      "Epoch 00005: val_loss improved from 12.61721 to 12.58892, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/005-12.5889.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 12.5002 - acc: 0.2131 - val_loss: 12.5889 - val_acc: 0.2122\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 8.8620 - acc: 0.2691\n",
      "Epoch 00006: val_loss improved from 12.58892 to 1.75727, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/006-1.7573.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 8.8612 - acc: 0.2691 - val_loss: 1.7573 - val_acc: 0.4554\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0794 - acc: 0.6649\n",
      "Epoch 00007: val_loss improved from 1.75727 to 0.86987, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/007-0.8699.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 1.0794 - acc: 0.6648 - val_loss: 0.8699 - val_acc: 0.7452\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7679\n",
      "Epoch 00008: val_loss did not improve from 0.86987\n",
      "36805/36805 [==============================] - 165s 4ms/sample - loss: 0.7737 - acc: 0.7678 - val_loss: 0.9485 - val_acc: 0.7128\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6583 - acc: 0.8042\n",
      "Epoch 00009: val_loss improved from 0.86987 to 0.65434, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/009-0.6543.hdf5\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.6583 - acc: 0.8042 - val_loss: 0.6543 - val_acc: 0.8034\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8298\n",
      "Epoch 00010: val_loss improved from 0.65434 to 0.60746, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/010-0.6075.hdf5\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.5739 - acc: 0.8298 - val_loss: 0.6075 - val_acc: 0.8300\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8520\n",
      "Epoch 00011: val_loss did not improve from 0.60746\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 0.5021 - acc: 0.8519 - val_loss: 0.6357 - val_acc: 0.8106\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8635\n",
      "Epoch 00012: val_loss did not improve from 0.60746\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.4588 - acc: 0.8634 - val_loss: 0.6518 - val_acc: 0.8169\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8754\n",
      "Epoch 00013: val_loss improved from 0.60746 to 0.58413, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/013-0.5841.hdf5\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.4157 - acc: 0.8753 - val_loss: 0.5841 - val_acc: 0.8337\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8945\n",
      "Epoch 00014: val_loss improved from 0.58413 to 0.57504, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/014-0.5750.hdf5\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.3517 - acc: 0.8945 - val_loss: 0.5750 - val_acc: 0.8386\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.9064\n",
      "Epoch 00015: val_loss did not improve from 0.57504\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.3100 - acc: 0.9063 - val_loss: 0.6643 - val_acc: 0.8102\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9159\n",
      "Epoch 00016: val_loss improved from 0.57504 to 0.55778, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv_checkpoint/016-0.5578.hdf5\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.2759 - acc: 0.9159 - val_loss: 0.5578 - val_acc: 0.8474\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9302\n",
      "Epoch 00017: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.2288 - acc: 0.9302 - val_loss: 0.6079 - val_acc: 0.8339\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9451\n",
      "Epoch 00018: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.1835 - acc: 0.9451 - val_loss: 0.6191 - val_acc: 0.8404\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9543\n",
      "Epoch 00019: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.1556 - acc: 0.9543 - val_loss: 0.5697 - val_acc: 0.8577\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9605\n",
      "Epoch 00020: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.1320 - acc: 0.9605 - val_loss: 0.7697 - val_acc: 0.8209\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9587\n",
      "Epoch 00021: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.1344 - acc: 0.9587 - val_loss: 0.6949 - val_acc: 0.8316\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9705\n",
      "Epoch 00022: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0993 - acc: 0.9704 - val_loss: 0.7923 - val_acc: 0.8192\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9684\n",
      "Epoch 00023: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.1057 - acc: 0.9684 - val_loss: 0.6743 - val_acc: 0.8523\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9770\n",
      "Epoch 00024: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0787 - acc: 0.9770 - val_loss: 0.6665 - val_acc: 0.8472\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9765\n",
      "Epoch 00025: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0790 - acc: 0.9764 - val_loss: 0.8268 - val_acc: 0.8234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9724\n",
      "Epoch 00026: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0934 - acc: 0.9723 - val_loss: 0.6588 - val_acc: 0.8537\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9782\n",
      "Epoch 00027: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0746 - acc: 0.9782 - val_loss: 0.6601 - val_acc: 0.8551\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9843\n",
      "Epoch 00028: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0539 - acc: 0.9843 - val_loss: 0.6547 - val_acc: 0.8644\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9891\n",
      "Epoch 00029: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0408 - acc: 0.9891 - val_loss: 0.6851 - val_acc: 0.8626\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9876\n",
      "Epoch 00030: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0434 - acc: 0.9876 - val_loss: 0.6726 - val_acc: 0.8609\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9886\n",
      "Epoch 00031: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0438 - acc: 0.9886 - val_loss: 0.7149 - val_acc: 0.8425\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9870\n",
      "Epoch 00032: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0470 - acc: 0.9870 - val_loss: 0.6677 - val_acc: 0.8609\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9905\n",
      "Epoch 00033: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0365 - acc: 0.9905 - val_loss: 0.6829 - val_acc: 0.8677\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9919\n",
      "Epoch 00034: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0313 - acc: 0.9919 - val_loss: 0.6942 - val_acc: 0.8621\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9922\n",
      "Epoch 00035: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0296 - acc: 0.9922 - val_loss: 0.6653 - val_acc: 0.8619\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9927\n",
      "Epoch 00036: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0294 - acc: 0.9927 - val_loss: 0.6855 - val_acc: 0.8656\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9937\n",
      "Epoch 00037: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0256 - acc: 0.9937 - val_loss: 0.7128 - val_acc: 0.8642\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9934\n",
      "Epoch 00038: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0274 - acc: 0.9934 - val_loss: 0.6571 - val_acc: 0.8675\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9943\n",
      "Epoch 00039: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0246 - acc: 0.9942 - val_loss: 0.9714 - val_acc: 0.8290\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9826\n",
      "Epoch 00040: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0608 - acc: 0.9825 - val_loss: 0.7204 - val_acc: 0.8535\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9906\n",
      "Epoch 00041: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0351 - acc: 0.9906 - val_loss: 0.7259 - val_acc: 0.8635\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9940\n",
      "Epoch 00042: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0248 - acc: 0.9940 - val_loss: 0.6974 - val_acc: 0.8647\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9955\n",
      "Epoch 00043: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0214 - acc: 0.9955 - val_loss: 0.6903 - val_acc: 0.8717\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9958\n",
      "Epoch 00044: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0197 - acc: 0.9958 - val_loss: 0.7431 - val_acc: 0.8649\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9948\n",
      "Epoch 00045: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0223 - acc: 0.9948 - val_loss: 0.6759 - val_acc: 0.8684\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0206 - acc: 0.9951 - val_loss: 0.7542 - val_acc: 0.8679\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9954\n",
      "Epoch 00047: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0192 - acc: 0.9954 - val_loss: 0.7319 - val_acc: 0.8689\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9959\n",
      "Epoch 00048: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0186 - acc: 0.9959 - val_loss: 0.8905 - val_acc: 0.8404\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9924\n",
      "Epoch 00049: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0278 - acc: 0.9924 - val_loss: 0.7153 - val_acc: 0.8737\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9965\n",
      "Epoch 00050: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0177 - acc: 0.9965 - val_loss: 0.6968 - val_acc: 0.8691\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9955\n",
      "Epoch 00051: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0206 - acc: 0.9955 - val_loss: 0.6837 - val_acc: 0.8700\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9965\n",
      "Epoch 00052: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0174 - acc: 0.9965 - val_loss: 0.7426 - val_acc: 0.8661\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9979\n",
      "Epoch 00053: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0134 - acc: 0.9979 - val_loss: 0.7208 - val_acc: 0.8721\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9975\n",
      "Epoch 00054: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0125 - acc: 0.9975 - val_loss: 0.7321 - val_acc: 0.8693\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9977\n",
      "Epoch 00055: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0130 - acc: 0.9977 - val_loss: 0.6947 - val_acc: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9977\n",
      "Epoch 00056: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 161s 4ms/sample - loss: 0.0121 - acc: 0.9977 - val_loss: 0.7458 - val_acc: 0.8658\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9977\n",
      "Epoch 00057: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0120 - acc: 0.9977 - val_loss: 0.7341 - val_acc: 0.8751\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9965\n",
      "Epoch 00058: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0149 - acc: 0.9965 - val_loss: 0.7415 - val_acc: 0.8700\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9975\n",
      "Epoch 00059: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0125 - acc: 0.9975 - val_loss: 0.7380 - val_acc: 0.8717\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9961\n",
      "Epoch 00060: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.7426 - val_acc: 0.8740\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9972\n",
      "Epoch 00061: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0134 - acc: 0.9972 - val_loss: 0.7269 - val_acc: 0.8719\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9973\n",
      "Epoch 00062: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 159s 4ms/sample - loss: 0.0140 - acc: 0.9973 - val_loss: 0.7638 - val_acc: 0.8710\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9982\n",
      "Epoch 00063: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0106 - acc: 0.9982 - val_loss: 0.7336 - val_acc: 0.8726\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9972\n",
      "Epoch 00064: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0144 - acc: 0.9972 - val_loss: 0.7288 - val_acc: 0.8728\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9973\n",
      "Epoch 00065: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0143 - acc: 0.9973 - val_loss: 0.7683 - val_acc: 0.8710\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9973\n",
      "Epoch 00066: val_loss did not improve from 0.55778\n",
      "36805/36805 [==============================] - 160s 4ms/sample - loss: 0.0145 - acc: 0.9973 - val_loss: 0.7549 - val_acc: 0.8661\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.7109 - acc: 0.8093\n",
      "Loss: 0.7108803769138372 Accuracy: 0.8093458\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.6062 - acc: 0.4777\n",
      "Epoch 00001: val_loss improved from inf to 0.98043, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/001-0.9804.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 3.6058 - acc: 0.4777 - val_loss: 0.9804 - val_acc: 0.7072\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7720\n",
      "Epoch 00002: val_loss did not improve from 0.98043\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.7776 - acc: 0.7720 - val_loss: 2.2823 - val_acc: 0.5076\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.8218\n",
      "Epoch 00003: val_loss improved from 0.98043 to 0.86832, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/003-0.8683.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.6052 - acc: 0.8217 - val_loss: 0.8683 - val_acc: 0.7533\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8614\n",
      "Epoch 00004: val_loss improved from 0.86832 to 0.46647, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/004-0.4665.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.4710 - acc: 0.8614 - val_loss: 0.4665 - val_acc: 0.8703\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8840\n",
      "Epoch 00005: val_loss improved from 0.46647 to 0.42027, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/005-0.4203.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.3916 - acc: 0.8840 - val_loss: 0.4203 - val_acc: 0.8798\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.9011\n",
      "Epoch 00006: val_loss improved from 0.42027 to 0.38133, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/006-0.3813.hdf5\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.3316 - acc: 0.9012 - val_loss: 0.3813 - val_acc: 0.8915\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9133\n",
      "Epoch 00007: val_loss improved from 0.38133 to 0.36146, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/007-0.3615.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.2885 - acc: 0.9133 - val_loss: 0.3615 - val_acc: 0.9036\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9246\n",
      "Epoch 00008: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.2526 - acc: 0.9245 - val_loss: 0.7960 - val_acc: 0.8102\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9299\n",
      "Epoch 00009: val_loss improved from 0.36146 to 0.32476, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/009-0.3248.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.2304 - acc: 0.9299 - val_loss: 0.3248 - val_acc: 0.9113\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9401\n",
      "Epoch 00010: val_loss did not improve from 0.32476\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1973 - acc: 0.9401 - val_loss: 1.7438 - val_acc: 0.6820\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9370\n",
      "Epoch 00011: val_loss did not improve from 0.32476\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.2058 - acc: 0.9370 - val_loss: 0.3345 - val_acc: 0.9119\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9496\n",
      "Epoch 00012: val_loss improved from 0.32476 to 0.27519, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv_checkpoint/012-0.2752.hdf5\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1633 - acc: 0.9496 - val_loss: 0.2752 - val_acc: 0.9245\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9567\n",
      "Epoch 00013: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1398 - acc: 0.9567 - val_loss: 0.4027 - val_acc: 0.8896\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9569\n",
      "Epoch 00014: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1361 - acc: 0.9569 - val_loss: 0.2853 - val_acc: 0.9229\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9645\n",
      "Epoch 00015: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.1135 - acc: 0.9644 - val_loss: 0.5069 - val_acc: 0.8777\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9596\n",
      "Epoch 00016: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.1307 - acc: 0.9596 - val_loss: 0.2919 - val_acc: 0.9185\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9718\n",
      "Epoch 00017: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0913 - acc: 0.9718 - val_loss: 0.2943 - val_acc: 0.9255\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9737\n",
      "Epoch 00018: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0830 - acc: 0.9737 - val_loss: 0.3429 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9742\n",
      "Epoch 00019: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0838 - acc: 0.9742 - val_loss: 0.3218 - val_acc: 0.9271\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9799\n",
      "Epoch 00020: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0659 - acc: 0.9799 - val_loss: 0.5087 - val_acc: 0.8791\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9699\n",
      "Epoch 00021: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0985 - acc: 0.9699 - val_loss: 0.3262 - val_acc: 0.9269\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9820\n",
      "Epoch 00022: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0595 - acc: 0.9820 - val_loss: 0.3239 - val_acc: 0.9271\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9860\n",
      "Epoch 00023: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0481 - acc: 0.9860 - val_loss: 0.3208 - val_acc: 0.9269\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9866\n",
      "Epoch 00024: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0458 - acc: 0.9866 - val_loss: 0.3077 - val_acc: 0.9313\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9882\n",
      "Epoch 00025: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0414 - acc: 0.9882 - val_loss: 0.4056 - val_acc: 0.9168\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9848\n",
      "Epoch 00026: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0512 - acc: 0.9848 - val_loss: 0.4655 - val_acc: 0.9015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9835\n",
      "Epoch 00027: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0560 - acc: 0.9835 - val_loss: 0.3134 - val_acc: 0.9317\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9897\n",
      "Epoch 00028: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0366 - acc: 0.9897 - val_loss: 0.2929 - val_acc: 0.9378\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9915\n",
      "Epoch 00029: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 0.3076 - val_acc: 0.9359\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9917\n",
      "Epoch 00030: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0292 - acc: 0.9917 - val_loss: 0.3217 - val_acc: 0.9350\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9930\n",
      "Epoch 00031: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0256 - acc: 0.9930 - val_loss: 0.3680 - val_acc: 0.9236\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9900\n",
      "Epoch 00032: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0365 - acc: 0.9900 - val_loss: 0.3094 - val_acc: 0.9378\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9942\n",
      "Epoch 00033: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0212 - acc: 0.9942 - val_loss: 0.3523 - val_acc: 0.9313\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9949\n",
      "Epoch 00034: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0189 - acc: 0.9949 - val_loss: 0.3237 - val_acc: 0.9371\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9946\n",
      "Epoch 00035: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0205 - acc: 0.9946 - val_loss: 0.3449 - val_acc: 0.9355\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9930\n",
      "Epoch 00036: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 0.3904 - val_acc: 0.9201\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9897\n",
      "Epoch 00037: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0376 - acc: 0.9897 - val_loss: 0.3436 - val_acc: 0.9294\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9943\n",
      "Epoch 00038: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0222 - acc: 0.9943 - val_loss: 0.3260 - val_acc: 0.9364\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9949\n",
      "Epoch 00039: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0197 - acc: 0.9949 - val_loss: 0.3239 - val_acc: 0.9369\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9968\n",
      "Epoch 00040: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0138 - acc: 0.9968 - val_loss: 0.3352 - val_acc: 0.9371\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9963\n",
      "Epoch 00041: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0147 - acc: 0.9963 - val_loss: 0.3161 - val_acc: 0.9392\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9963\n",
      "Epoch 00042: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0146 - acc: 0.9963 - val_loss: 0.3246 - val_acc: 0.9392\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9966\n",
      "Epoch 00043: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0142 - acc: 0.9966 - val_loss: 0.3452 - val_acc: 0.9350\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9966\n",
      "Epoch 00044: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0136 - acc: 0.9966 - val_loss: 0.3294 - val_acc: 0.9406\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9973\n",
      "Epoch 00045: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0118 - acc: 0.9973 - val_loss: 0.3322 - val_acc: 0.9373\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9971\n",
      "Epoch 00046: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0119 - acc: 0.9971 - val_loss: 0.3342 - val_acc: 0.9404\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9984\n",
      "Epoch 00047: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0083 - acc: 0.9984 - val_loss: 0.3433 - val_acc: 0.9392\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9976\n",
      "Epoch 00048: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0108 - acc: 0.9976 - val_loss: 0.3570 - val_acc: 0.9366\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9975\n",
      "Epoch 00049: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0109 - acc: 0.9975 - val_loss: 0.3292 - val_acc: 0.9376\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9969\n",
      "Epoch 00050: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0124 - acc: 0.9969 - val_loss: 0.7325 - val_acc: 0.8682\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9901\n",
      "Epoch 00051: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0337 - acc: 0.9901 - val_loss: 0.4940 - val_acc: 0.9066\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9898\n",
      "Epoch 00052: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0353 - acc: 0.9898 - val_loss: 0.3246 - val_acc: 0.9348\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 00053: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0136 - acc: 0.9968 - val_loss: 0.3760 - val_acc: 0.9257\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9930\n",
      "Epoch 00054: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0255 - acc: 0.9930 - val_loss: 0.3231 - val_acc: 0.9362\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0163 - acc: 0.9955 - val_loss: 0.3125 - val_acc: 0.9380\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9972\n",
      "Epoch 00056: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0116 - acc: 0.9972 - val_loss: 0.3158 - val_acc: 0.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0111 - acc: 0.9974 - val_loss: 0.3259 - val_acc: 0.9399\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9982\n",
      "Epoch 00058: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0089 - acc: 0.9982 - val_loss: 0.3471 - val_acc: 0.9411\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9986\n",
      "Epoch 00059: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0071 - acc: 0.9986 - val_loss: 0.3491 - val_acc: 0.9404\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9981\n",
      "Epoch 00060: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 164s 4ms/sample - loss: 0.0082 - acc: 0.9981 - val_loss: 0.3415 - val_acc: 0.9390\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9978\n",
      "Epoch 00061: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 163s 4ms/sample - loss: 0.0094 - acc: 0.9978 - val_loss: 0.3436 - val_acc: 0.9392\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9981\n",
      "Epoch 00062: val_loss did not improve from 0.27519\n",
      "36805/36805 [==============================] - 162s 4ms/sample - loss: 0.0084 - acc: 0.9981 - val_loss: 0.3358 - val_acc: 0.9373\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3453 - acc: 0.9061\n",
      "Loss: 0.34529382100605394 Accuracy: 0.9061267\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.6323\n",
      "Epoch 00001: val_loss improved from inf to 1.12809, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/001-1.1281.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 1.3247 - acc: 0.6323 - val_loss: 1.1281 - val_acc: 0.6983\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8426\n",
      "Epoch 00002: val_loss improved from 1.12809 to 0.64607, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/002-0.6461.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.5213 - acc: 0.8426 - val_loss: 0.6461 - val_acc: 0.7959\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8871\n",
      "Epoch 00003: val_loss improved from 0.64607 to 0.29517, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/003-0.2952.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.3685 - acc: 0.8871 - val_loss: 0.2952 - val_acc: 0.9199\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9127\n",
      "Epoch 00004: val_loss did not improve from 0.29517\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2854 - acc: 0.9127 - val_loss: 0.3814 - val_acc: 0.8912\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9226\n",
      "Epoch 00005: val_loss improved from 0.29517 to 0.22571, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/005-0.2257.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.2462 - acc: 0.9226 - val_loss: 0.2257 - val_acc: 0.9406\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9364\n",
      "Epoch 00006: val_loss did not improve from 0.22571\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.2034 - acc: 0.9364 - val_loss: 0.2749 - val_acc: 0.9259\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9439\n",
      "Epoch 00007: val_loss did not improve from 0.22571\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1807 - acc: 0.9438 - val_loss: 0.2363 - val_acc: 0.9355\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9478\n",
      "Epoch 00008: val_loss did not improve from 0.22571\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1642 - acc: 0.9478 - val_loss: 0.3296 - val_acc: 0.9159\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9533\n",
      "Epoch 00009: val_loss did not improve from 0.22571\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.1461 - acc: 0.9532 - val_loss: 0.2455 - val_acc: 0.9324\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9546\n",
      "Epoch 00010: val_loss improved from 0.22571 to 0.17882, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/010-0.1788.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1403 - acc: 0.9546 - val_loss: 0.1788 - val_acc: 0.9518\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9635\n",
      "Epoch 00011: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.1131 - acc: 0.9635 - val_loss: 0.2209 - val_acc: 0.9429\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9692\n",
      "Epoch 00012: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0974 - acc: 0.9691 - val_loss: 0.2491 - val_acc: 0.9320\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9693\n",
      "Epoch 00013: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0982 - acc: 0.9693 - val_loss: 0.2342 - val_acc: 0.9394\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9694\n",
      "Epoch 00014: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0953 - acc: 0.9693 - val_loss: 0.2077 - val_acc: 0.9441\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9703\n",
      "Epoch 00015: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0912 - acc: 0.9703 - val_loss: 0.2470 - val_acc: 0.9357\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9757\n",
      "Epoch 00016: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0786 - acc: 0.9757 - val_loss: 0.3062 - val_acc: 0.9273\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9785\n",
      "Epoch 00017: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0692 - acc: 0.9785 - val_loss: 0.3649 - val_acc: 0.9164\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9764\n",
      "Epoch 00018: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0726 - acc: 0.9764 - val_loss: 0.2738 - val_acc: 0.9341\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9780\n",
      "Epoch 00019: val_loss did not improve from 0.17882\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0694 - acc: 0.9780 - val_loss: 0.2134 - val_acc: 0.9429\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9792\n",
      "Epoch 00020: val_loss improved from 0.17882 to 0.17529, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/020-0.1753.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0632 - acc: 0.9792 - val_loss: 0.1753 - val_acc: 0.9543\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9850\n",
      "Epoch 00021: val_loss did not improve from 0.17529\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0480 - acc: 0.9850 - val_loss: 0.1759 - val_acc: 0.9520\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9887\n",
      "Epoch 00022: val_loss improved from 0.17529 to 0.17383, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/022-0.1738.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0386 - acc: 0.9887 - val_loss: 0.1738 - val_acc: 0.9576\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9876\n",
      "Epoch 00023: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0411 - acc: 0.9876 - val_loss: 0.2473 - val_acc: 0.9434\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9863\n",
      "Epoch 00024: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0417 - acc: 0.9863 - val_loss: 0.1969 - val_acc: 0.9541\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9899\n",
      "Epoch 00025: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0333 - acc: 0.9899 - val_loss: 0.1806 - val_acc: 0.9604\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9897\n",
      "Epoch 00026: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0338 - acc: 0.9896 - val_loss: 0.7622 - val_acc: 0.8626\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
      "Epoch 00027: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0487 - acc: 0.9850 - val_loss: 0.1876 - val_acc: 0.9550\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9921\n",
      "Epoch 00028: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0262 - acc: 0.9921 - val_loss: 0.2769 - val_acc: 0.9387\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9893\n",
      "Epoch 00029: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0364 - acc: 0.9892 - val_loss: 0.8591 - val_acc: 0.8085\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9837\n",
      "Epoch 00030: val_loss improved from 0.17383 to 0.16945, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/030-0.1695.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0524 - acc: 0.9838 - val_loss: 0.1695 - val_acc: 0.9599\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9919\n",
      "Epoch 00031: val_loss did not improve from 0.16945\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0267 - acc: 0.9919 - val_loss: 0.1704 - val_acc: 0.9585\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9934\n",
      "Epoch 00032: val_loss improved from 0.16945 to 0.16922, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/032-0.1692.hdf5\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0226 - acc: 0.9934 - val_loss: 0.1692 - val_acc: 0.9616\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9933\n",
      "Epoch 00033: val_loss did not improve from 0.16922\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.4271 - val_acc: 0.9255\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9883\n",
      "Epoch 00034: val_loss improved from 0.16922 to 0.16760, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv_checkpoint/034-0.1676.hdf5\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0388 - acc: 0.9883 - val_loss: 0.1676 - val_acc: 0.9618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9949\n",
      "Epoch 00035: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0193 - acc: 0.9949 - val_loss: 0.1935 - val_acc: 0.9550\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9954\n",
      "Epoch 00036: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0175 - acc: 0.9954 - val_loss: 0.1694 - val_acc: 0.9602\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9964\n",
      "Epoch 00037: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0133 - acc: 0.9964 - val_loss: 0.1761 - val_acc: 0.9613\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9967\n",
      "Epoch 00038: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2163 - val_acc: 0.9567\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9936\n",
      "Epoch 00039: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0218 - acc: 0.9936 - val_loss: 0.1745 - val_acc: 0.9606\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 00040: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.1960 - val_acc: 0.9585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9971\n",
      "Epoch 00041: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0113 - acc: 0.9971 - val_loss: 0.2030 - val_acc: 0.9581\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9945\n",
      "Epoch 00042: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0182 - acc: 0.9945 - val_loss: 0.1906 - val_acc: 0.9599\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 00043: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.2200 - val_acc: 0.9576\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9924\n",
      "Epoch 00044: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0277 - acc: 0.9924 - val_loss: 0.1803 - val_acc: 0.9611\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9935\n",
      "Epoch 00045: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0212 - acc: 0.9935 - val_loss: 0.2034 - val_acc: 0.9571\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9968\n",
      "Epoch 00046: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0121 - acc: 0.9968 - val_loss: 0.1794 - val_acc: 0.9609\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9974\n",
      "Epoch 00047: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0097 - acc: 0.9974 - val_loss: 0.2596 - val_acc: 0.9485\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 00048: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0150 - acc: 0.9955 - val_loss: 0.2218 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9964\n",
      "Epoch 00049: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0121 - acc: 0.9964 - val_loss: 0.1835 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 00050: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.1918 - val_acc: 0.9604\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 00051: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0069 - acc: 0.9983 - val_loss: 0.1868 - val_acc: 0.9616\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9984\n",
      "Epoch 00052: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0066 - acc: 0.9984 - val_loss: 0.1818 - val_acc: 0.9618\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 00053: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 0.1930 - val_acc: 0.9595\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00054: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.1992 - val_acc: 0.9616\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1924 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9984\n",
      "Epoch 00056: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0069 - acc: 0.9984 - val_loss: 0.1770 - val_acc: 0.9653\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00057: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.5108 - val_acc: 0.9073\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9903\n",
      "Epoch 00058: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.2405 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9914\n",
      "Epoch 00059: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0279 - acc: 0.9914 - val_loss: 0.2292 - val_acc: 0.9520\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00060: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0089 - acc: 0.9976 - val_loss: 0.2001 - val_acc: 0.9569\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9948\n",
      "Epoch 00061: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0185 - acc: 0.9948 - val_loss: 0.1840 - val_acc: 0.9581\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9974\n",
      "Epoch 00062: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0093 - acc: 0.9974 - val_loss: 0.2387 - val_acc: 0.9529\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9986\n",
      "Epoch 00063: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.1918 - val_acc: 0.9611\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 00064: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.2001 - val_acc: 0.9597\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 00065: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.1798 - val_acc: 0.9632\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9987\n",
      "Epoch 00066: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1999 - val_acc: 0.9618\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9990\n",
      "Epoch 00067: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.1823 - val_acc: 0.9639\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
      "Epoch 00068: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.1896 - val_acc: 0.9630\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00069: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.1975 - val_acc: 0.9625\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9990\n",
      "Epoch 00070: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0036 - acc: 0.9990 - val_loss: 0.2329 - val_acc: 0.9595\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9983\n",
      "Epoch 00071: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0065 - acc: 0.9983 - val_loss: 0.1879 - val_acc: 0.9634\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00072: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.2014 - val_acc: 0.9632\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 0.1775 - val_acc: 0.9651\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00074: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 167s 5ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1753 - val_acc: 0.9648\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9995\n",
      "Epoch 00075: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.2469 - val_acc: 0.9560\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00076: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2010 - val_acc: 0.9623\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00077: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1857 - val_acc: 0.9644\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00078: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.2052 - val_acc: 0.9618\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00079: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0029 - acc: 0.9993 - val_loss: 0.2337 - val_acc: 0.9541\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9968\n",
      "Epoch 00080: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0113 - acc: 0.9968 - val_loss: 0.1949 - val_acc: 0.9618\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00081: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0036 - acc: 0.9993 - val_loss: 0.1841 - val_acc: 0.9618\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 00082: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.2002 - val_acc: 0.9613\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9993\n",
      "Epoch 00083: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0029 - acc: 0.9993 - val_loss: 0.2213 - val_acc: 0.9602\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00084: val_loss did not improve from 0.16760\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1986 - val_acc: 0.9616\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1878 - acc: 0.9506\n",
      "Loss: 0.18781093235822977 Accuracy: 0.9505711\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.6928\n",
      "Epoch 00001: val_loss improved from inf to 0.60705, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/001-0.6070.hdf5\n",
      "36805/36805 [==============================] - 178s 5ms/sample - loss: 0.9971 - acc: 0.6928 - val_loss: 0.6070 - val_acc: 0.8183\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8808\n",
      "Epoch 00002: val_loss did not improve from 0.60705\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.3828 - acc: 0.8808 - val_loss: 1.7125 - val_acc: 0.6513\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9122\n",
      "Epoch 00003: val_loss improved from 0.60705 to 0.39889, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/003-0.3989.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.2774 - acc: 0.9122 - val_loss: 0.3989 - val_acc: 0.8884\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9333\n",
      "Epoch 00004: val_loss improved from 0.39889 to 0.16827, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/004-0.1683.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.2115 - acc: 0.9333 - val_loss: 0.1683 - val_acc: 0.9511\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9442\n",
      "Epoch 00005: val_loss did not improve from 0.16827\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1762 - acc: 0.9442 - val_loss: 0.1802 - val_acc: 0.9425\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9528\n",
      "Epoch 00006: val_loss improved from 0.16827 to 0.13779, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/006-0.1378.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1480 - acc: 0.9528 - val_loss: 0.1378 - val_acc: 0.9630\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9571\n",
      "Epoch 00007: val_loss did not improve from 0.13779\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1314 - acc: 0.9571 - val_loss: 0.1552 - val_acc: 0.9555\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9621\n",
      "Epoch 00008: val_loss did not improve from 0.13779\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1173 - acc: 0.9621 - val_loss: 0.2113 - val_acc: 0.9390\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9668\n",
      "Epoch 00009: val_loss did not improve from 0.13779\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.1021 - acc: 0.9667 - val_loss: 0.2731 - val_acc: 0.9276\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9706\n",
      "Epoch 00010: val_loss improved from 0.13779 to 0.13073, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/010-0.1307.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0911 - acc: 0.9706 - val_loss: 0.1307 - val_acc: 0.9574\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9740\n",
      "Epoch 00011: val_loss did not improve from 0.13073\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0801 - acc: 0.9740 - val_loss: 1.7532 - val_acc: 0.7058\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9679\n",
      "Epoch 00012: val_loss did not improve from 0.13073\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1026 - acc: 0.9679 - val_loss: 0.1320 - val_acc: 0.9644\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9739\n",
      "Epoch 00013: val_loss improved from 0.13073 to 0.12864, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/013-0.1286.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0776 - acc: 0.9739 - val_loss: 0.1286 - val_acc: 0.9627\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9791\n",
      "Epoch 00014: val_loss improved from 0.12864 to 0.12655, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/014-0.1266.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0649 - acc: 0.9791 - val_loss: 0.1266 - val_acc: 0.9630\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9776\n",
      "Epoch 00015: val_loss improved from 0.12655 to 0.11682, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/015-0.1168.hdf5\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0679 - acc: 0.9776 - val_loss: 0.1168 - val_acc: 0.9653\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9833\n",
      "Epoch 00016: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0513 - acc: 0.9833 - val_loss: 0.4163 - val_acc: 0.8933\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9770\n",
      "Epoch 00017: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0693 - acc: 0.9770 - val_loss: 0.1296 - val_acc: 0.9620\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9806\n",
      "Epoch 00018: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0586 - acc: 0.9806 - val_loss: 0.1442 - val_acc: 0.9592\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9874\n",
      "Epoch 00019: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0389 - acc: 0.9873 - val_loss: 0.1789 - val_acc: 0.9525\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9834\n",
      "Epoch 00020: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0515 - acc: 0.9834 - val_loss: 0.1270 - val_acc: 0.9662\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9901\n",
      "Epoch 00021: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0336 - acc: 0.9901 - val_loss: 0.1396 - val_acc: 0.9604\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9879\n",
      "Epoch 00022: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0371 - acc: 0.9879 - val_loss: 0.1297 - val_acc: 0.9655\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 00023: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0277 - acc: 0.9914 - val_loss: 0.1380 - val_acc: 0.9625\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9908\n",
      "Epoch 00024: val_loss did not improve from 0.11682\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0308 - acc: 0.9908 - val_loss: 0.1277 - val_acc: 0.9683\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9928\n",
      "Epoch 00025: val_loss improved from 0.11682 to 0.11088, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/025-0.1109.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0254 - acc: 0.9928 - val_loss: 0.1109 - val_acc: 0.9690\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9930\n",
      "Epoch 00026: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0235 - acc: 0.9930 - val_loss: 0.1603 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9933\n",
      "Epoch 00027: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0235 - acc: 0.9932 - val_loss: 0.4301 - val_acc: 0.9189\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9859\n",
      "Epoch 00028: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0431 - acc: 0.9859 - val_loss: 0.1386 - val_acc: 0.9653\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9916\n",
      "Epoch 00029: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0281 - acc: 0.9916 - val_loss: 0.1337 - val_acc: 0.9679\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9942\n",
      "Epoch 00030: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0198 - acc: 0.9942 - val_loss: 0.1117 - val_acc: 0.9711\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9937\n",
      "Epoch 00031: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.1608 - val_acc: 0.9639\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9924\n",
      "Epoch 00032: val_loss did not improve from 0.11088\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0247 - acc: 0.9924 - val_loss: 0.1800 - val_acc: 0.9604\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9923\n",
      "Epoch 00033: val_loss improved from 0.11088 to 0.11027, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/033-0.1103.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0254 - acc: 0.9923 - val_loss: 0.1103 - val_acc: 0.9693\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9955\n",
      "Epoch 00034: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0159 - acc: 0.9955 - val_loss: 0.1764 - val_acc: 0.9583\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9941\n",
      "Epoch 00035: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0207 - acc: 0.9941 - val_loss: 0.1162 - val_acc: 0.9725\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9965\n",
      "Epoch 00036: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0127 - acc: 0.9965 - val_loss: 0.1243 - val_acc: 0.9702\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 00037: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0098 - acc: 0.9973 - val_loss: 0.1877 - val_acc: 0.9597\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9914\n",
      "Epoch 00038: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0282 - acc: 0.9914 - val_loss: 0.2192 - val_acc: 0.9495\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9916\n",
      "Epoch 00039: val_loss did not improve from 0.11027\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0265 - acc: 0.9916 - val_loss: 0.1166 - val_acc: 0.9697\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 00040: val_loss improved from 0.11027 to 0.09888, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv_checkpoint/040-0.0989.hdf5\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0989 - val_acc: 0.9753\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9974\n",
      "Epoch 00041: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0100 - acc: 0.9974 - val_loss: 0.1216 - val_acc: 0.9727\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9978\n",
      "Epoch 00042: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0078 - acc: 0.9978 - val_loss: 0.1322 - val_acc: 0.9706\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9986\n",
      "Epoch 00043: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.1316 - val_acc: 0.9697\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9963\n",
      "Epoch 00044: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0124 - acc: 0.9963 - val_loss: 0.1322 - val_acc: 0.9713\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9983\n",
      "Epoch 00045: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0068 - acc: 0.9983 - val_loss: 0.1370 - val_acc: 0.9704\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9952\n",
      "Epoch 00046: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0163 - acc: 0.9952 - val_loss: 0.1081 - val_acc: 0.9734\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 00047: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.1376 - val_acc: 0.9706\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00048: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.1255 - val_acc: 0.9716\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00049: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0066 - acc: 0.9985 - val_loss: 0.1144 - val_acc: 0.9753\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 00050: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1388 - val_acc: 0.9709\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00051: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.1377 - val_acc: 0.9709\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 00052: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0100 - acc: 0.9973 - val_loss: 0.1121 - val_acc: 0.9748\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00053: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1200 - val_acc: 0.9746\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00054: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1114 - val_acc: 0.9758\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 00055: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 1.2197 - val_acc: 0.8393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9883\n",
      "Epoch 00056: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0374 - acc: 0.9883 - val_loss: 0.1272 - val_acc: 0.9725\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0116 - acc: 0.9964 - val_loss: 0.1843 - val_acc: 0.9653\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 00058: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0151 - acc: 0.9954 - val_loss: 0.1491 - val_acc: 0.9641\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0126 - acc: 0.9964 - val_loss: 0.1196 - val_acc: 0.9734\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9985\n",
      "Epoch 00060: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 0.1071 - val_acc: 0.9751\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9986\n",
      "Epoch 00061: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.1201 - val_acc: 0.9737\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00062: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0044 - acc: 0.9989 - val_loss: 0.1341 - val_acc: 0.9693\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00063: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0038 - acc: 0.9992 - val_loss: 0.1323 - val_acc: 0.9690\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00064: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0058 - acc: 0.9985 - val_loss: 0.1214 - val_acc: 0.9720\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00065: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.1242 - val_acc: 0.9748\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 00066: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0054 - acc: 0.9985 - val_loss: 0.1748 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9948\n",
      "Epoch 00067: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0166 - acc: 0.9948 - val_loss: 0.1750 - val_acc: 0.9599\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9946\n",
      "Epoch 00068: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0178 - acc: 0.9946 - val_loss: 0.1296 - val_acc: 0.9720\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00069: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.1530 - val_acc: 0.9667\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00070: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.3429 - val_acc: 0.9401\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9938\n",
      "Epoch 00071: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0219 - acc: 0.9938 - val_loss: 0.1175 - val_acc: 0.9734\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 00072: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0061 - acc: 0.9985 - val_loss: 0.1618 - val_acc: 0.9634\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9966\n",
      "Epoch 00073: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0120 - acc: 0.9966 - val_loss: 0.1186 - val_acc: 0.9737\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00074: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.1482 - val_acc: 0.9655\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9970\n",
      "Epoch 00075: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0112 - acc: 0.9970 - val_loss: 0.1708 - val_acc: 0.9669\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9933\n",
      "Epoch 00076: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0225 - acc: 0.9933 - val_loss: 0.1298 - val_acc: 0.9693\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9985\n",
      "Epoch 00077: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0062 - acc: 0.9985 - val_loss: 0.1916 - val_acc: 0.9604\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9945\n",
      "Epoch 00078: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0180 - acc: 0.9945 - val_loss: 0.1428 - val_acc: 0.9702\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00079: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0106 - acc: 0.9968 - val_loss: 0.1322 - val_acc: 0.9690\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 00080: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0195 - acc: 0.9942 - val_loss: 0.1467 - val_acc: 0.9667\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 00081: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0156 - acc: 0.9955 - val_loss: 0.1313 - val_acc: 0.9718\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 00082: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0057 - acc: 0.9988 - val_loss: 0.1265 - val_acc: 0.9725\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00083: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1314 - val_acc: 0.9730\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
      "Epoch 00084: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 168s 5ms/sample - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1230 - val_acc: 0.9751\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00085: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1192 - val_acc: 0.9746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00086: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1247 - val_acc: 0.9739\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986\n",
      "Epoch 00087: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 169s 5ms/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.1234 - val_acc: 0.9741\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992\n",
      "Epoch 00088: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1492 - val_acc: 0.9713\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9973\n",
      "Epoch 00089: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0101 - acc: 0.9973 - val_loss: 0.1467 - val_acc: 0.9702\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9974\n",
      "Epoch 00090: val_loss did not improve from 0.09888\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0093 - acc: 0.9974 - val_loss: 0.1438 - val_acc: 0.9700\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1449 - acc: 0.9664\n",
      "Loss: 0.14492283910435555 Accuracy: 0.96635514\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7277\n",
      "Epoch 00001: val_loss improved from inf to 0.51616, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/001-0.5162.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 0.8881 - acc: 0.7277 - val_loss: 0.5162 - val_acc: 0.8367\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9010\n",
      "Epoch 00002: val_loss did not improve from 0.51616\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.3180 - acc: 0.9009 - val_loss: 0.5763 - val_acc: 0.8227\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9275\n",
      "Epoch 00003: val_loss improved from 0.51616 to 0.19759, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/003-0.1976.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.2295 - acc: 0.9275 - val_loss: 0.1976 - val_acc: 0.9359\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9465\n",
      "Epoch 00004: val_loss improved from 0.19759 to 0.16065, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/004-0.1607.hdf5\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.1668 - acc: 0.9466 - val_loss: 0.1607 - val_acc: 0.9504\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9575\n",
      "Epoch 00005: val_loss improved from 0.16065 to 0.11476, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/005-0.1148.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.1333 - acc: 0.9575 - val_loss: 0.1148 - val_acc: 0.9655\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9632\n",
      "Epoch 00006: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.1109 - acc: 0.9632 - val_loss: 0.1628 - val_acc: 0.9525\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9691\n",
      "Epoch 00007: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0967 - acc: 0.9691 - val_loss: 0.2185 - val_acc: 0.9371\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9670\n",
      "Epoch 00008: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.1001 - acc: 0.9669 - val_loss: 0.3545 - val_acc: 0.9255\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9720\n",
      "Epoch 00009: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0881 - acc: 0.9720 - val_loss: 0.1662 - val_acc: 0.9488\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9790\n",
      "Epoch 00010: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0649 - acc: 0.9790 - val_loss: 0.1930 - val_acc: 0.9427\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9810\n",
      "Epoch 00011: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0593 - acc: 0.9809 - val_loss: 0.1931 - val_acc: 0.9492\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9796\n",
      "Epoch 00012: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0615 - acc: 0.9796 - val_loss: 0.1281 - val_acc: 0.9665\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9859\n",
      "Epoch 00013: val_loss did not improve from 0.11476\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0426 - acc: 0.9859 - val_loss: 0.1188 - val_acc: 0.9672\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9875\n",
      "Epoch 00014: val_loss improved from 0.11476 to 0.11049, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/014-0.1105.hdf5\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0398 - acc: 0.9875 - val_loss: 0.1105 - val_acc: 0.9704\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9905\n",
      "Epoch 00015: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0306 - acc: 0.9905 - val_loss: 0.2589 - val_acc: 0.9383\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9861\n",
      "Epoch 00016: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0438 - acc: 0.9861 - val_loss: 0.5291 - val_acc: 0.8940\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9796\n",
      "Epoch 00017: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0649 - acc: 0.9796 - val_loss: 0.1618 - val_acc: 0.9578\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9832\n",
      "Epoch 00018: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0520 - acc: 0.9832 - val_loss: 0.1118 - val_acc: 0.9695\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9921\n",
      "Epoch 00019: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0264 - acc: 0.9921 - val_loss: 0.2179 - val_acc: 0.9532\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9882\n",
      "Epoch 00020: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0380 - acc: 0.9882 - val_loss: 0.1179 - val_acc: 0.9693\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9929\n",
      "Epoch 00021: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0236 - acc: 0.9929 - val_loss: 0.1112 - val_acc: 0.9713\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9947\n",
      "Epoch 00022: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0196 - acc: 0.9946 - val_loss: 0.1956 - val_acc: 0.9555\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9892\n",
      "Epoch 00023: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0343 - acc: 0.9892 - val_loss: 0.5612 - val_acc: 0.8931\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9883\n",
      "Epoch 00024: val_loss did not improve from 0.11049\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0367 - acc: 0.9883 - val_loss: 0.1671 - val_acc: 0.9595\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9890\n",
      "Epoch 00025: val_loss improved from 0.11049 to 0.10371, saving model to model/checkpoint/1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv_checkpoint/025-0.1037.hdf5\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0358 - acc: 0.9891 - val_loss: 0.1037 - val_acc: 0.9741\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9959\n",
      "Epoch 00026: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0158 - acc: 0.9959 - val_loss: 0.1041 - val_acc: 0.9734\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9974\n",
      "Epoch 00027: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0104 - acc: 0.9974 - val_loss: 0.1144 - val_acc: 0.9734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9978\n",
      "Epoch 00028: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0096 - acc: 0.9978 - val_loss: 0.2023 - val_acc: 0.9618\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9917\n",
      "Epoch 00029: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0290 - acc: 0.9917 - val_loss: 0.1092 - val_acc: 0.9725\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9973\n",
      "Epoch 00030: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0104 - acc: 0.9973 - val_loss: 0.1149 - val_acc: 0.9727\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9975\n",
      "Epoch 00031: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0097 - acc: 0.9975 - val_loss: 0.1412 - val_acc: 0.9702\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9965\n",
      "Epoch 00032: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1190 - val_acc: 0.9734\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9985\n",
      "Epoch 00033: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.1117 - val_acc: 0.9746\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9981\n",
      "Epoch 00034: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0068 - acc: 0.9980 - val_loss: 0.1816 - val_acc: 0.9604\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9924\n",
      "Epoch 00035: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 177s 5ms/sample - loss: 0.0264 - acc: 0.9924 - val_loss: 0.1999 - val_acc: 0.9550\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9942\n",
      "Epoch 00036: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.3394 - val_acc: 0.9352\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9923\n",
      "Epoch 00037: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0279 - acc: 0.9923 - val_loss: 0.1059 - val_acc: 0.9744\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9973\n",
      "Epoch 00038: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0107 - acc: 0.9973 - val_loss: 0.2097 - val_acc: 0.9578\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9916\n",
      "Epoch 00039: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0277 - acc: 0.9916 - val_loss: 0.1186 - val_acc: 0.9716\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 00040: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0134 - acc: 0.9955 - val_loss: 0.1152 - val_acc: 0.9744\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9966\n",
      "Epoch 00041: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0119 - acc: 0.9966 - val_loss: 0.1061 - val_acc: 0.9769\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
      "Epoch 00042: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0068 - acc: 0.9982 - val_loss: 0.1255 - val_acc: 0.9753\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00043: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.5730 - val_acc: 0.8998\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9921\n",
      "Epoch 00044: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0273 - acc: 0.9921 - val_loss: 0.1480 - val_acc: 0.9660\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9976\n",
      "Epoch 00045: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0087 - acc: 0.9976 - val_loss: 0.1236 - val_acc: 0.9716\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00046: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.1868 - val_acc: 0.9583\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 00047: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0194 - acc: 0.9942 - val_loss: 0.1278 - val_acc: 0.9706\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9981\n",
      "Epoch 00048: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0067 - acc: 0.9981 - val_loss: 0.1232 - val_acc: 0.9725\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 00049: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 175s 5ms/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 0.1586 - val_acc: 0.9660\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9959\n",
      "Epoch 00050: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 176s 5ms/sample - loss: 0.0159 - acc: 0.9959 - val_loss: 0.1198 - val_acc: 0.9748\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 00051: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 174s 5ms/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.1131 - val_acc: 0.9746\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9992\n",
      "Epoch 00052: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0035 - acc: 0.9992 - val_loss: 0.1209 - val_acc: 0.9751\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00053: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1173 - val_acc: 0.9767\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00054: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1167 - val_acc: 0.9762\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00055: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1153 - val_acc: 0.9767\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00056: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 170s 5ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1285 - val_acc: 0.9776\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00057: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1219 - val_acc: 0.9753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00058: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1146 - val_acc: 0.9760\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00059: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1236 - val_acc: 0.9732\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9977\n",
      "Epoch 00060: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0083 - acc: 0.9977 - val_loss: 0.1238 - val_acc: 0.9765\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00061: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1338 - val_acc: 0.9751\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00062: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0027 - acc: 0.9992 - val_loss: 0.3195 - val_acc: 0.9448\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 00063: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0162 - acc: 0.9955 - val_loss: 0.1253 - val_acc: 0.9746\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 00064: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0048 - acc: 0.9987 - val_loss: 0.1386 - val_acc: 0.9713\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 00065: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.1229 - val_acc: 0.9739\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993\n",
      "Epoch 00066: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0025 - acc: 0.9993 - val_loss: 0.1207 - val_acc: 0.9762\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00067: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 173s 5ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.1228 - val_acc: 0.9772\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00068: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.1748 - val_acc: 0.9683\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0125 - acc: 0.9963 - val_loss: 0.1496 - val_acc: 0.9709\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00070: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1344 - val_acc: 0.9723\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00071: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1246 - val_acc: 0.9753\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9993\n",
      "Epoch 00072: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1232 - val_acc: 0.9758\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00073: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1215 - val_acc: 0.9762\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00074: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 171s 5ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1094 - val_acc: 0.9786\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 00075: val_loss did not improve from 0.10371\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.1249 - val_acc: 0.9760\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1419 - acc: 0.9618\n",
      "Loss: 0.14189552459569632 Accuracy: 0.9617861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_128_DO_025_DO_BN_SGD'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_128_DO_BN(conv_num=i)\n",
    "        \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_42_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_42_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_42_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16)           3805712     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Concatenate)           (None, 16)           0           sequential_7[1][0]               \n",
      "                                                                 sequential_7[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 13.6140 - acc: 0.1553\n",
      "Loss: 13.613969903274489 Accuracy: 0.15534787\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16)           1461392     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Concatenate)           (None, 16)           0           sequential_8[1][0]               \n",
      "                                                                 sequential_8[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.8486 - acc: 0.7362\n",
      "Loss: 0.8485809605193411 Accuracy: 0.7362409\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_49_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_49_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_49_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1221008     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.7109 - acc: 0.8093\n",
      "Loss: 0.7108803769138372 Accuracy: 0.8093458\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_54_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_54_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_54_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1009296     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3453 - acc: 0.9061\n",
      "Loss: 0.34529382100605394 Accuracy: 0.9061267\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_60_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_60_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_60_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1158032     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1878 - acc: 0.9506\n",
      "Loss: 0.18781093235822977 Accuracy: 0.9505711\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_67_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_67_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_67_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1429648     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1449 - acc: 0.9664\n",
      "Loss: 0.14492283910435555 Accuracy: 0.96635514\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_75_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 16000, 1)     0           conv1d_75_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 16000, 1)     0           conv1d_75_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           2075280     lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1419 - acc: 0.9618\n",
      "Loss: 0.14189552459569632 Accuracy: 0.9617861\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_128_DO_025_DO_BN_SGD'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_42_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           conv1d_42_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           conv1d_42_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16)           3805712     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Concatenate)           (None, 16)           0           sequential_7[1][0]               \n",
      "                                                                 sequential_7[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,805,712\n",
      "Trainable params: 3,804,944\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 13.6551 - acc: 0.1524\n",
      "Loss: 13.655118088014152 Accuracy: 0.1524403\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_45_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           conv1d_45_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16)           1461392     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Concatenate)           (None, 16)           0           sequential_8[1][0]               \n",
      "                                                                 sequential_8[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,461,392\n",
      "Trainable params: 1,460,368\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.5772 - acc: 0.7587\n",
      "Loss: 1.5771726283205136 Accuracy: 0.7586708\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_49_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           conv1d_49_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           conv1d_49_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16)           1221008     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Concatenate)           (None, 16)           0           sequential_9[1][0]               \n",
      "                                                                 sequential_9[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,221,008\n",
      "Trainable params: 1,219,472\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.9329 - acc: 0.8347\n",
      "Loss: 0.9328936758071091 Accuracy: 0.8346833\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_54_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           conv1d_54_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           conv1d_54_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 16)           1009296     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 16)           0           sequential_10[1][0]              \n",
      "                                                                 sequential_10[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,009,296\n",
      "Trainable params: 1,007,248\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.4260 - acc: 0.9161\n",
      "Loss: 0.4260353553057614 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_60_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           conv1d_60_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           conv1d_60_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 16)           1158032     lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           sequential_11[1][0]              \n",
      "                                                                 sequential_11[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,158,032\n",
      "Trainable params: 1,155,472\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.2093 - acc: 0.9570\n",
      "Loss: 0.20933819176693982 Accuracy: 0.9570094\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_67_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 16000, 1)     0           conv1d_67_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 16000, 1)     0           conv1d_67_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 16)           1429648     lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Concatenate)          (None, 16)           0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,429,648\n",
      "Trainable params: 1,426,576\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1526 - acc: 0.9655\n",
      "Loss: 0.15257831057079083 Accuracy: 0.9655244\n",
      "\n",
      "1D_CNN_custom_ch_128_DO_025_DO_BN_SGD_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_75_input (InputLayer)    (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 16000, 1)     0           conv1d_75_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 16000, 1)     0           conv1d_75_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 16)           2075280     lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           sequential_13[1][0]              \n",
      "                                                                 sequential_13[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,075,280\n",
      "Trainable params: 2,071,184\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1497 - acc: 0.9680\n",
      "Loss: 0.14973689708711244 Accuracy: 0.9680166\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
