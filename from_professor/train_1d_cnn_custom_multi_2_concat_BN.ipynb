{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 151616)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 151616)       606464      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2425872     batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 3,074,576\n",
      "Trainable params: 2,770,960\n",
      "Non-trainable params: 303,616\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50496)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 50496)        201984      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           807952      batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,072,976\n",
      "Trainable params: 971,472\n",
      "Non-trainable params: 101,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20928)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 20928)        83712       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           334864      batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 523,216\n",
      "Trainable params: 480,592\n",
      "Non-trainable params: 42,624\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11008)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 11008)        44032       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           176144      batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 407,376\n",
      "Trainable params: 384,336\n",
      "Non-trainable params: 23,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3584)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 3584)         14336       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           57360       batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 341,456\n",
      "Trainable params: 333,008\n",
      "Non-trainable params: 8,448\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1152)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1152)         4608        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           18448       batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 375,376\n",
      "Trainable params: 371,536\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5795 - acc: 0.4361\n",
      "Epoch 00001: val_loss improved from inf to 2.24707, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_3_conv_checkpoint/001-2.2471.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.5795 - acc: 0.4361 - val_loss: 2.2471 - val_acc: 0.4400\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1725 - acc: 0.7113\n",
      "Epoch 00002: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.1730 - acc: 0.7113 - val_loss: 2.4336 - val_acc: 0.5141\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.8270\n",
      "Epoch 00003: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6659 - acc: 0.8270 - val_loss: 2.4155 - val_acc: 0.5367\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8872\n",
      "Epoch 00004: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4365 - acc: 0.8872 - val_loss: 3.0308 - val_acc: 0.4864\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9226\n",
      "Epoch 00005: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3089 - acc: 0.9226 - val_loss: 2.4932 - val_acc: 0.5462\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9382\n",
      "Epoch 00006: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2561 - acc: 0.9381 - val_loss: 2.4958 - val_acc: 0.5809\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9399\n",
      "Epoch 00007: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2500 - acc: 0.9399 - val_loss: 2.8641 - val_acc: 0.5565\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9512\n",
      "Epoch 00008: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2062 - acc: 0.9512 - val_loss: 2.7681 - val_acc: 0.5639\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9490\n",
      "Epoch 00009: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2140 - acc: 0.9490 - val_loss: 3.4719 - val_acc: 0.5041\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9532\n",
      "Epoch 00010: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2001 - acc: 0.9531 - val_loss: 3.1005 - val_acc: 0.5516\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9518\n",
      "Epoch 00011: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2138 - acc: 0.9518 - val_loss: 3.0983 - val_acc: 0.5721\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9628\n",
      "Epoch 00012: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1653 - acc: 0.9628 - val_loss: 3.4392 - val_acc: 0.5448\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9654\n",
      "Epoch 00013: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1587 - acc: 0.9654 - val_loss: 3.2943 - val_acc: 0.5646\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9691\n",
      "Epoch 00014: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1428 - acc: 0.9691 - val_loss: 3.1123 - val_acc: 0.5945\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9699\n",
      "Epoch 00015: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1479 - acc: 0.9698 - val_loss: 3.8552 - val_acc: 0.5379\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9648\n",
      "Epoch 00016: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1645 - acc: 0.9648 - val_loss: 3.5574 - val_acc: 0.5653\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9752\n",
      "Epoch 00017: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1188 - acc: 0.9751 - val_loss: 3.5878 - val_acc: 0.5740\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9698\n",
      "Epoch 00018: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1507 - acc: 0.9697 - val_loss: 4.0079 - val_acc: 0.5276\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9718\n",
      "Epoch 00019: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1401 - acc: 0.9719 - val_loss: 4.4253 - val_acc: 0.5262\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9793\n",
      "Epoch 00020: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1092 - acc: 0.9792 - val_loss: 3.5538 - val_acc: 0.5840\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9707\n",
      "Epoch 00021: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1534 - acc: 0.9707 - val_loss: 3.6672 - val_acc: 0.5756\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9743\n",
      "Epoch 00022: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1258 - acc: 0.9743 - val_loss: 3.9659 - val_acc: 0.5597\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9766\n",
      "Epoch 00023: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1208 - acc: 0.9766 - val_loss: 4.0561 - val_acc: 0.5549\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9809\n",
      "Epoch 00024: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1073 - acc: 0.9808 - val_loss: 3.7177 - val_acc: 0.5816\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9775\n",
      "Epoch 00025: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1302 - acc: 0.9775 - val_loss: 3.7757 - val_acc: 0.5891\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9809\n",
      "Epoch 00026: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1058 - acc: 0.9809 - val_loss: 3.5567 - val_acc: 0.6014\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9803\n",
      "Epoch 00027: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1081 - acc: 0.9803 - val_loss: 3.6520 - val_acc: 0.6108\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9822\n",
      "Epoch 00028: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0968 - acc: 0.9822 - val_loss: 4.0520 - val_acc: 0.5686\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9810\n",
      "Epoch 00029: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1072 - acc: 0.9810 - val_loss: 4.1278 - val_acc: 0.5693\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9823\n",
      "Epoch 00030: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1007 - acc: 0.9823 - val_loss: 4.0033 - val_acc: 0.5793\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9798\n",
      "Epoch 00031: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1138 - acc: 0.9798 - val_loss: 3.9142 - val_acc: 0.5912\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9829\n",
      "Epoch 00032: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0983 - acc: 0.9829 - val_loss: 3.8038 - val_acc: 0.5966\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9797\n",
      "Epoch 00033: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1138 - acc: 0.9797 - val_loss: 3.8166 - val_acc: 0.5975\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9867\n",
      "Epoch 00034: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0835 - acc: 0.9867 - val_loss: 4.0982 - val_acc: 0.5784\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9839\n",
      "Epoch 00035: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1009 - acc: 0.9839 - val_loss: 4.0467 - val_acc: 0.5823\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9789\n",
      "Epoch 00036: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1301 - acc: 0.9789 - val_loss: 4.1510 - val_acc: 0.5870\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9847\n",
      "Epoch 00037: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0928 - acc: 0.9846 - val_loss: 3.9920 - val_acc: 0.5966\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9859\n",
      "Epoch 00038: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0874 - acc: 0.9858 - val_loss: 4.2965 - val_acc: 0.5821\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9811\n",
      "Epoch 00039: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1087 - acc: 0.9811 - val_loss: 4.5089 - val_acc: 0.5684\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9876\n",
      "Epoch 00040: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0812 - acc: 0.9875 - val_loss: 4.2992 - val_acc: 0.5691\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9882\n",
      "Epoch 00041: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0775 - acc: 0.9882 - val_loss: 3.9916 - val_acc: 0.6094\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9865\n",
      "Epoch 00042: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0875 - acc: 0.9864 - val_loss: 4.9475 - val_acc: 0.5360\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9833\n",
      "Epoch 00043: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1093 - acc: 0.9832 - val_loss: 4.3766 - val_acc: 0.5919\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9853\n",
      "Epoch 00044: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0955 - acc: 0.9853 - val_loss: 4.4715 - val_acc: 0.5823\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9872\n",
      "Epoch 00045: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0833 - acc: 0.9872 - val_loss: 4.2779 - val_acc: 0.5896\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9890\n",
      "Epoch 00046: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0706 - acc: 0.9890 - val_loss: 5.0160 - val_acc: 0.5313\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9894\n",
      "Epoch 00047: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0732 - acc: 0.9893 - val_loss: 4.4282 - val_acc: 0.5861\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9861\n",
      "Epoch 00048: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0892 - acc: 0.9861 - val_loss: 4.6879 - val_acc: 0.5642\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9881\n",
      "Epoch 00049: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0792 - acc: 0.9881 - val_loss: 4.4761 - val_acc: 0.5775\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9823\n",
      "Epoch 00050: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1130 - acc: 0.9823 - val_loss: 4.5887 - val_acc: 0.5805\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9903\n",
      "Epoch 00051: val_loss did not improve from 2.24707\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0720 - acc: 0.9903 - val_loss: 4.0243 - val_acc: 0.6082\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 608us/sample - loss: 2.2996 - acc: 0.4289\n",
      "Loss: 2.2996431459890347 Accuracy: 0.42886811\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8067 - acc: 0.4923\n",
      "Epoch 00001: val_loss improved from inf to 1.72821, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_4_conv_checkpoint/001-1.7282.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.8066 - acc: 0.4924 - val_loss: 1.7282 - val_acc: 0.4873\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0115 - acc: 0.7015\n",
      "Epoch 00002: val_loss improved from 1.72821 to 1.40755, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_4_conv_checkpoint/002-1.4075.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0115 - acc: 0.7015 - val_loss: 1.4075 - val_acc: 0.6110\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6557 - acc: 0.8027\n",
      "Epoch 00003: val_loss improved from 1.40755 to 1.30774, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_4_conv_checkpoint/003-1.3077.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6558 - acc: 0.8027 - val_loss: 1.3077 - val_acc: 0.6483\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8721\n",
      "Epoch 00004: val_loss improved from 1.30774 to 1.24245, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_4_conv_checkpoint/004-1.2424.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4417 - acc: 0.8721 - val_loss: 1.2424 - val_acc: 0.6678\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9189\n",
      "Epoch 00005: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3002 - acc: 0.9189 - val_loss: 1.2628 - val_acc: 0.6727\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9444\n",
      "Epoch 00006: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2188 - acc: 0.9444 - val_loss: 1.2589 - val_acc: 0.6734\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9623\n",
      "Epoch 00007: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1653 - acc: 0.9623 - val_loss: 1.4157 - val_acc: 0.6480\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9690\n",
      "Epoch 00008: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1403 - acc: 0.9690 - val_loss: 1.4244 - val_acc: 0.6690\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9761\n",
      "Epoch 00009: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1194 - acc: 0.9761 - val_loss: 1.3774 - val_acc: 0.6839\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9754\n",
      "Epoch 00010: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1117 - acc: 0.9753 - val_loss: 1.4894 - val_acc: 0.6769\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9799\n",
      "Epoch 00011: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0981 - acc: 0.9799 - val_loss: 1.4731 - val_acc: 0.6811\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9815\n",
      "Epoch 00012: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0889 - acc: 0.9815 - val_loss: 1.5712 - val_acc: 0.6646\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9850\n",
      "Epoch 00013: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0782 - acc: 0.9849 - val_loss: 1.6783 - val_acc: 0.6527\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9838\n",
      "Epoch 00014: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0763 - acc: 0.9838 - val_loss: 1.8403 - val_acc: 0.6252\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9885\n",
      "Epoch 00015: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0612 - acc: 0.9884 - val_loss: 1.6109 - val_acc: 0.6771\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9841\n",
      "Epoch 00016: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0722 - acc: 0.9841 - val_loss: 1.6525 - val_acc: 0.6657\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9895\n",
      "Epoch 00017: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0535 - acc: 0.9894 - val_loss: 1.6728 - val_acc: 0.6718\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9862\n",
      "Epoch 00018: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0610 - acc: 0.9862 - val_loss: 1.6403 - val_acc: 0.6818\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9921\n",
      "Epoch 00019: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0464 - acc: 0.9921 - val_loss: 1.7022 - val_acc: 0.6699\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9875\n",
      "Epoch 00020: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0547 - acc: 0.9875 - val_loss: 1.6709 - val_acc: 0.6771\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9900\n",
      "Epoch 00021: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0492 - acc: 0.9900 - val_loss: 1.8665 - val_acc: 0.6627\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9893\n",
      "Epoch 00022: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0519 - acc: 0.9893 - val_loss: 1.9241 - val_acc: 0.6576\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9867\n",
      "Epoch 00023: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0587 - acc: 0.9867 - val_loss: 1.8488 - val_acc: 0.6746\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9925\n",
      "Epoch 00024: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0392 - acc: 0.9925 - val_loss: 1.9485 - val_acc: 0.6667\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9907\n",
      "Epoch 00025: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0459 - acc: 0.9907 - val_loss: 2.4244 - val_acc: 0.6147\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9893\n",
      "Epoch 00026: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0528 - acc: 0.9892 - val_loss: 1.9112 - val_acc: 0.6683\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9887\n",
      "Epoch 00027: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0506 - acc: 0.9887 - val_loss: 1.9802 - val_acc: 0.6569\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9924\n",
      "Epoch 00028: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0376 - acc: 0.9924 - val_loss: 2.0217 - val_acc: 0.6601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9910\n",
      "Epoch 00029: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0443 - acc: 0.9910 - val_loss: 2.1334 - val_acc: 0.6497\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9915\n",
      "Epoch 00030: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0443 - acc: 0.9915 - val_loss: 2.0172 - val_acc: 0.6643\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9873\n",
      "Epoch 00031: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0533 - acc: 0.9873 - val_loss: 1.9394 - val_acc: 0.6774\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9922\n",
      "Epoch 00032: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0376 - acc: 0.9921 - val_loss: 1.9700 - val_acc: 0.6699\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9826\n",
      "Epoch 00033: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0750 - acc: 0.9826 - val_loss: 2.2089 - val_acc: 0.6525\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9909\n",
      "Epoch 00034: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0457 - acc: 0.9909 - val_loss: 2.1377 - val_acc: 0.6601\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9957\n",
      "Epoch 00035: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0269 - acc: 0.9956 - val_loss: 2.2064 - val_acc: 0.6511\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9933\n",
      "Epoch 00036: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0344 - acc: 0.9933 - val_loss: 2.1202 - val_acc: 0.6615\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9923\n",
      "Epoch 00037: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0385 - acc: 0.9923 - val_loss: 2.1750 - val_acc: 0.6618\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9960\n",
      "Epoch 00038: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0248 - acc: 0.9960 - val_loss: 2.0407 - val_acc: 0.6702\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9915\n",
      "Epoch 00039: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0407 - acc: 0.9915 - val_loss: 2.2280 - val_acc: 0.6527\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9921\n",
      "Epoch 00040: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0364 - acc: 0.9921 - val_loss: 2.1838 - val_acc: 0.6725\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9932\n",
      "Epoch 00041: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0351 - acc: 0.9932 - val_loss: 2.3191 - val_acc: 0.6499\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0338 - acc: 0.9936 - val_loss: 2.4644 - val_acc: 0.6355\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0393 - acc: 0.9918 - val_loss: 2.3408 - val_acc: 0.6532\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9944\n",
      "Epoch 00044: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0301 - acc: 0.9943 - val_loss: 2.1087 - val_acc: 0.6741\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9893\n",
      "Epoch 00045: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0488 - acc: 0.9893 - val_loss: 2.3257 - val_acc: 0.6522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9909\n",
      "Epoch 00046: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0442 - acc: 0.9908 - val_loss: 2.2675 - val_acc: 0.6723\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9913\n",
      "Epoch 00047: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0407 - acc: 0.9913 - val_loss: 2.1880 - val_acc: 0.6839\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9933\n",
      "Epoch 00048: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0351 - acc: 0.9933 - val_loss: 2.1541 - val_acc: 0.6816\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9936\n",
      "Epoch 00049: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0330 - acc: 0.9936 - val_loss: 2.1977 - val_acc: 0.6730\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9915\n",
      "Epoch 00050: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0412 - acc: 0.9915 - val_loss: 2.1709 - val_acc: 0.6811\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9974\n",
      "Epoch 00051: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0205 - acc: 0.9974 - val_loss: 2.3382 - val_acc: 0.6699\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9966\n",
      "Epoch 00052: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0219 - acc: 0.9966 - val_loss: 2.3524 - val_acc: 0.6608\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9903\n",
      "Epoch 00053: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0458 - acc: 0.9902 - val_loss: 2.5175 - val_acc: 0.6525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9909\n",
      "Epoch 00054: val_loss did not improve from 1.24245\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0424 - acc: 0.9909 - val_loss: 2.3938 - val_acc: 0.6683\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 628us/sample - loss: 1.3310 - acc: 0.6368\n",
      "Loss: 1.3310384488427627 Accuracy: 0.6367601\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6366 - acc: 0.5101\n",
      "Epoch 00001: val_loss improved from inf to 1.52557, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/001-1.5256.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.6370 - acc: 0.5100 - val_loss: 1.5256 - val_acc: 0.5399\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0289 - acc: 0.6939\n",
      "Epoch 00002: val_loss improved from 1.52557 to 1.14110, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/002-1.1411.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 1.0290 - acc: 0.6938 - val_loss: 1.1411 - val_acc: 0.6758\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8055 - acc: 0.7617\n",
      "Epoch 00003: val_loss improved from 1.14110 to 0.90797, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/003-0.9080.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.8055 - acc: 0.7617 - val_loss: 0.9080 - val_acc: 0.7501\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6573 - acc: 0.8074\n",
      "Epoch 00004: val_loss did not improve from 0.90797\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.6574 - acc: 0.8074 - val_loss: 0.9891 - val_acc: 0.7228\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8399\n",
      "Epoch 00005: val_loss improved from 0.90797 to 0.86471, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/005-0.8647.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.5373 - acc: 0.8399 - val_loss: 0.8647 - val_acc: 0.7626\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8717\n",
      "Epoch 00006: val_loss improved from 0.86471 to 0.85229, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/006-0.8523.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.4404 - acc: 0.8716 - val_loss: 0.8523 - val_acc: 0.7652\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8908\n",
      "Epoch 00007: val_loss improved from 0.85229 to 0.80709, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/007-0.8071.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.3807 - acc: 0.8907 - val_loss: 0.8071 - val_acc: 0.7750\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9136\n",
      "Epoch 00008: val_loss did not improve from 0.80709\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.3078 - acc: 0.9136 - val_loss: 0.9183 - val_acc: 0.7503\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9306\n",
      "Epoch 00009: val_loss did not improve from 0.80709\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2603 - acc: 0.9306 - val_loss: 0.8218 - val_acc: 0.7822\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9421\n",
      "Epoch 00010: val_loss improved from 0.80709 to 0.78887, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/010-0.7889.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.2164 - acc: 0.9421 - val_loss: 0.7889 - val_acc: 0.8006\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9559\n",
      "Epoch 00011: val_loss did not improve from 0.78887\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1777 - acc: 0.9558 - val_loss: 0.8241 - val_acc: 0.7838\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9605\n",
      "Epoch 00012: val_loss did not improve from 0.78887\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1584 - acc: 0.9605 - val_loss: 0.8234 - val_acc: 0.7862\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9689\n",
      "Epoch 00013: val_loss did not improve from 0.78887\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1343 - acc: 0.9688 - val_loss: 0.9735 - val_acc: 0.7631\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9660\n",
      "Epoch 00014: val_loss did not improve from 0.78887\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.1439 - acc: 0.9660 - val_loss: 0.8395 - val_acc: 0.7943\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9765\n",
      "Epoch 00015: val_loss did not improve from 0.78887\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1062 - acc: 0.9764 - val_loss: 0.7964 - val_acc: 0.8085\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9793\n",
      "Epoch 00016: val_loss improved from 0.78887 to 0.78738, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/016-0.7874.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0965 - acc: 0.9793 - val_loss: 0.7874 - val_acc: 0.8113\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9853\n",
      "Epoch 00017: val_loss did not improve from 0.78738\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0749 - acc: 0.9852 - val_loss: 0.8152 - val_acc: 0.7980\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9801\n",
      "Epoch 00018: val_loss did not improve from 0.78738\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0911 - acc: 0.9801 - val_loss: 0.8240 - val_acc: 0.7987\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9835\n",
      "Epoch 00019: val_loss did not improve from 0.78738\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0783 - acc: 0.9835 - val_loss: 0.8480 - val_acc: 0.7999\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9889\n",
      "Epoch 00020: val_loss improved from 0.78738 to 0.74435, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_5_conv_checkpoint/020-0.7443.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0636 - acc: 0.9888 - val_loss: 0.7443 - val_acc: 0.8230\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9849\n",
      "Epoch 00021: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0729 - acc: 0.9848 - val_loss: 0.9041 - val_acc: 0.7941\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9851\n",
      "Epoch 00022: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0695 - acc: 0.9851 - val_loss: 0.8603 - val_acc: 0.7927\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9899\n",
      "Epoch 00023: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0536 - acc: 0.9899 - val_loss: 0.8947 - val_acc: 0.7883\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9902\n",
      "Epoch 00024: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0515 - acc: 0.9902 - val_loss: 0.8970 - val_acc: 0.7945\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9898\n",
      "Epoch 00025: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0528 - acc: 0.9898 - val_loss: 0.8587 - val_acc: 0.8053\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9912\n",
      "Epoch 00026: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0457 - acc: 0.9913 - val_loss: 0.8714 - val_acc: 0.8055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9937\n",
      "Epoch 00027: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0389 - acc: 0.9938 - val_loss: 0.8902 - val_acc: 0.8099\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9926\n",
      "Epoch 00028: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0402 - acc: 0.9926 - val_loss: 0.8722 - val_acc: 0.8092\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9912\n",
      "Epoch 00029: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0445 - acc: 0.9911 - val_loss: 1.2669 - val_acc: 0.7391\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9851\n",
      "Epoch 00030: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0596 - acc: 0.9850 - val_loss: 0.8658 - val_acc: 0.8097\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9903\n",
      "Epoch 00031: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0456 - acc: 0.9903 - val_loss: 0.8917 - val_acc: 0.8118\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9892\n",
      "Epoch 00032: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0474 - acc: 0.9892 - val_loss: 0.9734 - val_acc: 0.7929\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9928\n",
      "Epoch 00033: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0371 - acc: 0.9927 - val_loss: 0.9608 - val_acc: 0.8015\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9937\n",
      "Epoch 00034: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0330 - acc: 0.9937 - val_loss: 0.9010 - val_acc: 0.8088\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9955\n",
      "Epoch 00035: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0281 - acc: 0.9955 - val_loss: 0.8309 - val_acc: 0.8227\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9942\n",
      "Epoch 00036: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0317 - acc: 0.9942 - val_loss: 0.9759 - val_acc: 0.7983\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9912\n",
      "Epoch 00037: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0403 - acc: 0.9911 - val_loss: 1.0478 - val_acc: 0.7843\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9889\n",
      "Epoch 00038: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0469 - acc: 0.9889 - val_loss: 0.9271 - val_acc: 0.8125\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9942\n",
      "Epoch 00039: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0299 - acc: 0.9942 - val_loss: 0.9690 - val_acc: 0.7980\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9913\n",
      "Epoch 00040: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0388 - acc: 0.9913 - val_loss: 0.9576 - val_acc: 0.8039\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9945\n",
      "Epoch 00041: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0290 - acc: 0.9945 - val_loss: 1.0183 - val_acc: 0.7952\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0337 - acc: 0.9936 - val_loss: 0.9031 - val_acc: 0.8204\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9878\n",
      "Epoch 00043: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0503 - acc: 0.9877 - val_loss: 0.9205 - val_acc: 0.8157\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9927\n",
      "Epoch 00044: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0356 - acc: 0.9926 - val_loss: 0.8696 - val_acc: 0.8181\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9962\n",
      "Epoch 00045: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0237 - acc: 0.9961 - val_loss: 0.9375 - val_acc: 0.8090\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9933\n",
      "Epoch 00046: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0318 - acc: 0.9933 - val_loss: 0.9790 - val_acc: 0.8090\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9955\n",
      "Epoch 00047: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0236 - acc: 0.9954 - val_loss: 0.9429 - val_acc: 0.8195\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9938\n",
      "Epoch 00048: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0291 - acc: 0.9938 - val_loss: 0.9347 - val_acc: 0.8162\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9902\n",
      "Epoch 00049: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0416 - acc: 0.9902 - val_loss: 0.9768 - val_acc: 0.8055\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9969\n",
      "Epoch 00050: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0209 - acc: 0.9968 - val_loss: 0.9974 - val_acc: 0.8090\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0264 - acc: 0.9946 - val_loss: 0.9426 - val_acc: 0.8139\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9923\n",
      "Epoch 00052: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0329 - acc: 0.9923 - val_loss: 0.9767 - val_acc: 0.8139\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9939\n",
      "Epoch 00053: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0287 - acc: 0.9939 - val_loss: 1.1402 - val_acc: 0.7957\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9921\n",
      "Epoch 00054: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0350 - acc: 0.9921 - val_loss: 0.9380 - val_acc: 0.8227\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9937\n",
      "Epoch 00055: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0289 - acc: 0.9937 - val_loss: 1.1418 - val_acc: 0.7878\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9931\n",
      "Epoch 00056: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0308 - acc: 0.9931 - val_loss: 0.9473 - val_acc: 0.8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9970\n",
      "Epoch 00057: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0189 - acc: 0.9970 - val_loss: 1.0064 - val_acc: 0.8118\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0253 - acc: 0.9947 - val_loss: 1.0830 - val_acc: 0.8043\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9894\n",
      "Epoch 00059: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0454 - acc: 0.9894 - val_loss: 0.9037 - val_acc: 0.8237\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9972\n",
      "Epoch 00060: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0190 - acc: 0.9971 - val_loss: 1.1328 - val_acc: 0.7859\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0260 - acc: 0.9946 - val_loss: 0.9865 - val_acc: 0.8178\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9938\n",
      "Epoch 00062: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0275 - acc: 0.9938 - val_loss: 0.9814 - val_acc: 0.8192\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9974\n",
      "Epoch 00063: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0157 - acc: 0.9974 - val_loss: 0.9020 - val_acc: 0.8288\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9974\n",
      "Epoch 00064: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0172 - acc: 0.9974 - val_loss: 0.9862 - val_acc: 0.8185\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9936\n",
      "Epoch 00065: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0276 - acc: 0.9936 - val_loss: 1.0102 - val_acc: 0.8181\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9927\n",
      "Epoch 00066: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0323 - acc: 0.9927 - val_loss: 1.0610 - val_acc: 0.8062\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9937\n",
      "Epoch 00067: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0273 - acc: 0.9937 - val_loss: 1.0082 - val_acc: 0.8123\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9954\n",
      "Epoch 00068: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0221 - acc: 0.9954 - val_loss: 0.9882 - val_acc: 0.8099\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9971\n",
      "Epoch 00069: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0175 - acc: 0.9971 - val_loss: 0.9984 - val_acc: 0.8143\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9965\n",
      "Epoch 00070: val_loss did not improve from 0.74435\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0193 - acc: 0.9965 - val_loss: 1.0967 - val_acc: 0.7948\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 646us/sample - loss: 0.8024 - acc: 0.7994\n",
      "Loss: 0.8024348169719938 Accuracy: 0.79937696\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6082 - acc: 0.5063\n",
      "Epoch 00001: val_loss improved from inf to 1.28036, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/001-1.2804.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 1.6081 - acc: 0.5063 - val_loss: 1.2804 - val_acc: 0.5926\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9375 - acc: 0.7218\n",
      "Epoch 00002: val_loss improved from 1.28036 to 0.88036, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/002-0.8804.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.9378 - acc: 0.7217 - val_loss: 0.8804 - val_acc: 0.7338\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7083 - acc: 0.7936\n",
      "Epoch 00003: val_loss improved from 0.88036 to 0.77724, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/003-0.7772.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.7083 - acc: 0.7936 - val_loss: 0.7772 - val_acc: 0.7713\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8384\n",
      "Epoch 00004: val_loss improved from 0.77724 to 0.61322, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/004-0.6132.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5599 - acc: 0.8384 - val_loss: 0.6132 - val_acc: 0.8281\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8668\n",
      "Epoch 00005: val_loss improved from 0.61322 to 0.55656, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/005-0.5566.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4611 - acc: 0.8668 - val_loss: 0.5566 - val_acc: 0.8463\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8886\n",
      "Epoch 00006: val_loss did not improve from 0.55656\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3854 - acc: 0.8886 - val_loss: 0.5679 - val_acc: 0.8355\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.9021\n",
      "Epoch 00007: val_loss did not improve from 0.55656\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3341 - acc: 0.9021 - val_loss: 0.5623 - val_acc: 0.8355\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9179\n",
      "Epoch 00008: val_loss improved from 0.55656 to 0.48958, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/008-0.4896.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2832 - acc: 0.9178 - val_loss: 0.4896 - val_acc: 0.8626\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9301\n",
      "Epoch 00009: val_loss improved from 0.48958 to 0.42854, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/009-0.4285.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2459 - acc: 0.9300 - val_loss: 0.4285 - val_acc: 0.8828\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9348\n",
      "Epoch 00010: val_loss did not improve from 0.42854\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2229 - acc: 0.9348 - val_loss: 0.4294 - val_acc: 0.8889\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9496\n",
      "Epoch 00011: val_loss did not improve from 0.42854\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1822 - acc: 0.9496 - val_loss: 0.5341 - val_acc: 0.8488\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9548\n",
      "Epoch 00012: val_loss improved from 0.42854 to 0.39068, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/012-0.3907.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1611 - acc: 0.9547 - val_loss: 0.3907 - val_acc: 0.8977\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9634\n",
      "Epoch 00013: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1388 - acc: 0.9634 - val_loss: 0.4056 - val_acc: 0.8835\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9685\n",
      "Epoch 00014: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1190 - acc: 0.9685 - val_loss: 0.5411 - val_acc: 0.8537\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9694\n",
      "Epoch 00015: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1164 - acc: 0.9694 - val_loss: 0.3934 - val_acc: 0.8915\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9726\n",
      "Epoch 00016: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1052 - acc: 0.9726 - val_loss: 0.4336 - val_acc: 0.8852\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9757\n",
      "Epoch 00017: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0961 - acc: 0.9757 - val_loss: 0.3945 - val_acc: 0.8996\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9820\n",
      "Epoch 00018: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0795 - acc: 0.9819 - val_loss: 0.4576 - val_acc: 0.8796\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9816\n",
      "Epoch 00019: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0779 - acc: 0.9816 - val_loss: 0.5141 - val_acc: 0.8656\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9856\n",
      "Epoch 00020: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0637 - acc: 0.9855 - val_loss: 0.4443 - val_acc: 0.8861\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9874\n",
      "Epoch 00021: val_loss did not improve from 0.39068\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0583 - acc: 0.9874 - val_loss: 0.4114 - val_acc: 0.8924\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9863\n",
      "Epoch 00022: val_loss improved from 0.39068 to 0.38021, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/022-0.3802.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0597 - acc: 0.9863 - val_loss: 0.3802 - val_acc: 0.9012\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9877\n",
      "Epoch 00023: val_loss improved from 0.38021 to 0.36326, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_6_conv_checkpoint/023-0.3633.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0545 - acc: 0.9876 - val_loss: 0.3633 - val_acc: 0.9045\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9913\n",
      "Epoch 00024: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0451 - acc: 0.9913 - val_loss: 0.3852 - val_acc: 0.8987\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9875\n",
      "Epoch 00025: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0537 - acc: 0.9875 - val_loss: 0.3969 - val_acc: 0.9022\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9941\n",
      "Epoch 00026: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0342 - acc: 0.9941 - val_loss: 0.4268 - val_acc: 0.8861\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9909\n",
      "Epoch 00027: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0437 - acc: 0.9909 - val_loss: 0.5383 - val_acc: 0.8623\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9934\n",
      "Epoch 00028: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0356 - acc: 0.9934 - val_loss: 0.4326 - val_acc: 0.8938\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9919\n",
      "Epoch 00029: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0378 - acc: 0.9919 - val_loss: 0.4505 - val_acc: 0.8891\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0344 - acc: 0.9932 - val_loss: 0.4083 - val_acc: 0.8980\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9909\n",
      "Epoch 00031: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0403 - acc: 0.9909 - val_loss: 0.3892 - val_acc: 0.9040\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9957\n",
      "Epoch 00032: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9957 - val_loss: 0.3756 - val_acc: 0.9038\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9942\n",
      "Epoch 00033: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0288 - acc: 0.9942 - val_loss: 0.4078 - val_acc: 0.8987\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9910\n",
      "Epoch 00034: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0401 - acc: 0.9909 - val_loss: 0.4515 - val_acc: 0.8919\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9911\n",
      "Epoch 00035: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0374 - acc: 0.9911 - val_loss: 0.3914 - val_acc: 0.9043\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9982\n",
      "Epoch 00036: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0156 - acc: 0.9982 - val_loss: 0.4618 - val_acc: 0.8940\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9958\n",
      "Epoch 00037: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0226 - acc: 0.9958 - val_loss: 0.4707 - val_acc: 0.8875\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9933\n",
      "Epoch 00038: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0312 - acc: 0.9933 - val_loss: 0.4070 - val_acc: 0.9010\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9966\n",
      "Epoch 00039: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0187 - acc: 0.9966 - val_loss: 0.4249 - val_acc: 0.9010\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9919\n",
      "Epoch 00040: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0328 - acc: 0.9919 - val_loss: 0.4059 - val_acc: 0.9050\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9971\n",
      "Epoch 00041: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0166 - acc: 0.9971 - val_loss: 0.4216 - val_acc: 0.8970\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9967\n",
      "Epoch 00042: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0180 - acc: 0.9967 - val_loss: 0.4971 - val_acc: 0.8814\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9936\n",
      "Epoch 00043: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0275 - acc: 0.9936 - val_loss: 0.4158 - val_acc: 0.9047\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9967\n",
      "Epoch 00044: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0177 - acc: 0.9967 - val_loss: 0.3968 - val_acc: 0.9092\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9972\n",
      "Epoch 00045: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0164 - acc: 0.9972 - val_loss: 0.4220 - val_acc: 0.9087\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9955\n",
      "Epoch 00046: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0210 - acc: 0.9954 - val_loss: 0.4231 - val_acc: 0.9061\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9931\n",
      "Epoch 00047: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0291 - acc: 0.9931 - val_loss: 0.4234 - val_acc: 0.9080\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9983\n",
      "Epoch 00048: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0132 - acc: 0.9982 - val_loss: 0.4830 - val_acc: 0.8959\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9940\n",
      "Epoch 00049: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0244 - acc: 0.9940 - val_loss: 0.4433 - val_acc: 0.8991\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0226 - acc: 0.9945 - val_loss: 0.3829 - val_acc: 0.9152\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9982\n",
      "Epoch 00051: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0121 - acc: 0.9982 - val_loss: 0.4026 - val_acc: 0.9073\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9919\n",
      "Epoch 00052: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0318 - acc: 0.9918 - val_loss: 0.4055 - val_acc: 0.9101\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9956\n",
      "Epoch 00053: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0182 - acc: 0.9956 - val_loss: 0.4080 - val_acc: 0.9101\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9949\n",
      "Epoch 00054: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0205 - acc: 0.9948 - val_loss: 0.4015 - val_acc: 0.9133\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9949\n",
      "Epoch 00055: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0230 - acc: 0.9949 - val_loss: 0.3898 - val_acc: 0.9157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9982\n",
      "Epoch 00056: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0114 - acc: 0.9982 - val_loss: 0.4188 - val_acc: 0.9078\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9973\n",
      "Epoch 00057: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0136 - acc: 0.9973 - val_loss: 0.4191 - val_acc: 0.9136\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9950\n",
      "Epoch 00058: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0205 - acc: 0.9950 - val_loss: 0.4128 - val_acc: 0.9061\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9943\n",
      "Epoch 00059: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0237 - acc: 0.9943 - val_loss: 0.4415 - val_acc: 0.9078\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9967\n",
      "Epoch 00060: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0158 - acc: 0.9967 - val_loss: 0.3987 - val_acc: 0.9140\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9990\n",
      "Epoch 00061: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0081 - acc: 0.9990 - val_loss: 0.4182 - val_acc: 0.9138\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9907\n",
      "Epoch 00062: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0349 - acc: 0.9906 - val_loss: 0.4394 - val_acc: 0.9080\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9968\n",
      "Epoch 00063: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0151 - acc: 0.9968 - val_loss: 0.4080 - val_acc: 0.9092\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0143 - acc: 0.9969 - val_loss: 0.3963 - val_acc: 0.9180\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9937\n",
      "Epoch 00065: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0250 - acc: 0.9938 - val_loss: 0.3905 - val_acc: 0.9157\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9990\n",
      "Epoch 00066: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0080 - acc: 0.9989 - val_loss: 0.4194 - val_acc: 0.9150\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9944\n",
      "Epoch 00067: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0206 - acc: 0.9944 - val_loss: 0.4028 - val_acc: 0.9157\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9989\n",
      "Epoch 00068: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0085 - acc: 0.9989 - val_loss: 0.4263 - val_acc: 0.9071\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9995\n",
      "Epoch 00069: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0065 - acc: 0.9995 - val_loss: 0.4180 - val_acc: 0.9140\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9955\n",
      "Epoch 00070: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0181 - acc: 0.9955 - val_loss: 0.4339 - val_acc: 0.9068\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9973\n",
      "Epoch 00071: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0130 - acc: 0.9972 - val_loss: 0.5320 - val_acc: 0.8889\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9945\n",
      "Epoch 00072: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0205 - acc: 0.9945 - val_loss: 0.4311 - val_acc: 0.9096\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9964\n",
      "Epoch 00073: val_loss did not improve from 0.36326\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0158 - acc: 0.9964 - val_loss: 0.4605 - val_acc: 0.9064\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 686us/sample - loss: 0.4072 - acc: 0.8889\n",
      "Loss: 0.40719286103114904 Accuracy: 0.8888889\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5850 - acc: 0.5192\n",
      "Epoch 00001: val_loss improved from inf to 1.24229, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/001-1.2423.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.5849 - acc: 0.5193 - val_loss: 1.2423 - val_acc: 0.6145\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7901 - acc: 0.7698\n",
      "Epoch 00002: val_loss improved from 1.24229 to 0.65981, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/002-0.6598.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.7905 - acc: 0.7697 - val_loss: 0.6598 - val_acc: 0.8004\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8462\n",
      "Epoch 00003: val_loss improved from 0.65981 to 0.47559, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/003-0.4756.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5377 - acc: 0.8461 - val_loss: 0.4756 - val_acc: 0.8640\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8831\n",
      "Epoch 00004: val_loss improved from 0.47559 to 0.40360, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/004-0.4036.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4087 - acc: 0.8831 - val_loss: 0.4036 - val_acc: 0.8891\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.9060\n",
      "Epoch 00005: val_loss improved from 0.40360 to 0.34321, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/005-0.3432.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3280 - acc: 0.9059 - val_loss: 0.3432 - val_acc: 0.8982\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9222\n",
      "Epoch 00006: val_loss did not improve from 0.34321\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2708 - acc: 0.9222 - val_loss: 0.4259 - val_acc: 0.8696\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9357\n",
      "Epoch 00007: val_loss improved from 0.34321 to 0.30954, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/007-0.3095.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2275 - acc: 0.9357 - val_loss: 0.3095 - val_acc: 0.9082\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9444\n",
      "Epoch 00008: val_loss improved from 0.30954 to 0.27443, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/008-0.2744.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1963 - acc: 0.9444 - val_loss: 0.2744 - val_acc: 0.9185\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9509\n",
      "Epoch 00009: val_loss improved from 0.27443 to 0.25541, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/009-0.2554.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1709 - acc: 0.9508 - val_loss: 0.2554 - val_acc: 0.9252\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9564\n",
      "Epoch 00010: val_loss did not improve from 0.25541\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1540 - acc: 0.9564 - val_loss: 0.2794 - val_acc: 0.9161\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9619\n",
      "Epoch 00011: val_loss improved from 0.25541 to 0.22663, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/011-0.2266.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1344 - acc: 0.9619 - val_loss: 0.2266 - val_acc: 0.9352\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9671\n",
      "Epoch 00012: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1178 - acc: 0.9670 - val_loss: 0.2474 - val_acc: 0.9280\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9707\n",
      "Epoch 00013: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1039 - acc: 0.9707 - val_loss: 0.2467 - val_acc: 0.9269\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9767\n",
      "Epoch 00014: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0880 - acc: 0.9767 - val_loss: 0.2531 - val_acc: 0.9248\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9796\n",
      "Epoch 00015: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0784 - acc: 0.9796 - val_loss: 0.2622 - val_acc: 0.9210\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9822\n",
      "Epoch 00016: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0683 - acc: 0.9822 - val_loss: 0.2338 - val_acc: 0.9334\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9860\n",
      "Epoch 00017: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0587 - acc: 0.9860 - val_loss: 0.2466 - val_acc: 0.9308\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9801\n",
      "Epoch 00018: val_loss did not improve from 0.22663\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0725 - acc: 0.9801 - val_loss: 0.2291 - val_acc: 0.9348\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9895\n",
      "Epoch 00019: val_loss improved from 0.22663 to 0.22224, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/019-0.2222.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0454 - acc: 0.9895 - val_loss: 0.2222 - val_acc: 0.9371\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9827\n",
      "Epoch 00020: val_loss improved from 0.22224 to 0.21420, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/020-0.2142.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0659 - acc: 0.9827 - val_loss: 0.2142 - val_acc: 0.9369\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9916\n",
      "Epoch 00021: val_loss improved from 0.21420 to 0.21347, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/021-0.2135.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0388 - acc: 0.9916 - val_loss: 0.2135 - val_acc: 0.9383\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9933\n",
      "Epoch 00022: val_loss did not improve from 0.21347\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0334 - acc: 0.9933 - val_loss: 0.3941 - val_acc: 0.8891\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9927\n",
      "Epoch 00023: val_loss did not improve from 0.21347\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0336 - acc: 0.9927 - val_loss: 0.2884 - val_acc: 0.9255\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9892\n",
      "Epoch 00024: val_loss improved from 0.21347 to 0.21068, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/024-0.2107.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0442 - acc: 0.9892 - val_loss: 0.2107 - val_acc: 0.9364\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9943\n",
      "Epoch 00025: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0272 - acc: 0.9943 - val_loss: 0.2298 - val_acc: 0.9394\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9932\n",
      "Epoch 00026: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0298 - acc: 0.9932 - val_loss: 0.2171 - val_acc: 0.9380\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9956\n",
      "Epoch 00027: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0225 - acc: 0.9955 - val_loss: 0.2398 - val_acc: 0.9371\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9921\n",
      "Epoch 00028: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0324 - acc: 0.9921 - val_loss: 0.2326 - val_acc: 0.9359\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9962\n",
      "Epoch 00029: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0193 - acc: 0.9963 - val_loss: 0.2526 - val_acc: 0.9315\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9961\n",
      "Epoch 00030: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0202 - acc: 0.9961 - val_loss: 0.2812 - val_acc: 0.9243\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9918\n",
      "Epoch 00031: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0312 - acc: 0.9918 - val_loss: 0.2298 - val_acc: 0.9366\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9979\n",
      "Epoch 00032: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0136 - acc: 0.9979 - val_loss: 0.2241 - val_acc: 0.9439\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9970\n",
      "Epoch 00033: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0162 - acc: 0.9969 - val_loss: 0.2335 - val_acc: 0.9373\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9938\n",
      "Epoch 00034: val_loss did not improve from 0.21068\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0256 - acc: 0.9938 - val_loss: 0.2314 - val_acc: 0.9422\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9978\n",
      "Epoch 00035: val_loss improved from 0.21068 to 0.20968, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/035-0.2097.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0130 - acc: 0.9978 - val_loss: 0.2097 - val_acc: 0.9420\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9940\n",
      "Epoch 00036: val_loss did not improve from 0.20968\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0239 - acc: 0.9940 - val_loss: 0.2707 - val_acc: 0.9320\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9954\n",
      "Epoch 00037: val_loss did not improve from 0.20968\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0200 - acc: 0.9954 - val_loss: 0.2743 - val_acc: 0.9352\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9979\n",
      "Epoch 00038: val_loss did not improve from 0.20968\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0118 - acc: 0.9978 - val_loss: 0.2361 - val_acc: 0.9359\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9938\n",
      "Epoch 00039: val_loss did not improve from 0.20968\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0242 - acc: 0.9937 - val_loss: 0.2620 - val_acc: 0.9371\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9945\n",
      "Epoch 00040: val_loss improved from 0.20968 to 0.20807, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/040-0.2081.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0223 - acc: 0.9944 - val_loss: 0.2081 - val_acc: 0.9446\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9976\n",
      "Epoch 00041: val_loss improved from 0.20807 to 0.20115, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/041-0.2011.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0122 - acc: 0.9976 - val_loss: 0.2011 - val_acc: 0.9490\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9990\n",
      "Epoch 00042: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0081 - acc: 0.9990 - val_loss: 0.2492 - val_acc: 0.9392\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9935\n",
      "Epoch 00043: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0223 - acc: 0.9935 - val_loss: 0.2214 - val_acc: 0.9453\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9987\n",
      "Epoch 00044: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0082 - acc: 0.9987 - val_loss: 0.2159 - val_acc: 0.9453\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 00045: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0137 - acc: 0.9968 - val_loss: 0.3174 - val_acc: 0.9264\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9963\n",
      "Epoch 00046: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0151 - acc: 0.9963 - val_loss: 0.2133 - val_acc: 0.9488\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9991\n",
      "Epoch 00047: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0073 - acc: 0.9990 - val_loss: 0.2059 - val_acc: 0.9490\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9927\n",
      "Epoch 00048: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0269 - acc: 0.9927 - val_loss: 0.2094 - val_acc: 0.9513\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00049: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0069 - acc: 0.9990 - val_loss: 0.2342 - val_acc: 0.9457\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0199 - acc: 0.9945 - val_loss: 0.2117 - val_acc: 0.9455\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9986\n",
      "Epoch 00051: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0080 - acc: 0.9986 - val_loss: 0.2189 - val_acc: 0.9467\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9983\n",
      "Epoch 00052: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0091 - acc: 0.9983 - val_loss: 0.2153 - val_acc: 0.9515\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9980\n",
      "Epoch 00053: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0100 - acc: 0.9980 - val_loss: 0.2567 - val_acc: 0.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9975\n",
      "Epoch 00054: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0111 - acc: 0.9975 - val_loss: 0.2421 - val_acc: 0.9392\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9986\n",
      "Epoch 00055: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.2356 - val_acc: 0.9453\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9982\n",
      "Epoch 00056: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0089 - acc: 0.9982 - val_loss: 0.2316 - val_acc: 0.9446\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0106 - acc: 0.9973 - val_loss: 0.2880 - val_acc: 0.9331\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9949- ETA: 0s - loss: 0.0182 - acc: 0.99\n",
      "Epoch 00058: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0182 - acc: 0.9949 - val_loss: 0.2273 - val_acc: 0.9471\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00059: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.2352 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9991\n",
      "Epoch 00060: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0066 - acc: 0.9990 - val_loss: 0.2655 - val_acc: 0.9366\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9932\n",
      "Epoch 00061: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0244 - acc: 0.9932 - val_loss: 0.2328 - val_acc: 0.9439\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9997\n",
      "Epoch 00062: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0038 - acc: 0.9997 - val_loss: 0.2054 - val_acc: 0.9495\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 00063: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0028 - acc: 0.9998 - val_loss: 0.2322 - val_acc: 0.9446\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0047 - acc: 0.9993 - val_loss: 0.2777 - val_acc: 0.9401\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9976\n",
      "Epoch 00065: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0098 - acc: 0.9976 - val_loss: 0.2448 - val_acc: 0.9476\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9946\n",
      "Epoch 00066: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0196 - acc: 0.9946 - val_loss: 0.2484 - val_acc: 0.9411\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9974\n",
      "Epoch 00067: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0104 - acc: 0.9974 - val_loss: 0.2079 - val_acc: 0.9497\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9983\n",
      "Epoch 00068: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0075 - acc: 0.9983 - val_loss: 0.2314 - val_acc: 0.9455\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9962\n",
      "Epoch 00069: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0147 - acc: 0.9962 - val_loss: 0.2262 - val_acc: 0.9511\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00070: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0038 - acc: 0.9996 - val_loss: 0.2121 - val_acc: 0.9520\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 00071: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.2242 - val_acc: 0.9455\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9970\n",
      "Epoch 00072: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0121 - acc: 0.9969 - val_loss: 0.2222 - val_acc: 0.9467\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9954\n",
      "Epoch 00073: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0163 - acc: 0.9954 - val_loss: 0.2105 - val_acc: 0.9485\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 00074: val_loss did not improve from 0.20115\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0033 - acc: 0.9996 - val_loss: 0.2089 - val_acc: 0.9497\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 00075: val_loss improved from 0.20115 to 0.19738, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_7_conv_checkpoint/075-0.1974.hdf5\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0035 - acc: 0.9996 - val_loss: 0.1974 - val_acc: 0.9520\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0088 - acc: 0.9977 - val_loss: 0.2074 - val_acc: 0.9502\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 00077: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0069 - acc: 0.9983 - val_loss: 0.2589 - val_acc: 0.9390\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9981\n",
      "Epoch 00078: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0075 - acc: 0.9981 - val_loss: 0.2545 - val_acc: 0.9455\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9939\n",
      "Epoch 00079: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0203 - acc: 0.9939 - val_loss: 0.2174 - val_acc: 0.9520\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0094 - acc: 0.9975 - val_loss: 0.2139 - val_acc: 0.9515\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00081: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2105 - val_acc: 0.9548\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9990\n",
      "Epoch 00082: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0050 - acc: 0.9990 - val_loss: 0.2280 - val_acc: 0.9462\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 00083: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0145 - acc: 0.9961 - val_loss: 0.2802 - val_acc: 0.9371\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9944\n",
      "Epoch 00084: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0193 - acc: 0.9943 - val_loss: 0.2063 - val_acc: 0.9541\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9973\n",
      "Epoch 00085: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0116 - acc: 0.9973 - val_loss: 0.2090 - val_acc: 0.9548\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 00086: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0038 - acc: 0.9994 - val_loss: 0.2034 - val_acc: 0.9513\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9976\n",
      "Epoch 00087: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0101 - acc: 0.9975 - val_loss: 0.2215 - val_acc: 0.9509\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n",
      "Epoch 00088: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0059 - acc: 0.9985 - val_loss: 0.2403 - val_acc: 0.9467\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00089: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.2435 - val_acc: 0.9522\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00090: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.2306 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9981\n",
      "Epoch 00091: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0078 - acc: 0.9981 - val_loss: 0.3045 - val_acc: 0.9371\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
      "Epoch 00092: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0058 - acc: 0.9988 - val_loss: 0.2102 - val_acc: 0.9557\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9989\n",
      "Epoch 00093: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0049 - acc: 0.9989 - val_loss: 0.3857 - val_acc: 0.9131\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9951\n",
      "Epoch 00094: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0174 - acc: 0.9951 - val_loss: 0.2163 - val_acc: 0.9522\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00095: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0033 - acc: 0.9994 - val_loss: 0.2161 - val_acc: 0.9499\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00096: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0038 - acc: 0.9992 - val_loss: 0.2517 - val_acc: 0.9462\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00097: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2966 - val_acc: 0.9364\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9953\n",
      "Epoch 00098: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0172 - acc: 0.9953 - val_loss: 0.1986 - val_acc: 0.9527\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 00099: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0044 - acc: 0.9990 - val_loss: 0.2205 - val_acc: 0.9499\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9981\n",
      "Epoch 00100: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0076 - acc: 0.9981 - val_loss: 0.2214 - val_acc: 0.9497\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9984\n",
      "Epoch 00101: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0062 - acc: 0.9984 - val_loss: 0.2025 - val_acc: 0.9543\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00102: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.2212 - val_acc: 0.9481\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 00103: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.2643 - val_acc: 0.9441\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9969\n",
      "Epoch 00104: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.2255 - val_acc: 0.9527\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00105: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.2128 - val_acc: 0.9555\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991\n",
      "Epoch 00106: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.2405 - val_acc: 0.9490\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00107: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0041 - acc: 0.9991 - val_loss: 0.2799 - val_acc: 0.9443\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9960\n",
      "Epoch 00108: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0130 - acc: 0.9960 - val_loss: 0.2378 - val_acc: 0.9455\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9995\n",
      "Epoch 00109: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 0.2437 - val_acc: 0.9485\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 00110: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0018 - acc: 0.9997 - val_loss: 0.2202 - val_acc: 0.9509\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9982\n",
      "Epoch 00111: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2264 - val_acc: 0.9520\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 00112: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.2344 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9951\n",
      "Epoch 00113: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 59s 2ms/sample - loss: 0.0172 - acc: 0.9951 - val_loss: 0.2188 - val_acc: 0.9534\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00114: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.2339 - val_acc: 0.9525\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00115: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.2197 - val_acc: 0.9539\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00116: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.2160 - val_acc: 0.9511\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00117: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.2081 - val_acc: 0.9569\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9972\n",
      "Epoch 00118: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.2014 - val_acc: 0.9522\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00119: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2177 - val_acc: 0.9529\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00120: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.2570 - val_acc: 0.9502\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9965\n",
      "Epoch 00121: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.2507 - val_acc: 0.9439\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 00122: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0039 - acc: 0.9990 - val_loss: 0.1997 - val_acc: 0.9578\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9990\n",
      "Epoch 00123: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.2263 - val_acc: 0.9513\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 00124: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2763 - val_acc: 0.9434\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00125: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0038 - acc: 0.9992 - val_loss: 0.2424 - val_acc: 0.9518\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 724us/sample - loss: 0.2588 - acc: 0.9389\n",
      "Loss: 0.25881171178455664 Accuracy: 0.9389408\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3659 - acc: 0.5829\n",
      "Epoch 00001: val_loss improved from inf to 0.98348, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/001-0.9835.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.3658 - acc: 0.5829 - val_loss: 0.9835 - val_acc: 0.7142\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8350\n",
      "Epoch 00002: val_loss improved from 0.98348 to 0.40348, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/002-0.4035.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5617 - acc: 0.8350 - val_loss: 0.4035 - val_acc: 0.8856\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8897\n",
      "Epoch 00003: val_loss improved from 0.40348 to 0.31269, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/003-0.3127.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3723 - acc: 0.8897 - val_loss: 0.3127 - val_acc: 0.9085\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9161\n",
      "Epoch 00004: val_loss improved from 0.31269 to 0.26516, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/004-0.2652.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2866 - acc: 0.9161 - val_loss: 0.2652 - val_acc: 0.9187\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9339\n",
      "Epoch 00005: val_loss did not improve from 0.26516\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2283 - acc: 0.9339 - val_loss: 0.2783 - val_acc: 0.9157\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9443\n",
      "Epoch 00006: val_loss improved from 0.26516 to 0.23310, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/006-0.2331.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1914 - acc: 0.9442 - val_loss: 0.2331 - val_acc: 0.9299\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9521\n",
      "Epoch 00007: val_loss did not improve from 0.23310\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1648 - acc: 0.9521 - val_loss: 0.2431 - val_acc: 0.9224\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9610\n",
      "Epoch 00008: val_loss improved from 0.23310 to 0.22597, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/008-0.2260.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1340 - acc: 0.9610 - val_loss: 0.2260 - val_acc: 0.9313\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9661\n",
      "Epoch 00009: val_loss improved from 0.22597 to 0.18261, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/009-0.1826.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1186 - acc: 0.9661 - val_loss: 0.1826 - val_acc: 0.9404\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9708\n",
      "Epoch 00010: val_loss did not improve from 0.18261\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1033 - acc: 0.9708 - val_loss: 0.2141 - val_acc: 0.9329\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9775\n",
      "Epoch 00011: val_loss did not improve from 0.18261\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0847 - acc: 0.9775 - val_loss: 0.2031 - val_acc: 0.9364\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9761\n",
      "Epoch 00012: val_loss did not improve from 0.18261\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0819 - acc: 0.9761 - val_loss: 0.1899 - val_acc: 0.9404\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9776\n",
      "Epoch 00013: val_loss improved from 0.18261 to 0.17944, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/013-0.1794.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0809 - acc: 0.9776 - val_loss: 0.1794 - val_acc: 0.9434\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9842\n",
      "Epoch 00014: val_loss did not improve from 0.17944\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0603 - acc: 0.9842 - val_loss: 0.2077 - val_acc: 0.9364\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9892\n",
      "Epoch 00015: val_loss improved from 0.17944 to 0.16415, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_BN_8_conv_checkpoint/015-0.1642.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0472 - acc: 0.9892 - val_loss: 0.1642 - val_acc: 0.9525\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9890\n",
      "Epoch 00016: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0444 - acc: 0.9890 - val_loss: 0.2122 - val_acc: 0.9373\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9918\n",
      "Epoch 00017: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0371 - acc: 0.9918 - val_loss: 0.1962 - val_acc: 0.9422\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9921\n",
      "Epoch 00018: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0350 - acc: 0.9921 - val_loss: 0.1822 - val_acc: 0.9457\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9888\n",
      "Epoch 00019: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0422 - acc: 0.9888 - val_loss: 0.2505 - val_acc: 0.9290\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9944\n",
      "Epoch 00020: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0261 - acc: 0.9944 - val_loss: 0.1959 - val_acc: 0.9467\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9929\n",
      "Epoch 00021: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0315 - acc: 0.9929 - val_loss: 0.1825 - val_acc: 0.9488\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9965\n",
      "Epoch 00022: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0186 - acc: 0.9965 - val_loss: 0.2020 - val_acc: 0.9481\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9942\n",
      "Epoch 00023: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0241 - acc: 0.9942 - val_loss: 0.1889 - val_acc: 0.9492\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9935\n",
      "Epoch 00024: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0269 - acc: 0.9935 - val_loss: 0.1675 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9968\n",
      "Epoch 00025: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0163 - acc: 0.9967 - val_loss: 0.1969 - val_acc: 0.9511\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9889\n",
      "Epoch 00026: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0400 - acc: 0.9889 - val_loss: 0.1756 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9966\n",
      "Epoch 00027: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0171 - acc: 0.9966 - val_loss: 0.1682 - val_acc: 0.9546\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9984\n",
      "Epoch 00028: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0104 - acc: 0.9984 - val_loss: 0.1857 - val_acc: 0.9539\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9976\n",
      "Epoch 00029: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0128 - acc: 0.9976 - val_loss: 0.2012 - val_acc: 0.9478\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9974\n",
      "Epoch 00030: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0140 - acc: 0.9974 - val_loss: 0.2243 - val_acc: 0.9406\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9929\n",
      "Epoch 00031: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 0.2089 - val_acc: 0.9483\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9946\n",
      "Epoch 00032: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0218 - acc: 0.9946 - val_loss: 0.1767 - val_acc: 0.9590\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9951\n",
      "Epoch 00033: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0201 - acc: 0.9951 - val_loss: 0.1668 - val_acc: 0.9599\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9991\n",
      "Epoch 00034: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0067 - acc: 0.9991 - val_loss: 0.1910 - val_acc: 0.9525\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9963\n",
      "Epoch 00035: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0143 - acc: 0.9963 - val_loss: 0.1826 - val_acc: 0.9529\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9961\n",
      "Epoch 00036: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0166 - acc: 0.9961 - val_loss: 0.1677 - val_acc: 0.9574\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9976\n",
      "Epoch 00037: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0114 - acc: 0.9976 - val_loss: 0.1903 - val_acc: 0.9492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9975\n",
      "Epoch 00038: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0107 - acc: 0.9975 - val_loss: 0.2012 - val_acc: 0.9518\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 00039: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0093 - acc: 0.9977 - val_loss: 0.2271 - val_acc: 0.9443\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00040: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0201 - acc: 0.9941 - val_loss: 0.1860 - val_acc: 0.9543\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9988\n",
      "Epoch 00041: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0063 - acc: 0.9988 - val_loss: 0.1850 - val_acc: 0.9574\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0215 - acc: 0.9938 - val_loss: 0.1795 - val_acc: 0.9539\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9989\n",
      "Epoch 00043: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0057 - acc: 0.9989 - val_loss: 0.1949 - val_acc: 0.9522\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9978\n",
      "Epoch 00044: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0100 - acc: 0.9978 - val_loss: 0.2223 - val_acc: 0.9499\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9967\n",
      "Epoch 00045: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0127 - acc: 0.9967 - val_loss: 0.1853 - val_acc: 0.9536\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9985\n",
      "Epoch 00046: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0070 - acc: 0.9985 - val_loss: 0.2054 - val_acc: 0.9539\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9959\n",
      "Epoch 00047: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0158 - acc: 0.9959 - val_loss: 0.2391 - val_acc: 0.9429\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 00048: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0057 - acc: 0.9987 - val_loss: 0.1744 - val_acc: 0.9592\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9986\n",
      "Epoch 00049: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.2028 - val_acc: 0.9520\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00050: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.2407 - val_acc: 0.9457\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9968\n",
      "Epoch 00051: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0127 - acc: 0.9968 - val_loss: 0.2383 - val_acc: 0.9450\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9956\n",
      "Epoch 00052: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0138 - acc: 0.9956 - val_loss: 0.1835 - val_acc: 0.9567\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9968\n",
      "Epoch 00053: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.1726 - val_acc: 0.9576\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9995\n",
      "Epoch 00054: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0034 - acc: 0.9995 - val_loss: 0.1892 - val_acc: 0.9578\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0156 - acc: 0.9955 - val_loss: 0.1887 - val_acc: 0.9562\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 00056: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1948 - val_acc: 0.9543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9954\n",
      "Epoch 00057: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0143 - acc: 0.9954 - val_loss: 0.1915 - val_acc: 0.9555\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00058: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.1956 - val_acc: 0.9546\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00059: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1858 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9967\n",
      "Epoch 00060: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0111 - acc: 0.9967 - val_loss: 0.2802 - val_acc: 0.9348\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 00061: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0125 - acc: 0.9963 - val_loss: 0.2422 - val_acc: 0.9460\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 00062: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0057 - acc: 0.9987 - val_loss: 0.1698 - val_acc: 0.9625\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00063: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.2321 - val_acc: 0.9474\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9944\n",
      "Epoch 00064: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0179 - acc: 0.9943 - val_loss: 0.2236 - val_acc: 0.9525\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9952\n",
      "Epoch 00065: val_loss did not improve from 0.16415\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0173 - acc: 0.9952 - val_loss: 0.1859 - val_acc: 0.9564\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 717us/sample - loss: 0.2275 - acc: 0.9288\n",
      "Loss: 0.22747659368802206 Accuracy: 0.9287643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 151616)       606464      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,074,576\n",
      "Trainable params: 2,770,960\n",
      "Non-trainable params: 303,616\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 701us/sample - loss: 2.2996 - acc: 0.4289\n",
      "Loss: 2.2996431459890347 Accuracy: 0.42886811\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 50496)        201984      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,072,976\n",
      "Trainable params: 971,472\n",
      "Non-trainable params: 101,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 699us/sample - loss: 1.3310 - acc: 0.6368\n",
      "Loss: 1.3310384488427627 Accuracy: 0.6367601\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 20928)        83712       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 523,216\n",
      "Trainable params: 480,592\n",
      "Non-trainable params: 42,624\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 756us/sample - loss: 0.8024 - acc: 0.7994\n",
      "Loss: 0.8024348169719938 Accuracy: 0.79937696\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 11008)        44032       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 407,376\n",
      "Trainable params: 384,336\n",
      "Non-trainable params: 23,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 761us/sample - loss: 0.4072 - acc: 0.8889\n",
      "Loss: 0.40719286103114904 Accuracy: 0.8888889\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 3584)         14336       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 341,456\n",
      "Trainable params: 333,008\n",
      "Non-trainable params: 8,448\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 813us/sample - loss: 0.2588 - acc: 0.9389\n",
      "Loss: 0.25881171178455664 Accuracy: 0.9389408\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 1152)         4608        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 375,376\n",
      "Trainable params: 371,536\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 799us/sample - loss: 0.2275 - acc: 0.9288\n",
      "Loss: 0.22747659368802206 Accuracy: 0.9287643\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 151616)       606464      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,074,576\n",
      "Trainable params: 2,770,960\n",
      "Non-trainable params: 303,616\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 809us/sample - loss: 4.3362 - acc: 0.5842\n",
      "Loss: 4.336159222544415 Accuracy: 0.584216\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 50496)        201984      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,072,976\n",
      "Trainable params: 971,472\n",
      "Non-trainable params: 101,504\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 840us/sample - loss: 2.6378 - acc: 0.6368\n",
      "Loss: 2.6378005140171865 Accuracy: 0.6367601\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 20928)        83712       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 523,216\n",
      "Trainable params: 480,592\n",
      "Non-trainable params: 42,624\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 818us/sample - loss: 1.2021 - acc: 0.7832\n",
      "Loss: 1.2021141990818092 Accuracy: 0.78317755\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 11008)        44032       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 407,376\n",
      "Trainable params: 384,336\n",
      "Non-trainable params: 23,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 838us/sample - loss: 0.4866 - acc: 0.8964\n",
      "Loss: 0.48655820212647055 Accuracy: 0.8963655\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 3584)         14336       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 341,456\n",
      "Trainable params: 333,008\n",
      "Non-trainable params: 8,448\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 897us/sample - loss: 0.3065 - acc: 0.9367\n",
      "Loss: 0.30654326564693285 Accuracy: 0.9366563\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 1152)         4608        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 375,376\n",
      "Trainable params: 371,536\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 956us/sample - loss: 0.2415 - acc: 0.9406\n",
      "Loss: 0.2414763518275455 Accuracy: 0.9406023\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
