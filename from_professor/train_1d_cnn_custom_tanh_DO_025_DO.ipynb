{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', \n",
    "                      input_shape=input_shape)) \n",
    "    model.add(Activation('tanh'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), strides=1, \n",
    "                          padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7422 - acc: 0.1095\n",
      "Epoch 00001: val_loss improved from inf to 2.73753, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_1_conv_checkpoint/001-2.7375.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 2.7420 - acc: 0.1096 - val_loss: 2.7375 - val_acc: 0.1046\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4746 - acc: 0.2691\n",
      "Epoch 00002: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 2.4746 - acc: 0.2691 - val_loss: 2.8316 - val_acc: 0.1085\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2111 - acc: 0.3570\n",
      "Epoch 00003: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 2.2110 - acc: 0.3571 - val_loss: 2.9666 - val_acc: 0.1095\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9891 - acc: 0.4270\n",
      "Epoch 00004: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.9892 - acc: 0.4270 - val_loss: 3.1276 - val_acc: 0.1165\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8163 - acc: 0.4779\n",
      "Epoch 00005: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 1.8164 - acc: 0.4779 - val_loss: 3.2977 - val_acc: 0.1165\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6770 - acc: 0.5205\n",
      "Epoch 00006: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 1.6771 - acc: 0.5204 - val_loss: 3.4690 - val_acc: 0.1141\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5602 - acc: 0.5555\n",
      "Epoch 00007: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 1.5602 - acc: 0.5555 - val_loss: 3.6321 - val_acc: 0.1181\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4603 - acc: 0.5842\n",
      "Epoch 00008: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 1.4602 - acc: 0.5842 - val_loss: 3.7973 - val_acc: 0.1141\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3713 - acc: 0.6135\n",
      "Epoch 00009: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 1.3713 - acc: 0.6136 - val_loss: 3.9835 - val_acc: 0.1151\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2899 - acc: 0.6380\n",
      "Epoch 00010: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 1.2898 - acc: 0.6381 - val_loss: 4.1701 - val_acc: 0.1139\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2198 - acc: 0.6607\n",
      "Epoch 00011: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 1.2197 - acc: 0.6607 - val_loss: 4.3358 - val_acc: 0.1153\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1528 - acc: 0.6790\n",
      "Epoch 00012: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 1.1529 - acc: 0.6790 - val_loss: 4.5118 - val_acc: 0.1118\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0942 - acc: 0.6981\n",
      "Epoch 00013: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 1.0943 - acc: 0.6981 - val_loss: 4.6946 - val_acc: 0.1120\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0369 - acc: 0.7157\n",
      "Epoch 00014: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 1.0370 - acc: 0.7157 - val_loss: 4.8812 - val_acc: 0.1137\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9857 - acc: 0.7294\n",
      "Epoch 00015: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.9857 - acc: 0.7294 - val_loss: 5.0367 - val_acc: 0.1109\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9398 - acc: 0.7482\n",
      "Epoch 00016: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.9398 - acc: 0.7482 - val_loss: 5.2408 - val_acc: 0.1095\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8946 - acc: 0.7579\n",
      "Epoch 00017: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.8947 - acc: 0.7579 - val_loss: 5.4015 - val_acc: 0.1076\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8526 - acc: 0.7708\n",
      "Epoch 00018: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.8525 - acc: 0.7709 - val_loss: 5.5809 - val_acc: 0.1099\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.7837\n",
      "Epoch 00019: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.8122 - acc: 0.7838 - val_loss: 5.7467 - val_acc: 0.1125\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7936\n",
      "Epoch 00020: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.7770 - acc: 0.7936 - val_loss: 5.9121 - val_acc: 0.1081\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.8046\n",
      "Epoch 00021: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.7420 - acc: 0.8045 - val_loss: 6.0978 - val_acc: 0.1085\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.8130\n",
      "Epoch 00022: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.7116 - acc: 0.8130 - val_loss: 6.2548 - val_acc: 0.1090\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.8220\n",
      "Epoch 00023: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.6786 - acc: 0.8219 - val_loss: 6.3872 - val_acc: 0.1081\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.8310\n",
      "Epoch 00024: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.6518 - acc: 0.8310 - val_loss: 6.5392 - val_acc: 0.1058\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8396\n",
      "Epoch 00025: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.6255 - acc: 0.8395 - val_loss: 6.6721 - val_acc: 0.1044\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8452\n",
      "Epoch 00026: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.6002 - acc: 0.8452 - val_loss: 6.8255 - val_acc: 0.1023\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8540\n",
      "Epoch 00027: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.5742 - acc: 0.8540 - val_loss: 6.9636 - val_acc: 0.1032\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8581\n",
      "Epoch 00028: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.5555 - acc: 0.8579 - val_loss: 7.0931 - val_acc: 0.1016\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.8650\n",
      "Epoch 00029: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.5339 - acc: 0.8650 - val_loss: 7.2360 - val_acc: 0.1039\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8713\n",
      "Epoch 00030: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.5106 - acc: 0.8713 - val_loss: 7.3564 - val_acc: 0.1016\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8774\n",
      "Epoch 00031: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.4898 - acc: 0.8774 - val_loss: 7.4695 - val_acc: 0.1020\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8834\n",
      "Epoch 00032: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.4720 - acc: 0.8834 - val_loss: 7.6012 - val_acc: 0.1027\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8882\n",
      "Epoch 00033: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.4542 - acc: 0.8882 - val_loss: 7.7172 - val_acc: 0.1027\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8913\n",
      "Epoch 00034: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.4396 - acc: 0.8913 - val_loss: 7.8415 - val_acc: 0.1006\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8967\n",
      "Epoch 00035: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.4231 - acc: 0.8966 - val_loss: 7.9546 - val_acc: 0.0999\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.9009\n",
      "Epoch 00036: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.4062 - acc: 0.9009 - val_loss: 8.0493 - val_acc: 0.1023\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.9047\n",
      "Epoch 00037: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.3940 - acc: 0.9047 - val_loss: 8.1568 - val_acc: 0.1016\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.9087\n",
      "Epoch 00038: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.3793 - acc: 0.9087 - val_loss: 8.2819 - val_acc: 0.1013\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3667 - acc: 0.9116\n",
      "Epoch 00039: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3669 - acc: 0.9114 - val_loss: 8.3455 - val_acc: 0.0990\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.9157\n",
      "Epoch 00040: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3533 - acc: 0.9157 - val_loss: 8.4278 - val_acc: 0.1004\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.9190\n",
      "Epoch 00041: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.3400 - acc: 0.9191 - val_loss: 8.5365 - val_acc: 0.0997\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.9233\n",
      "Epoch 00042: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3288 - acc: 0.9233 - val_loss: 8.6300 - val_acc: 0.1004\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.9252\n",
      "Epoch 00043: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3202 - acc: 0.9252 - val_loss: 8.7220 - val_acc: 0.0990\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9282\n",
      "Epoch 00044: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3063 - acc: 0.9282 - val_loss: 8.7999 - val_acc: 0.0971\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9324\n",
      "Epoch 00045: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.2989 - acc: 0.9324 - val_loss: 8.8905 - val_acc: 0.0981\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9350\n",
      "Epoch 00046: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.2859 - acc: 0.9350 - val_loss: 8.9492 - val_acc: 0.0983\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9364\n",
      "Epoch 00047: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.2788 - acc: 0.9364 - val_loss: 9.0100 - val_acc: 0.0974\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9399\n",
      "Epoch 00048: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2702 - acc: 0.9400 - val_loss: 9.1289 - val_acc: 0.0990\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9420\n",
      "Epoch 00049: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.2619 - acc: 0.9420 - val_loss: 9.1823 - val_acc: 0.0974\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2537 - acc: 0.9446\n",
      "Epoch 00050: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.2537 - acc: 0.9446 - val_loss: 9.2763 - val_acc: 0.0978\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9457\n",
      "Epoch 00051: val_loss did not improve from 2.73753\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.2468 - acc: 0.9457 - val_loss: 9.3501 - val_acc: 0.0999\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 302us/sample - loss: 2.7266 - acc: 0.1051\n",
      "Loss: 2.7266184161137694 Accuracy: 0.105088264\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3320 - acc: 0.2661\n",
      "Epoch 00001: val_loss improved from inf to 2.12189, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_2_conv_checkpoint/001-2.1219.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.3320 - acc: 0.2661 - val_loss: 2.1219 - val_acc: 0.3433\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8159 - acc: 0.4377\n",
      "Epoch 00002: val_loss improved from 2.12189 to 2.04540, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_2_conv_checkpoint/002-2.0454.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.8157 - acc: 0.4378 - val_loss: 2.0454 - val_acc: 0.3580\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5138 - acc: 0.5363\n",
      "Epoch 00003: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.5139 - acc: 0.5363 - val_loss: 2.0845 - val_acc: 0.3611\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2913 - acc: 0.6058\n",
      "Epoch 00004: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.2911 - acc: 0.6058 - val_loss: 2.1733 - val_acc: 0.3620\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1091 - acc: 0.6652\n",
      "Epoch 00005: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.1091 - acc: 0.6652 - val_loss: 2.2964 - val_acc: 0.3524\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9597 - acc: 0.7138\n",
      "Epoch 00006: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9597 - acc: 0.7137 - val_loss: 2.3946 - val_acc: 0.3583\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7535\n",
      "Epoch 00007: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8332 - acc: 0.7535 - val_loss: 2.5196 - val_acc: 0.3594\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7917\n",
      "Epoch 00008: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7161 - acc: 0.7917 - val_loss: 2.6653 - val_acc: 0.3508\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.8216\n",
      "Epoch 00009: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6233 - acc: 0.8216 - val_loss: 2.8254 - val_acc: 0.3564\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.8465\n",
      "Epoch 00010: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5413 - acc: 0.8464 - val_loss: 2.9248 - val_acc: 0.3508\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8699\n",
      "Epoch 00011: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4725 - acc: 0.8699 - val_loss: 3.1119 - val_acc: 0.3438\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8909\n",
      "Epoch 00012: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4065 - acc: 0.8909 - val_loss: 3.2138 - val_acc: 0.3506\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.9073\n",
      "Epoch 00013: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3529 - acc: 0.9073 - val_loss: 3.3783 - val_acc: 0.3424\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9206\n",
      "Epoch 00014: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3064 - acc: 0.9206 - val_loss: 3.5549 - val_acc: 0.3438\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9308\n",
      "Epoch 00015: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2715 - acc: 0.9308 - val_loss: 3.6370 - val_acc: 0.3394\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9416\n",
      "Epoch 00016: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2360 - acc: 0.9416 - val_loss: 3.7638 - val_acc: 0.3473\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9515\n",
      "Epoch 00017: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2047 - acc: 0.9516 - val_loss: 3.9002 - val_acc: 0.3440\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9581\n",
      "Epoch 00018: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1825 - acc: 0.9581 - val_loss: 3.9790 - val_acc: 0.3494\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9599\n",
      "Epoch 00019: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1675 - acc: 0.9599 - val_loss: 4.1344 - val_acc: 0.3457\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9663\n",
      "Epoch 00020: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1523 - acc: 0.9663 - val_loss: 4.2299 - val_acc: 0.3494\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9681\n",
      "Epoch 00021: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1416 - acc: 0.9681 - val_loss: 4.3126 - val_acc: 0.3464\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9742\n",
      "Epoch 00022: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1230 - acc: 0.9741 - val_loss: 4.3927 - val_acc: 0.3431\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9684\n",
      "Epoch 00023: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1436 - acc: 0.9684 - val_loss: 4.4364 - val_acc: 0.3468\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9795\n",
      "Epoch 00024: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0979 - acc: 0.9795 - val_loss: 4.5494 - val_acc: 0.3445\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9816\n",
      "Epoch 00025: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0928 - acc: 0.9816 - val_loss: 4.6882 - val_acc: 0.3436\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9806\n",
      "Epoch 00026: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0953 - acc: 0.9806 - val_loss: 4.7448 - val_acc: 0.3427\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9821\n",
      "Epoch 00027: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0880 - acc: 0.9821 - val_loss: 4.7411 - val_acc: 0.3541\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9805\n",
      "Epoch 00028: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0874 - acc: 0.9805 - val_loss: 4.8077 - val_acc: 0.3548\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9849\n",
      "Epoch 00029: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0791 - acc: 0.9849 - val_loss: 4.8577 - val_acc: 0.3503\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9814\n",
      "Epoch 00030: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0827 - acc: 0.9814 - val_loss: 4.9255 - val_acc: 0.3517\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9824\n",
      "Epoch 00031: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0779 - acc: 0.9824 - val_loss: 5.0001 - val_acc: 0.3492\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9855\n",
      "Epoch 00032: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0703 - acc: 0.9855 - val_loss: 5.0222 - val_acc: 0.3508\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9882\n",
      "Epoch 00033: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0586 - acc: 0.9882 - val_loss: 5.0590 - val_acc: 0.3545\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9893\n",
      "Epoch 00034: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0568 - acc: 0.9893 - val_loss: 5.1302 - val_acc: 0.3557\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9845\n",
      "Epoch 00035: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0727 - acc: 0.9845 - val_loss: 5.1781 - val_acc: 0.3501\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9869\n",
      "Epoch 00036: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0615 - acc: 0.9869 - val_loss: 5.2314 - val_acc: 0.3501\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9832\n",
      "Epoch 00037: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0700 - acc: 0.9832 - val_loss: 5.2261 - val_acc: 0.3541\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9863\n",
      "Epoch 00038: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0610 - acc: 0.9863 - val_loss: 5.3248 - val_acc: 0.3515\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9871\n",
      "Epoch 00039: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0578 - acc: 0.9871 - val_loss: 5.2931 - val_acc: 0.3571\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9911\n",
      "Epoch 00040: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0444 - acc: 0.9911 - val_loss: 5.3555 - val_acc: 0.3531\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9879\n",
      "Epoch 00041: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0545 - acc: 0.9879 - val_loss: 5.3682 - val_acc: 0.3548\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9892\n",
      "Epoch 00042: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0494 - acc: 0.9892 - val_loss: 5.4343 - val_acc: 0.3541\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9872\n",
      "Epoch 00043: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0541 - acc: 0.9872 - val_loss: 5.4678 - val_acc: 0.3569\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9881\n",
      "Epoch 00044: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0536 - acc: 0.9881 - val_loss: 5.5288 - val_acc: 0.3536\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9883\n",
      "Epoch 00045: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0531 - acc: 0.9883 - val_loss: 5.5115 - val_acc: 0.3592\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9911\n",
      "Epoch 00046: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0464 - acc: 0.9911 - val_loss: 5.5542 - val_acc: 0.3475\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9910\n",
      "Epoch 00047: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0431 - acc: 0.9910 - val_loss: 5.6093 - val_acc: 0.3557\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9869\n",
      "Epoch 00048: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0581 - acc: 0.9869 - val_loss: 5.6389 - val_acc: 0.3557\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9864\n",
      "Epoch 00049: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0592 - acc: 0.9864 - val_loss: 5.6512 - val_acc: 0.3599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9902\n",
      "Epoch 00050: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0458 - acc: 0.9902 - val_loss: 5.7301 - val_acc: 0.3508\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9912\n",
      "Epoch 00051: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0399 - acc: 0.9912 - val_loss: 5.6641 - val_acc: 0.3604\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9904\n",
      "Epoch 00052: val_loss did not improve from 2.04540\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0438 - acc: 0.9904 - val_loss: 5.7438 - val_acc: 0.3543\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 479us/sample - loss: 2.0960 - acc: 0.3429\n",
      "Loss: 2.095974131140506 Accuracy: 0.3428868\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0069 - acc: 0.3694\n",
      "Epoch 00001: val_loss improved from inf to 1.64994, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_3_conv_checkpoint/001-1.6499.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.0069 - acc: 0.3694 - val_loss: 1.6499 - val_acc: 0.4852\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4076 - acc: 0.5598\n",
      "Epoch 00002: val_loss improved from 1.64994 to 1.56820, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_3_conv_checkpoint/002-1.5682.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4077 - acc: 0.5598 - val_loss: 1.5682 - val_acc: 0.5085\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1689 - acc: 0.6381\n",
      "Epoch 00003: val_loss improved from 1.56820 to 1.52279, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_3_conv_checkpoint/003-1.5228.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1689 - acc: 0.6381 - val_loss: 1.5228 - val_acc: 0.5218\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9962 - acc: 0.6970\n",
      "Epoch 00004: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9962 - acc: 0.6969 - val_loss: 1.5654 - val_acc: 0.5281\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7457\n",
      "Epoch 00005: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8477 - acc: 0.7457 - val_loss: 1.6032 - val_acc: 0.5306\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7151 - acc: 0.7918\n",
      "Epoch 00006: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7151 - acc: 0.7918 - val_loss: 1.6133 - val_acc: 0.5365\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.8311\n",
      "Epoch 00007: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5994 - acc: 0.8312 - val_loss: 1.6528 - val_acc: 0.5439\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8628\n",
      "Epoch 00008: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4987 - acc: 0.8628 - val_loss: 1.7215 - val_acc: 0.5460\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8896\n",
      "Epoch 00009: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4109 - acc: 0.8897 - val_loss: 1.7858 - val_acc: 0.5509\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.9120\n",
      "Epoch 00010: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3468 - acc: 0.9120 - val_loss: 1.8148 - val_acc: 0.5488\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9331\n",
      "Epoch 00011: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2836 - acc: 0.9331 - val_loss: 1.8346 - val_acc: 0.5539\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9462\n",
      "Epoch 00012: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2354 - acc: 0.9461 - val_loss: 1.9006 - val_acc: 0.5637\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9556\n",
      "Epoch 00013: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2103 - acc: 0.9556 - val_loss: 1.9349 - val_acc: 0.5581\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9653\n",
      "Epoch 00014: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1723 - acc: 0.9653 - val_loss: 1.9650 - val_acc: 0.5693\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9745\n",
      "Epoch 00015: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1422 - acc: 0.9745 - val_loss: 1.9666 - val_acc: 0.5716\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9801\n",
      "Epoch 00016: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1225 - acc: 0.9801 - val_loss: 2.0515 - val_acc: 0.5707\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9801\n",
      "Epoch 00017: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1209 - acc: 0.9801 - val_loss: 2.0705 - val_acc: 0.5656\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9833\n",
      "Epoch 00018: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1015 - acc: 0.9833 - val_loss: 2.0966 - val_acc: 0.5714\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9858\n",
      "Epoch 00019: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0972 - acc: 0.9858 - val_loss: 2.1308 - val_acc: 0.5758\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9891\n",
      "Epoch 00020: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0795 - acc: 0.9891 - val_loss: 2.2254 - val_acc: 0.5679\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9903\n",
      "Epoch 00021: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0699 - acc: 0.9903 - val_loss: 2.2512 - val_acc: 0.5723\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9879\n",
      "Epoch 00022: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0790 - acc: 0.9879 - val_loss: 2.2609 - val_acc: 0.5807\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9896\n",
      "Epoch 00023: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0711 - acc: 0.9896 - val_loss: 2.3165 - val_acc: 0.5726\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9900\n",
      "Epoch 00024: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0685 - acc: 0.9900 - val_loss: 2.3746 - val_acc: 0.5709\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9903\n",
      "Epoch 00025: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0696 - acc: 0.9903 - val_loss: 2.3729 - val_acc: 0.5830\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9928\n",
      "Epoch 00026: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0556 - acc: 0.9928 - val_loss: 2.3565 - val_acc: 0.5795\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9921\n",
      "Epoch 00027: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0561 - acc: 0.9921 - val_loss: 2.4000 - val_acc: 0.5807\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9936\n",
      "Epoch 00028: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0491 - acc: 0.9936 - val_loss: 2.4486 - val_acc: 0.5788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9915\n",
      "Epoch 00029: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0556 - acc: 0.9915 - val_loss: 2.4358 - val_acc: 0.5840\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9937\n",
      "Epoch 00030: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0470 - acc: 0.9937 - val_loss: 2.5025 - val_acc: 0.5823\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9918\n",
      "Epoch 00031: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0515 - acc: 0.9918 - val_loss: 2.5235 - val_acc: 0.5802\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9932\n",
      "Epoch 00032: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0457 - acc: 0.9932 - val_loss: 2.5384 - val_acc: 0.5842\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9952\n",
      "Epoch 00033: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0379 - acc: 0.9952 - val_loss: 2.5532 - val_acc: 0.5812\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9939\n",
      "Epoch 00034: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0442 - acc: 0.9939 - val_loss: 2.5731 - val_acc: 0.5849\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9932\n",
      "Epoch 00035: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0458 - acc: 0.9932 - val_loss: 2.6189 - val_acc: 0.5837\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9924\n",
      "Epoch 00036: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0505 - acc: 0.9924 - val_loss: 2.5837 - val_acc: 0.5854\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9958\n",
      "Epoch 00037: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0344 - acc: 0.9958 - val_loss: 2.6163 - val_acc: 0.5821\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9954\n",
      "Epoch 00038: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0359 - acc: 0.9954 - val_loss: 2.6266 - val_acc: 0.5856\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9935\n",
      "Epoch 00039: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0381 - acc: 0.9935 - val_loss: 2.6284 - val_acc: 0.5842\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9948\n",
      "Epoch 00040: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0381 - acc: 0.9948 - val_loss: 2.7236 - val_acc: 0.5821\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9927\n",
      "Epoch 00041: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0465 - acc: 0.9927 - val_loss: 2.6520 - val_acc: 0.5924\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9950\n",
      "Epoch 00042: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0338 - acc: 0.9950 - val_loss: 2.6861 - val_acc: 0.5912\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9928\n",
      "Epoch 00043: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0502 - acc: 0.9928 - val_loss: 2.6933 - val_acc: 0.5840\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9937\n",
      "Epoch 00044: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0423 - acc: 0.9937 - val_loss: 2.7134 - val_acc: 0.5840\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9957\n",
      "Epoch 00045: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0305 - acc: 0.9957 - val_loss: 2.7255 - val_acc: 0.5931\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9956\n",
      "Epoch 00046: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0320 - acc: 0.9956 - val_loss: 2.7730 - val_acc: 0.5840\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9951\n",
      "Epoch 00047: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0328 - acc: 0.9951 - val_loss: 2.8135 - val_acc: 0.5858\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9952\n",
      "Epoch 00048: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0352 - acc: 0.9952 - val_loss: 2.7518 - val_acc: 0.5903\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.0311 - acc: 0.9955 - val_loss: 2.7953 - val_acc: 0.5928\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9949\n",
      "Epoch 00050: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0349 - acc: 0.9949 - val_loss: 2.7979 - val_acc: 0.5907\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9963\n",
      "Epoch 00051: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0288 - acc: 0.9963 - val_loss: 2.7801 - val_acc: 0.5905\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9945\n",
      "Epoch 00052: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0368 - acc: 0.9945 - val_loss: 2.8491 - val_acc: 0.5877\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9949\n",
      "Epoch 00053: val_loss did not improve from 1.52279\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0337 - acc: 0.9949 - val_loss: 2.8474 - val_acc: 0.5858\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XecVNX5+PHPmbpltrMLuywdBKQtVQygRmM3YENsscZeQoxGkphojOZnYom9YItEAY2ISuQrEQXRWAFBEEFA2lK2sr1MO78/zsxsYXdZYGdnd/Z5v173de/M3Lnz3NnZ+9xzz7nnKK01QgghBIAl0gEIIYToOCQpCCGECJGkIIQQIkSSghBCiBBJCkIIIUIkKQghhAiRpCCEECJEkoIQQogQSQpCCCFCbJEO4FB169ZN9+3bN9JhCCFEp7Jq1apCrXX6wdbrdEmhb9++rFy5MtJhCCFEp6KU2tGa9eTykRBCiBBJCkIIIUIkKQghhAjpdHUKTfF4POTm5lJTUxPpUDqtmJgYsrOzsdvtkQ5FCBFBUZEUcnNzSUhIoG/fviilIh1Op6O1pqioiNzcXPr16xfpcIQQERQVl49qampIS0uThHCYlFKkpaVJSUsIER1JAZCEcITk+xNCQJRcPhJCiKjxzjvwzTdgt4PDYebB5ZwcGD8+rB8vSaENlJSUMHfuXG688cZDfu8ZZ5zB3LlzSU5ObtX699xzDy6Xi9tvv/2QP0sI0YF5vfDb38I//tH8OrNmhT0pRM3lo0gqKSnh6aefbvI1r9fb4nsXL17c6oQghGhnWrfP55SUwFlnmYRwyy1QWws1NVBeDsXFkJcHu3aZpBFmkhTawKxZs9i6dSs5OTnccccdLF++nClTpjB16lSOPvpoAM4++2zGjh3LsGHDmD17dui9ffv2pbCwkO3btzN06FCuueYahg0bximnnEJ1dXWLn7tmzRomTpzIyJEjOeecc9i/fz8Ajz/+OEcffTQjR47kwgsvBODjjz8mJyeHnJwcRo8eTXl5eZi+DSGiQF6eOQCnpsLNN5sDdLhs3gwTJ8KHH8Jzz8Hjj5tLRU4nuFyQkgIZGZCdbZbDLOouH23ePJOKijVtuk2XK4dBgx5t9vUHHniA9evXs2aN+dzly5ezevVq1q9fH2ri+dJLL5Gamkp1dTXjx4/nvPPOIy0trVHsm5k3bx7PP/88F1xwAQsWLODSSy9t9nMvu+wynnjiCY4//nj+9Kc/8ec//5lHH32UBx54gG3btuF0OikpKQHgoYce4qmnnmLSpElUVFQQExNzpF+LENEnNxcefBBmzwa3GyZNgqeegk8/hTfegKOOavp9Xi/8618wbx5UVJgkUn+KiYGxY+GYY2DCBLMcHw9Ll8IFF4DFYpaPP75997cJUlIIkwkTJjRo8//4448zatQoJk6cyK5du9i8efMB7+nXrx85OTkAjB07lu3btze7/dLSUkpKSjg+8CO6/PLLWbFiBQAjR47kkksu4dVXX8VmM3l/0qRJ3HbbbTz++OOUlJSEnhdCANu3w/XXw4AB8PTTcNFF8P33sGIF/Oc/JlmMGWMO/PX5/fD66zB8OFx1lbnEEx8PPXua5yZPNpeFxo+HVavgjjvMgT8pCUaOhNNOM+t+/XWHSAgQhSWFls7o21N8fHxoefny5SxdupTPP/+cuLg4TjjhhCbvCXA6naFlq9V60MtHzXnvvfdYsWIFixYt4v7772fdunXMmjWLM888k8WLFzNp0iSWLFnCkCFDDmv7QkSN0lK47z547DFQyhzY77wT6nfPf+aZsGYNXHwxXHYZfPQRPPEELF8Od90Fa9fCsGGwcCFMm2a205z8fPjqKzN9+aVJFo8+CgkJ4d7TVou6pBAJCQkJLV6jLy0tJSUlhbi4ODZu3MgXX3xxxJ+ZlJRESkoKn3zyCVOmTOFf//oXxx9/PH6/n127dvHTn/6UyZMnM3/+fCoqKigqKmLEiBGMGDGCr7/+mo0bN0pSEF2X1wsvvAB/+hMUFsIVV8C995rr9k3JzjbJ4N57TRJZsMBUAg8YAK++ChdeCFbrwT83I8OUHM46q013py1JUmgDaWlpTJo0ieHDh3P66adz5plnNnj9tNNO49lnn2Xo0KEMHjyYiRMntsnnvvLKK1x//fVUVVXRv39/Xn75ZXw+H5deeimlpaVorbn11ltJTk7mj3/8I8uWLcNisTBs2DBOP/30NolBiE5n6VL49a9h/Xo47jjT4mfMmIO/z2YzSeGEE0xiuOgik0yirL8wpduryVUbGTdunG48yM7333/P0KFDIxRR9JDvUbSbigrTuuecc+Dkk8PzGfv3m5Y9W7aY+ebNpp5g9Wro1w8eesh8fhe5m18ptUprPe5g60lJQQjRvrxec7nlvfdMK5+nnoLrrjuybWoNP/xgKoZXrIBPPoEd9QYaUwp69YJBg0wyuPlm0+RTHECSghCi/WhtDsjvvQcPP2za5l9/vTmb/9vfTNPM5uTnw759Zp6fb+4lyM83JYBPPjHLYK7bT5kCN95ompAOGmSu/Usz7FaRpCCEaD9/+5u5QWvWLLjtNrj1VvjVr8zZ+48/miafcXF161dXm/sDnnnGtNZpzG43JYBTTzX1A8cdZ5JAF7kkFA6SFIQQ7WPuXPjd70wF7f33m+dsNnjySXMgv+02U4n77rumqehzz8E//2nqBoYMgQcegIEDTUkgOCUnSwJoY5IUhBDh9/HHcOWV5gatl19ueJlIKZg501T+XnwxDB4MZWUmYZx7Ltxwg3mfHPzbhSQFIUT4+HywYQOcfbY5y1+4sPkK3mnTTCXxb38LJ54IV18NPXq0b7xCkkKkuFwuKioqWv28EGFTWgr/+5+5Hu9yHXz98nLTrHP7dtPCJzjfscP06Onx1E1+v3lPjx6wePHBO3QbO9ZUPouIkaQgRFfldsOzz5obsoqKIDHR3Ix1000HdvymtanofeEFmD8fKivrXsvMNN1CjB8PaWl1A8PUn59/PvTp0557Jw6X1rpTTWPHjtWNbdiw4YDn2tOdd96pn3zyydDju+++Wz/44IO6vLxcn3jiiXr06NF6+PDh+u233w6tEx8f3+S2gs/7/X59++2362HDhunhw4fr+fPna6213rNnj54yZYoeNWqUHjZsmF6xYoX2er368ssvD637yCOPHNZ+RPp7FO3E79f63//WeuBArUHrE080jy+5RGu73Tx36qlaL1qkdV6e1v/4h9bDhpnn4+O1/uUvtf6//9P6hx+0rq6O9N6IVgJW6lYcY6OvpDBzpum8qi3l5JhOq5oxY8YMZs6cyU033QTAG2+8wZIlS4iJiWHhwoUkJiZSWFjIxIkTmTp1aqvGQ37rrbdYs2YNa9eupbCwkPHjx3Pccccxd+5cTj31VP7whz/g8/moqqpizZo17N69m/Xr1wOEussWogG/33QBfeed8MUXphfPxYtNT51KmbP5hx82N5Q9+yz8/Od17z3mGHj+eZgxo0N13ibaXvQlhQgYPXo0+fn57Nmzh4KCAlJSUujVqxcej4ff//73rFixAovFwu7du8nLy6NHKyrPPv30Uy666CKsVivdu3fn+OOP5+uvv2b8+PFcddVVeDwezj77bHJycujfvz8//vgjt9xyC2eeeSannHJKO+y16PDcbnPt/5NPzPS//5lr/llZ8OKLcPnlB3bi1r07/PGP5j6ChQtN/0DTp8OIEZHZB9Huoi8ptHBGH07Tp0/nzTffZN++fcyYMQOA1157jYKCAlatWoXdbqdv375Ndpl9KI477jhWrFjBe++9xxVXXMFtt93GZZddxtq1a1myZAnPPvssb7zxBi+99FJb7JboLCorzQF83TozrV1rumcOdr8+aJBpATRlijnI1+vavUl2uxn85YILwh+76FDClhSUUr2AOUB3QAOztdaPNVrnBOAdYFvgqbe01veGK6ZwmjFjBtdccw2FhYV8/PHHgOkyOyMjA7vdzrJly9hRvy+Wg5gyZQrPPfccl19+OcXFxaxYsYIHH3yQHTt2kJ2dzTXXXENtbS2rV6/mjDPOwOFwcN555zF48OAWR2sTnZzPZ+78/fZbc/D/9lsz/fhj3XjC8fHm0tA115gkMHmyNO0UrRbOkoIX+I3WerVSKgFYpZT6QGu9odF6n2itO27n4q00bNgwysvL6dmzJ5mZmQBccskl/PznP2fEiBGMGzfukMYvOOecc/j8888ZNWoUSin+/ve/06NHD1555RUefPBB7HY7LpeLOXPmsHv3bq688kr8geZ//+///b+w7KOIgPx8c+PX8uVmdK716+vO/i0W0/Z/9GhzKWjECDOaV9++LfchJEQL2q3rbKXUO8CTWusP6j13AnD7oSQF6To7fOR77AAqK+H//s8kgWXLzI1fYO4fGD8eRo0yB/4RI+Dooxv2EyRECzpU19lKqb7AaKCJHq04Vim1FtiDSRDftUdMQnQ4Gzea6/6bNplLQJMnm+EfTzjBDAITZYO5iI4p7ElBKeUCFgAztdZljV5eDfTRWlcopc4A3gYGNbGNa4FrAXr37h3miIWIgLffNgkgNtZ0K33yyZIERESE9cKjUsqOSQivaa3favy61rpMa10RWF4M2JVS3ZpYb7bWepzWelx6eno4Qxaiffn9Zpzgc84xPYGuXAlnnCEJQURMOFsfKeBF4Hut9SPNrNMDyNNaa6XUBEySKgpXTEK0u6IiUwrIzDS9f/btW3dvQEkJXHqpKRlceSU8/bQMBCMiLpyXjyYBvwDWKaWCtxj/HugNoLV+FjgfuEEp5QWqgQt1e9V8CxFOXq8ZGObuu814AEEOh7lnYPBg05R0+3YzHOUNN0jX0KJDCFtS0Fp/CrT4K9daPwk8Ga4YhIiIDz80o4l99x2cdBL89a+mx9BNm8y0caN5zWKBjz4y9xII0UFE3x3NEVBSUsLcuXO58cYbD/m9Z5xxBnPnziU5OTkMkYl2tW0b/OY3pnuIfv3grbdMa6JgCWDSpMjGJ0QryB0ubaCkpISnn366yde8Xm+L7128eLEkhM4uPx9+/WsYOhSWLIH77jP3F5xzjlwSEp2OJIU2MGvWLLZu3UpOTg533HEHy5cvZ8qUKUydOpWjjz4agLPPPpuxY8cybNgwZs+eHXpv3759KSwsZPv27QwdOpRrrrmGYcOGccopp1AdvHO1nkWLFnHMMccwevRofvazn5GXlwdARUUFV155JSNGjGDkyJEsWLAAgPfff58xY8YwatQoTjrppHb4NjqhwkIz4tdnn5nWP2vXmoP65s0Nxw1orKQE7roL+veHxx83Q0lu2gR/+INUGItOK+ouH0Wg52weeOAB1q9fz5rABy9fvpzVq1ezfv16+vXrB8BLL71Eamoq1dXVjB8/nvPOO4+0tLQG29m8eTPz5s3j+eef54ILLmDBggUH9GM0efJkvvjiC5RSvPDCC/z973/n4Ycf5i9/+QtJSUmsW7cOgP3791NQUMA111zDihUr6NevH8XFxW34rUSJefPgxhvNAb4pFotpKjpmjBkVbMwYMwDNyy/D3/9u3jdjBvz5z6byWIhOLuqSQkcxYcKEUEIAePzxx1m4cCEAu3btYvPmzQckhX79+pGTkwPA2LFj2b59+wHbzc3NZcaMGezduxe32x36jKVLlzJ//vzQeikpKSxatIjjjjsutE5qamqb7mOntn+/SQbz58PEieZeAYvFdDcdHErS7YatW0330x99BK++2nAbZ50Ff/mLOWsQIkpEXVKIUM/ZB4iv1zXx8uXLWbp0KZ9//jlxcXGccMIJTXah7aw3oLnVam3y8tEtt9zCbbfdxtSpU1m+fDn33HNPWOKPakuXmmEn8/LM9f877wRbK/4V9u2Db74xndJNngzHHhv2UIVob1Kn0AYSEhIoLy9v9vXS0lJSUlKIi4tj48aNfPHFF4f9WaWlpfTs2ROAV155JfT8ySefzFNPPRV6vH//fiZOnMiKFSvYts30TN7lLx9VV5umoiefbEYP++ILc/2/NQkBTPfTp58Od9whCUFELUkKbSAtLY1JkyYxfPhw7rjjjgNeP+200/B6vQwdOpRZs2YxceLEw/6se+65h+nTpzN27Fi6davrEeSuu+5i//79DB8+nFGjRrFs2TLS09OZPXs25557LqNGjQoN/tMlff656WL68cfhlltg1SpTRyCEaKDdus5uK9J1dvhE5fdYXW2Gl3zkEejVywxD+bOfRToqIdpdh+o6W4iI+Owz06fQDz/AddeZ1kKJiZGOSogOTZKCiB5amw7odu6EuXPrSgcffCClAyFaSZKC6JzKyswIZR9+aLqX2LXLJIP6Lbauuw4efNBUKgshWkWSgug8du+Gd9+Fd94x9w14PJCSYm4mGzECzjzTlAx69zZdTkRb/YgQ7UCSguj49u6FCy6ATz81jwcONE1Lp00zTUOD4xMIIY6YJAXRsZWXmxLADz/A/febXkeHDpWO5oQIE0kKEeJyuaioqIh0GB2bxwPTp5vBaBYtMjeOCSHCSpKC6Ji0NhXFS5bACy9IQhCincgdzW1g1qxZDbqYuOeee3jooYeoqKjgpJNOYsyYMYwYMYJ33nnnoNtqrovtprrAbq677Khwzz2mJ9K774arr450NEJ0GVFXUpj5/kzW7GvbvrNzeuTw6GnN97Q3Y8YMZs6cyU033QTAG2+8wZIlS4iJiWHhwoUkJiZSWFjIxIkTmTp1KqqF6+FNdbHt9/ub7AK7qe6yo8ILL8C998JVV5mkIIRoN1GXFCJh9OjR5Ofns2fPHgoKCkhJSaFXr154PB5+//vfs2LFCiwWC7t37yYvL48ePXo0u62mutguKChosgvsprrL7vQWL4brr4fTToNnn5UKZSHaWdQlhZbO6MNp+vTpvPnmm+zbty/U8dxrr71GQUEBq1atwm6307dv3ya7zA5qbRfbUcfnMzeiPfOMmY8eDf/+N9jtkY5MiC5H6hTayIwZM5g/fz5vvvkm06dPB0w31xkZGdjtdpYtW8aOHTta3EZzXWw31wV2U91ldyr79plmpv37w89/bgazuesu+O9/weWKdHRCdEmSFNrIsGHDKC8vp2fPnmRmZgJwySWXsHLlSkaMGMGcOXMYMmRIi9torovt5rrAbqq77A6vuBj+9S8zqH2vXiYJDBpkSgY7d5q6hEYj0gkh2o90nS1CwvY97tpluqZYuBA+/thcLsrKMmMbX3edjG0sRDuQrrNFZGgNmzebbquD03ffmdeGDoXf/tbclTxunBkTWQjRoUhSEG3js8/ggQfMCGeFhea55GTTN9EvfmESgZQIhOjwwpYUlFK9gDlAd0ADs7XWjzVaRwGPAWcAVcAVWuvVh/N5WusW2/+Llh3RZcRPPjFNSJOSTIXxT35ipiFDpDQgRCcTzpKCF/iN1nq1UioBWKWU+kBrvaHeOqcDgwLTMcAzgfkhiYmJoaioiLS0NEkMh0FrTVFRETExMYf+5s8+gzPOMJXGy5ebwe2FEJ1W2JKC1novsDewXK6U+h7oCdRPCtOAOdqcpn6hlEpWSmUG3ttq2dnZ5ObmUlBQ0Ow6fr8Hv78SqzURpeTstbGYmBiys7MP7U1ffmlKCJmZZnwDSQhCdHrtUqeglOoLjAa+bPRST2BXvce5gecOKSnY7fbQ3b7NKSh4m+++O4cxY74iMXH8oWxeNGXlSjj1VEhPNwkhKyvSEQkh2kDYT5mVUi5gATBTa112mNu4Vim1Uim1sqXSQEtiYwcCUF295bDeL+r55hs4+WQz6tmyZXCoJQwhRIcV1pKCUsqOSQivaa3famKV3UCveo+zA881oLWeDcwGc5/C4cQSG9sfgOrqrYfz9q5Fa1NXsHgxVFSA12vGNvB4zPLixZCYaBJC796RjlYI0YbC2fpIAS8C32utH2lmtXeBm5VS8zEVzKWHWp/QWlZrHA5HlpQUWrJpE7z2Grz6KmzbBjYbxMebPojsdvPYbjf3G7zyCvTtG+mIhRBtLJwlhUnAL4B1SqlgX9a/B3oDaK2fBRZjmqNuwTRJvTKM8RAbO1CSQmMlJSYJzJkDX39tmpCedBL8+c/m3oKEhEhHKIRoR+FsffQp0GL70ECro5vCFUNjsbEDKS5e3F4f17GtWmV6JZ07F6qrIScHHn4YLrxQKo2F6MK61B3NsbEDcLv34fVWYLN1wV44q6rg9ddNMvj6a4iLg0svhRtuMN1VCyG6vC6WFEwLpJqarbhcoyIcTTv77DOYPh327DF1Ak88YbqfSEqKdGRCiA6kS93FVdcstQu1QNIannoKjj/elAw++sh0UHfzzZIQhBAH6GIlhQFAF7pXobraDG05Zw6cdZYZxyA5OdJRCSE6sC5VUrDZkrDbu3WNpLBtG0yaZBLCPfeY8QwkIQghDqJLlRQg2Cw1ii8f5eWZ4SxnzjSD2SxaZEoJQgjRCl0uKcTEDKC09JNIh9E2tIYffzRdV3/yCXz6Kfzwg3ltxAh46y0YODCyMQohOpUulxRiYweSnz8Xv78Wi8UZ6XAOz/ffw7x5MH++GeUMTD9EkyfD1VfDlClmZDO7PbJxCiE6nS6ZFEBTXb2N+PghkQ6n9bZvN0lg/nxYuxaUgp/+FH71K9Oy6OijZUAbIcQR64JJoa4FUqdICtXV5sD//PPm8cSJ8Nhj5p6DzMzIxiaEiDpdMCl0oi60f/wRzj/fdFV9223m3oKDjBshhBBHosslBbu9G1ZrIjU1HbwF0rvvwmWXmUtC//kPnHlmpCMSQnQBXe4itFKqY/eW6vXC734H06bBgAGm4zpJCEKIdtLlSgpg6hUqKr6JdBgHys+HGTNg+XK47jp49FGIiYl0VEKILqTLlRTA1CvU1GzH7/dGOpQ6a9aYZqRffmkGsHn2WUkIQoh212WTgtZeamt3RjoUY8EC0yWF1uYGtMsui3REQoguqosmhQ7SMZ7fb/olOv98GDXKjHEwZkxkYxJCdGldtE6hA3ShXVkJl19uSgmXXw7PPQfOTnqHtRAianSdkkJJCdx1F3g8OByZWCyxkSkpaG0qkidNgoULzRCYL78sCUEI0SF0naSweDHcfz9ccAHK4yUmpn/7JgW/33RffeyxpnuKffvM/Qe33Wa6rBBCiA6g6ySFiy82Q1C+/Tacdx5x1n7tc/nI4zGD24wcCWefbbq2fvppM97B6aeH//OFEOIQdK06hZtvNj2HXn89/cr68s1de9Haj1JtkBv37oV162DHjobTpk3m/oPhw+HVV819CLau9bULITqPrnd0uu46sNuJ++UvGTZLU/vBVmJSBx3+9lauhIcegjffNIPaAFit0LMn9OkDp55qOq8780zpxVQI0eF1vaQAcNVVVNZsJPmWB/FNnQ7vfwouV+vf7/fDe++ZSuKPP4bERPj1r80IZ337moQgpQEhRCfUZU9drVfewPe/A+vn38LPfgbffnvwN3k88NJLZuyCqVNNvcAjj8CuXfDgg2Zcgz59JCEIITqtLpsUnM5eFJxsI/+Jaea6f04OXHEF7GziLmefD+bMgaFDzchmcXEwdy5s2WJKCImJ7R6/EEKEQ9iSglLqJaVUvlJqfTOvn6CUKlVKrQlMfwpXLE2xWGzExPSj8Hg7bN0Kv/mNGdXsqKPgzjth/35zmWj+fBg2zNxglpgIixaZnksvukiGuxRCRJ1WJQWl1K+UUonKeFEptVopdcpB3vZP4LSDrPOJ1jonMN3bmljaUqgL7dRUc/ln0ya44AKzPGCAaTEUPPgvWGCSwVlnyX0FQoio1dqSwlVa6zLgFCAF+AXwQEtv0FqvAIqPLLzwio0dQHX1FrTW5ok+fcxlom++MTeZxcXVjYl87rmSDIQQUa+1NaLBo+EZwL+01t8p1SZHyGOVUmuBPcDtWuvv2mCbrRYbOxCfrxyPpxCHI73uhVGjTOsiIYToYlpbUlillPovJiksUUolAP4j/OzVQB+t9SjgCeDt5lZUSl2rlFqplFpZUFBwhB9bp1ON1yyEEO2gtUnhamAWMF5rXQXYgSuP5IO11mVa64rA8mLArpTq1sy6s7XW47TW49LT05ta5bDExHSQLrSFEKKDaG1SOBbYpLUuUUpdCtwFlB7JByulegQvQSmlJgRiKTqSbR6q2Nh+gIpsF9pCCNGBtLZO4RlglFJqFPAb4AVgDnB8c29QSs0DTgC6KaVygbsxJQy01s8C5wM3KKW8QDVwoQ7V+LYPi8WJ09lbSgpCCBHQ2qTg1VprpdQ04Emt9YtKqatbeoPW+qKDvP4k8GQrPz9sgi2QhBBCtP7yUblS6neYpqjvKdOtaFTcuRUbO5CaGrl8JIQQ0PqkMAOoxdyvsA/IBh4MW1TtKD5+OB5PIVVVP0Q6FCGEiLhWJYVAIngNSFJKnQXUaK3nhDWydpKefi6gyM+fF+lQhBAi4lrbzcUFwFfAdOAC4Eul1PnhDKy9OJ09SU4+nry8ebRzPbcQQnQ4rb189AfMPQqXa60vAyYAfwxfWO0rI+Miqqs3UVHxTaRDEUKIiGptUrBorfPrPS46hPd2eOnp56GUXS4hCSG6vNYe2N9XSi1RSl2hlLoCeA9YHL6w2pfdnkZq6qnk589H6yPtvUMIITqv1lY03wHMBkYGptla6zvDGVh7y8i4mNraXEpLP410KEIIETGtHjdSa70AWBDGWCKqW7epWCxx5OXNJTn5uEiHI4QQEdFiSUEpVa6UKmtiKldKlbVXkO3Bao2nW7dpFBT8G7/fHelwhBAiIlpMClrrBK11YhNTgtY66gYmzsi4GK+3mP37P4h0KEIIERFR04KoLaSmnoLNlkJenrRCEkJ0TZIU6rFYHKSnT6ew8G18vqpIhyOEEO1OkkIjGRkX4fdXUlS0KNKhCCFEu5Ok0Ehy8hQcjp7k5c2NdChCCNHuJCk0opSVjIwLKS7+Pzye/ZEORwgh2pUkhSZ0734RWnsoKIja2zKEEKJJkhSa4HKNITb2KPLzX4t0KEII0a4kKTRBKUVm5lWUlCynvHx1pMMRQoh2I0mhGVlZN2CzJbNjx/2RDkUIIdqNJIVm2GyJ9Ox5K4WFb1FZ+V2kwxFCiHYhSaEF2dm3YrHEs2PHXyMdihC09nXlAAAgAElEQVRCtAtJCi2w29Po2fNG8vPnU1W1OdLhCCFE2ElSOIjs7NuwWBzs3PlApEMRQoiwk6RwEE5nDzIzryEvbw41NTsjHY4QQoSVJIVW6NXrDkCxc+ffIx2KEEKEVdiSglLqJaVUvlJqfTOvK6XU40qpLUqpb5VSY8IVy5GKielFjx6Xs3fvC9TW7o10OEIIETbhLCn8EzithddPBwYFpmuBZ8IYyxHr3XsWWnvYtevhSIcihBBhE7akoLVeARS3sMo0YI42vgCSlVKZ4YrnSMXGDqB794vZs+cZ3O7CSIcjhBBhYYvgZ/cEdtV7nBt4rsNen+nd+3fk5b1Kbu4/6N9f7nQOJ63B44GqqrrJ4wGfD7xeM/l84PeDwwFOJ8TE1M3tdrO+291w7vWabTc3+f0NH3u9UFNjptraumUAiwWsVjMPTl6v+ZzgFIzTbjexBWN1Os173W6z3eDkdpv1HY6Gk90OStXte/3J7a6bgttwu008NtuBU/14G8ff+DmtG/4NKivN3O028VgsdfPge5uKHeq+j+B35PWaeOp/L8H3NP5eamvNe4KvB79Dp9Nsw+8331tw8vsP/G6Ck9/f9G+u8b4El+v/JusvN/W7aWo7wW003nev1zwf/I7qf2dKNdxucH7iiXDmmW33f9aUSCaFVlNKXYu5xETv3r0jFkd8/NGkp19Abu6j9Ox5E05nVsRiiZSaGti7F4qLYf9+MwWXKyubfk/w4F5d3fAAEzzANj7gVlebbfl87btv0cBqNQdKu70uqQWn5g6GraUUxMVBfHzd9usfsIIH5PpJuDl2uzmYBw+SBxMTY9b3eMxv5VDUP+Da7eY7aqzxvvj9dZNSDb+D+sv1E2Pwtaa2o3XdPttsdcvBk59gwgrun9YNk0tw7nJFd1LYDfSq9zg78NwBtNazgdkA48aN002t01769/8rhYVvs23bXQwZ8lIkQ2lzbjfk5sKuXbBzp5lycxtOhS1cOav/j1GfzWYOJsEpNtZMcXGQkmL+4YNn+U6nOejUXz8+3qxf/5/KajVzpcw/Uv3EEjxbrn8GZrfXvT8YZ+Op/j93cLLbG5ZAgstK1Z2d1j+AWK11nxWcLJa6f/z6JQKv98CzXqez4fr1p8YHluD3ENxGcwe8oODZc/BgVT92n6/uwF7/Oaj7e8TENP33bU79A55SdQfD+mffwbjqfz8eT13JKrhf9T83uN3gd+n11pVurNa6ZZut7qxbtF4kk8K7wM1KqfnAMUCp1rrDXjoKio0dQHb2reza9TA9e95CQsLoSIfUKlpDfj5s324O+sEDf3B51y7Yt69hERkgLQ2ys810zDFmnpVlnk9JgdRUM09JMQdu+QfsuCwWc5BsL0rVnaG3xGKpSwAJCYe23dasLw5N2JKCUmoecALQTSmVC9wN2AG01s8Ci4EzgC1AFXBluGJpa717/4G9e19m69bfMGrUh6gOdCTU2hz4166FjRsbTqWlDdeNj4devcw0YoSZ9+5dN8/ONmeIQoiuI2xJQWt90UFe18BN4fr8cLLbk+nX789s3nwzRUWL6NZtakTj2bEDli2D5cvNfGe9G6+zsmDIELjkEhg8GPr3rzvwJyfLmb0QoqFOUdHcEWVmXsvu3U+ydevtpKaehsXSfuXyggL46CP44AMz37bNPJ+WBiecAHfcARMmmGSQmNhuYQkhooAkhcNksdgZMOAh1q07iz17niU7+9awfVZtLaxYYZLA0qXwzTfm+aQk+OlPYeZMkwyGDz+wEk8IIQ6FJIUjkJp6BikpP2P79nvo3v1S7PbUNtt2RQW8/z4sWADvvQfl5aYVxk9+AvfdBz/7GYwda1pYCCFEW5FDyhFQSjFgwMOsXJnDjh33MXDgI0e0vfJyePttkwiWLDHNK9PT4cILYdo0UxqIj2+b2IUQoimSFI6QyzWSzMyr2b37SbKybiAubtAhvd/vN5eGXn4Z3nzT3NSVnQ3XXgvnnguTJ7fc9lwIIdqSJIU20LfvX8jPn8+WLTMZMeI/rWqiumMHvPIK/POfpqI4MREuvRSuuAImTpRWQUKIyJBqyTbgdPagb997KS5eTEHBv1tcd9UqmD4d+vWDe+4xTURffdV0HfHcc3DssZIQhBCRI0mhjfTseQsu1xi2bPkVHk9Jg9e0NvcPnHIKjBtnWhHNmmVKCEuXmnsI5CYxIURHIEmhjVgsNgYPfh63O58ff5wFmGTw7rvm7P/EE+Hbb+FvfzOXjv76V+jTJ8JBCyFEI5IU2lBCwhiys2eyd+9z/Pe/a5k82bQaKiiAZ54x3U/89rfm/gIhhOiIpKK5jXm993LvvSeybNkoevTQPP+84oor5H4CIUTnICWFNlJUZO4sHjkynq++OpUrrribpUv/zi9/KQlBCNF5SFI4Ql4vPPEEDBxo5ldcAZs327j99o0UFt5NVdUPkQ5RCCFaTZLCEfjwQ8jJgVtvNa2K1q6F2bMhMxMGDnwMiyWGH364Ht14kAIhhOigJCkchm3b4LzzTP9DVVWwcCH897+mQ7ogp7MHAwb8jZKSZezd+2LkghVCiEMgSeEQeL3wl7/A0KGms7r77oMNG+Dss5u+4Swz8xqSk09ky5ZfUVW1qf0DFkKIQyRJoZW2bDH9EP3pT6aZ6aZN8Ic/mHFrm6OUhaFD52CxxLJhw0X4/Yc44rgQQrQzSQoHoTW8+KKpO9i0CebPh9dfN53WtYbT2ZMhQ16kouIbfvzxD+ENVgghjpAkhRYUFpq6g1/+0oxk9u23MGPGoW+nW7dpZGXdSG7uwxQXL2n7QIUQoo1IUmjG8uUwcqQZ4Oahh0wfRb16Hf72Bgx4iPj44Xz//eW43fltFqcQQrQlSQpNWLgQTj3VdEfx1Vfwm98c+TCXVmssQ4fOw+stYePGK6WZqhCiQ5Kk0MicOaZr6zFj4LPPYNSottu2yzWcgQMfprh4Mbt3P952GxZCiDYiHTDU88QT5ka0k04yw2K6XG3/GVlZN1Jc/F+2bv0tSUmTSUgY2/YfIkQno7UmvzKf7SXb2V2+G601VosVm8WGVZm5w+ogPT6djPgMUmNTsajWndO6fW72lu8ltyyX3LJcLMpCdmI22YnZZCZkYrMc/DCotabWV0tZbRnlteWUu8up9lRT66ulxlsTmjw+Dw6rA4fVgdPmNHOrE5vFhlIKhWm7Hly2WWzE2GKItceauc3MHVZHqwbrCgdJCpgWRvfdZ5qbnn02zJvXclPTI6GUYvDgF1m1aizr1k1j7NivcTozw/NhnYTP76PCXUFZbRkV7gpcDhfdXd1xWB2ten+lu5K9FXvZU76HveVmXlpbSkZ8BlkJWWS6MslMyKSHq0eL2yyrLWNz0WY2F29mc9FmtuzfQo23hnh7PPH2eOLsccQ7zFyh8Pq9eP1efNqH1+/Fr/3YLDbsFnvowOCwOrAoC+XuckpqSkJTaW0ple5Ks77Vjt1ibzB3WusOKMEDTPB7qnBXUOGpCC0rFLH2WGJtscTZ40IHFrfPTaWnkgp3RWhe5anCoiyhA23wwGtRFrx+Lx6fB4/fE5prrc027WbbwcmiLFR5qqh0V1LpqQwtB7+D4GS32kPfSf2DpMPqwGaxkVeZx/aS7ews3UmNt6bVvxmrsoYSREpMSpMH0PLacnLLcsmrzGt2OxZloYerBz0TeuK0OXH73Lh9bjw+D26fm1pfLRXuCspry/H4Pa2Ory00/h3ZrXZuGn8TsybPCuvndvmkoDXcfjs88ghcdplpfhruDuwcjm6MGLGI1at/wvr108jJ+RirNTYsn6W1prS2lJ2lO9lVuguLspASm0JKTAqpsakkxyRjt9opqy1ja/FWftz/Y2jaUbqDKk8Vtb5aar3mjKjWV4vH5zEHFosVq7KG5k6bkyRnEskxyaEpyZmEUor91fsprimmuLpuKq0ppay2jEpPZZOxp8Wm0cPVg8yETNLj0vH4PZTVloXO1spqyyipKaHcXd7q7yPBkdDg4OuwOrBb7Oyv2U9+ZcMGAL0SexFnj2tw0Kv1NX+viVVZ8Wlfs69blKXB9xLviKfaW33AgTh4YApOtd5aPH4PCoXL4SLBmYDL4cLlcBFvj0ejKasoo9pbTbWnmipPFTXeGhxWh1nHYZKay+EiLTYNjTbJzO8LJTWPz4PNYiPeEd8gQSmlQtssrSllX8U+qjxV+Py+UIKMt8eT6cok3hGPVVlDydLj9zRINGW1ZdR6axvsW0Z8BiO7j+TnR/2cvsl96ZPUh+zEbKwWa4P4vH4vNd4aCqsKya/MD015lXmU1JQ0WUeXHp/O6B6jQ6WC7MRseib2xK/95Jblsrtsd6j0kFuei9fvxeVw1R2EAwdll8NFojORBEeCmQe+/zh7HDG2mAaTzWLD4/NQ66sN/e3cPncowQJodGjZ4/dQ462h2lNt5oG/Yf3vqP5vYkDKgFb/1g+X6mwVnuPGjdMrV65ss+09+ij8+tdwyy1m+UgrlA9FYeE7rFt3NmnpF9J/0POhH0hpbSmlNaUN5tWe6gOKpA6rA7/2Nzj7DC7vq9jHztKd7CzdedCDZqwtlmpvdYPn0mLT6JPcB5fDhdPqxGlzEmOLwWl1Yrfa8Ws/Pr8Pn/aF5rXe2gYxlNSUUOGuAMzBODU2NTSlxKaQ7EwO/ZMF/+lcDhcV7gr2VuxlX8W+0JRfmY/T5gz9Y4be50gkMyEzVCLISsgiKyGLBGcCBZUF7K3Yy97yvaF5cXVxgwNwcNnlcDEodRCD0gZxVNpRDEgZQKz9wETt9Xup9pjvqvGZNpgkHNxm8B/Zp30kOhOJt8cf9iWB4P9ppC4piM5PKbVKaz3uoOuFMykopU4DHgOswAta6wcavX4F8CCwO/DUk1rrF1raZlsmBY/HjJE8aJDp3O5I/t/82k95bXmDM+HgFDyjyavMM8sVeRRUFZgzb28tmrb7GyQ4EkiKSSIjPoM+SX3ondQ7NGUnZqO1Zn/NfvZX7w/NS2tL6R7fnf4p/UNTUkzbjATk9XvRWmO32ttke0KIw9PapBC2CyVKKSvwFHAykAt8rZR6V2u9odGqr2utbw5XHC156y3IzTWjogUTgs/vY1vJNr4v+J4dpTsoqiqiqLqIwqpCiqqLKKoqotxd3qByqdZb2+JlBYDU2FS6x3enu6s7YzLH0C2uG3H2OBxWB6XF7+KuWkevrCvpljyBRGciSc4kkmKSQvNYW+yBlxV8tQ0uSSQ6E1tVadaeOlo8QoiWhfM/dgKwRWv9I4BSaj4wDWicFCLmkcdryJqygi/jPuWVf3/PxsKN/FD0A26fu8F6Sc4kusV1Iy0ujYz4DAakDghV5gUnp9VJUkySuTQSuF4fvEySHpfe4pmyz/cH1qw5gcrKNxgz5Fe4XG3YDlYIIQ5BOJNCT2BXvce5wDFNrHeeUuo44Afg11rrXU2s0ya01mwq2sT7W97n9VVL+Oqny8Few18/tdA/pT9Dug3h9IGnM7TbUIZ0G0L/lP6kxaWF/WzXao1l+PC3WbVqPOvWTWXMmM9wOnuG9TOFEKIpkS7bLwLmaa1rlVLXAa8AJzZeSSl1LXAtQO/evQ/rgxZvXswN793AztKdACTUDsax4Vrm/eU0Th18HPGO+MPdhzbhdGYyYsS7rFlzPGvX/oycnBU4HOkRjUkI0fWEs63NbqB+b0HZ1FUoA6C1LtJaBy/GvwA0eSeX1nq21nqc1npcevrhHSizErIYkzmGZ898lv9N30bV3zdy66DHOHfE6RFPCEEJCWMYMeI9amp28O23p+DxlEQ6JCFEFxPOpPA1MEgp1U8p5QAuBN6tv4JSqv5dW1OB78MVTE6PHBbOWMh1467jnVf6ojXcHJHq7ZYlJx/H8OELqaz8jnXrTsfrrYh0SEKILiRsSUFr7QVuBpZgDvZvaK2/U0rdq5SaGljtVqXUd0qptcCtwBXhiieostKMo3zeedCnT7g/7fCkpp7K0Ue/TlnZ16xfPw2fr/V3egohxJHocjevPfMM3Hgj/O9/8JOftGFgYbBv36ts3HgZqalnMHz4W1gsrev2QQghGmvtfQpdqpdUv9/ctTx+PBx7bKSjObgePS7lqKOeobj4PTZsuBifr/rgbxJCiCPQpZLC++/DDz/AzJlHdvdye8rKuo4BAx6hsHABq1cfS1XVlkiHJISIYl0qKTz6KGRlmfESOpNevX7NiBH/obZ2J6tWjaWgYGGkQxJCRKkukxTWr4cPPjAtjuydsBuetLQzGTfuG+LiBvPdd+eydesd+Nu5K18hRPTrMklh924YMgSuvTbSkRy+mJg+jB79CVlZN7Fr10OsXXsitbV7Ih2WECKKdJmkcOqpsGEDpKVFOpIjY7E4OeqoJxk69DXKy79h5cociouXRjosIUSU6DJJATpP5XJrdO9+MWPHfo3dnsG3357C9u1/RrcwwIsQQrRGl0oK0SY+fihjx35J9+6/YPv2e/j229Nwu/MP/kYhhGiGJIVOzmqNZ8iQfzJ48IuUln7KypU5lJSsiHRYQohOSpJCFFBKkZl5FWPGfInV6mLNmhPZtu1ufL6qSIcmhOhkJClEEZdrJGPHrqR794vYseNevvpqKPn5rzc5qLkQQjRFkkKUsdkSGTr0X+TkfIzdnsqGDReyZs1xlJevjnRoQohOQJJClEpOPo6xY1dy1FGzqaraxKpV49i06RrpJkMI0aJIj7wmwkgpK1lZ15CRcQHbt/+F3bsfZ+/eF4iLG0Ja2lmkpf2cxMSfYAnzcKNCiM6jy3Wd3ZXV1OyisPBtiooWUVKyHK092GyppKaeTmbm1SQnn4CKpps5hBAhre06W5JCF+X1llFc/F+Kiv5DUdEivN5iXK6x9O59B926nSelByGiTGuTgvznd1E2WyIZGeeTkXE+Pl81eXlz2LXrYTZsuJCYmL5kZ/+azMyrsVo7xvjVQoj2IRXNAqs1lqys65gwYSPDhi3E4chiy5Zf8fnnvfjhh5soLf1cmrUK0UVISUGEKGUhPf1s0tPPprT0M3bvfoJ9+15iz56niYkZQPful9K9+yXExQ2KdKhCiDCROgXRIq+3jIKCt8jLe5WSko8ATULCeJKTf0pS0mSSkiZht6dGOkwhxEFIRbNoc7W1u8nLm0dh4ULKy79GazPIT1zcMJKSJuNyjcJicaKUPTDZsFjsOByZuFw5WCyOCO+BEF2XJAURVj5fNeXlX1Na+imlpZ9QWvoZPl9Zs+tbLLEkJEwIlS4SE4/Fbk9ux4iF6Nqk9ZEIK6s1luTk40hOPg4ArX243Xlo7cHv96B13VRdvY2ysv9RWvopO3c+APgAhd2egc2WiNWagNWaEFhOJC7uKBISJpCYOB67vZOPiiREJyNJQbQJpaw4nVlNvpaQMJaMjPMB8PkqKSv7ktLS/1Fbm4vPV47XW4bPV05tbS5ebwn5+XMBU4KNiRlAYuIEEhLGExs7iJiYXjidvbHZkg+40c7rNdsw2ynF4UjH4eiBw9EDqzVRbswTohUkKYh2ZbXGk5JyIikpJza7jtdbTnn5SsrLv6Ks7CtKSz8hP39eo+24cDp7Ybdn4PEUBBJMy5evHI4e2O1pWCwxB0xgRWsv4ENrL1r70NqHw5FBXNww4uOPJj5+GA5HliQXEdWkTkF0Cm53HjU126mp2Ult7a7Q3OPJx27PwOnsidOZHZpstmQ8ngLc7n243fuord2L270Pr7cIv78Wv7+m3lSL1l6UsgUmK0rZAAtu9248nsJQHDZbMnFxQ7FaEwCN1n5MqUYHXk/B4cjAbk/Hbk/H4cjAak3C6y3B4ymsNxXg99fgcPTA6czC4egZmGdhsyXg9Zbj85XVK0mVAQqbLRW7PbXB3GqNP6JEpbUPv9+Nz1eG11uG11uKz1eK12uSbHz8CGJj+6NU+G9r0lrj85VjtSYcdJ98vhoqK9fjdu/Gbs+oVyqMDXucnVGHqFNQSp0GPAZYgRe01g80et0JzAHGAkXADK319nDGJDonh6M7Dkd3EhOPaffPdrsLqKz8jqqq7wLz7/F6SwMHSQWowLKmqmojpaWfBBJJ0ydc5oDeDYslhrKyL/B4jmwIVYslJnRAtNu7B5Yz8Ptr8HiK8HiK8HqLAsvFaF3boN6nuTjrs1oTcblySEgYg8s1mpiYvoESlQetvQ22Z5JuLVq7Q3OTPBvz4/EUUlu7B7d7T2C+F609gbqlwcTGHkVc3GDi4gZjt2dQWbmeiorVlJevpqrqu0DprnGsSTidmTid2cTGDgxNMTEDiI3tj9UaB5gEVH8fzOTHlBb9gTHPTcIMnkiY764WrT1YLHFYra5AfVgCVqsLiyUGn68Kn68Sn68Cv9/MtfYF/u5p2O2pWCzOULxaa7ze/bjde0MnL+Y7iMNiicViiQ0tOxxZOJ09Du+H0kphKykopazAD8DJQC7wNXCR1npDvXVuBEZqra9XSl0InKO1ntHSdqWkIDoDrX14PMV4PPl4vaXYbMnY7enYbCkH9Cvl97sDpZnduN178PkqsFoTQxXvwUp4rf14vfvxeovxeIoD86J6JaK8UMnI4ylEKUfgIJSG3d4Nuz0Nmy21QbNhiyXYfNgR+jybLQmbLQmrNRGtPVRUrKWi4hsqKlZTUbEWv7+6zb4nmy05cKDLwuHIxOHIwm5PpaZmJ9XVm6iq2kRt7a4G77Hbu+FyjSUhYQwJCWNxOnvj8RQG9n1v6DuoqdlBdfVWvN6iBu9Xyhm6VBgpFkt8qBGFSQLuVr2vV6/fMmDA3w7rMztCSWECsEVr/WMgoPnANGBDvXWmAfcElt8EnlRKKd3ZrmkJ0YhS1kBFd/pB17VYHMTE9CYmpncrtpzdqs83Z7mWNqn/SEwc32C75kC9p15CsQcuuwWTjBOLxRlIPg4sFgfmHPFAzT1fn89XSXX1FtzufcTFDcPp7HlI++Xx7Ke6ekto8vkqQvEG76UJXjoEK0pZAnFZApcSHaH9MXVQTpSy4fNV4/OV4/NVhOZ+fzUWSzxWa3ygFOEK9B9mqZfM60pt4A8kw0yczsxAKS8TpRz4/dX4/VX4fGbu91cTE9O/1ft9uMKZFHoC9VN8LtC47B9aR2vtVUqVAmlAIUKIw9aag+3hbtdUuh8dlu03xWqNx+UaBYw6rPfb7SnY7eMbJDfRvE7RIZ5S6lql1Eql1MqCgoJIhyOEEFErnElhN9Cr3uPswHNNrqNM2S0JU+HcgNZ6ttZ6nNZ6XHr6wYvjQgghDk84k8LXwCClVD+llAO4EHi30TrvApcHls8HPpL6BCGEiJyw1SkE6ghuBpZgmqS+pLX+Til1L7BSa/0u8CLwL6XUFqAYkziEEEJESFjvU9BaLwYWN3ruT/WWa4Dp4YxBCCFE63WKimYhhBDtQ5KCEEKIEEkKQgghQjpdh3hKqQJgx2G+vRtd58a4rrKvXWU/QfY1GrXnfvbRWh+0TX+nSwpHQim1sjV9f0SDrrKvXWU/QfY1GnXE/ZTLR0IIIUIkKQghhAjpaklhdqQDaEddZV+7yn6C7Gs06nD72aXqFIQQQrSsq5UUhBBCtKDLJAWl1GlKqU1KqS1KqVmRjqctKaVeUkrlK6XW13suVSn1gVJqc2CeEskY24JSqpdSaplSaoNS6jul1K8Cz0fVviqlYpRSXyml1gb288+B5/sppb4M/IZfD3Q0GRWUUlal1DdKqf8EHkflviqltiul1iml1iilVgae61C/3y6RFAJDgz4FnA4cDVyklGq/UULC75/AaY2emwV8qLUeBHwYeNzZeYHfaK2PBiYCNwX+jtG2r7XAiVrrUUAOcJpSaiLwN+AfWuuBwH7g6gjG2NZ+BXxf73E07+tPtdY59Zqidqjfb5dICtQbGlSbwVCDQ4NGBa31Ckwvs/VNA14JLL8CnN2uQYWB1nqv1np1YLkccxDpSZTtqzYqAg/tgUkDJ2KGrYUo2M8gpVQ2cCbwQuCxIkr3tRkd6vfbVZJCU0OD9oxQLO2lu9Z6b2B5H9A9ksG0NaVUX2A08CVRuK+ByylrgHzgA2ArUKLNiPMQXb/hR4HfAv7A4zSid1818F+l1Cql1LWB5zrU7zesXWeLjkFrrZVSUdPMTCnlAhYAM7XWZfUHcY+WfdVa+4AcpVQysBAYEuGQwkIpdRaQr7VepZQ6IdLxtIPJWuvdSqkM4AOl1Mb6L3aE329XKSm0ZmjQaJOnlMoECMzzIxxPm1BK2TEJ4TWt9VuBp6NyXwG01iXAMuBYIDkwbC1Ez294EjBVKbUdc1n3ROAxonNf0VrvDszzMcl+Ah3s99tVkkJrhgaNNvWHOr0ceCeCsbSJwLXmF4HvtdaP1HspqvZVKZUeKCGglIoFTsbUnyzDDFsLUbCfAFrr32mts7XWfTH/lx9prS8hCvdVKRWvlEoILgOnAOvpYL/fLnPzmlLqDMy1y+DQoPdHOKQ2o5SaB5yA6XExD7gbeBt4A+iN6VX2Aq1148roTkUpNRn4BFhH3fXn32PqFaJmX5VSIzEVjlbMidsbWut7lVL9MWfTqcA3wKVa69rIRdq2ApePbtdanxWN+xrYp4WBhzZgrtb6fqVUGh3o99tlkoIQQoiD6yqXj4QQQrSCJAUhhBAhkhSEEEKESFIQQggRIklBCCFEiCQFIdqRUuqEYE+gQnREkhSEEEKESFIQoglKqUsDYxqsUUo9F+igrkIp9Y/AGAcfKqXSA+vmKKW+UEp9q5RaGOwPXyk1UCm1NDAuwmql1IDA5l1KqTeVUhuVUq+p+p03CRFhkhSEaEQpNRSYAUzSWucAPuASIB5YqbUeBnyMuXMcYA5wp9Z6JOZu6+DzrwFPBcZF+AkQ7AlzNDATM7ZHf0z/P0J0CNJLqhAHOgkYC3wdOImPxXRS5gdeD6zzKvCWUioJSNZafxx4/hXg34E+bnpqrRcCaK1rAALb+0prnRt4vAboC3wa/t0S4uAkKQhxIAW8opsbdGAAAADTSURBVLX+XYMnlfpjo/UOt4+Y+n34+JD/Q9GByOUjIQ70IXB+oM/74Bi6fTD/L8GeOy8GPtValwL7lVJTAs//Avg4MDJcrlLq7MA2nEqpuHbdCyEOg5yhCNGI1nqDUuouzAhZFsAD3ARUAhMCr+Vj6h3AdHf8bOCg/yNwZeD5XwDPKaXuDWxjejvuhhCHRXpJFaKVlFIVWmtXpOMQIpzk8pEQQogQKSkIIYQIkZKCEEKIEEkKQgghQiQpCCGECJGkIIQQIkSSghBCiBBJCkIIIUL+P4kv5ttQuITOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 536us/sample - loss: 1.5866 - acc: 0.4987\n",
      "Loss: 1.5865661711955243 Accuracy: 0.49865004\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8428 - acc: 0.4080\n",
      "Epoch 00001: val_loss improved from inf to 1.47112, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/001-1.4711.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.8427 - acc: 0.4081 - val_loss: 1.4711 - val_acc: 0.5425\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3299 - acc: 0.5908\n",
      "Epoch 00002: val_loss improved from 1.47112 to 1.28120, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/002-1.2812.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3299 - acc: 0.5908 - val_loss: 1.2812 - val_acc: 0.6182\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1235 - acc: 0.6636\n",
      "Epoch 00003: val_loss improved from 1.28120 to 1.22590, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/003-1.2259.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1234 - acc: 0.6637 - val_loss: 1.2259 - val_acc: 0.6250\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9842 - acc: 0.7078\n",
      "Epoch 00004: val_loss improved from 1.22590 to 1.19797, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/004-1.1980.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9842 - acc: 0.7078 - val_loss: 1.1980 - val_acc: 0.6308\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.7455\n",
      "Epoch 00005: val_loss did not improve from 1.19797\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8681 - acc: 0.7455 - val_loss: 1.2318 - val_acc: 0.6308\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7764\n",
      "Epoch 00006: val_loss did not improve from 1.19797\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7672 - acc: 0.7764 - val_loss: 1.2063 - val_acc: 0.6417\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6741 - acc: 0.8075\n",
      "Epoch 00007: val_loss improved from 1.19797 to 1.19774, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/007-1.1977.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6741 - acc: 0.8075 - val_loss: 1.1977 - val_acc: 0.6401\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5991 - acc: 0.8317\n",
      "Epoch 00008: val_loss did not improve from 1.19774\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5992 - acc: 0.8317 - val_loss: 1.2274 - val_acc: 0.6382\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.8542\n",
      "Epoch 00009: val_loss improved from 1.19774 to 1.19682, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_4_conv_checkpoint/009-1.1968.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5327 - acc: 0.8542 - val_loss: 1.1968 - val_acc: 0.6483\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8763\n",
      "Epoch 00010: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4683 - acc: 0.8763 - val_loss: 1.2641 - val_acc: 0.6413\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8959\n",
      "Epoch 00011: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4122 - acc: 0.8959 - val_loss: 1.2173 - val_acc: 0.6564\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.9095\n",
      "Epoch 00012: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3655 - acc: 0.9095 - val_loss: 1.2431 - val_acc: 0.6578\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.9228\n",
      "Epoch 00013: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3231 - acc: 0.9228 - val_loss: 1.2325 - val_acc: 0.6632\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9315\n",
      "Epoch 00014: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2919 - acc: 0.9315 - val_loss: 1.2618 - val_acc: 0.6625\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9427\n",
      "Epoch 00015: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2554 - acc: 0.9427 - val_loss: 1.2753 - val_acc: 0.6643\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9500\n",
      "Epoch 00016: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2299 - acc: 0.9500 - val_loss: 1.2845 - val_acc: 0.6650\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9573\n",
      "Epoch 00017: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2072 - acc: 0.9573 - val_loss: 1.3098 - val_acc: 0.6697\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9621\n",
      "Epoch 00018: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1866 - acc: 0.9621 - val_loss: 1.3066 - val_acc: 0.6681\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9675\n",
      "Epoch 00019: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1685 - acc: 0.9675 - val_loss: 1.3412 - val_acc: 0.6723\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9736\n",
      "Epoch 00020: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1479 - acc: 0.9736 - val_loss: 1.3702 - val_acc: 0.6692\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9762\n",
      "Epoch 00021: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1352 - acc: 0.9762 - val_loss: 1.3843 - val_acc: 0.6667\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9789\n",
      "Epoch 00022: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1246 - acc: 0.9789 - val_loss: 1.3765 - val_acc: 0.6739\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9817\n",
      "Epoch 00023: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1118 - acc: 0.9817 - val_loss: 1.4014 - val_acc: 0.6751\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9831\n",
      "Epoch 00024: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1088 - acc: 0.9831 - val_loss: 1.4340 - val_acc: 0.6713\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9857\n",
      "Epoch 00025: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0971 - acc: 0.9857 - val_loss: 1.4451 - val_acc: 0.6718\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9852\n",
      "Epoch 00026: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0951 - acc: 0.9852 - val_loss: 1.4684 - val_acc: 0.6720\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9870\n",
      "Epoch 00027: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0860 - acc: 0.9870 - val_loss: 1.4520 - val_acc: 0.6769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9884\n",
      "Epoch 00028: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0784 - acc: 0.9884 - val_loss: 1.4770 - val_acc: 0.6758\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9884\n",
      "Epoch 00029: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0777 - acc: 0.9884 - val_loss: 1.4934 - val_acc: 0.6776\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9913\n",
      "Epoch 00030: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0683 - acc: 0.9913 - val_loss: 1.5051 - val_acc: 0.6748\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9914\n",
      "Epoch 00031: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0655 - acc: 0.9914 - val_loss: 1.5198 - val_acc: 0.6797\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9908\n",
      "Epoch 00032: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0640 - acc: 0.9908 - val_loss: 1.5124 - val_acc: 0.6795\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9917\n",
      "Epoch 00033: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0608 - acc: 0.9917 - val_loss: 1.5604 - val_acc: 0.6732\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9932\n",
      "Epoch 00034: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0546 - acc: 0.9932 - val_loss: 1.5698 - val_acc: 0.6806\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9922\n",
      "Epoch 00035: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0564 - acc: 0.9922 - val_loss: 1.6592 - val_acc: 0.6795\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9919\n",
      "Epoch 00036: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0567 - acc: 0.9919 - val_loss: 1.6053 - val_acc: 0.6783\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9936\n",
      "Epoch 00037: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0498 - acc: 0.9936 - val_loss: 1.5870 - val_acc: 0.6765\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9944\n",
      "Epoch 00038: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0459 - acc: 0.9944 - val_loss: 1.6141 - val_acc: 0.6739\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9937\n",
      "Epoch 00039: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0473 - acc: 0.9937 - val_loss: 1.6274 - val_acc: 0.6846\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9947\n",
      "Epoch 00040: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0441 - acc: 0.9947 - val_loss: 1.6226 - val_acc: 0.6839\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9939\n",
      "Epoch 00041: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0437 - acc: 0.9939 - val_loss: 1.6215 - val_acc: 0.6862\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9950\n",
      "Epoch 00042: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0415 - acc: 0.9950 - val_loss: 1.6505 - val_acc: 0.6809\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9947\n",
      "Epoch 00043: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0400 - acc: 0.9947 - val_loss: 1.6763 - val_acc: 0.6862\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9937\n",
      "Epoch 00044: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0439 - acc: 0.9937 - val_loss: 1.6659 - val_acc: 0.6867\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9954\n",
      "Epoch 00045: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0375 - acc: 0.9954 - val_loss: 1.6610 - val_acc: 0.6848\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9949\n",
      "Epoch 00046: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0373 - acc: 0.9949 - val_loss: 1.6899 - val_acc: 0.6820\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9949\n",
      "Epoch 00047: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0379 - acc: 0.9949 - val_loss: 1.6845 - val_acc: 0.6860\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9955\n",
      "Epoch 00048: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0356 - acc: 0.9955 - val_loss: 1.6900 - val_acc: 0.6881\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9943\n",
      "Epoch 00049: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0411 - acc: 0.9943 - val_loss: 1.7056 - val_acc: 0.6797\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9963\n",
      "Epoch 00050: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0315 - acc: 0.9963 - val_loss: 1.7178 - val_acc: 0.6869\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0351 - acc: 0.9948 - val_loss: 1.7454 - val_acc: 0.6795\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9960\n",
      "Epoch 00052: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0308 - acc: 0.9960 - val_loss: 1.7391 - val_acc: 0.6874\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9957\n",
      "Epoch 00053: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0334 - acc: 0.9957 - val_loss: 1.7575 - val_acc: 0.6883\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9961\n",
      "Epoch 00054: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0309 - acc: 0.9961 - val_loss: 1.7552 - val_acc: 0.6820\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0342 - acc: 0.9955 - val_loss: 1.7643 - val_acc: 0.6858\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9957\n",
      "Epoch 00056: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0301 - acc: 0.9957 - val_loss: 1.7387 - val_acc: 0.6907\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9958\n",
      "Epoch 00057: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0308 - acc: 0.9958 - val_loss: 1.7874 - val_acc: 0.6851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9958\n",
      "Epoch 00058: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0312 - acc: 0.9958 - val_loss: 1.8210 - val_acc: 0.6739\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 1.19682\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0318 - acc: 0.9956 - val_loss: 1.8056 - val_acc: 0.6825\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 569us/sample - loss: 1.2426 - acc: 0.6278\n",
      "Loss: 1.2425976790619293 Accuracy: 0.6278297\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8096 - acc: 0.4205\n",
      "Epoch 00001: val_loss improved from inf to 1.47831, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/001-1.4783.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.8096 - acc: 0.4205 - val_loss: 1.4783 - val_acc: 0.5344\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3702 - acc: 0.5798\n",
      "Epoch 00002: val_loss improved from 1.47831 to 1.24806, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/002-1.2481.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3704 - acc: 0.5798 - val_loss: 1.2481 - val_acc: 0.6296\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1944 - acc: 0.6414\n",
      "Epoch 00003: val_loss improved from 1.24806 to 1.15143, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/003-1.1514.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1943 - acc: 0.6414 - val_loss: 1.1514 - val_acc: 0.6560\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.6811\n",
      "Epoch 00004: val_loss improved from 1.15143 to 1.08133, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/004-1.0813.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0697 - acc: 0.6811 - val_loss: 1.0813 - val_acc: 0.6723\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9786 - acc: 0.7126\n",
      "Epoch 00005: val_loss improved from 1.08133 to 1.03422, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/005-1.0342.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9786 - acc: 0.7126 - val_loss: 1.0342 - val_acc: 0.6862\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9002 - acc: 0.7373\n",
      "Epoch 00006: val_loss improved from 1.03422 to 0.99247, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/006-0.9925.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9001 - acc: 0.7373 - val_loss: 0.9925 - val_acc: 0.6976\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8265 - acc: 0.7611\n",
      "Epoch 00007: val_loss improved from 0.99247 to 0.98710, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/007-0.9871.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8266 - acc: 0.7610 - val_loss: 0.9871 - val_acc: 0.7077\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7806\n",
      "Epoch 00008: val_loss improved from 0.98710 to 0.94458, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/008-0.9446.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7617 - acc: 0.7806 - val_loss: 0.9446 - val_acc: 0.7156\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7022 - acc: 0.7986\n",
      "Epoch 00009: val_loss improved from 0.94458 to 0.92729, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/009-0.9273.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7023 - acc: 0.7986 - val_loss: 0.9273 - val_acc: 0.7268\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.8149\n",
      "Epoch 00010: val_loss improved from 0.92729 to 0.89338, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/010-0.8934.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6562 - acc: 0.8149 - val_loss: 0.8934 - val_acc: 0.7372\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8308\n",
      "Epoch 00011: val_loss improved from 0.89338 to 0.88490, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/011-0.8849.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6012 - acc: 0.8308 - val_loss: 0.8849 - val_acc: 0.7382\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5531 - acc: 0.8468\n",
      "Epoch 00012: val_loss did not improve from 0.88490\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5532 - acc: 0.8468 - val_loss: 0.9039 - val_acc: 0.7349\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8601\n",
      "Epoch 00013: val_loss did not improve from 0.88490\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5117 - acc: 0.8600 - val_loss: 0.8885 - val_acc: 0.7470\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4740 - acc: 0.8711\n",
      "Epoch 00014: val_loss improved from 0.88490 to 0.85569, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/014-0.8557.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4740 - acc: 0.8711 - val_loss: 0.8557 - val_acc: 0.7531\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8845\n",
      "Epoch 00015: val_loss did not improve from 0.85569\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4342 - acc: 0.8845 - val_loss: 0.8725 - val_acc: 0.7559\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8924\n",
      "Epoch 00016: val_loss did not improve from 0.85569\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4039 - acc: 0.8924 - val_loss: 0.8730 - val_acc: 0.7529\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.9031\n",
      "Epoch 00017: val_loss did not improve from 0.85569\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3703 - acc: 0.9031 - val_loss: 0.8701 - val_acc: 0.7591\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.9148\n",
      "Epoch 00018: val_loss did not improve from 0.85569\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3398 - acc: 0.9148 - val_loss: 0.8598 - val_acc: 0.7568\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.9236\n",
      "Epoch 00019: val_loss improved from 0.85569 to 0.85215, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_5_conv_checkpoint/019-0.8521.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3124 - acc: 0.9237 - val_loss: 0.8521 - val_acc: 0.7615\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9299\n",
      "Epoch 00020: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2887 - acc: 0.9299 - val_loss: 0.8749 - val_acc: 0.7561\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9371\n",
      "Epoch 00021: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2666 - acc: 0.9372 - val_loss: 0.8699 - val_acc: 0.7591\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9464\n",
      "Epoch 00022: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2417 - acc: 0.9464 - val_loss: 0.8912 - val_acc: 0.7584\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9516\n",
      "Epoch 00023: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2236 - acc: 0.9516 - val_loss: 0.9127 - val_acc: 0.7570\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9551\n",
      "Epoch 00024: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2076 - acc: 0.9551 - val_loss: 0.9115 - val_acc: 0.7573\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9613\n",
      "Epoch 00025: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1897 - acc: 0.9613 - val_loss: 0.9101 - val_acc: 0.7591\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9660\n",
      "Epoch 00026: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1738 - acc: 0.9660 - val_loss: 0.9012 - val_acc: 0.7582\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9681\n",
      "Epoch 00027: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1634 - acc: 0.9681 - val_loss: 0.9123 - val_acc: 0.7626\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9726\n",
      "Epoch 00028: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1503 - acc: 0.9725 - val_loss: 0.9916 - val_acc: 0.7538\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9735\n",
      "Epoch 00029: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1405 - acc: 0.9735 - val_loss: 0.9373 - val_acc: 0.7594\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9773\n",
      "Epoch 00030: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1312 - acc: 0.9773 - val_loss: 0.9347 - val_acc: 0.7668\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9805\n",
      "Epoch 00031: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1171 - acc: 0.9805 - val_loss: 0.9488 - val_acc: 0.7610\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9812\n",
      "Epoch 00032: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1121 - acc: 0.9813 - val_loss: 0.9667 - val_acc: 0.7638\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9835\n",
      "Epoch 00033: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1030 - acc: 0.9835 - val_loss: 0.9558 - val_acc: 0.7633\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9848\n",
      "Epoch 00034: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0963 - acc: 0.9848 - val_loss: 0.9928 - val_acc: 0.7552\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9857\n",
      "Epoch 00035: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0941 - acc: 0.9857 - val_loss: 0.9772 - val_acc: 0.7666\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9878\n",
      "Epoch 00036: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0842 - acc: 0.9878 - val_loss: 1.0017 - val_acc: 0.7633\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9896\n",
      "Epoch 00037: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0795 - acc: 0.9896 - val_loss: 1.0185 - val_acc: 0.7603\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9889\n",
      "Epoch 00038: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0772 - acc: 0.9889 - val_loss: 1.0280 - val_acc: 0.7659\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9909\n",
      "Epoch 00039: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0732 - acc: 0.9909 - val_loss: 1.0333 - val_acc: 0.7631\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9907\n",
      "Epoch 00040: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0652 - acc: 0.9907 - val_loss: 1.0666 - val_acc: 0.7580\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9924\n",
      "Epoch 00041: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0644 - acc: 0.9924 - val_loss: 1.0227 - val_acc: 0.7671\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9914\n",
      "Epoch 00042: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0637 - acc: 0.9914 - val_loss: 1.0473 - val_acc: 0.7615\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9915\n",
      "Epoch 00043: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0602 - acc: 0.9915 - val_loss: 1.0540 - val_acc: 0.7680\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9928\n",
      "Epoch 00044: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0567 - acc: 0.9928 - val_loss: 1.0734 - val_acc: 0.7631\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9924\n",
      "Epoch 00045: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0574 - acc: 0.9924 - val_loss: 1.0694 - val_acc: 0.7689\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9941\n",
      "Epoch 00046: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0497 - acc: 0.9941 - val_loss: 1.0867 - val_acc: 0.7687\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9946\n",
      "Epoch 00047: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0461 - acc: 0.9946 - val_loss: 1.0941 - val_acc: 0.7680\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9940\n",
      "Epoch 00048: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0486 - acc: 0.9940 - val_loss: 1.0991 - val_acc: 0.7647\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9937\n",
      "Epoch 00049: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0496 - acc: 0.9937 - val_loss: 1.0928 - val_acc: 0.7678\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9949\n",
      "Epoch 00050: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0435 - acc: 0.9949 - val_loss: 1.0973 - val_acc: 0.7643\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9944\n",
      "Epoch 00051: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0434 - acc: 0.9944 - val_loss: 1.0937 - val_acc: 0.7673\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9951\n",
      "Epoch 00052: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0393 - acc: 0.9951 - val_loss: 1.1373 - val_acc: 0.7664\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9946\n",
      "Epoch 00053: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0406 - acc: 0.9946 - val_loss: 1.1252 - val_acc: 0.7694\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9958\n",
      "Epoch 00054: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0397 - acc: 0.9958 - val_loss: 1.1734 - val_acc: 0.7666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9950\n",
      "Epoch 00055: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0403 - acc: 0.9950 - val_loss: 1.1322 - val_acc: 0.7661\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9962\n",
      "Epoch 00056: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0337 - acc: 0.9963 - val_loss: 1.1805 - val_acc: 0.7638\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9952\n",
      "Epoch 00057: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0364 - acc: 0.9952 - val_loss: 1.1623 - val_acc: 0.7671\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0332 - acc: 0.9961 - val_loss: 1.1607 - val_acc: 0.7647\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0351 - acc: 0.9956 - val_loss: 1.2010 - val_acc: 0.7631\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9956\n",
      "Epoch 00060: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0343 - acc: 0.9956 - val_loss: 1.1882 - val_acc: 0.7666\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9967\n",
      "Epoch 00061: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0309 - acc: 0.9967 - val_loss: 1.2186 - val_acc: 0.7610\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9964\n",
      "Epoch 00062: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0306 - acc: 0.9964 - val_loss: 1.1970 - val_acc: 0.7626\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9962\n",
      "Epoch 00063: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0301 - acc: 0.9962 - val_loss: 1.1807 - val_acc: 0.7706\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9968\n",
      "Epoch 00064: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0300 - acc: 0.9968 - val_loss: 1.1610 - val_acc: 0.7768\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9968\n",
      "Epoch 00065: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0283 - acc: 0.9968 - val_loss: 1.2291 - val_acc: 0.7671\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9960\n",
      "Epoch 00066: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0313 - acc: 0.9960 - val_loss: 1.2221 - val_acc: 0.7675\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9967\n",
      "Epoch 00067: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0278 - acc: 0.9967 - val_loss: 1.2157 - val_acc: 0.7652\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9962\n",
      "Epoch 00068: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0295 - acc: 0.9962 - val_loss: 1.1857 - val_acc: 0.7715\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9968\n",
      "Epoch 00069: val_loss did not improve from 0.85215\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0278 - acc: 0.9968 - val_loss: 1.2263 - val_acc: 0.7654\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 582us/sample - loss: 0.9325 - acc: 0.7310\n",
      "Loss: 0.9325461771134151 Accuracy: 0.7310488\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9048 - acc: 0.3830\n",
      "Epoch 00001: val_loss improved from inf to 1.49195, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/001-1.4919.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.9046 - acc: 0.3830 - val_loss: 1.4919 - val_acc: 0.5353\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4427 - acc: 0.5533\n",
      "Epoch 00002: val_loss improved from 1.49195 to 1.26672, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/002-1.2667.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4426 - acc: 0.5533 - val_loss: 1.2667 - val_acc: 0.6164\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2510 - acc: 0.6259\n",
      "Epoch 00003: val_loss improved from 1.26672 to 1.11371, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/003-1.1137.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.2509 - acc: 0.6259 - val_loss: 1.1137 - val_acc: 0.6725\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1093 - acc: 0.6744\n",
      "Epoch 00004: val_loss improved from 1.11371 to 1.03699, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/004-1.0370.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.1092 - acc: 0.6744 - val_loss: 1.0370 - val_acc: 0.7035\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9977 - acc: 0.7058\n",
      "Epoch 00005: val_loss improved from 1.03699 to 0.90567, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/005-0.9057.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9976 - acc: 0.7058 - val_loss: 0.9057 - val_acc: 0.7389\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9166 - acc: 0.7334\n",
      "Epoch 00006: val_loss improved from 0.90567 to 0.84058, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/006-0.8406.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.9165 - acc: 0.7334 - val_loss: 0.8406 - val_acc: 0.7615\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8508 - acc: 0.7481\n",
      "Epoch 00007: val_loss improved from 0.84058 to 0.79736, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/007-0.7974.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8507 - acc: 0.7481 - val_loss: 0.7974 - val_acc: 0.7766\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8029 - acc: 0.7640\n",
      "Epoch 00008: val_loss improved from 0.79736 to 0.77069, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/008-0.7707.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.8029 - acc: 0.7640 - val_loss: 0.7707 - val_acc: 0.7736\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7544 - acc: 0.7787\n",
      "Epoch 00009: val_loss improved from 0.77069 to 0.72792, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/009-0.7279.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7543 - acc: 0.7787 - val_loss: 0.7279 - val_acc: 0.7990\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7903\n",
      "Epoch 00010: val_loss improved from 0.72792 to 0.70003, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/010-0.7000.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7215 - acc: 0.7904 - val_loss: 0.7000 - val_acc: 0.7976\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6839 - acc: 0.8013\n",
      "Epoch 00011: val_loss improved from 0.70003 to 0.69003, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/011-0.6900.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6840 - acc: 0.8012 - val_loss: 0.6900 - val_acc: 0.8025\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6596 - acc: 0.8083\n",
      "Epoch 00012: val_loss improved from 0.69003 to 0.65706, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/012-0.6571.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6595 - acc: 0.8083 - val_loss: 0.6571 - val_acc: 0.8120\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.8156\n",
      "Epoch 00013: val_loss improved from 0.65706 to 0.64432, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/013-0.6443.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6286 - acc: 0.8156 - val_loss: 0.6443 - val_acc: 0.8153\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6039 - acc: 0.8227\n",
      "Epoch 00014: val_loss improved from 0.64432 to 0.62977, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/014-0.6298.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.6041 - acc: 0.8227 - val_loss: 0.6298 - val_acc: 0.8190\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5774 - acc: 0.8314\n",
      "Epoch 00015: val_loss improved from 0.62977 to 0.62156, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/015-0.6216.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5775 - acc: 0.8314 - val_loss: 0.6216 - val_acc: 0.8204\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.8380\n",
      "Epoch 00016: val_loss improved from 0.62156 to 0.59773, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/016-0.5977.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5575 - acc: 0.8381 - val_loss: 0.5977 - val_acc: 0.8337\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8455\n",
      "Epoch 00017: val_loss did not improve from 0.59773\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5370 - acc: 0.8455 - val_loss: 0.5979 - val_acc: 0.8318\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8515\n",
      "Epoch 00018: val_loss improved from 0.59773 to 0.57809, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/018-0.5781.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5163 - acc: 0.8515 - val_loss: 0.5781 - val_acc: 0.8314\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4967 - acc: 0.8558\n",
      "Epoch 00019: val_loss did not improve from 0.57809\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4968 - acc: 0.8558 - val_loss: 0.5811 - val_acc: 0.8348\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.8603\n",
      "Epoch 00020: val_loss improved from 0.57809 to 0.56931, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/020-0.5693.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4812 - acc: 0.8603 - val_loss: 0.5693 - val_acc: 0.8393\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8649\n",
      "Epoch 00021: val_loss improved from 0.56931 to 0.54249, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/021-0.5425.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4642 - acc: 0.8649 - val_loss: 0.5425 - val_acc: 0.8379\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8701\n",
      "Epoch 00022: val_loss did not improve from 0.54249\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4493 - acc: 0.8701 - val_loss: 0.5447 - val_acc: 0.8421\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8735\n",
      "Epoch 00023: val_loss improved from 0.54249 to 0.53749, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/023-0.5375.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4355 - acc: 0.8735 - val_loss: 0.5375 - val_acc: 0.8465\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8814\n",
      "Epoch 00024: val_loss did not improve from 0.53749\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4142 - acc: 0.8814 - val_loss: 0.5641 - val_acc: 0.8460\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8827\n",
      "Epoch 00025: val_loss improved from 0.53749 to 0.52473, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/025-0.5247.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4041 - acc: 0.8827 - val_loss: 0.5247 - val_acc: 0.8530\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8884\n",
      "Epoch 00026: val_loss improved from 0.52473 to 0.51143, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/026-0.5114.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3891 - acc: 0.8884 - val_loss: 0.5114 - val_acc: 0.8537\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8909\n",
      "Epoch 00027: val_loss did not improve from 0.51143\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3793 - acc: 0.8909 - val_loss: 0.5279 - val_acc: 0.8523\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8931\n",
      "Epoch 00028: val_loss improved from 0.51143 to 0.50693, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/028-0.5069.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3659 - acc: 0.8931 - val_loss: 0.5069 - val_acc: 0.8586\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8974\n",
      "Epoch 00029: val_loss improved from 0.50693 to 0.49950, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/029-0.4995.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3569 - acc: 0.8974 - val_loss: 0.4995 - val_acc: 0.8581\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.9010\n",
      "Epoch 00030: val_loss did not improve from 0.49950\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3413 - acc: 0.9010 - val_loss: 0.5115 - val_acc: 0.8563\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9044\n",
      "Epoch 00031: val_loss improved from 0.49950 to 0.48950, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/031-0.4895.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3297 - acc: 0.9044 - val_loss: 0.4895 - val_acc: 0.8630\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9069\n",
      "Epoch 00032: val_loss did not improve from 0.48950\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3227 - acc: 0.9069 - val_loss: 0.5036 - val_acc: 0.8551\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9110\n",
      "Epoch 00033: val_loss improved from 0.48950 to 0.47391, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/033-0.4739.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3090 - acc: 0.9110 - val_loss: 0.4739 - val_acc: 0.8686\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9135\n",
      "Epoch 00034: val_loss did not improve from 0.47391\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3011 - acc: 0.9135 - val_loss: 0.4742 - val_acc: 0.8724\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9175\n",
      "Epoch 00035: val_loss improved from 0.47391 to 0.46632, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/035-0.4663.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2883 - acc: 0.9175 - val_loss: 0.4663 - val_acc: 0.8707\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9210\n",
      "Epoch 00036: val_loss did not improve from 0.46632\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2805 - acc: 0.9210 - val_loss: 0.4791 - val_acc: 0.8698\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9218\n",
      "Epoch 00037: val_loss did not improve from 0.46632\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2728 - acc: 0.9218 - val_loss: 0.4825 - val_acc: 0.8714\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9244\n",
      "Epoch 00038: val_loss did not improve from 0.46632\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2639 - acc: 0.9244 - val_loss: 0.4742 - val_acc: 0.8700\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9249\n",
      "Epoch 00039: val_loss did not improve from 0.46632\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2572 - acc: 0.9249 - val_loss: 0.4760 - val_acc: 0.8670\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9291\n",
      "Epoch 00040: val_loss did not improve from 0.46632\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2484 - acc: 0.9291 - val_loss: 0.4679 - val_acc: 0.8737\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9307\n",
      "Epoch 00041: val_loss improved from 0.46632 to 0.46252, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/041-0.4625.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2412 - acc: 0.9307 - val_loss: 0.4625 - val_acc: 0.8719\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9332\n",
      "Epoch 00042: val_loss improved from 0.46252 to 0.45602, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/042-0.4560.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2351 - acc: 0.9332 - val_loss: 0.4560 - val_acc: 0.8735\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9335\n",
      "Epoch 00043: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2312 - acc: 0.9335 - val_loss: 0.4634 - val_acc: 0.8726\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9374\n",
      "Epoch 00044: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2189 - acc: 0.9374 - val_loss: 0.4678 - val_acc: 0.8754\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9398\n",
      "Epoch 00045: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2131 - acc: 0.9398 - val_loss: 0.4609 - val_acc: 0.8721\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9421\n",
      "Epoch 00046: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2056 - acc: 0.9421 - val_loss: 0.4710 - val_acc: 0.8747\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9406\n",
      "Epoch 00047: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2045 - acc: 0.9406 - val_loss: 0.4709 - val_acc: 0.8756\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9447\n",
      "Epoch 00048: val_loss did not improve from 0.45602\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1965 - acc: 0.9447 - val_loss: 0.4611 - val_acc: 0.8828\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9449\n",
      "Epoch 00049: val_loss improved from 0.45602 to 0.45182, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/049-0.4518.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1937 - acc: 0.9449 - val_loss: 0.4518 - val_acc: 0.8786\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9483\n",
      "Epoch 00050: val_loss improved from 0.45182 to 0.45094, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/050-0.4509.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1844 - acc: 0.9483 - val_loss: 0.4509 - val_acc: 0.8786\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9506\n",
      "Epoch 00051: val_loss did not improve from 0.45094\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1759 - acc: 0.9506 - val_loss: 0.4664 - val_acc: 0.8747\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9508\n",
      "Epoch 00052: val_loss did not improve from 0.45094\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1750 - acc: 0.9508 - val_loss: 0.4733 - val_acc: 0.8777\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9532\n",
      "Epoch 00053: val_loss did not improve from 0.45094\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1663 - acc: 0.9532 - val_loss: 0.4621 - val_acc: 0.8770\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9541\n",
      "Epoch 00054: val_loss did not improve from 0.45094\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1639 - acc: 0.9541 - val_loss: 0.4652 - val_acc: 0.8735\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9541\n",
      "Epoch 00055: val_loss improved from 0.45094 to 0.44482, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_6_conv_checkpoint/055-0.4448.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1604 - acc: 0.9541 - val_loss: 0.4448 - val_acc: 0.8814\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9565\n",
      "Epoch 00056: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1571 - acc: 0.9566 - val_loss: 0.4587 - val_acc: 0.8807\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9596\n",
      "Epoch 00057: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1471 - acc: 0.9597 - val_loss: 0.4675 - val_acc: 0.8817\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9602\n",
      "Epoch 00058: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1452 - acc: 0.9602 - val_loss: 0.4607 - val_acc: 0.8791\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9591\n",
      "Epoch 00059: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1438 - acc: 0.9591 - val_loss: 0.4712 - val_acc: 0.8742\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9605\n",
      "Epoch 00060: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1393 - acc: 0.9605 - val_loss: 0.4532 - val_acc: 0.8789\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9638\n",
      "Epoch 00061: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1345 - acc: 0.9638 - val_loss: 0.4555 - val_acc: 0.8782\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9653\n",
      "Epoch 00062: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1294 - acc: 0.9653 - val_loss: 0.4660 - val_acc: 0.8814\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9652\n",
      "Epoch 00063: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1295 - acc: 0.9652 - val_loss: 0.4724 - val_acc: 0.8784\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9666\n",
      "Epoch 00064: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1221 - acc: 0.9666 - val_loss: 0.4732 - val_acc: 0.8800\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9690\n",
      "Epoch 00065: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1177 - acc: 0.9691 - val_loss: 0.4635 - val_acc: 0.8824\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9672\n",
      "Epoch 00066: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1187 - acc: 0.9672 - val_loss: 0.4539 - val_acc: 0.8838\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9684\n",
      "Epoch 00067: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1133 - acc: 0.9684 - val_loss: 0.4570 - val_acc: 0.8845\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9687\n",
      "Epoch 00068: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1131 - acc: 0.9687 - val_loss: 0.4808 - val_acc: 0.8819\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9703\n",
      "Epoch 00069: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1102 - acc: 0.9703 - val_loss: 0.4666 - val_acc: 0.8868\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9716\n",
      "Epoch 00070: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1067 - acc: 0.9716 - val_loss: 0.4852 - val_acc: 0.8800\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9729\n",
      "Epoch 00071: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1039 - acc: 0.9728 - val_loss: 0.4760 - val_acc: 0.8833\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9734\n",
      "Epoch 00072: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0994 - acc: 0.9734 - val_loss: 0.4679 - val_acc: 0.8856\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9730\n",
      "Epoch 00073: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0982 - acc: 0.9729 - val_loss: 0.5124 - val_acc: 0.8765\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9751\n",
      "Epoch 00074: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0956 - acc: 0.9751 - val_loss: 0.4700 - val_acc: 0.8849\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9753\n",
      "Epoch 00075: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0959 - acc: 0.9753 - val_loss: 0.4767 - val_acc: 0.8821\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9762\n",
      "Epoch 00076: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0900 - acc: 0.9762 - val_loss: 0.4661 - val_acc: 0.8861\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9777\n",
      "Epoch 00077: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0849 - acc: 0.9777 - val_loss: 0.4697 - val_acc: 0.8882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9767\n",
      "Epoch 00078: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0879 - acc: 0.9767 - val_loss: 0.4825 - val_acc: 0.8863\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9779\n",
      "Epoch 00079: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0842 - acc: 0.9779 - val_loss: 0.4807 - val_acc: 0.8842\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9781\n",
      "Epoch 00080: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0845 - acc: 0.9781 - val_loss: 0.4906 - val_acc: 0.8863\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9784\n",
      "Epoch 00081: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0821 - acc: 0.9784 - val_loss: 0.4912 - val_acc: 0.8863\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9792\n",
      "Epoch 00082: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0795 - acc: 0.9792 - val_loss: 0.5091 - val_acc: 0.8803\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9768\n",
      "Epoch 00083: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0860 - acc: 0.9768 - val_loss: 0.5031 - val_acc: 0.8831\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9825\n",
      "Epoch 00084: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0718 - acc: 0.9825 - val_loss: 0.4864 - val_acc: 0.8845\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9816\n",
      "Epoch 00085: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0730 - acc: 0.9816 - val_loss: 0.5046 - val_acc: 0.8814\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9816\n",
      "Epoch 00086: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0731 - acc: 0.9816 - val_loss: 0.4966 - val_acc: 0.8842\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9830\n",
      "Epoch 00087: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0696 - acc: 0.9830 - val_loss: 0.4901 - val_acc: 0.8847\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9841\n",
      "Epoch 00088: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0665 - acc: 0.9841 - val_loss: 0.4853 - val_acc: 0.8849\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9826\n",
      "Epoch 00089: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0668 - acc: 0.9826 - val_loss: 0.5203 - val_acc: 0.8798\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9829\n",
      "Epoch 00090: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0672 - acc: 0.9829 - val_loss: 0.5229 - val_acc: 0.8831\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9832\n",
      "Epoch 00091: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0652 - acc: 0.9832 - val_loss: 0.5177 - val_acc: 0.8812\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9842\n",
      "Epoch 00092: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0654 - acc: 0.9842 - val_loss: 0.5068 - val_acc: 0.8856\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9835\n",
      "Epoch 00093: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0639 - acc: 0.9835 - val_loss: 0.5104 - val_acc: 0.8868\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9847\n",
      "Epoch 00094: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0612 - acc: 0.9847 - val_loss: 0.4913 - val_acc: 0.8833\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9865\n",
      "Epoch 00095: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0586 - acc: 0.9865 - val_loss: 0.4867 - val_acc: 0.8875\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9851\n",
      "Epoch 00096: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0596 - acc: 0.9851 - val_loss: 0.5023 - val_acc: 0.8838\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9864\n",
      "Epoch 00097: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0566 - acc: 0.9864 - val_loss: 0.5008 - val_acc: 0.8884\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9862\n",
      "Epoch 00098: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0569 - acc: 0.9863 - val_loss: 0.5040 - val_acc: 0.8831\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9870\n",
      "Epoch 00099: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0558 - acc: 0.9870 - val_loss: 0.5055 - val_acc: 0.8842\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9871\n",
      "Epoch 00100: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0542 - acc: 0.9871 - val_loss: 0.5036 - val_acc: 0.8859\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9881\n",
      "Epoch 00101: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0522 - acc: 0.9881 - val_loss: 0.4991 - val_acc: 0.8903\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9871\n",
      "Epoch 00102: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0532 - acc: 0.9871 - val_loss: 0.5199 - val_acc: 0.8833\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9875\n",
      "Epoch 00103: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0532 - acc: 0.9875 - val_loss: 0.4967 - val_acc: 0.8931\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9886\n",
      "Epoch 00104: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0511 - acc: 0.9886 - val_loss: 0.5030 - val_acc: 0.8921\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9888\n",
      "Epoch 00105: val_loss did not improve from 0.44482\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0500 - acc: 0.9888 - val_loss: 0.5045 - val_acc: 0.8859\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 617us/sample - loss: 0.5230 - acc: 0.8582\n",
      "Loss: 0.5229580372912366 Accuracy: 0.8581516\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9028 - acc: 0.3868\n",
      "Epoch 00001: val_loss improved from inf to 1.44088, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/001-1.4409.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.9028 - acc: 0.3868 - val_loss: 1.4409 - val_acc: 0.5558\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4223 - acc: 0.5587\n",
      "Epoch 00002: val_loss improved from 1.44088 to 1.21072, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/002-1.2107.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.4224 - acc: 0.5586 - val_loss: 1.2107 - val_acc: 0.6324\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2227 - acc: 0.6337\n",
      "Epoch 00003: val_loss improved from 1.21072 to 1.02143, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/003-1.0214.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.2227 - acc: 0.6337 - val_loss: 1.0214 - val_acc: 0.7130\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0517 - acc: 0.6937\n",
      "Epoch 00004: val_loss improved from 1.02143 to 0.91240, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/004-0.9124.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0515 - acc: 0.6937 - val_loss: 0.9124 - val_acc: 0.7405\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9190 - acc: 0.7336\n",
      "Epoch 00005: val_loss improved from 0.91240 to 0.79056, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/005-0.7906.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.9190 - acc: 0.7336 - val_loss: 0.7906 - val_acc: 0.7820\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8204 - acc: 0.7620\n",
      "Epoch 00006: val_loss improved from 0.79056 to 0.70576, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/006-0.7058.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.8205 - acc: 0.7619 - val_loss: 0.7058 - val_acc: 0.8083\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7293 - acc: 0.7905\n",
      "Epoch 00007: val_loss improved from 0.70576 to 0.65742, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/007-0.6574.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.7293 - acc: 0.7905 - val_loss: 0.6574 - val_acc: 0.8220\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.8128\n",
      "Epoch 00008: val_loss improved from 0.65742 to 0.59088, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/008-0.5909.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6598 - acc: 0.8128 - val_loss: 0.5909 - val_acc: 0.8367\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8249\n",
      "Epoch 00009: val_loss improved from 0.59088 to 0.53607, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/009-0.5361.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.6075 - acc: 0.8249 - val_loss: 0.5361 - val_acc: 0.8472\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8429\n",
      "Epoch 00010: val_loss improved from 0.53607 to 0.49397, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/010-0.4940.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5551 - acc: 0.8429 - val_loss: 0.4940 - val_acc: 0.8628\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8537\n",
      "Epoch 00011: val_loss improved from 0.49397 to 0.45185, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/011-0.4519.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.5168 - acc: 0.8537 - val_loss: 0.4519 - val_acc: 0.8786\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8642\n",
      "Epoch 00012: val_loss improved from 0.45185 to 0.43549, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/012-0.4355.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4822 - acc: 0.8642 - val_loss: 0.4355 - val_acc: 0.8744\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.8691\n",
      "Epoch 00013: val_loss improved from 0.43549 to 0.42112, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/013-0.4211.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4532 - acc: 0.8691 - val_loss: 0.4211 - val_acc: 0.8812\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.8793\n",
      "Epoch 00014: val_loss improved from 0.42112 to 0.38674, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/014-0.3867.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4291 - acc: 0.8793 - val_loss: 0.3867 - val_acc: 0.8917\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8848\n",
      "Epoch 00015: val_loss improved from 0.38674 to 0.37176, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/015-0.3718.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.4047 - acc: 0.8848 - val_loss: 0.3718 - val_acc: 0.8987\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3829 - acc: 0.8916\n",
      "Epoch 00016: val_loss improved from 0.37176 to 0.35524, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/016-0.3552.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3828 - acc: 0.8916 - val_loss: 0.3552 - val_acc: 0.8991\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8965\n",
      "Epoch 00017: val_loss improved from 0.35524 to 0.34546, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/017-0.3455.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3647 - acc: 0.8965 - val_loss: 0.3455 - val_acc: 0.9064\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.9002\n",
      "Epoch 00018: val_loss improved from 0.34546 to 0.34372, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/018-0.3437.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3508 - acc: 0.9002 - val_loss: 0.3437 - val_acc: 0.9036\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.9052\n",
      "Epoch 00019: val_loss improved from 0.34372 to 0.31678, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/019-0.3168.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3339 - acc: 0.9052 - val_loss: 0.3168 - val_acc: 0.9113\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9099\n",
      "Epoch 00020: val_loss improved from 0.31678 to 0.30938, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/020-0.3094.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3209 - acc: 0.9099 - val_loss: 0.3094 - val_acc: 0.9145\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9132\n",
      "Epoch 00021: val_loss did not improve from 0.30938\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.3052 - acc: 0.9132 - val_loss: 0.3109 - val_acc: 0.9113\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9154\n",
      "Epoch 00022: val_loss improved from 0.30938 to 0.29182, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/022-0.2918.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2982 - acc: 0.9154 - val_loss: 0.2918 - val_acc: 0.9187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9165\n",
      "Epoch 00023: val_loss did not improve from 0.29182\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2900 - acc: 0.9165 - val_loss: 0.2974 - val_acc: 0.9201\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9208\n",
      "Epoch 00024: val_loss improved from 0.29182 to 0.29044, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/024-0.2904.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2761 - acc: 0.9208 - val_loss: 0.2904 - val_acc: 0.9194\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9235\n",
      "Epoch 00025: val_loss did not improve from 0.29044\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2683 - acc: 0.9234 - val_loss: 0.2908 - val_acc: 0.9187\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9267\n",
      "Epoch 00026: val_loss improved from 0.29044 to 0.28288, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/026-0.2829.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2591 - acc: 0.9267 - val_loss: 0.2829 - val_acc: 0.9220\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9278\n",
      "Epoch 00027: val_loss did not improve from 0.28288\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2495 - acc: 0.9278 - val_loss: 0.2835 - val_acc: 0.9206\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9302\n",
      "Epoch 00028: val_loss improved from 0.28288 to 0.27179, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/028-0.2718.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2443 - acc: 0.9303 - val_loss: 0.2718 - val_acc: 0.9255\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9329\n",
      "Epoch 00029: val_loss improved from 0.27179 to 0.25849, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/029-0.2585.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2348 - acc: 0.9329 - val_loss: 0.2585 - val_acc: 0.9269\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9331\n",
      "Epoch 00030: val_loss did not improve from 0.25849\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2283 - acc: 0.9331 - val_loss: 0.2652 - val_acc: 0.9257\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9356\n",
      "Epoch 00031: val_loss did not improve from 0.25849\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2213 - acc: 0.9356 - val_loss: 0.2617 - val_acc: 0.9287\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9381\n",
      "Epoch 00032: val_loss did not improve from 0.25849\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2166 - acc: 0.9381 - val_loss: 0.2591 - val_acc: 0.9290\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9404\n",
      "Epoch 00033: val_loss improved from 0.25849 to 0.25824, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/033-0.2582.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2083 - acc: 0.9403 - val_loss: 0.2582 - val_acc: 0.9301\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9415\n",
      "Epoch 00034: val_loss improved from 0.25824 to 0.24186, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/034-0.2419.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2027 - acc: 0.9415 - val_loss: 0.2419 - val_acc: 0.9306\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9433\n",
      "Epoch 00035: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1971 - acc: 0.9433 - val_loss: 0.2546 - val_acc: 0.9304\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9434\n",
      "Epoch 00036: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1931 - acc: 0.9434 - val_loss: 0.2469 - val_acc: 0.9290\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9437\n",
      "Epoch 00037: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1898 - acc: 0.9437 - val_loss: 0.2586 - val_acc: 0.9262\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9478\n",
      "Epoch 00038: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1834 - acc: 0.9478 - val_loss: 0.2472 - val_acc: 0.9324\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9469\n",
      "Epoch 00039: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1811 - acc: 0.9469 - val_loss: 0.2430 - val_acc: 0.9320\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9482\n",
      "Epoch 00040: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1761 - acc: 0.9482 - val_loss: 0.2431 - val_acc: 0.9320\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9511\n",
      "Epoch 00041: val_loss did not improve from 0.24186\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1680 - acc: 0.9511 - val_loss: 0.2423 - val_acc: 0.9320\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9527\n",
      "Epoch 00042: val_loss improved from 0.24186 to 0.23223, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/042-0.2322.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1633 - acc: 0.9527 - val_loss: 0.2322 - val_acc: 0.9373\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9530\n",
      "Epoch 00043: val_loss did not improve from 0.23223\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1613 - acc: 0.9530 - val_loss: 0.2466 - val_acc: 0.9341\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9535\n",
      "Epoch 00044: val_loss improved from 0.23223 to 0.22661, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/044-0.2266.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1591 - acc: 0.9535 - val_loss: 0.2266 - val_acc: 0.9369\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9534\n",
      "Epoch 00045: val_loss did not improve from 0.22661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1581 - acc: 0.9534 - val_loss: 0.2327 - val_acc: 0.9352\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9540\n",
      "Epoch 00046: val_loss did not improve from 0.22661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1512 - acc: 0.9540 - val_loss: 0.2318 - val_acc: 0.9362\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9575\n",
      "Epoch 00047: val_loss did not improve from 0.22661\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1466 - acc: 0.9575 - val_loss: 0.2540 - val_acc: 0.9276\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9585\n",
      "Epoch 00048: val_loss improved from 0.22661 to 0.22370, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/048-0.2237.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1459 - acc: 0.9585 - val_loss: 0.2237 - val_acc: 0.9348\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9588\n",
      "Epoch 00049: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1394 - acc: 0.9588 - val_loss: 0.2283 - val_acc: 0.9373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9611\n",
      "Epoch 00050: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1334 - acc: 0.9611 - val_loss: 0.2268 - val_acc: 0.9366\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9611\n",
      "Epoch 00051: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1341 - acc: 0.9611 - val_loss: 0.2453 - val_acc: 0.9311\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9607\n",
      "Epoch 00052: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1300 - acc: 0.9607 - val_loss: 0.2299 - val_acc: 0.9394\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9628\n",
      "Epoch 00053: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1264 - acc: 0.9628 - val_loss: 0.2397 - val_acc: 0.9348\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9635\n",
      "Epoch 00054: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1245 - acc: 0.9635 - val_loss: 0.2315 - val_acc: 0.9385\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9641\n",
      "Epoch 00055: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1229 - acc: 0.9641 - val_loss: 0.2399 - val_acc: 0.9369\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9651\n",
      "Epoch 00056: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1178 - acc: 0.9651 - val_loss: 0.2238 - val_acc: 0.9404\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9656\n",
      "Epoch 00057: val_loss did not improve from 0.22370\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1177 - acc: 0.9656 - val_loss: 0.2344 - val_acc: 0.9348\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9653\n",
      "Epoch 00058: val_loss improved from 0.22370 to 0.21947, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/058-0.2195.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1164 - acc: 0.9653 - val_loss: 0.2195 - val_acc: 0.9380\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9668\n",
      "Epoch 00059: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1113 - acc: 0.9669 - val_loss: 0.2317 - val_acc: 0.9364\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9689\n",
      "Epoch 00060: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1070 - acc: 0.9689 - val_loss: 0.2295 - val_acc: 0.9378\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9687\n",
      "Epoch 00061: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1072 - acc: 0.9687 - val_loss: 0.2383 - val_acc: 0.9336\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9695\n",
      "Epoch 00062: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1056 - acc: 0.9695 - val_loss: 0.2308 - val_acc: 0.9341\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9701\n",
      "Epoch 00063: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1016 - acc: 0.9701 - val_loss: 0.2315 - val_acc: 0.9362\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9713\n",
      "Epoch 00064: val_loss did not improve from 0.21947\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0996 - acc: 0.9713 - val_loss: 0.2384 - val_acc: 0.9371\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9725\n",
      "Epoch 00065: val_loss improved from 0.21947 to 0.21780, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/065-0.2178.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0958 - acc: 0.9725 - val_loss: 0.2178 - val_acc: 0.9432\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9724\n",
      "Epoch 00066: val_loss improved from 0.21780 to 0.21665, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_7_conv_checkpoint/066-0.2167.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0956 - acc: 0.9724 - val_loss: 0.2167 - val_acc: 0.9413\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9727\n",
      "Epoch 00067: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0925 - acc: 0.9727 - val_loss: 0.2241 - val_acc: 0.9392\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9734\n",
      "Epoch 00068: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0919 - acc: 0.9734 - val_loss: 0.2419 - val_acc: 0.9311\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9745\n",
      "Epoch 00069: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0879 - acc: 0.9745 - val_loss: 0.2388 - val_acc: 0.9371\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9748\n",
      "Epoch 00070: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0865 - acc: 0.9748 - val_loss: 0.2240 - val_acc: 0.9380\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9759\n",
      "Epoch 00071: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0856 - acc: 0.9759 - val_loss: 0.2189 - val_acc: 0.9408\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9761\n",
      "Epoch 00072: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0822 - acc: 0.9761 - val_loss: 0.2352 - val_acc: 0.9411\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9730\n",
      "Epoch 00073: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0890 - acc: 0.9730 - val_loss: 0.2290 - val_acc: 0.9413\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9762\n",
      "Epoch 00074: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0801 - acc: 0.9762 - val_loss: 0.2261 - val_acc: 0.9441\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9777\n",
      "Epoch 00075: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0768 - acc: 0.9777 - val_loss: 0.2345 - val_acc: 0.9357\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9772\n",
      "Epoch 00076: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0762 - acc: 0.9771 - val_loss: 0.2459 - val_acc: 0.9357\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9778\n",
      "Epoch 00077: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0749 - acc: 0.9778 - val_loss: 0.2257 - val_acc: 0.9392\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9789\n",
      "Epoch 00078: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0755 - acc: 0.9789 - val_loss: 0.2279 - val_acc: 0.9411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9795\n",
      "Epoch 00079: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0729 - acc: 0.9795 - val_loss: 0.2311 - val_acc: 0.9394\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9806\n",
      "Epoch 00080: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0699 - acc: 0.9806 - val_loss: 0.2307 - val_acc: 0.9397\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9804\n",
      "Epoch 00081: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0709 - acc: 0.9804 - val_loss: 0.2279 - val_acc: 0.9406\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9805\n",
      "Epoch 00082: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0681 - acc: 0.9805 - val_loss: 0.2265 - val_acc: 0.9378\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9797\n",
      "Epoch 00083: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0693 - acc: 0.9797 - val_loss: 0.2289 - val_acc: 0.9397\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9818\n",
      "Epoch 00084: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0651 - acc: 0.9818 - val_loss: 0.2422 - val_acc: 0.9385\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9817\n",
      "Epoch 00085: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0627 - acc: 0.9817 - val_loss: 0.2270 - val_acc: 0.9399\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9832\n",
      "Epoch 00086: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0613 - acc: 0.9832 - val_loss: 0.2394 - val_acc: 0.9397\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9830\n",
      "Epoch 00087: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0610 - acc: 0.9830 - val_loss: 0.2290 - val_acc: 0.9413\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9826\n",
      "Epoch 00088: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0608 - acc: 0.9826 - val_loss: 0.2297 - val_acc: 0.9397\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9830\n",
      "Epoch 00089: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0606 - acc: 0.9830 - val_loss: 0.2482 - val_acc: 0.9383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9830\n",
      "Epoch 00090: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0605 - acc: 0.9830 - val_loss: 0.2321 - val_acc: 0.9399\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9840\n",
      "Epoch 00091: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0554 - acc: 0.9841 - val_loss: 0.2451 - val_acc: 0.9397\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9842\n",
      "Epoch 00092: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0573 - acc: 0.9842 - val_loss: 0.2421 - val_acc: 0.9397\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9835\n",
      "Epoch 00093: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0573 - acc: 0.9835 - val_loss: 0.2357 - val_acc: 0.9415\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9846\n",
      "Epoch 00094: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0539 - acc: 0.9846 - val_loss: 0.2319 - val_acc: 0.9453\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9850\n",
      "Epoch 00095: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0532 - acc: 0.9850 - val_loss: 0.2349 - val_acc: 0.9406\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9859\n",
      "Epoch 00096: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0512 - acc: 0.9859 - val_loss: 0.2367 - val_acc: 0.9411\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9852\n",
      "Epoch 00097: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0506 - acc: 0.9852 - val_loss: 0.2357 - val_acc: 0.9413\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9859\n",
      "Epoch 00098: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0510 - acc: 0.9859 - val_loss: 0.2406 - val_acc: 0.9364\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9864\n",
      "Epoch 00099: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0493 - acc: 0.9864 - val_loss: 0.2362 - val_acc: 0.9397\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9873\n",
      "Epoch 00100: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0461 - acc: 0.9873 - val_loss: 0.2259 - val_acc: 0.9464\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9880\n",
      "Epoch 00101: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0443 - acc: 0.9880 - val_loss: 0.2598 - val_acc: 0.9392\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9865\n",
      "Epoch 00102: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0495 - acc: 0.9866 - val_loss: 0.2426 - val_acc: 0.9413\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9878\n",
      "Epoch 00103: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0465 - acc: 0.9878 - val_loss: 0.2343 - val_acc: 0.9427\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9874\n",
      "Epoch 00104: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0463 - acc: 0.9874 - val_loss: 0.2392 - val_acc: 0.9401\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9886\n",
      "Epoch 00105: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0417 - acc: 0.9886 - val_loss: 0.2316 - val_acc: 0.9429\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9886\n",
      "Epoch 00106: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0435 - acc: 0.9886 - val_loss: 0.2476 - val_acc: 0.9394\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9886\n",
      "Epoch 00107: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0419 - acc: 0.9886 - val_loss: 0.2444 - val_acc: 0.9429\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9886\n",
      "Epoch 00108: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0425 - acc: 0.9886 - val_loss: 0.2411 - val_acc: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9879\n",
      "Epoch 00109: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0415 - acc: 0.9879 - val_loss: 0.2470 - val_acc: 0.9397\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9891\n",
      "Epoch 00110: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0391 - acc: 0.9891 - val_loss: 0.2566 - val_acc: 0.9413\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9891\n",
      "Epoch 00111: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0413 - acc: 0.9891 - val_loss: 0.2299 - val_acc: 0.9404\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9896\n",
      "Epoch 00112: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0394 - acc: 0.9896 - val_loss: 0.2435 - val_acc: 0.9415\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9915\n",
      "Epoch 00113: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0348 - acc: 0.9915 - val_loss: 0.2362 - val_acc: 0.9429\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9901\n",
      "Epoch 00114: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0375 - acc: 0.9901 - val_loss: 0.2463 - val_acc: 0.9404\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9896\n",
      "Epoch 00115: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0372 - acc: 0.9896 - val_loss: 0.2433 - val_acc: 0.9441\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9901\n",
      "Epoch 00116: val_loss did not improve from 0.21665\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0371 - acc: 0.9901 - val_loss: 0.2428 - val_acc: 0.9404\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 646us/sample - loss: 0.2609 - acc: 0.9215\n",
      "Loss: 0.26094469949835186 Accuracy: 0.9214953\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8508 - acc: 0.4058\n",
      "Epoch 00001: val_loss improved from inf to 1.38881, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/001-1.3888.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.8506 - acc: 0.4058 - val_loss: 1.3888 - val_acc: 0.5921\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2727 - acc: 0.6158\n",
      "Epoch 00002: val_loss improved from 1.38881 to 1.00392, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/002-1.0039.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2726 - acc: 0.6158 - val_loss: 1.0039 - val_acc: 0.7098\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0120 - acc: 0.6987\n",
      "Epoch 00003: val_loss improved from 1.00392 to 0.81271, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/003-0.8127.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.0120 - acc: 0.6987 - val_loss: 0.8127 - val_acc: 0.7668\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8292 - acc: 0.7544\n",
      "Epoch 00004: val_loss improved from 0.81271 to 0.66928, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/004-0.6693.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8292 - acc: 0.7545 - val_loss: 0.6693 - val_acc: 0.8160\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6787 - acc: 0.8038\n",
      "Epoch 00005: val_loss improved from 0.66928 to 0.53232, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/005-0.5323.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6788 - acc: 0.8037 - val_loss: 0.5323 - val_acc: 0.8549\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8342\n",
      "Epoch 00006: val_loss improved from 0.53232 to 0.46534, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/006-0.4653.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5734 - acc: 0.8343 - val_loss: 0.4653 - val_acc: 0.8707\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8595\n",
      "Epoch 00007: val_loss improved from 0.46534 to 0.38580, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/007-0.3858.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4935 - acc: 0.8594 - val_loss: 0.3858 - val_acc: 0.8942\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4417 - acc: 0.8717\n",
      "Epoch 00008: val_loss improved from 0.38580 to 0.34267, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/008-0.3427.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4417 - acc: 0.8717 - val_loss: 0.3427 - val_acc: 0.9052\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.8866\n",
      "Epoch 00009: val_loss improved from 0.34267 to 0.31785, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/009-0.3178.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3979 - acc: 0.8866 - val_loss: 0.3178 - val_acc: 0.9110\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8976\n",
      "Epoch 00010: val_loss improved from 0.31785 to 0.28832, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/010-0.2883.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3616 - acc: 0.8976 - val_loss: 0.2883 - val_acc: 0.9189\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9055\n",
      "Epoch 00011: val_loss improved from 0.28832 to 0.26678, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/011-0.2668.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3297 - acc: 0.9056 - val_loss: 0.2668 - val_acc: 0.9262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9111\n",
      "Epoch 00012: val_loss improved from 0.26678 to 0.25754, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/012-0.2575.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3087 - acc: 0.9111 - val_loss: 0.2575 - val_acc: 0.9243\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9168\n",
      "Epoch 00013: val_loss improved from 0.25754 to 0.24869, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/013-0.2487.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2896 - acc: 0.9168 - val_loss: 0.2487 - val_acc: 0.9285\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9223\n",
      "Epoch 00014: val_loss did not improve from 0.24869\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2727 - acc: 0.9222 - val_loss: 0.2552 - val_acc: 0.9259\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9260\n",
      "Epoch 00015: val_loss improved from 0.24869 to 0.21707, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/015-0.2171.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2533 - acc: 0.9260 - val_loss: 0.2171 - val_acc: 0.9348\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9293\n",
      "Epoch 00016: val_loss improved from 0.21707 to 0.20955, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/016-0.2095.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2443 - acc: 0.9293 - val_loss: 0.2095 - val_acc: 0.9422\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9354\n",
      "Epoch 00017: val_loss did not improve from 0.20955\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2260 - acc: 0.9354 - val_loss: 0.2254 - val_acc: 0.9334\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9368\n",
      "Epoch 00018: val_loss improved from 0.20955 to 0.19995, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/018-0.2000.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.2184 - acc: 0.9367 - val_loss: 0.2000 - val_acc: 0.9450\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9405\n",
      "Epoch 00019: val_loss improved from 0.19995 to 0.18483, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/019-0.1848.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2067 - acc: 0.9405 - val_loss: 0.1848 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9435\n",
      "Epoch 00020: val_loss did not improve from 0.18483\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1975 - acc: 0.9435 - val_loss: 0.1890 - val_acc: 0.9448\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9431\n",
      "Epoch 00021: val_loss improved from 0.18483 to 0.18175, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/021-0.1818.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1927 - acc: 0.9431 - val_loss: 0.1818 - val_acc: 0.9483\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9458\n",
      "Epoch 00022: val_loss did not improve from 0.18175\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1857 - acc: 0.9457 - val_loss: 0.1876 - val_acc: 0.9434\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9486\n",
      "Epoch 00023: val_loss improved from 0.18175 to 0.17340, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/023-0.1734.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1776 - acc: 0.9486 - val_loss: 0.1734 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9518\n",
      "Epoch 00024: val_loss did not improve from 0.17340\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1688 - acc: 0.9518 - val_loss: 0.1750 - val_acc: 0.9490\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9524\n",
      "Epoch 00025: val_loss improved from 0.17340 to 0.16587, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/025-0.1659.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1631 - acc: 0.9524 - val_loss: 0.1659 - val_acc: 0.9525\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9527\n",
      "Epoch 00026: val_loss improved from 0.16587 to 0.16002, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/026-0.1600.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1593 - acc: 0.9527 - val_loss: 0.1600 - val_acc: 0.9525\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9545\n",
      "Epoch 00027: val_loss did not improve from 0.16002\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1516 - acc: 0.9545 - val_loss: 0.1683 - val_acc: 0.9481\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9579\n",
      "Epoch 00028: val_loss did not improve from 0.16002\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1463 - acc: 0.9579 - val_loss: 0.1666 - val_acc: 0.9515\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1411 - acc: 0.9585\n",
      "Epoch 00029: val_loss did not improve from 0.16002\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1412 - acc: 0.9585 - val_loss: 0.1628 - val_acc: 0.9515\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9591\n",
      "Epoch 00030: val_loss improved from 0.16002 to 0.15982, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/030-0.1598.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1391 - acc: 0.9591 - val_loss: 0.1598 - val_acc: 0.9553\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9614\n",
      "Epoch 00031: val_loss improved from 0.15982 to 0.15813, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/031-0.1581.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1301 - acc: 0.9614 - val_loss: 0.1581 - val_acc: 0.9567\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9635\n",
      "Epoch 00032: val_loss did not improve from 0.15813\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1231 - acc: 0.9634 - val_loss: 0.1613 - val_acc: 0.9534\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9623\n",
      "Epoch 00033: val_loss improved from 0.15813 to 0.15424, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/033-0.1542.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1246 - acc: 0.9623 - val_loss: 0.1542 - val_acc: 0.9569\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9634\n",
      "Epoch 00034: val_loss did not improve from 0.15424\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1202 - acc: 0.9634 - val_loss: 0.1644 - val_acc: 0.9509\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9665\n",
      "Epoch 00035: val_loss did not improve from 0.15424\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1137 - acc: 0.9664 - val_loss: 0.1559 - val_acc: 0.9550\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9666\n",
      "Epoch 00036: val_loss did not improve from 0.15424\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1130 - acc: 0.9666 - val_loss: 0.1545 - val_acc: 0.9553\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9681\n",
      "Epoch 00037: val_loss improved from 0.15424 to 0.14791, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/037-0.1479.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1056 - acc: 0.9681 - val_loss: 0.1479 - val_acc: 0.9543\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9701\n",
      "Epoch 00038: val_loss improved from 0.14791 to 0.14526, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/038-0.1453.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1020 - acc: 0.9701 - val_loss: 0.1453 - val_acc: 0.9585\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9706\n",
      "Epoch 00039: val_loss did not improve from 0.14526\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.1014 - acc: 0.9706 - val_loss: 0.1469 - val_acc: 0.9557\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9718\n",
      "Epoch 00040: val_loss improved from 0.14526 to 0.14513, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/040-0.1451.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0974 - acc: 0.9718 - val_loss: 0.1451 - val_acc: 0.9576\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9727\n",
      "Epoch 00041: val_loss improved from 0.14513 to 0.14453, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/041-0.1445.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0932 - acc: 0.9727 - val_loss: 0.1445 - val_acc: 0.9557\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9729\n",
      "Epoch 00042: val_loss did not improve from 0.14453\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0919 - acc: 0.9729 - val_loss: 0.1721 - val_acc: 0.9495\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9739\n",
      "Epoch 00043: val_loss improved from 0.14453 to 0.14160, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/043-0.1416.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0882 - acc: 0.9739 - val_loss: 0.1416 - val_acc: 0.9599\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9756\n",
      "Epoch 00044: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0850 - acc: 0.9756 - val_loss: 0.1588 - val_acc: 0.9525\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9764\n",
      "Epoch 00045: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0812 - acc: 0.9764 - val_loss: 0.1442 - val_acc: 0.9583\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9763\n",
      "Epoch 00046: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0790 - acc: 0.9763 - val_loss: 0.1526 - val_acc: 0.9583\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9775\n",
      "Epoch 00047: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0760 - acc: 0.9775 - val_loss: 0.1455 - val_acc: 0.9599\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9769\n",
      "Epoch 00048: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0779 - acc: 0.9769 - val_loss: 0.1542 - val_acc: 0.9520\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9783\n",
      "Epoch 00049: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0727 - acc: 0.9783 - val_loss: 0.1432 - val_acc: 0.9592\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9788\n",
      "Epoch 00050: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0713 - acc: 0.9788 - val_loss: 0.1532 - val_acc: 0.9569\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9806\n",
      "Epoch 00051: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0685 - acc: 0.9806 - val_loss: 0.1482 - val_acc: 0.9548\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9809\n",
      "Epoch 00052: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0659 - acc: 0.9809 - val_loss: 0.1541 - val_acc: 0.9548\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9806\n",
      "Epoch 00053: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0664 - acc: 0.9806 - val_loss: 0.1541 - val_acc: 0.9555\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9827\n",
      "Epoch 00054: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0619 - acc: 0.9827 - val_loss: 0.1460 - val_acc: 0.9588\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9830\n",
      "Epoch 00055: val_loss did not improve from 0.14160\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0607 - acc: 0.9830 - val_loss: 0.1495 - val_acc: 0.9576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9832\n",
      "Epoch 00056: val_loss improved from 0.14160 to 0.14053, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/056-0.1405.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0578 - acc: 0.9832 - val_loss: 0.1405 - val_acc: 0.9567\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9846\n",
      "Epoch 00057: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0551 - acc: 0.9846 - val_loss: 0.1436 - val_acc: 0.9609\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9849\n",
      "Epoch 00058: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0522 - acc: 0.9849 - val_loss: 0.1486 - val_acc: 0.9592\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9850\n",
      "Epoch 00059: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0533 - acc: 0.9850 - val_loss: 0.1453 - val_acc: 0.9590\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9857\n",
      "Epoch 00060: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0496 - acc: 0.9857 - val_loss: 0.1467 - val_acc: 0.9567\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9859\n",
      "Epoch 00061: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0502 - acc: 0.9859 - val_loss: 0.1459 - val_acc: 0.9583\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9867\n",
      "Epoch 00062: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0474 - acc: 0.9867 - val_loss: 0.1567 - val_acc: 0.9562\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9877\n",
      "Epoch 00063: val_loss did not improve from 0.14053\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0452 - acc: 0.9877 - val_loss: 0.1643 - val_acc: 0.9578\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9886\n",
      "Epoch 00064: val_loss improved from 0.14053 to 0.13864, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_8_conv_checkpoint/064-0.1386.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0431 - acc: 0.9886 - val_loss: 0.1386 - val_acc: 0.9634\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9889\n",
      "Epoch 00065: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0421 - acc: 0.9889 - val_loss: 0.1488 - val_acc: 0.9571\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9881\n",
      "Epoch 00066: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0414 - acc: 0.9881 - val_loss: 0.1439 - val_acc: 0.9581\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9882\n",
      "Epoch 00067: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0414 - acc: 0.9882 - val_loss: 0.1549 - val_acc: 0.9553\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0376 - acc: 0.9895 - val_loss: 0.1495 - val_acc: 0.9599\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9895\n",
      "Epoch 00069: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0386 - acc: 0.9895 - val_loss: 0.1504 - val_acc: 0.9557\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9898\n",
      "Epoch 00070: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0370 - acc: 0.9898 - val_loss: 0.1511 - val_acc: 0.9623\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9912\n",
      "Epoch 00071: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0342 - acc: 0.9912 - val_loss: 0.1513 - val_acc: 0.9569\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9896\n",
      "Epoch 00072: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0375 - acc: 0.9896 - val_loss: 0.1520 - val_acc: 0.9578\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9921\n",
      "Epoch 00073: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0306 - acc: 0.9921 - val_loss: 0.1507 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9917\n",
      "Epoch 00074: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0314 - acc: 0.9917 - val_loss: 0.1617 - val_acc: 0.9571\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9912\n",
      "Epoch 00075: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0322 - acc: 0.9912 - val_loss: 0.1671 - val_acc: 0.9564\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9927\n",
      "Epoch 00076: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0294 - acc: 0.9927 - val_loss: 0.1493 - val_acc: 0.9602\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9926\n",
      "Epoch 00077: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0286 - acc: 0.9926 - val_loss: 0.1497 - val_acc: 0.9597\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9928\n",
      "Epoch 00078: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0280 - acc: 0.9928 - val_loss: 0.1550 - val_acc: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9928\n",
      "Epoch 00079: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0282 - acc: 0.9928 - val_loss: 0.1493 - val_acc: 0.9597\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9935\n",
      "Epoch 00080: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0254 - acc: 0.9935 - val_loss: 0.1529 - val_acc: 0.9602\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9932\n",
      "Epoch 00081: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0258 - acc: 0.9932 - val_loss: 0.1563 - val_acc: 0.9604\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9940\n",
      "Epoch 00082: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0256 - acc: 0.9940 - val_loss: 0.1448 - val_acc: 0.9613\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9935\n",
      "Epoch 00083: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0247 - acc: 0.9935 - val_loss: 0.1547 - val_acc: 0.9602\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9935\n",
      "Epoch 00084: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0256 - acc: 0.9935 - val_loss: 0.1627 - val_acc: 0.9581\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9941\n",
      "Epoch 00085: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0241 - acc: 0.9941 - val_loss: 0.1540 - val_acc: 0.9590\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9951\n",
      "Epoch 00086: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0208 - acc: 0.9951 - val_loss: 0.1498 - val_acc: 0.9625\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9947\n",
      "Epoch 00087: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0223 - acc: 0.9947 - val_loss: 0.1629 - val_acc: 0.9539\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9948\n",
      "Epoch 00088: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0217 - acc: 0.9948 - val_loss: 0.1578 - val_acc: 0.9592\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9950\n",
      "Epoch 00089: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0208 - acc: 0.9950 - val_loss: 0.1625 - val_acc: 0.9562\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9899\n",
      "Epoch 00090: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0385 - acc: 0.9899 - val_loss: 0.1513 - val_acc: 0.9581\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9936\n",
      "Epoch 00091: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0257 - acc: 0.9936 - val_loss: 0.1496 - val_acc: 0.9597\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9957\n",
      "Epoch 00092: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0183 - acc: 0.9957 - val_loss: 0.1519 - val_acc: 0.9620\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9953\n",
      "Epoch 00093: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0199 - acc: 0.9953 - val_loss: 0.1548 - val_acc: 0.9637\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9968\n",
      "Epoch 00094: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0164 - acc: 0.9968 - val_loss: 0.1608 - val_acc: 0.9613\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9952\n",
      "Epoch 00095: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0192 - acc: 0.9952 - val_loss: 0.1664 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9951\n",
      "Epoch 00096: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0203 - acc: 0.9951 - val_loss: 0.1698 - val_acc: 0.9585\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9961\n",
      "Epoch 00097: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 0.1645 - val_acc: 0.9585\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9967\n",
      "Epoch 00098: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0145 - acc: 0.9967 - val_loss: 0.1569 - val_acc: 0.9616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9960\n",
      "Epoch 00099: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0159 - acc: 0.9960 - val_loss: 0.1600 - val_acc: 0.9606\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9951\n",
      "Epoch 00100: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0186 - acc: 0.9951 - val_loss: 0.1577 - val_acc: 0.9595\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9964\n",
      "Epoch 00101: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0156 - acc: 0.9964 - val_loss: 0.1658 - val_acc: 0.9590\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9961\n",
      "Epoch 00102: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 0.1511 - val_acc: 0.9627\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9968\n",
      "Epoch 00103: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0134 - acc: 0.9968 - val_loss: 0.1788 - val_acc: 0.9557\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9957\n",
      "Epoch 00104: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.1627 - val_acc: 0.9597\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9957\n",
      "Epoch 00105: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0168 - acc: 0.9957 - val_loss: 0.1596 - val_acc: 0.9602\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9971\n",
      "Epoch 00106: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0127 - acc: 0.9971 - val_loss: 0.1694 - val_acc: 0.9567\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9964\n",
      "Epoch 00107: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0144 - acc: 0.9964 - val_loss: 0.1725 - val_acc: 0.9578\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9961\n",
      "Epoch 00108: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0147 - acc: 0.9961 - val_loss: 0.1712 - val_acc: 0.9602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9974\n",
      "Epoch 00109: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0123 - acc: 0.9974 - val_loss: 0.1624 - val_acc: 0.9616\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9970\n",
      "Epoch 00110: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0135 - acc: 0.9970 - val_loss: 0.1816 - val_acc: 0.9576\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9963\n",
      "Epoch 00111: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0156 - acc: 0.9963 - val_loss: 0.1634 - val_acc: 0.9613\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 00112: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0121 - acc: 0.9973 - val_loss: 0.1628 - val_acc: 0.9585\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9963\n",
      "Epoch 00113: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0143 - acc: 0.9963 - val_loss: 0.1612 - val_acc: 0.9618\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9972\n",
      "Epoch 00114: val_loss did not improve from 0.13864\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.1587 - val_acc: 0.9623\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmSUzkz2ZJGwhJKxCCAQICD9w33BD1CpatWqrtn20Ptbn0Ydqn9bu2mq1tlqlPlqtilqtVZSWagVxQw3IprJI2BII2ffZ5/z+ODNJgCQEyJCQfN+v17ySuev33pk533vOvfdcpbVGCCGEOBRLbwcghBDi+CAJQwghRLdIwhBCCNEtkjCEEEJ0iyQMIYQQ3SIJQwghRLdIwhBCCNEtkjCEEEJ0iyQMIYQQ3WLr7QB6UkZGhs7Nze3tMIQQ4rixevXqKq11Znem7VcJIzc3l+Li4t4OQwghjhtKqZ3dnVaapIQQQnSLJAwhhBDdIglDCCFEt/SrcxgdCQQClJaW4vV6ezuU45LT6SQ7Oxu73d7boQghelm/TxilpaUkJSWRm5uLUqq3wzmuaK2prq6mtLSUvLy83g5HCNHL+n2TlNfrxe12S7I4Akop3G631M6EEMAASBiAJIujIPtOCBE1IBLGofh8ewgG63s7DCGE6NMkYQB+fznBYENMll1XV8ejjz56RPOed9551NXVdXv6e+65h/vvv/+I1iWEEIciCQNQygqEYrLsrhJGMBjsct6lS5eSmpoai7CEEOKwScIAwILW4ZgseeHChWzbto3CwkLuuOMOVqxYwUknncS8efOYMGECAPPnz2fatGnk5+ezaNGi1nlzc3Opqqpix44djB8/nhtvvJH8/HzOPvtsPB5Pl+tdu3YtM2fOZNKkSVx88cXU1tYC8PDDDzNhwgQmTZrEFVdcAcC7775LYWEhhYWFTJkyhcbGxpjsCyHE8a3fX1bb3tatt9HUtPag4eFwM2DBYnEd9jITEwsZM+ahTsffe++9bNy4kbVrzXpXrFjBmjVr2LhxY+ulqk8++STp6el4PB6mT5/OpZdeitvtPiD2rSxevJg//elPXH755bzyyitcffXVna73G9/4Br///e855ZRT+NGPfsRPfvITHnroIe699162b9+Ow+Fobe66//77eeSRR5g9ezZNTU04nc7D3g9CiP5PahgAHNsrgWbMmLHffQ0PP/wwkydPZubMmezevZutW7ceNE9eXh6FhYUATJs2jR07dnS6/Pr6eurq6jjllFMAuPbaa1m5ciUAkyZN4qqrruLZZ5/FZjPHC7Nnz+b222/n4Ycfpq6urnW4EEK0N6BKhs5qAi0tm9E6TELC+GMSR0JCQuv/K1as4O233+ajjz4iPj6eU089tcP7HhwOR+v/Vqv1kE1SnXnzzTdZuXIlS5Ys4Re/+AUbNmxg4cKFnH/++SxdupTZs2ezbNkyTjjhhCNavhCi/5IaBgBWIDbnMJKSkro8J1BfX09aWhrx8fFs2rSJVatWHfU6U1JSSEtL47333gPgL3/5C6eccgrhcJjdu3dz2mmncd9991FfX09TUxPbtm2joKCA//mf/2H69Ols2rTpqGMQQvQ/MathKKWeBC4AKrTWEzsYfwdwVbs4xgOZWusapdQOoBFz6VJQa10UqzhNLBbC4dgkDLfbzezZs5k4cSLnnnsu559//n7j586dy2OPPcb48eMZN24cM2fO7JH1Pv3003znO9+hpaWFkSNH8tRTTxEKhbj66qupr69Ha82tt95Kamoq//u//8vy5cuxWCzk5+dz7rnn9kgMQoj+RWmtY7NgpU4GmoBnOkoYB0x7IfB9rfXpkfc7gCKtddXhrLOoqEgf+AClL7/8kvHju25q8np3EAzWk5g4+XBWN2B0Zx8KIY5PSqnV3T0oj1mTlNZ6JVDTzcmvBBbHKpZDs6J1bO7DEEKI/qLXz2EopeKBucAr7QZr4F9KqdVKqZtiH4MFCBOr2pYQQvQHfeEqqQuBD7TW7Wsjc7TWZUqpLOAtpdSmSI3lIJGEchNATk7OEYYQzZuaY32JrRBCHC96vYYBXMEBzVFa67LI3wrgVWBGZzNrrRdprYu01kWZmZlHFICpYSDNUkII0YVeTRhKqRTgFOC1dsMSlFJJ0f+Bs4GNsY0kuhtic6WUEEL0B7G8rHYxcCqQoZQqBX4M2AG01o9FJrsY+JfWurndrIOAVyPPYbABz2ut/xmrOE2sVkxckjCEEKIzMUsYWusruzHNn4E/HzCsBDjG17f2rRpGYmIiTU1N3R4uhBDHQl84h9Hr2s5h9I2EIYQQfZEkDNoSRiyeibFw4UIeeeSR1vfRhxw1NTVxxhlnMHXqVAoKCnjttde6WMr+tNbccccdTJw4kYKCAl588UUA9u7dy8knn0xhYSETJ07kvffeIxQKcd1117VO++CDD/b4NgohBoa+cFntsXPbbbD24O7NLTqMK9xsujdXh7lLCgvhoc67N1+wYAG33XYbN998MwAvvfQSy5Ytw+l08uqrr5KcnExVVRUzZ85k3rx53XqG9t/+9jfWrl3LunXrqKqqYvr06Zx88sk8//zznHPOOdx9992EQiFaWlpYu3YtZWVlbNxorhs4nCf4CSFEewMrYXSmtYzu+Rv3pkyZQkVFBXv27KGyspK0tDSGDx9OIBDgrrvuYuXKlVgsFsrKyti3bx+DBw8+5DLff/99rrzySqxWK4MGDeKUU07h008/Zfr06Xzzm98kEAgwf/58CgsLGTlyJCUlJXzve9/j/PPP5+yzz+7xbRRCDAwDK2F0UhPQ4QCe5nU4HDnExWX1+Govu+wyXn75ZcrLy1mwYAEAzz33HJWVlaxevRq73U5ubm6H3ZofjpNPPpmVK1fy5ptvct1113H77bfzjW98g3Xr1rFs2TIee+wxXnrpJZ588sme2CwhxAAj5zCI/Y17CxYs4IUXXuDll1/msssuA0y35llZWdjtdpYvX87OnTu7vbyTTjqJF198kVAoRGVlJStXrmTGjBns3LmTQYMGceONN3LDDTewZs0aqqqqCIfDXHrppfz85z9nzZo1MdlGIUT/N7BqGJ2K7WW1+fn5NDY2MmzYMIYMGQLAVVddxYUXXkhBQQFFRUWH9cCiiy++mI8++ojJkyejlOLXv/41gwcP5umnn+Y3v/kNdrudxMREnnnmGcrKyrj++utbu2//1a9+FZNtFEL0fzHr3rw3HGn35gCNjWuw2zNxOofHKrzjlnRvLkT/1Se6Nz/eRHusFUII0TFJGK3kmRhCCNEVSRgRUsMQQoiuScJoZZGuQYQQoguSMCKUkoQhhBBdkYTRykos+pISQoj+QhJGRKxqGHV1dTz66KNHNO95550nfT8JIfoMSRgRsTrp3VXCCAaDXc67dOlSUlNTezwmIYQ4EpIwWsWmhrFw4UK2bdtGYWEhd9xxBytWrOCkk05i3rx5TJgwAYD58+czbdo08vPzWbRoUeu8ubm5VFVVsWPHDsaPH8+NN95Ifn4+Z599Nh6P56B1LVmyhBNPPJEpU6Zw5plnsm/fPgCampq4/vrrKSgoYNKkSbzyyisA/POf/2Tq1KlMnjyZM844o8e3XQjRvwyorkE66d0cgHB4EFqnYbVq2nVfe0iH6N2ce++9l40bN7I2suIVK1awZs0aNm7cSF5eHgBPPvkk6enpeDwepk+fzqWXXorb7d5vOVu3bmXx4sX86U9/4vLLL+eVV17h6quv3m+aOXPmsGrVKpRSPPHEE/z617/mgQce4Gc/+xkpKSls2LABgNraWiorK7nxxhtZuXIleXl51NTUdHubhRADUyyf6f0kcAFQobWe2MH4U4HXgO2RQX/TWv80Mm4u8DvMmegntNb3xirO3jBjxozWZAHw8MMP8+qrrwKwe/dutm7delDCyMvLo7CwEIBp06axY8eOg5ZbWlrKggUL2Lt3L36/v3Udb7/9Ni+88ELrdGlpaSxZsoSTTz65dZr09PQe3UYhRP8TyxrGn4E/AM90Mc17WusL2g9QSlmBR4CzgFLgU6XU61rrL442oK5qAn5/HT7fbhISCrFYYlvxSkhIaP1/xYoVvP3223z00UfEx8dz6qmndtjNucPhaP3farV22CT1ve99j9tvv5158+axYsUK7rnnnpjEL4QYmGJ2DkNrvRI4knaOGcBXWusSrbUfeAG4qEeD61BsHtOalJREY2Njp+Pr6+tJS0sjPj6eTZs2sWrVqiNeV319PcOGDQPg6aefbh1+1lln7feY2NraWmbOnMnKlSvZvt1U8KRJSghxKL190nuWUmqdUuofSqn8yLBhwO5205RGhsVU2zMxevbEt9vtZvbs2UycOJE77rjjoPFz584lGAwyfvx4Fi5cyMyZM494Xffccw+XXXYZ06ZNIyMjo3X4D3/4Q2pra5k4cSKTJ09m+fLlZGZmsmjRIi655BImT57c+mAnIYToTEy7N1dK5QJvdHIOIxkIa62blFLnAb/TWo9RSn0NmKu1viEy3TXAiVrrWzpZx03ATQA5OTnTDnwQUXe75g4E6vB6vyI+fjxWa8Ihpx9IpHtzIfqv46J7c611g9a6KfL/UsCulMoAyoD2D6XIjgzrbDmLtNZFWuuizMzMI44nVjUMIYToL3otYSilBiulVOT/GZFYqoFPgTFKqTylVBxwBfB67OOJ7VP3hBDieBfLy2oXA6cCGUqpUuDHgB1Aa/0Y8DXgu0qpIOABrtCmfSyolLoFWIa5rPZJrfXnsYqzjRUTm/QnJYQQHYlZwtBaX3mI8X/AXHbb0bilwNJYxNUZaZISQoiu9fZVUn2INEkJIURXJGFESA1DCCG6JgmjVWxu3DsSiYmJvR2CEEIcRBJGhLlgS566J4QQnZGE0U4snomxcOHC/brluOeee7j//vtpamrijDPOYOrUqRQUFPDaa68dclmddYPeUTflnXVpLoQQR2pgdW/+z9tYW95J/+ZAKNSMUlYsFme3l1k4uJCH5nbeq+GCBQu47bbbuPnmmwF46aWXWLZsGU6nk1dffZXk5GSqqqqYOXMm8+bNi9R0OtZRN+jhcLjDbso76tJcCCGOxoBKGJ1qbga7PXIrRs92lTJlyhQqKirYs2cPlZWVpKWlMXz4cAKBAHfddRcrV67EYrFQVlbGvn37GDx4cKfL6qgb9MrKyg67Ke+oS3MhhDgaAyphdFoTWLsW0tJozmhBKSvx8WN7dL2XXXYZL7/8MuXl5a2d/D333HNUVlayevVq7HY7ubm5HXZrHtXdbtCFECJW5BwGgNUKoRBKxeak94IFC3jhhRd4+eWXueyyywDTFXlWVhZ2u53ly5dzYKeJB+qsG/TOuinvqEtzIYQ4GpIwwCSMYBCzO3o+YeTn59PY2MiwYcMYMmQIAFdddRXFxcUUFBTwzDPPcMIJJ3S5jM66Qe+sm/KOujQXQoijEdPuzY+1oqIiXVxcvN+wbnXNvWULhEJ4ch2EQs0kJhbEMMrjj3RvLkT/dVx0b96nRJqkYlXDEEKI/kASBoDNBsFgzM5hCCFEfzAgEsYhm92iJ72lhnGQ/tRkKYQ4Ov0+YTidTqqrq7su+KxW0Bq0ArTUMiK01lRXV+N0dv9GRiFE/9Xv78PIzs6mtLSUysrKzidqbISaGkKbAwR0PQ7Hl+2ewDewOZ1OsrOzezsMIUQf0O8Tht1ub70LulMvvQQLFlDxzj18oe5h5sxdOJ3Du55HCCEGGDmMBkhNBcDWaJqiQqHm3oxGCCH6pJglDKXUk0qpCqXUxk7GX6WUWq+U2qCU+lApNbnduB2R4WuVUsUdzd+jIv0s2RrNszDCYUkYQghxoFjWMP4MzO1i/HbgFK11AfAzYNEB40/TWhd294aSoxJJGNZIwggGG2K+SiGEON7E7ByG1nqlUiq3i/Eftnu7Cui9M6utTVKma3G/v7zXQhFCiL6qr5zD+Bbwj3bvNfAvpdRqpdRNMV/7AecwJGEIIcTBev0qKaXUaZiEMafd4Dla6zKlVBbwllJqk9Z6ZSfz3wTcBJCTk3NkQdhskJSEpcGLUg78/r1HthwhhOjHerWGoZSaBDwBXKS1ro4O11qXRf5WAK8CMzpbhtZ6kda6SGtdlJmZeeTBpKai6uqIixssNQwhhOhAryUMpVQO8DfgGq31lnbDE5RSSdH/gbOBDq+06lFpaVBbi8MxRGoYQgjRgZg1SSmlFgOnAhlKqVLgx4AdQGv9GPAjwA08GnmOdTByRdQg4NXIMBvwvNb6n7GKs1VaGkRqGC0tW2O+OiGEON7E8iqpKw8x/gbghg6GlwCTD54jxlJToaSEuLh86ureO+arF0KIvq6vXCXV+yJNUnFxQwgGqwmH/b0dkRBC9CmSMKLaNUkB+P37ejkgIYToWyRhRKWmQlMTccpcaSUnvoUQYn+SMKIi3YM4PAmAJAwhhDiQJIyo1oRhHhYk92IIIcT+JGFEtXYPYgUUPp/UMIQQoj1JGFGRGoalvhG7PUNqGEIIcQBJGFGRhGGulJK7vYUQ4kCSMKIiTVLmXozBkjCEEOIAkjCiojWMyM170iQlhBD7k4QR5XRCXFzrzXt+fzla696OSggh+gxJGFFK7ddjrdYBAoHqQ88nhBADhCSM9tr1JwVyL4YQQrQnCaO91NQD+pOSE99CCBElCaM9qWEIIUSnJGG015owpIYhhBAHkoTRXqRJymZLwmJJkIQhhBDtSMJoL/JMDMLh1ktrhRBCGDFNGEqpJ5VSFUqpjZ2MV0qph5VSXyml1iulprYbd61SamvkdW0s42yVlgbhMDQ24nAMkQ4IhRCinVjXMP4MzO1i/LnAmMjrJuCPAEqpdODHwInADODHSqm0mEYKbd2DtPYnJTUMIYSIimnC0FqvBGq6mOQi4BltrAJSlVJDgHOAt7TWNVrrWuAtuk48PWO/7kGG4vOVyt3eQggRYevl9Q8Ddrd7XxoZ1tnw2MrIMH8rKnBNyCMcbiYQqCQuLivmqxbiWAiFoLERfD5ITjY94mgNlZVQXm7G22zmFRdnXqEQ1NebVzBoOkVQygwPhcz8YIYFg+D1muUrBVarGef1gsdj/o8uVynzPhyGQAD8fjNNczO0tJjxNlvbMsCsKxQy84wYAdOmwfjxJvatW2HHDrMtVVUmBpvNLKepycTv80FKimlMcLnaYvD5zDo9HhOH32/W5XCYl8sF8fEm7qYmaGgw22S1mlcwaJbh95tpExPNvo0u3+Mx629sNDFFlxvd136/WWZjo5nebjcvh8OsM7qMlhYzfXy8GRfdr0lJ8OqrMfvatOpWwlBK/SfwFNAIPAFMARZqrf8Vw9i6RSl1E6Y5i5ycnKNbWF6e+bt9O86powDweLZJwhAH8fnaCia/f//CU2vzI44WAH6/KVCCwbbCtX3BGp0vHG4rAKKFQ7QQ83jMuOhywBRITqcpsKLzNzWZdba0mGmiw0OhtnW2Z7e3je9LLJa2pHQkEhNNgRpNLgkJJlE4HOZzqa01+yL6eUUTgtO5fyHt85npvF6zT/1+s+xoso1+7tEkYLe3Jb3ovtbaTJuSYgr26Occ/eyDQTN/dDyYBOr1tiU+MNvgcpnl19SY8U6nGZaScnT7u7u6W8P4ptb6d0qpc4A04BrgL8DRJowyYHi799mRYWXAqQcMX9HRArTWi4BFAEVFRUfXfjR0qPnES0pwuU4GTMJISZl1VIsVRy8cbitEPZ62QjYYNMOam80RXG2tudAt+oP1+838Fov5v6nJjLNYzEdtsZgCtqGhrUCIHu1VV5tlBYNtBVf0iDK63KNlsZjlKWX+d7nMy+lsKyBcLnC7zTC73RQuWpuCxOMx+ya6rJwcU5i5XOZ9+3XYbGaZycltBWddnRk3ZAgMHmwKymDQFFjRo36LxRRIKSlt69a6bV9YLG3D7Pa2fjyjtQFoS25Kte3jKKXaah3R7bbbzfDoMqJH6tHtAdi2DVavhk2bYNgwGDPGHPNlZprtFz2vuwkj+nGdB/xFa/25Uu0/wiP2OnCLUuoFzAnueq31XqXUMuCX7U50nw38oAfW1zWrFXJzTQ3DmQcovN5tMV9tf6G1KbSbmsyPPjHRDGtuNoVyWZlpMog2fWhtCvW6OjNfVZV51dSYwjB6lNXU1NaccbiihYvWprBLSjKxaW0KrXDYDEtObmtySEw0BVB6umm6sNvbCvXoEWFiImRlmVbMaNNC9KhYqbajvqQkMz5auEbHW62mcLR18xeotabaU43b5aZnfnp9mzl3qFqbpToyerR5dSYQClDvq6fR10iqM5VUZ2pM9l0oHMKiLL36uWitj8n6u5swViul/gXkAT9QSiUB4UPNpJRajKkpZCilSjFXPtkBtNaPAUsxSegroAW4PjKuRin1M+DTyKJ+qrXu6uR5z8nLg5ISrFYnDkc2Hs/ASxjhsCnAq6th1y5zJLdrV1v7rs8HTf4mqvVWKr37qG5soK6lmabywYSq8sCbCqnbIX0b+BNg1xxoyTQLT6gw4yxBUGHQFpy2eJJcDhKG7sI68guYspNEMkkmG5c1EVw1hOKqcTnspDrcuB2DGOWaTootE6tVU8rHLG/4E844C+fkXcDccadTFdxJccX7bK7+Eo1Ga4036KXWW0ujv5Epg6dwyfhLKBxcyEe7P2Lp1qXsbdrL4MTBDEoYRFZCFlkJWbjj3ViVaUQP6RDeoBdf0Mfepr2U1JZQ3FBKnIoj0ZpIgj2BeHs8LruLiuYKNn6+ka01W4mzxpHmTCPZkYzVYsWiLCTaExmcOJishCz8IT+13lpaAi24XW4yEzJxWB3Uemupbqlm7b61rCpdRVVLFcOShnFa3mnkZ+bjC/rwBD0kxSUxPGU4GfEZbK3eyrp966hormCcexz5WflUNlfyzo53WFW6iuzkbE4cdiL5mfkEw0FaAi34Q37COkxYh3HYHLhsLlx2Fw6rA4fNQZO/iS3VW9hasxVv0IvdYsdhc5DmTMPtcuOwOaj2VFPdUk1LoIVAOEAwHCQUDhHW4db1eIIeLMpCgj2BxLhE3PFushKySHGktG7LvuZ9fFXzFTvqdlCQVcAVE6/grJFnsbl6M5+UfUK1p5qc5BxyUnII6zDVnmpqPDXUemrN/vJUU95UTnlTOQ2+hv2+13HWOIYnD2fqkKkUDS0i1ZlKeVM5Fc0VBMOmnc8b9LK7YTc763bS5G/CZXcRb49vjTkhLgG7xY7dam/dL9trtzMkaQjnjDqHk3JOYl/zPjZVbaK0oZRAOEAgFCDdlc6otFHkpeUR1mGa/E3Ue+vZ17yP8qZyqj3VNPgaaPI3MSRxCOMzxzMkcQg76nawpXoL3qCX7ORshiUPo95bT0ltCaUNpXiDXoLhIJkJmez9r9jfBqC6cxWQUsoCFAIlWuu6yGWv2Vrr9bEO8HAUFRXp4uLio1vId78LL70E1dWsXXsa4bCfqVM/6JkAe1hZQxn7mvfR4GvAH/KT4kgh1ZlKo7+RbTXb2Fm/k8GuHIaqqQRrh/HhtrWs3vcxu+vLqG8I09AUxBtXSiBxG6HE3aYAB/AmQ8VE87L5IP0rSNmJsoZM04CjgXDC4X05s6xj8KpaGoJVh5w2MS6RJn/TIaebmDURm8XG2vK1JMUloZQ6qJBIjEvEbrED4LQ5SXOl4bQ5Wb9vPcFwEKuyEtIh7BY7Q5KGUNFcgTfo7Wh1HW9XQhahcIgmfxO+kK91uEIxKn0U49zjCOkQtR6TqMI6TCgcotHfSEVzBWFt9rlFWXDZXDQHmvdbvkIx1j2WWcNncYL7BNaUr2HFjhVUNFcAphD0h/ZvH4smvGgBD5Cfmc/s4bMpbSzl49KPqfa0dd1vt9ixWqwoFP6Qn5A++MRBqjOVse6xJNgTCIQD+II+ajw1VHuq8QV9uOPduF3u1gLVZrFhtVixKitWi9UkUpsLjabJ30SDr4HqlmoqWyqp99bjtDmJt8fjjnczJn0M2cnZvLfrPT4p+6Q1BqfNSUZ8Bnsa97TuNwCXzUW6K51UZyrueHdr0s+IzyDNmUaSI4k6bx3lTeVsq93G6j2r2V63vXX+NGcacVZz0sJutZOdnE1uai7Jccl4gh5aAi00B5pp8jfR7G9uTQIuu4ux7rGMShvF1pqtvF3yNnXeutbPYETqCBxWB3arnaqWKr6q+YqWQMt+2xON1R3vJsWRQoI9gdLGUr6s/JI9jXvIS8tjrHss8fZ4ShtKKWsoI8WZQl5qHjkpObhsLmwWGynOFO6cfWd3vrIHUUqt1loXdWfa7tYwZgFrtdbNSqmrganA744our5u5EjTJlJfj9M5iurqN3otlIrmCv6x9R+8sfUNqlqquOSESzg/93I+2PoFf1x/Hx9VLjuyBTvisWRasQ6ykhAeyqDwKFL0yVixY1HgS6qiJm0DFWPeIc7qIC9lNGMyx+GwmR9VvD2eMeljGOMew7CkYSQ7knHZXext3Mv2uu3UeevIS81jVPooajw1rNy5ko/LPibDlUF+Vj6j00fjsDqwKAshHcIT8OAJehiaNJQJmRPIiM/AG/RS2lDaetSd7krHH/JT7ammrKGMD3Z/wPIdy6nz1vHoeY9y9aSrcdgcvL/rfd7d8S4j00YyJ2cOI9NGdlhVr/XU8saWN1hbvpaTRpzEGXlnkORIQmtNg6+BiuYKKlsqqW6p3q9Qd9qcOGyO1gLBaXO2LjMYDuIJmAImuk+6EgqHqPHU4LA5SIxLxKIs+II+Klsq8Yf8+9VK2tNa0xJowWlzYrVY8QQ8lDaUUtFcwaj0UQxOHNy6/B11O0iIS2gdFp2/zluHw+bAaXNiUZb9xgXCATwBD76QD1/Q11pQ90aTS0ltCR/u/pAJmRMoyCrAbrUTCAXY07gHq8WK2+U+5H7uSLQ2NChxUGuyOFrBcJAt1VsYkjiENNfBt41pralqqcJutZNgT8ButffIeo+l7tYw1gOTgUmYm/GeAC7XWp8S0+gOU4/UMF5+GS67DD77jJ1p/2D79ruYM6cRmy2xR2Ks89ahUCQ7kgHY07iHzys/p8HXwNCkoWTGZ/Lervd4fsPzvLP9HTSaVOsQrH431dZ2N8w3DYJPboF9k7DrZLIH23EPqydCv7SBAAAgAElEQVRlcC1JzniSg6NJCA7Hkr4TT+oagvGlzMydzDkTZzAya3An0e2vL7TNCiFiKxY1jKDWWiulLgL+oLX+P6XUt448xD5s5Ejzt6QE1xxzaa3Xu43ExMlHtDitNcV7inljyxss/WopxXtMQrMqKw6bY78qanuqdhR6/d2w6WLq9k7B4VAUnf45CdNfYeSgwZw3+RtknO8kLw+GD287uXuwVEyuP3wHHtkKIQa27iaMRqXUDzCX054UOadx/NWnuqPdvRius04FzKW1h5MwKpsr+WD3B/y75N/8ffPfKW0oxaIszMyeyU9O/Qnal8jGkmp2lLVQ89Vodq/OJ9CQBkl7Sc3ey/iMCUwfNoOxlypGjTI5LDcX4uLygfwe32QhhOiO7iaMBcDXMfdjlCulcoDfxC6sXpSWZq6lLCnB6TSVqO5cKdXoa+SptU/x+OrH+aLyC8CcjDtn9Dn87NRfkFV3Pu+86eYvv4KvvjLz2O0wdSrMvxBmzYKZMyeTnR2zLRNCiKPSrYQRSRLPAdOVUhcAn2itn4ltaL0ocmmt3Z6KzZbeZcLQWvOr93/FfR/cR4OvgVnZs7jvzPuYPXw2YxKn8dzTTn6y0Nx/YLfDmWfCd74DM2eaZCE3GAkhjhfd7RrkckyNYgXmJr7fK6Xu0Fq/HMPYes/IkbDRnGB2uUZ3evNeKBzi5qU38/jqx5l/wnx+MOcHzBg2A68X7r8fzv21uWHtpJPg5z+HCy44drfwCyFET+tuk9TdwHStdQWAUioTeBvonwkjLw+WLIFwGJdrFA0Nqw6aJBAKcO3fr2XxxsX8YM4P+MXpvwAUb7wBt91mbna75BK46y7TQZoQQhzvutu9uSWaLCKqD2Pe48/IkeaW5r17cblG4fXuIhwOtI7+bO9nzPq/WSzeuJh7z7iXX5z+S958UzFrFlx4oWl6eusteOUVSRZCiP6juzWMf0b6d1oceb8A061H/xS9UqqkBOfoUUAIr3cncc5c7vr3Xfz2o9+SEZ/By5e9zDk5l3LWWfDvf5vulh97DK6/vq23SyGE6C+6e9L7DqXUpcDsyKBFWutj0Pt6L4nei7F9O66CtnsxHvnsVX7z4W+4YcoN/PqsXxMXTuO88+D99+H3v4dvf9vULoQQoj/q9gOUtNavAK/EMJa+Y8QI06VoSQku15kAfFH+ET9acR8XjbuIRRcuoqVFccEFJlk89xxccUUvxyyEEDHWZcJQSjUCHfUdogCttU6OSVS9zeEw/Vtv305c3BCUcnLHyieJs8bxyHmPoLXi61+HlSvh2WclWQghBoYuE4bWOulYBdLnjBwJJSUopfhXtZtV+3bz+AWPMyx5GD/8Ibz+Ojz8MFx5ZW8HKoQQx0b/vdLpaEVu3mvwNfDI5kqmpDm4YeoNvPgi/OIXcOONcMstvR2kEEIcO5IwOnPCCbBnD79bcR/1fj/fzvOxdYuX66+HOXPgD3/Y/7GRQgjR30nC6ExBAXVOeKD4YebmzWBcEtxzj+lZ9sUX5bJZIcTAIwmjMxMn8ttZUB9s4scn38WuXeN46aV0br4Zhg7t7eCEEOLY6/ZltUdCKTUX82Q+K/CE1vreA8Y/CJwWeRsPZGmtUyPjQsCGyLhdWut5sYz1QNUZCTw0E77mHcWMnPO5+2kvTmeAO+90HMswhBCiz4hZwlBKWYFHgLOAUuBTpdTrWusvotNorb/fbvrvAVPaLcKjtS6MVXyH8uDHD9EUB/d8nMIXX9hYvvwyrr/+FTIzL+utkIQQolfFsklqBvCV1rpEa+0HXgAu6mL6K2nreqRXtQRa+GPxH5nvyyX/4+385Cea+Hgvl1/+y94OTQghek0sE8YwYHe796WRYQdRSo0A8oB32g12KqWKlVKrlFLzYxfmwf6y7i/UeGr4ftZFbK1188orcO21n+FwrCUUaj6WoQghRJ8R03MYh+EK4GWtdajdsBFa6zKl1EjgHaXUBq31QQ+mUErdBNwEkJOTc9SBhHWYhz5+iKlDpjJn7HxuYQx2q+a7362nqgqam78kOblbz0sXQoh+JZY1jDJgeLv32ZFhHbmCA5qjtNZlkb8lmAc3TTl4NtBaL9JaF2mtizIzM482Zt7a9habqjbx/ZnfpyZ7Mk9xPVcVfs7IkaYTwpaWLw6xBCGE6J9imTA+BcYopfKUUnGYpPD6gRMppU4A0oCP2g1LU0o5Iv9nYHrJPSYl9YOrHmRI4hAuz7+cx15Mw0M8tw9ZjNM5CqXiaG7+/FiEIYQQfU7MEobWOgjcAiwDvgRe0lp/rpT6qVKq/SWyVwAvaK3bd3I4HihWSq0DlgP3tr+6Kla2VG9h2bZl3Dz9ZnQwjj/8Ac5xFzOx9J9YLDbi48dJwhBCDFgxPYehtV7KAQ9a0lr/6ID393Qw34dAQSxj68i/S/4NwNcLvs7ixVBeDn/52mpY8gUEgyQk5NPQ8PGxDksIIfoEudO7nU/2fEJWQha5qbk89xyMHQtnnO8Enw+2bSM+Ph+vd7tcKSWEGJAkYbTzcenHzBg2g4YGxbvvwvz5oCZFKjobNpCQMAEwV0oJIcRAIwkjot5bz6aqTZw47ESWLYNAAC68EBg/HiwW2LCBxMTJADQ1rendYIUQohdIwogo3lOMRjNj2AyWLAG3G2bNAlwuGDMG1q/H6RyJ3T6I+vr3ejtcIYQ45iRhRHxcZk5mT8maztKlcP75YLVGRk6aBOvXo5QiNfUk6uokYQghBh5JGBGflH3CWPdYNq1No6Ym0hwVNXkylJRAQwMpKXPw+Xbi9e7udFlCCNEfScIAtNZ8XGZOeL/+OtjtcPbZ7SaYbM5dsGEDKSknAUizlBBiwJGEAZQ2lFLeVM6Jw05kyRI47TRITm43QTRhrFtHYuJkrNYkSRhCiAFHEgZt5y8Gh2awefMBzVEA2dmQmho5j2ElOfn/yXkMIcSAIwkDc/4izhqHf5epScyZc8AESplaxrp1AKSmnkRLy+cEAtXHOFIhhOg9kjAwNYzCwYXsLTWPX83N7WCiyZNhwwYIh9udx/jw2AUphBC9bMAnjGA4SPGeYk4cdiI7d5pzF6mpHUw4eTI0N8O2bSQlTUcpu5zHEEIMKAM+YQC8ceUb3DTtJnbtgk6fwTRpkvm7bh1Wq4ukpOmSMIQQA8qATxg2i43T8k5jYtZEdu6EESM6mTA/33QR0noe42QaG4sJBhuOXbBCCNGLBnzCaK/LGobLBePGwfr1AKSnn4/WQWpq/nHsAhRCiF4kCSOiqQlqarqoYcB+V0qlpMzCbs+kqurvxyZAIYToZZIwInbtMn87rWGAOY+xcyfU1aGUFbd7HtXVbxIO+45JjEII0ZskYUTs3Gn+HrKGAa21jIyM+YRCjdTVrYhpbEII0RfENGEopeYqpTYrpb5SSi3sYPx1SqlKpdTayOuGduOuVUptjbyujWWc0M0axqxZEBcHfzfNUGlpZ2KxJEizlBBiQIhZwlBKWYFHgHOBCcCVSqkJHUz6ota6MPJ6IjJvOvBj4ERgBvBjpVRarGIFU8Ow2WDIkC4mSksz/YY8+yz4/VitTtzuc6mqeg2tw7EMTwghel0saxgzgK+01iVaaz/wAnBRN+c9B3hLa12jta4F3gLmxihOwNQwsrPbPQOjM9ddB1VV8A9zdVRGxnz8/r00NHwSy/CEEKLXxTJhDAPaPzSiNDLsQJcqpdYrpV5WSg0/zHlRSt2klCpWShVXVlYecbBd3oPR3jnnwKBB8PTTgLm8VikbVVWvHvG6hRDieNDbJ72XALla60mYWsTTh7sArfUirXWR1rooMzPziAPp8h6M9ux2uPpqWLIEKiux21NJSzuH8vKnCIVajnj9QgjR18UyYZQBw9u9z44Ma6W1rtZaR69JfQKY1t15e1IwCGVl3axhAFx7rZlp8WIAcnIWEghUsnfvn2IVohBC9LpYJoxPgTFKqTylVBxwBfB6+wmUUu1PMc8Dvoz8vww4WymVFjnZfXZkWEyUlUEo1M0aBkBBAUydCn/+MwCpqXNISTmFXbt+LfdkCCH6rZglDK11ELgFU9B/Cbyktf5cKfVTpdS8yGS3KqU+V0qtA24FrovMWwP8DJN0PgV+GhkWE9FLartdwwC45hr47DPYujUy7w/x+/dQXv7nHo9PCCH6AqW17u0YekxRUZEuLi4+7PmefdaU/5s2me6iumXnTvPgjPvugzvvRGvNmjWzCAT2MWPGFiwW+2HHIYQQx5pSarXWuqg70/b2Se8+IVrDGD686+n2M2KEaZaK3MSnlGLEiB/i9e6gvPywz90LIUSfJwkDU1nIzIT4+MOc8eKL4aOPYO9eANzu80lOnsX27T8kGGzs+UCFEKIXScLgMC6pPdD8+ebva68BppYxevRDBAL72LXrlz0XoBBC9AGSMDiMm/YOlJ8Po0e3NksBJCfPYNCga9i9+7d4PNt7LkghhOhlAz5haH0UNQylTLPUO+9AfX3r4JEjf4VSNrZtu6PnAhVCiF4mCUPD88+be/GOyPz5EAjAm2+2DnI4hpGT8wOqql6hqur1LmYWQojjx4BPGBYLzJsHhYVHuICZM2HwYHNtbjs5OXeSkDCZzZtvxO8/8j6uhBCirxjwCeOoWSxw662m99q33243OI7x4/9CMFjHli3fpj/d7yKEGJgkYfSE738fRo6E//xP0zwVkZhYQF7ez6mqepV9+57pxQCFEOLoScLoCU4nPPAAfPEFPPbYfqOGD7+dlJST2bLlZpqbP++lAIUQ4uhJwugpF10EZ54JP/qRecBShFJWJkx4Hqs1gY0bLyEYrO9iIUII0XdJwugpSsHvfgfNzXD++VBX1zrK4RhGfv5LeDzb2LTpOnmcqxDiuCQJoydNmAB//avpxfbss/dLGqmppzBq1G+oqvo7W7feSjgc7MVAhRDi8EnC6GkXXQSvvAJr18JZZ0FjW59S2dm3kZ19O3v2PMLGjRdK85QQ4rgiCSMWLrwQ/vY3U9O48krzdCaifU09wNixj1Nb+zZr1syipWVzLwcrhBDdIwkjVi64AH7/e3MH+H//936jhg69iUmT/oXfX8Hq1UVUVLzUS0EKIUT3ScKIpe9+19yb8dBDJnm0k5Z2GkVFn5GQUMAXXyxgy5abCYWaeylQIYQ4NEkYsfbAA6bvkVtvhf/9X9N5VYTTOZzCwhVkZ3+fPXsepbi4kPr6D3oxWCGE6FxME4ZSaq5SarNS6iul1MIOxt+ulPpCKbVeKfVvpdSIduNCSqm1kdfx24Of1Qovvwzf+hb8/Odw1VXm0tsIiyWO0aN/y+TJ76B1kM8+O4mSkrvROtSLQQshxMFiljCUUlbgEeBcYAJwpVJqwgGTfQYUaa0nAS8Dv243zqO1Loy85sUqzmPCboc//Ql+9StYvBiGDIFvfhPefRfC5p4M00S1nsGDv8muXb9k3bpzpNNCIUSfEssaxgzgK611idbaD7wAXNR+Aq31cq11S+TtKiA7hvH0LqVg4UL44AP42tfM/Rqnngq5ufA//wOPPYbt6hs54cx/Mu2fF9PQ8AHFxVPYtes3eL2lvR29EELENGEMA3a3e18aGdaZbwH/aPfeqZQqVkqtUkrN72wmpdRNkemKKyuPgyPy//f/4MknobwcnnsOJk2C3/7WnCB/7z1ITyfp/teZpv4PpzOHkpI7WbUqhw0bLsTnK+vt6IUQA5iKVbfbSqmvAXO11jdE3l8DnKi1vqWDaa8GbgFO0Vr7IsOGaa3LlFIjgXeAM7TW27paZ1FRkS4uLu7pTYm96mpzV/jIkeZvfj5kZsKnn9IS3MW+fX9h9+4HsFicnHDCU2RkXNjbEQsh+gml1GqtdVF3po1lDaMMGN7ufXZk2H6UUmcCdwPzoskCQGtdFvlbAqwApsQw1t7ldsOoUabZKi3N9Hi7fj3cey/x8aPJy/sJRUWrSWwaQuDaeZTdXcDevU/i9+/r7ciFEAOILYbL/hQYo5TKwySKK4Cvt59AKTUFeBxTE6loNzwNaNFa+5RSGcBs9j8h3r/Nm2fuEP/5z80Dxy+8kPh9+5h8525UPbB0I1/VfovNl1vIzLyUnJz/ISlpWm9HLYTo52KWMLTWQaXULcAywAo8qbX+XCn1U6BYa/068BsgEfirUgpgV+SKqPHA40qpMKYWdK/W+otYxdon/eEPpsbx17/C//0fAOrUU+GRR9D33MPoP/6V1PRT+fKcf1FZ+VdSU09j8OBvkpl5MVZrQu/GLoTol2J2DqM3HLfnMLri95uT4V4vnHeeSSKBAFxxBfztb4Svv4Y9t+ZR6n0Gr3cHFksCaWlnkpQ0lcTEqaSkzMFuT+3trRBC9FGHcw5DEsbxyu+Hu++GBx+ElBT0jTfi372e8MZi/M5mKqe1UDsVHJWKrC1DSWjKwnr73bhmXkKkNieE6MtCIXPjb4xJwhhINm6Em2+GlSth8GAYPx727TOPi43QVkXIobF6oHJeCoGr5+OOm43TnwZFReZekOOV1qbWJURvKykx91p961twzjldTxsKQUMDJCRAXNz+43w+02HpU0/Bvfea33cMv+OSMAYaraGlxXz5onbuNHeS5+TAjBn4GnYS+NEtJDy1HBXc/zMPzJyAuvqb2M65yFytpTV89BH8/e+mG5O0tLYrucaNM5f/HvglP5SdO2HQIPP8856gNdx2GyxZAitWmO3sj3buND0DHO7+PlqhEHz5JaxaZT7/Sy7pvcQcDoOlhy7o1Bp27zbb9fnnppDftQtGjIAZM2DmTJg6tev1aQ3r1kEwaA64wBykzZ4N2yJX/l9zjbm/KiPDvG9pMecjFy+GLVtMDMHIQ9Ti4szD1668Ek4+2fQ79+mnMHGiOSC88ELTgWlKipk2IaHn9geSMHo7jL6tpITA+g+p1aupblmOc8UGBi0LEx+5xTLgtqMsdmyVLei4OFRSkrk3JNSubyulYOhQU0gPGgTp6ZCUBHv3wo4d5sdx2mlw7rmwZw88/rj5ASQnmwdMzZtnakPJyWYZqUdwjuXHP4af/tRU2SdNgvffh/h488P93e/Mek48setldJRoOxMMQkVF2/usLLDF6JqRzz4zBcvrr8PmzXDKKbB0qdm+zmht9v2QIUeXlDdtgocfNjeVNjS0DZ8/33RvEy0Ao9rfQ9Q+oYTD5rMvKTH/n3zy/oXcli3mQWN/+5spuM891xSY557b9nls3gw/+pGZbvRoUzgnJ5uHk23caArU22+Hiy/uvOmmvt4U4h9+aGrhH3xg4gIT7/Dh5vXVV+a7A+Y7fcEFZp07d5qE4nSa76zVCm+8Adu3t+2Xe+6B6683++4f/4B//9t0A6Q1ZGeb7/i6dWZ/jh5tEtOIEeZeq5YWM/zdd+Hjj80yk5NN7eLii81nceedpgk6SimTPNxuE9OQIaaV4De/OZxPut3iJGGIbgqFPDQ2fIpnzRLUB6uI+3gLQU81VbND1M1OJHHoySQnTSc5NJ6k8mTsJZXmx7V7t/kxVVZCTY15suCgQZCXZ77QK1eaE/VgbkT8xjdMAfDqq1Bb2xaAxWKO6M44w0w3bJj5ITU2mukCAZOQ3O626vtf/2pqF9/8pjnyvfBCuOwyuPxy+M53oKrKLPsb34D/+A94+23TAWQwaJoKTj3VJLDFi822XHABfP/75se3aBE8+yy4XDBtmqlVrV1rjkjbdRqJ222S0nnnmR/8V1+Z5X/ta6ZgixaeoZDZ7tWrzT7LyTGFa2KiKcwaGkwh5HSahPvooyb52e0mzsmTzZHq6aebBGKzmQJr82YzbtIkU9g8+CAUF5v5CgvhhBNMIdPSYj4Hn8+8D4VMAR4KmWFer/kMkpJMHKtXg8MBCxbAmWeapPvmm6apxe02+yoUMp/PmjVtR9QZGaYXA4sFtm41+8Pna9tf48bBf/1XW79qH35ohp94IhQUmG0qLzf7LS/P7KeVK83ncM01Zt98+ik0NZntzs+Hf/3LJKThw2HsWHPgYbOZxF5eDqWl+z3xkuHD4aSTYNYsU5MoKDDbCqZwLy0161yyxBT8DQ1mm3NyzLaUl5v1n346XHqp+e7/4hfme2G1ms/nvPPM8j7/HF54wSTxHTvMNt1wg1l/ZzW1bdvgrbfM451Hjmwb/uWX5jMOBEwcDQ3mt1FVZWLau9f8Ltav796P/gCSMMRRCYU81Nb+m+rqJTQ0fEhz8+eA+Z44HDkkJBQQFzcIuz2TpKQi3O4LsFoPOKr1eEzBl5hofpzRH4nfb77YdXWmwNy40RyRrVplfhDdNW+eOfK02cyR1Z13muFTp8If/2iOXB98sO3IbPZsU/isXGmGKWVqQZMmmQQRTTJ2uzlqtNlMAVxSYqaZM8c0G1gspsB8/31TyEWPwq1WMy4QMIVXbq5JqDt3tiXO7sjLg1tuMUesaWlm2DPPwHXXmQRWVmYKiAONHQs33mi24+OPTdxOp9lml8sUjHFxbXFaLGa809mWAJqbTZL6zndMDaq9devg298222OzmfkmTTJHy6mp5vP76COz3LFjYcwYk2xHjjQxPfCASTBgkse3vmVqFNmR7uNCIVMovv++KWy3bjWxLFx4cCxRoRC89pr5/PbtazvAGDzYHLwMG2YK+0izLCNGdLycjkQL58TE/YcfeM6srAx++UvzXfra17q//D5EEoboUcFgI01Na2hsXE1jYzHNzV8QCFQSCFSidQCrNYXMzEvJyLiI1NTTsNmSDn8lHo85wisrM0duycmmwLTZTA2mutocLfv9bUfALpeZV2vTROV0wh13mEIfzFHu+++bI+VowdTUBJ98Yi4OGDKkbd0vvGAS2Ne/vn8B1VX7uc9nCsGsLFMoNTebJPb88yaRjBhhEkdBgSnsc3PNNpaUmHWmpJjtDIdNUrHZzNFvR80rTzxhTn6eeaYp0OfMgQ0bTO1n9GiYO7dH27V7nNamOUgpUxORCxX6DEkY4pjQOkRt7XL27XuWqqpXCIWaUMpOfPwEwmEPwWA9NlsSCQkFJCRMIiVlNikps7Fau2iLF50LBmN33kQMWIeTMOTbJ46YUlbS088kPf1MwuHHqa//gJqaZTQ3b8RqTcJmSyEYrKGpaT1VVX8HNErFkZg4BZstCaXicDpH4HafT2rq6Vitrt7epL5NkoXoZfINFD3CYnGQlnY6aWmndzg+GGyivv596ureobGxmFComXC4hrq6d9mz549YLC4SEgpwuUbjdOZhtSZgscRhs6URHz+O+Pjx2O3px3irhBDtScIQx4TNlojbPRe3e+5+w8NhH3V171JdvZSWls+pr/+AiorFRE+yt2e3Z5GQkB9JHm6s1mTsdjcu1xji48dgt2fJXexCxJAkDNGrLBYH6elnk55+duswrcNoHSAc9hMIVNDSspmWli9pbv6C5ubPqah4nmCwngOTisWSgMuVh9OZi8s1lvj4cbhco4mLG0xc3CBstnRJKEIcBUkYos9RyoJSDiwWBzZbEi7XKNzu8/abRuswoVBzJKFsxePZitdbgsezHa+3hNratwmHvQcs14HTORyHI5tw2EcgUEU47CMxcRJJSdOJjz8Bmy0FqzWFuLhBOBzDsFiO8R3WQvRhkjDEcUkpCzZbUmtCgf2burQO4/PtxuMpwe/fRyCwD5+vDK93Fz5fKVZrAg5HDkpZaWr6jOrqN+m4GcwkDodjGDZbWmvNx+EYQkLCZBITC7DZ3FitiZFkIyfuRf8lCUP0S0pZcDpH4HR272atYLABr3cnoVAjwWAdfn85Pt9ufL7SSKLZSTC4HoslDqVs1NYuIxRqOmg5dnsGDkcODkd2u9dwnM7h2O2ZhMMeQqFmLJZ44uPHYLOl9PSmCxEzkjCEAGy2ZBITC7o9vdZhvN7tNDd/QTBYTyjURDBYjde7G59vF17vdurr3yMYrO1yOXZ7Bko50Nrc5R4XNySSZIYRF5eF3Z4V+ZuJ3Z6B1ZoYuYLMhcXiRCm7nJcRx4wkDCGOgFIWXK5RkeawzoVCzfh8pXi9uwkEqrBa47FaEwgGG/F4tuDxbEPrIBZLHFqH8fv34POV0tj4KYFAFRA+VCSYh1JqlLLgcIwgPn4cDscwwmEvoVALFosdmy0Vmy2NuLihOBzZ2GzJeL278Hp3YLUmkJZ2FomJk1CqD98t3oHm5i8pLX2IrKzLSU09XZJnjEnCECKGrNaEyH0k4w57Xq1DBALVBAKV+P0VBAJVkftXWgiFWtDaRzjsQ+twZPogXu92Wlo209i4Gqs1HovFhdZBgsFagsFatA4esBZF9NyN3Z6JzZZKOOyNvDyEw16UimttWrPb07BYnO1qN3aUsgFhtA5js6XgcAzH4RgKWCLrM09aVsqKUnFYraZ2FA77CYc9aB1svdGzu1ezaa3Zs+dRtm37b8JhL3v3LiIl5RRGjLib1NTTsFikaIuFmO5VpdRc4HeYZ3o/obW+94DxDuAZYBpQDSzQWu+IjPsB8C0gBNyqtV4Wy1iF6GuUshIXZ5qkEhLyj3p5WmsCgUp8vlKCwfrIuZUcAoEqamvforZ2OVr7IwnB0drsFQ77IudzdtPcvLc1mZgLAAJoHUQpK6AIhRro6OKBw2G1JuNyjURrTTBYF+lyxobFEtcaVzjsx+PZTHr6uYwZ8yjV1UvYteuXrF9/NjZbKunpc3E4sgkEagmFmnA4huJyjcJmc+P378Xn200gUEMo1EQ47MHpNJ1qRm8QtVpTUMpCMFhPMFgfOadVSjBYQ1zcYByOHOz2DNoSZTJxcUOx29Pw+yvwencSCFSgdRCtQ9jtGSQkTGi9Vygc9hEMNrSOt1rjsdnS9kuU5oChhkCgErDgdOa2dvJprhJswmpNPKa1wpj1JaXMN2gLcBZQCnwKXKm1/qLdNP8BTNJaf0cpdQVwsdZ6gVJqArAYmAEMBd4GxmqtQweupz3pS0qI3hUOB/D7y/H7I8+cwIpSlkgtKNRaq4jWXExNxeqfV7EAAAgtSURBVEYo1EQoVI/Ptxevd9v/b+/eYuyq6jiOf38zZy69YKetQ6WttEUaFYkU2pAqahrgAZRYQlBRUEJi8KFGMBoFo1FJfDAxogaCEEALNlysRRvivRCUxN6gqNBqwFalpDfa0lo6dGZO/z6sNcNhOk13Zzgzs8/8Pi8ze599zqw1/5nzP3utvdefrq5tSM1UKh00N08iokpEd83Zz2tMn76UmTM/1/8mW612sW/fb9i791H27v011erB/ucfOfISR4929bezqWkCLS2deT6oja6ubVSrB+r++2lunpL70XXMY1Irra2nElHNv49DvDH5ira2WUT00t29h/RZWlQqU5kw4QwWLtwwpDaNlbWkzgdeiIituVEPAkuBzTXHLAW+lb9fCdymFP2lwIMRcQTYJumF/Hp/qWN7zWyYmppaaG9PV4WNtObmCXR2XkFn5xXHPBYRdHfvoKdnb/8l0m/8NB/5Muzn+88qIqpUKlP6h8rS3E9H/xlEb+/+/jOrdGXdDnp6Xqa19W20t8+hpWVGvo+nie7unRw+vJmurudpamqnUplKc/NbkCpIzVSrr9LdvZOenl1IlXxxwyn5YofOPNz4L7q6ttLU1EpLywwqlQ6q1YP09OzN7ai/eiaMWcCLNdvbgYEl0PqPiYheSQeA6Xn/2gHPnTXYD5F0PXA9wOmNWqbTzIZFEm1tM/PcyuCPt7efTnv7id9D2tpOo63ttJNswdlMm3bxST5n7CnXJRGDiIi7ImJRRCzq7Owc7eaYmTWseiaMl4Da89LZed+gxyhdajGFNPld5LlmZjaC6pkwNgDzJc2T1ApcBawecMxq4Nr8/ZXAY5Fm4VcDV0lqkzQPmA+sr2NbzczsBOo2h5HnJD4P/I50We29EfGcpFuAjRGxGrgHuD9Pau8jJRXycQ+TJsh7gWUnukLKzMzqyyVazczGsZO5rLb0k95mZjYynDDMzKwQJwwzMyukoeYwJO0B/jPEp78VePlNbM5Y4X6VRyP2CdyvsW5ORBS6ia2hEsZwSNpYdOKnTNyv8mjEPoH71Ug8JGVmZoU4YZiZWSFOGK+7a7QbUCfuV3k0Yp/A/WoYnsMwM7NCfIZhZmaFjPuEIekSSf+U9IKkm0a7PUMl6e2SHpe0WdJzkm7I+6dJ+oOk5/PXqaPd1qGQ1Cxpk6RH8/Y8Sety3B7KC1yWiqQOSSsl/UPSFknva4R4Sfpi/ht8VtIDktrLGC9J90raLenZmn2DxkfJj3L//ibpvNFref2M64SRy8jeDlwKnAV8MpeHLaNe4EsRcRawGFiW+3ITsCYi5gNr8nYZ3QBsqdn+LnBrRJwJ7CfVfy+bHwK/jYh3AeeQ+lfqeEmaBXwBWBQRZ5MWHr2Kcsbrp8AlA/YdLz6XklbVnk8q6HbHCLVxRI3rhEFNGdmI6Ab6ysiWTkTsiIin8/f/I735zCL1Z3k+bDlw+ei0cOgkzQY+AtydtwVcSCrrCyXsl6QpwIdIKzYTEd0R8QoNEC/SKtgTco2bicAOShiviPgTaRXtWseLz1LgvkjWAh2STrYs35g33hPGYGVkBy0FWyaS5gLnAuuAGRGxIz+0E5gxSs0ajh8AXwGO5u3pwCsR0Zu3yxi3ecAe4Cd5qO1uSZMoebwi4iXge8B/SYniAPAU5Y9Xn+PFpyHfSwYa7wmj4UiaDPwCuDEiDtY+lotTleqyOEmXAbsj4qnRbsubrAKcB9wREecCrzJg+Kmk8ZpK+rQ9D5gJTOLYYZ2GUMb4DNd4TxgNVQpWUgspWayIiFV5966+U+P8dfdotW+ILgA+KunfpCHDC0lj/x15yAPKGbftwPaIWJe3V5ISSNnjdTGwLSL2REQPsIoUw7LHq8/x4tNQ7yXHM94TRpEysqWQx/XvAbZExPdrHqotg3st8KuRbttwRMTNETE7IuaS4vNYRFwNPE4q6wvl7NdO4EVJ78y7LiJVmCx1vEhDUYslTcx/k339KnW8ahwvPquBz+SrpRYDB2qGrhrGuL9xT9KHSWPkfWVkvzPKTRoSSR8A/gz8ndfH+r9Gmsd4GDidtJLvxyNi4EReKUhaAnw5Ii6TdAbpjGMasAm4JiKOjGb7TpakBaSJ/FZgK3Ad6UNcqeMl6dvAJ0hX7m0CPksazy9VvCQ9ACwhrUq7C/gm8EsGiU9OjreRht8OA9dFRMOV/xz3CcPMzIoZ70NSZmZWkBOGmZkV4oRhZmaFOGGYmVkhThhmZlaIE4bZGCBpSd9KvGZjlROGmZkV4oRhdhIkXSNpvaRnJN2Z63QcknRrrgGxRlJnPnaBpLW5PsIjNbUTzpT0R0l/lfS0pHfkl59cUx9jRb4ZzGzMcMIwK0jSu0l3MF8QEQuAKnA1aYG9jRHxHuAJ0h3BAPcBX42I95LuwO/bvwK4PSLOAd5PWtUV0grDN5Jqs5xBWoPJbMyonPgQM8suAhYCG/KH/wmkxeeOAg/lY34GrMr1Ljoi4om8fznwc0mnALMi4hGAiHgNIL/e+ojYnrefAeYCT9a/W2bFOGGYFSdgeUTc/Iad0jcGHDfU9XZq11aq4v9PG2M8JGVW3BrgSkmnQn995zmk/6O+lVg/BTwZEQeA/ZI+mPd/GngiV0PcLuny/BptkiaOaC/MhsifYMwKiojNkr4O/F5SE9ADLCMVPzo/P7abNM8BafnrH+eE0LcaLaTkcaekW/JrfGwEu2E2ZF6t1myYJB2KiMmj3Q6zevOQlJmZFeIzDDMzK8RnGGZmVogThpmZFeKEYWZmhThhmJlZIU4YZmZWiBOGmZkV8n91RhFDEOJPVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 672us/sample - loss: 0.1818 - acc: 0.9441\n",
      "Loss: 0.181804375336549 Accuracy: 0.9441329\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5643 - acc: 0.5084\n",
      "Epoch 00001: val_loss improved from inf to 0.96322, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/001-0.9632.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.5642 - acc: 0.5084 - val_loss: 0.9632 - val_acc: 0.7067\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8646 - acc: 0.7398\n",
      "Epoch 00002: val_loss improved from 0.96322 to 0.62579, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/002-0.6258.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8646 - acc: 0.7398 - val_loss: 0.6258 - val_acc: 0.8157\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.8277\n",
      "Epoch 00003: val_loss improved from 0.62579 to 0.42023, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/003-0.4202.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5893 - acc: 0.8278 - val_loss: 0.4202 - val_acc: 0.8791\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.8730\n",
      "Epoch 00004: val_loss improved from 0.42023 to 0.33235, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/004-0.3323.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.4380 - acc: 0.8730 - val_loss: 0.3323 - val_acc: 0.9066\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8978\n",
      "Epoch 00005: val_loss improved from 0.33235 to 0.26673, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/005-0.2667.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.3500 - acc: 0.8978 - val_loss: 0.2667 - val_acc: 0.9234\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9138\n",
      "Epoch 00006: val_loss improved from 0.26673 to 0.26205, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/006-0.2621.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2952 - acc: 0.9138 - val_loss: 0.2621 - val_acc: 0.9273\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9234\n",
      "Epoch 00007: val_loss improved from 0.26205 to 0.20606, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/007-0.2061.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2570 - acc: 0.9234 - val_loss: 0.2061 - val_acc: 0.9418\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9333\n",
      "Epoch 00008: val_loss improved from 0.20606 to 0.19321, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/008-0.1932.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2270 - acc: 0.9334 - val_loss: 0.1932 - val_acc: 0.9455\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9399\n",
      "Epoch 00009: val_loss improved from 0.19321 to 0.17901, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/009-0.1790.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.2032 - acc: 0.9399 - val_loss: 0.1790 - val_acc: 0.9483\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9455\n",
      "Epoch 00010: val_loss improved from 0.17901 to 0.17403, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/010-0.1740.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1858 - acc: 0.9455 - val_loss: 0.1740 - val_acc: 0.9481\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9509\n",
      "Epoch 00011: val_loss did not improve from 0.17403\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1677 - acc: 0.9509 - val_loss: 0.1795 - val_acc: 0.9439\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9544\n",
      "Epoch 00012: val_loss improved from 0.17403 to 0.16965, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/012-0.1697.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1536 - acc: 0.9544 - val_loss: 0.1697 - val_acc: 0.9515\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9579\n",
      "Epoch 00013: val_loss improved from 0.16965 to 0.15714, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/013-0.1571.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1433 - acc: 0.9579 - val_loss: 0.1571 - val_acc: 0.9562\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9617\n",
      "Epoch 00014: val_loss improved from 0.15714 to 0.14789, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/014-0.1479.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1304 - acc: 0.9617 - val_loss: 0.1479 - val_acc: 0.9574\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9652\n",
      "Epoch 00015: val_loss did not improve from 0.14789\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1195 - acc: 0.9652 - val_loss: 0.1488 - val_acc: 0.9557\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9669\n",
      "Epoch 00016: val_loss improved from 0.14789 to 0.14668, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/016-0.1467.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1148 - acc: 0.9669 - val_loss: 0.1467 - val_acc: 0.9548\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9695\n",
      "Epoch 00017: val_loss improved from 0.14668 to 0.13945, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/017-0.1395.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.1063 - acc: 0.9694 - val_loss: 0.1395 - val_acc: 0.9599\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9729\n",
      "Epoch 00018: val_loss did not improve from 0.13945\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0961 - acc: 0.9729 - val_loss: 0.1443 - val_acc: 0.9590\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9744\n",
      "Epoch 00019: val_loss improved from 0.13945 to 0.13835, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/019-0.1384.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0899 - acc: 0.9744 - val_loss: 0.1384 - val_acc: 0.9616\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9763\n",
      "Epoch 00020: val_loss did not improve from 0.13835\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0845 - acc: 0.9763 - val_loss: 0.1510 - val_acc: 0.9576\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9771\n",
      "Epoch 00021: val_loss did not improve from 0.13835\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0802 - acc: 0.9771 - val_loss: 0.1419 - val_acc: 0.9562\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9797\n",
      "Epoch 00022: val_loss did not improve from 0.13835\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0747 - acc: 0.9796 - val_loss: 0.1407 - val_acc: 0.9604\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9798\n",
      "Epoch 00023: val_loss did not improve from 0.13835\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0729 - acc: 0.9798 - val_loss: 0.1419 - val_acc: 0.9574\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9820\n",
      "Epoch 00024: val_loss improved from 0.13835 to 0.13068, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/024-0.1307.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0645 - acc: 0.9820 - val_loss: 0.1307 - val_acc: 0.9630\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9844\n",
      "Epoch 00025: val_loss did not improve from 0.13068\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0605 - acc: 0.9844 - val_loss: 0.1386 - val_acc: 0.9585\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9837\n",
      "Epoch 00026: val_loss did not improve from 0.13068\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0565 - acc: 0.9837 - val_loss: 0.1352 - val_acc: 0.9611\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9865\n",
      "Epoch 00027: val_loss did not improve from 0.13068\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0515 - acc: 0.9865 - val_loss: 0.1505 - val_acc: 0.9581\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9858\n",
      "Epoch 00028: val_loss did not improve from 0.13068\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0508 - acc: 0.9858 - val_loss: 0.1413 - val_acc: 0.9595\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9875\n",
      "Epoch 00029: val_loss improved from 0.13068 to 0.12734, saving model to model/checkpoint/1D_CNN_custom_tanh_DO_025_DO_9_conv_checkpoint/029-0.1273.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0475 - acc: 0.9875 - val_loss: 0.1273 - val_acc: 0.9627\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9884\n",
      "Epoch 00030: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0436 - acc: 0.9884 - val_loss: 0.1343 - val_acc: 0.9623\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9894\n",
      "Epoch 00031: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0411 - acc: 0.9894 - val_loss: 0.1376 - val_acc: 0.9599\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9904\n",
      "Epoch 00032: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0380 - acc: 0.9904 - val_loss: 0.1308 - val_acc: 0.9630\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9912\n",
      "Epoch 00033: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0354 - acc: 0.9912 - val_loss: 0.1360 - val_acc: 0.9620\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9910\n",
      "Epoch 00034: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0349 - acc: 0.9910 - val_loss: 0.1375 - val_acc: 0.9613\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9915\n",
      "Epoch 00035: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0325 - acc: 0.9916 - val_loss: 0.1354 - val_acc: 0.9602\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9930\n",
      "Epoch 00036: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0301 - acc: 0.9930 - val_loss: 0.1488 - val_acc: 0.9597\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9930\n",
      "Epoch 00037: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0281 - acc: 0.9930 - val_loss: 0.1563 - val_acc: 0.9606\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9939\n",
      "Epoch 00038: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0258 - acc: 0.9939 - val_loss: 0.1401 - val_acc: 0.9637\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9945\n",
      "Epoch 00039: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0235 - acc: 0.9945 - val_loss: 0.1420 - val_acc: 0.9616\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9954\n",
      "Epoch 00040: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0213 - acc: 0.9954 - val_loss: 0.1414 - val_acc: 0.9639\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9943\n",
      "Epoch 00041: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0235 - acc: 0.9943 - val_loss: 0.1458 - val_acc: 0.9618\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9957\n",
      "Epoch 00042: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0204 - acc: 0.9957 - val_loss: 0.1470 - val_acc: 0.9641\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9956\n",
      "Epoch 00043: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0196 - acc: 0.9956 - val_loss: 0.1492 - val_acc: 0.9630\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9963\n",
      "Epoch 00044: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0188 - acc: 0.9963 - val_loss: 0.1504 - val_acc: 0.9609\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9950\n",
      "Epoch 00045: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0198 - acc: 0.9950 - val_loss: 0.1429 - val_acc: 0.9627\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9965\n",
      "Epoch 00046: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0158 - acc: 0.9965 - val_loss: 0.1530 - val_acc: 0.9620\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9965\n",
      "Epoch 00047: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0159 - acc: 0.9965 - val_loss: 0.1568 - val_acc: 0.9599\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9964\n",
      "Epoch 00048: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0155 - acc: 0.9964 - val_loss: 0.1474 - val_acc: 0.9623\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9977\n",
      "Epoch 00049: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0120 - acc: 0.9977 - val_loss: 0.1468 - val_acc: 0.9655\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9968\n",
      "Epoch 00050: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0137 - acc: 0.9968 - val_loss: 0.1633 - val_acc: 0.9620\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9960\n",
      "Epoch 00051: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0166 - acc: 0.9960 - val_loss: 0.1682 - val_acc: 0.9583\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9981\n",
      "Epoch 00052: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0105 - acc: 0.9981 - val_loss: 0.1725 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9975\n",
      "Epoch 00053: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0121 - acc: 0.9975 - val_loss: 0.1520 - val_acc: 0.9644\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 00054: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.1536 - val_acc: 0.9637\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9973\n",
      "Epoch 00055: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0115 - acc: 0.9973 - val_loss: 0.1576 - val_acc: 0.9620\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9981\n",
      "Epoch 00056: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0097 - acc: 0.9981 - val_loss: 0.1515 - val_acc: 0.9639\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9977\n",
      "Epoch 00057: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0101 - acc: 0.9977 - val_loss: 0.1638 - val_acc: 0.9618\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9976\n",
      "Epoch 00058: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0113 - acc: 0.9976 - val_loss: 0.1583 - val_acc: 0.9634\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9990\n",
      "Epoch 00059: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0065 - acc: 0.9990 - val_loss: 0.1596 - val_acc: 0.9648\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9970\n",
      "Epoch 00060: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0116 - acc: 0.9970 - val_loss: 0.1856 - val_acc: 0.9578\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9976\n",
      "Epoch 00061: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0101 - acc: 0.9976 - val_loss: 0.1905 - val_acc: 0.9564\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9987\n",
      "Epoch 00062: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0067 - acc: 0.9987 - val_loss: 0.1703 - val_acc: 0.9620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9986\n",
      "Epoch 00063: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.1657 - val_acc: 0.9623\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9987\n",
      "Epoch 00064: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0063 - acc: 0.9988 - val_loss: 0.1644 - val_acc: 0.9623\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9972\n",
      "Epoch 00065: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.2021 - val_acc: 0.9588\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9987\n",
      "Epoch 00066: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0066 - acc: 0.9987 - val_loss: 0.1660 - val_acc: 0.9639\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 00067: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0040 - acc: 0.9996 - val_loss: 0.1515 - val_acc: 0.9658\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979\n",
      "Epoch 00068: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 0.1757 - val_acc: 0.9632\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9978\n",
      "Epoch 00069: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0094 - acc: 0.9978 - val_loss: 0.1719 - val_acc: 0.9620\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00070: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1702 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9988\n",
      "Epoch 00071: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.1697 - val_acc: 0.9639\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9980\n",
      "Epoch 00072: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0089 - acc: 0.9980 - val_loss: 0.1738 - val_acc: 0.9627\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0070 - acc: 0.9987 - val_loss: 0.1871 - val_acc: 0.9599\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9992\n",
      "Epoch 00074: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1680 - val_acc: 0.9637\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9990\n",
      "Epoch 00075: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.1742 - val_acc: 0.9623\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9992\n",
      "Epoch 00076: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0051 - acc: 0.9992 - val_loss: 0.2017 - val_acc: 0.9536\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9979\n",
      "Epoch 00077: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0084 - acc: 0.9979 - val_loss: 0.1744 - val_acc: 0.9620\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9990\n",
      "Epoch 00078: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0055 - acc: 0.9990 - val_loss: 0.1825 - val_acc: 0.9625\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00079: val_loss did not improve from 0.12734\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.0072 - acc: 0.9982 - val_loss: 0.1855 - val_acc: 0.9623\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 677us/sample - loss: 0.1797 - acc: 0.9472\n",
      "Loss: 0.17967637111959922 Accuracy: 0.94724816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_tanh_DO_025_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 352us/sample - loss: 2.7266 - acc: 0.1051\n",
      "Loss: 2.7266184161137694 Accuracy: 0.105088264\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 542us/sample - loss: 2.0960 - acc: 0.3429\n",
      "Loss: 2.095974131140506 Accuracy: 0.3428868\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 595us/sample - loss: 1.5866 - acc: 0.4987\n",
      "Loss: 1.5865661711955243 Accuracy: 0.49865004\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 631us/sample - loss: 1.2426 - acc: 0.6278\n",
      "Loss: 1.2425976790619293 Accuracy: 0.6278297\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 667us/sample - loss: 0.9325 - acc: 0.7310\n",
      "Loss: 0.9325461771134151 Accuracy: 0.7310488\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 676us/sample - loss: 0.5230 - acc: 0.8582\n",
      "Loss: 0.5229580372912366 Accuracy: 0.8581516\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 701us/sample - loss: 0.2609 - acc: 0.9215\n",
      "Loss: 0.26094469949835186 Accuracy: 0.9214953\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 716us/sample - loss: 0.1818 - acc: 0.9441\n",
      "Loss: 0.181804375336549 Accuracy: 0.9441329\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 724us/sample - loss: 0.1797 - acc: 0.9472\n",
      "Loss: 0.17967637111959922 Accuracy: 0.94724816\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_tanh_DO_025_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 442us/sample - loss: 9.3682 - acc: 0.0966\n",
      "Loss: 9.368187146766163 Accuracy: 0.09657321\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 621us/sample - loss: 5.8874 - acc: 0.3416\n",
      "Loss: 5.887357475205498 Accuracy: 0.3416407\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 677us/sample - loss: 3.1866 - acc: 0.5460\n",
      "Loss: 3.1865988102038454 Accuracy: 0.5460021\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 713us/sample - loss: 1.9176 - acc: 0.6523\n",
      "Loss: 1.9176177687857876 Accuracy: 0.6523365\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 506,576\n",
      "Trainable params: 506,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 737us/sample - loss: 1.3502 - acc: 0.7288\n",
      "Loss: 1.3501875538939876 Accuracy: 0.7287643\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 318,288\n",
      "Trainable params: 318,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 771us/sample - loss: 0.6002 - acc: 0.8640\n",
      "Loss: 0.6002209250429338 Accuracy: 0.86396676\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 310,224\n",
      "Trainable params: 310,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 762us/sample - loss: 0.2792 - acc: 0.9252\n",
      "Loss: 0.2792014574459904 Accuracy: 0.92523366\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 363,600\n",
      "Trainable params: 363,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 778us/sample - loss: 0.2043 - acc: 0.9475\n",
      "Loss: 0.20429745706597097 Accuracy: 0.9474559\n",
      "\n",
      "1D_CNN_custom_tanh_DO_025_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 521,552\n",
      "Trainable params: 521,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 847us/sample - loss: 0.2284 - acc: 0.9485\n",
      "Loss: 0.2284342247282173 Accuracy: 0.9484943\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_tanh_DO'\n",
    "\n",
    "with open(path.join(log_dir, base+'_last'), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
