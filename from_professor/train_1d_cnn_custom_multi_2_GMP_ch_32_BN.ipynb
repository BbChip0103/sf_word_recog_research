{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 64)           256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1040        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 64)           256         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 96)           384         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1552        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 128)          512         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 128)          512         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 128)          512         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4523 - acc: 0.2255\n",
      "Epoch 00001: val_loss improved from inf to 2.51541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/001-2.5154.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 2.4516 - acc: 0.2259 - val_loss: 2.5154 - val_acc: 0.1819\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9689 - acc: 0.3566\n",
      "Epoch 00002: val_loss improved from 2.51541 to 1.82753, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/002-1.8275.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.9689 - acc: 0.3566 - val_loss: 1.8275 - val_acc: 0.3965\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7577 - acc: 0.4285\n",
      "Epoch 00003: val_loss improved from 1.82753 to 1.68715, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/003-1.6872.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.7574 - acc: 0.4286 - val_loss: 1.6872 - val_acc: 0.4554\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5757 - acc: 0.4939\n",
      "Epoch 00004: val_loss improved from 1.68715 to 1.48202, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/004-1.4820.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.5756 - acc: 0.4940 - val_loss: 1.4820 - val_acc: 0.5288\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4325 - acc: 0.5455\n",
      "Epoch 00005: val_loss improved from 1.48202 to 1.33136, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/005-1.3314.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4326 - acc: 0.5455 - val_loss: 1.3314 - val_acc: 0.5812\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3180 - acc: 0.5859\n",
      "Epoch 00006: val_loss improved from 1.33136 to 1.21616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/006-1.2162.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3179 - acc: 0.5859 - val_loss: 1.2162 - val_acc: 0.6196\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2175 - acc: 0.6176\n",
      "Epoch 00007: val_loss improved from 1.21616 to 1.12853, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/007-1.1285.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.2175 - acc: 0.6176 - val_loss: 1.1285 - val_acc: 0.6515\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1350 - acc: 0.6461\n",
      "Epoch 00008: val_loss improved from 1.12853 to 1.10542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/008-1.1054.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.1350 - acc: 0.6461 - val_loss: 1.1054 - val_acc: 0.6511\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0732 - acc: 0.6675\n",
      "Epoch 00009: val_loss improved from 1.10542 to 0.99039, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/009-0.9904.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.0733 - acc: 0.6675 - val_loss: 0.9904 - val_acc: 0.6909\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0225 - acc: 0.6826\n",
      "Epoch 00010: val_loss improved from 0.99039 to 0.97196, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/010-0.9720.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.0225 - acc: 0.6826 - val_loss: 0.9720 - val_acc: 0.7028\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9773 - acc: 0.6957\n",
      "Epoch 00011: val_loss improved from 0.97196 to 0.94116, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/011-0.9412.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9774 - acc: 0.6957 - val_loss: 0.9412 - val_acc: 0.7072\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9420 - acc: 0.7072\n",
      "Epoch 00012: val_loss improved from 0.94116 to 0.92053, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/012-0.9205.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9421 - acc: 0.7071 - val_loss: 0.9205 - val_acc: 0.7037\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9086 - acc: 0.7167\n",
      "Epoch 00013: val_loss improved from 0.92053 to 0.85403, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/013-0.8540.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.9086 - acc: 0.7167 - val_loss: 0.8540 - val_acc: 0.7191\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8815 - acc: 0.7244\n",
      "Epoch 00014: val_loss improved from 0.85403 to 0.84696, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/014-0.8470.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.8816 - acc: 0.7244 - val_loss: 0.8470 - val_acc: 0.7263\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8591 - acc: 0.7312\n",
      "Epoch 00015: val_loss improved from 0.84696 to 0.82592, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/015-0.8259.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.8591 - acc: 0.7312 - val_loss: 0.8259 - val_acc: 0.7424\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7383\n",
      "Epoch 00016: val_loss did not improve from 0.82592\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.8379 - acc: 0.7383 - val_loss: 0.8366 - val_acc: 0.7310\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8135 - acc: 0.7475\n",
      "Epoch 00017: val_loss improved from 0.82592 to 0.81207, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/017-0.8121.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.8135 - acc: 0.7475 - val_loss: 0.8121 - val_acc: 0.7431\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7994 - acc: 0.7500\n",
      "Epoch 00018: val_loss improved from 0.81207 to 0.78102, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/018-0.7810.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.7996 - acc: 0.7499 - val_loss: 0.7810 - val_acc: 0.7556\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7844 - acc: 0.7568\n",
      "Epoch 00019: val_loss improved from 0.78102 to 0.76263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/019-0.7626.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.7845 - acc: 0.7568 - val_loss: 0.7626 - val_acc: 0.7622\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.7589\n",
      "Epoch 00020: val_loss did not improve from 0.76263\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.7683 - acc: 0.7588 - val_loss: 0.7818 - val_acc: 0.7536\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7581 - acc: 0.7641\n",
      "Epoch 00021: val_loss did not improve from 0.76263\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.7581 - acc: 0.7641 - val_loss: 0.7882 - val_acc: 0.7407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7689\n",
      "Epoch 00022: val_loss improved from 0.76263 to 0.72265, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/022-0.7226.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.7429 - acc: 0.7689 - val_loss: 0.7226 - val_acc: 0.7827\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7295 - acc: 0.7720\n",
      "Epoch 00023: val_loss did not improve from 0.72265\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.7296 - acc: 0.7720 - val_loss: 0.7243 - val_acc: 0.7754\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7220 - acc: 0.7736\n",
      "Epoch 00024: val_loss improved from 0.72265 to 0.69664, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/024-0.6966.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.7221 - acc: 0.7736 - val_loss: 0.6966 - val_acc: 0.7808\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7099 - acc: 0.7778\n",
      "Epoch 00025: val_loss did not improve from 0.69664\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.7099 - acc: 0.7778 - val_loss: 0.6974 - val_acc: 0.7873\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7007 - acc: 0.7803\n",
      "Epoch 00026: val_loss did not improve from 0.69664\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.7009 - acc: 0.7802 - val_loss: 0.7062 - val_acc: 0.7827\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6879 - acc: 0.7853\n",
      "Epoch 00027: val_loss improved from 0.69664 to 0.67467, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/027-0.6747.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.6885 - acc: 0.7852 - val_loss: 0.6747 - val_acc: 0.7899\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6837 - acc: 0.7858\n",
      "Epoch 00028: val_loss did not improve from 0.67467\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.6837 - acc: 0.7859 - val_loss: 0.6813 - val_acc: 0.7815\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.7889\n",
      "Epoch 00029: val_loss did not improve from 0.67467\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.6762 - acc: 0.7888 - val_loss: 0.6829 - val_acc: 0.7880\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.7911\n",
      "Epoch 00030: val_loss did not improve from 0.67467\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.6705 - acc: 0.7910 - val_loss: 0.6916 - val_acc: 0.7799\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.7951\n",
      "Epoch 00031: val_loss improved from 0.67467 to 0.65583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/031-0.6558.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.6600 - acc: 0.7951 - val_loss: 0.6558 - val_acc: 0.7985\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6506 - acc: 0.7946\n",
      "Epoch 00032: val_loss did not improve from 0.65583\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.6508 - acc: 0.7946 - val_loss: 0.6688 - val_acc: 0.7906\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6461 - acc: 0.8016\n",
      "Epoch 00033: val_loss improved from 0.65583 to 0.65179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/033-0.6518.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.6462 - acc: 0.8016 - val_loss: 0.6518 - val_acc: 0.8001\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6385 - acc: 0.8002\n",
      "Epoch 00034: val_loss did not improve from 0.65179\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.6387 - acc: 0.8002 - val_loss: 0.6688 - val_acc: 0.7932\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.8032\n",
      "Epoch 00035: val_loss improved from 0.65179 to 0.64124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/035-0.6412.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.6327 - acc: 0.8032 - val_loss: 0.6412 - val_acc: 0.8032\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.8027\n",
      "Epoch 00036: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.6278 - acc: 0.8027 - val_loss: 0.6773 - val_acc: 0.7845\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.8064\n",
      "Epoch 00037: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.6235 - acc: 0.8064 - val_loss: 0.6449 - val_acc: 0.8001\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8083\n",
      "Epoch 00038: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.6177 - acc: 0.8082 - val_loss: 0.6476 - val_acc: 0.7939\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.8094\n",
      "Epoch 00039: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.6107 - acc: 0.8093 - val_loss: 0.6524 - val_acc: 0.7985\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8089\n",
      "Epoch 00040: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.6060 - acc: 0.8089 - val_loss: 0.6520 - val_acc: 0.7999\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8129\n",
      "Epoch 00041: val_loss did not improve from 0.64124\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.6014 - acc: 0.8128 - val_loss: 0.6475 - val_acc: 0.8011\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.8147\n",
      "Epoch 00042: val_loss improved from 0.64124 to 0.64067, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/042-0.6407.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.5973 - acc: 0.8147 - val_loss: 0.6407 - val_acc: 0.8001\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.8167\n",
      "Epoch 00043: val_loss did not improve from 0.64067\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.5924 - acc: 0.8168 - val_loss: 0.6407 - val_acc: 0.7973\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8170\n",
      "Epoch 00044: val_loss improved from 0.64067 to 0.63827, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/044-0.6383.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.5864 - acc: 0.8172 - val_loss: 0.6383 - val_acc: 0.8022\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8198\n",
      "Epoch 00045: val_loss improved from 0.63827 to 0.62328, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/045-0.6233.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5788 - acc: 0.8198 - val_loss: 0.6233 - val_acc: 0.8057\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.8204\n",
      "Epoch 00046: val_loss did not improve from 0.62328\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.5799 - acc: 0.8205 - val_loss: 0.6265 - val_acc: 0.8076\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8188\n",
      "Epoch 00047: val_loss improved from 0.62328 to 0.62258, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/047-0.6226.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5748 - acc: 0.8188 - val_loss: 0.6226 - val_acc: 0.8083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8224\n",
      "Epoch 00048: val_loss improved from 0.62258 to 0.59755, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/048-0.5976.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.5713 - acc: 0.8224 - val_loss: 0.5976 - val_acc: 0.8174\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8245\n",
      "Epoch 00049: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.5655 - acc: 0.8245 - val_loss: 0.6117 - val_acc: 0.8141\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8237\n",
      "Epoch 00050: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.5595 - acc: 0.8236 - val_loss: 0.7004 - val_acc: 0.7843\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5571 - acc: 0.8261\n",
      "Epoch 00051: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.5571 - acc: 0.8262 - val_loss: 0.6512 - val_acc: 0.7983\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8256\n",
      "Epoch 00052: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5539 - acc: 0.8256 - val_loss: 0.6561 - val_acc: 0.7983\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8284\n",
      "Epoch 00053: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.5503 - acc: 0.8284 - val_loss: 0.6611 - val_acc: 0.7922\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8281\n",
      "Epoch 00054: val_loss did not improve from 0.59755\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.5483 - acc: 0.8281 - val_loss: 0.6109 - val_acc: 0.8164\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8290\n",
      "Epoch 00055: val_loss improved from 0.59755 to 0.59079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/055-0.5908.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.5431 - acc: 0.8290 - val_loss: 0.5908 - val_acc: 0.8160\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5413 - acc: 0.8308\n",
      "Epoch 00056: val_loss improved from 0.59079 to 0.59066, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/056-0.5907.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.5413 - acc: 0.8308 - val_loss: 0.5907 - val_acc: 0.8246\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8298\n",
      "Epoch 00057: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.5408 - acc: 0.8298 - val_loss: 0.5912 - val_acc: 0.8216\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8329\n",
      "Epoch 00058: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5357 - acc: 0.8330 - val_loss: 0.6450 - val_acc: 0.8020\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.8328\n",
      "Epoch 00059: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.5342 - acc: 0.8327 - val_loss: 0.5918 - val_acc: 0.8174\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8352\n",
      "Epoch 00060: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5284 - acc: 0.8352 - val_loss: 0.5916 - val_acc: 0.8225\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.8365\n",
      "Epoch 00061: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.5265 - acc: 0.8365 - val_loss: 0.6075 - val_acc: 0.8104\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8358\n",
      "Epoch 00062: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.5227 - acc: 0.8358 - val_loss: 0.6104 - val_acc: 0.8148\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8348\n",
      "Epoch 00063: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.5218 - acc: 0.8348 - val_loss: 0.6052 - val_acc: 0.8178\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8376\n",
      "Epoch 00064: val_loss did not improve from 0.59066\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.5168 - acc: 0.8375 - val_loss: 0.6126 - val_acc: 0.8102\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.8398\n",
      "Epoch 00065: val_loss improved from 0.59066 to 0.58949, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/065-0.5895.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.5128 - acc: 0.8397 - val_loss: 0.5895 - val_acc: 0.8218\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8383\n",
      "Epoch 00066: val_loss did not improve from 0.58949\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.5109 - acc: 0.8384 - val_loss: 0.5897 - val_acc: 0.8218\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8438\n",
      "Epoch 00067: val_loss did not improve from 0.58949\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.5045 - acc: 0.8438 - val_loss: 0.5968 - val_acc: 0.8174\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8400\n",
      "Epoch 00068: val_loss improved from 0.58949 to 0.58029, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/068-0.5803.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.5078 - acc: 0.8400 - val_loss: 0.5803 - val_acc: 0.8244\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8416\n",
      "Epoch 00069: val_loss did not improve from 0.58029\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.5050 - acc: 0.8417 - val_loss: 0.5974 - val_acc: 0.8183\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8419\n",
      "Epoch 00070: val_loss did not improve from 0.58029\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.5022 - acc: 0.8420 - val_loss: 0.5809 - val_acc: 0.8255\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8428\n",
      "Epoch 00071: val_loss did not improve from 0.58029\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.4995 - acc: 0.8428 - val_loss: 0.5913 - val_acc: 0.8197\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8447\n",
      "Epoch 00072: val_loss did not improve from 0.58029\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4985 - acc: 0.8446 - val_loss: 0.7049 - val_acc: 0.7848\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8435\n",
      "Epoch 00073: val_loss improved from 0.58029 to 0.56015, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/073-0.5602.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4989 - acc: 0.8435 - val_loss: 0.5602 - val_acc: 0.8283\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8460\n",
      "Epoch 00074: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4946 - acc: 0.8460 - val_loss: 0.5709 - val_acc: 0.8251\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8463\n",
      "Epoch 00075: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4882 - acc: 0.8463 - val_loss: 0.6056 - val_acc: 0.8160\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8463\n",
      "Epoch 00076: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4908 - acc: 0.8463 - val_loss: 0.5846 - val_acc: 0.8216\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8469\n",
      "Epoch 00077: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4877 - acc: 0.8469 - val_loss: 0.5744 - val_acc: 0.8239\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4837 - acc: 0.8473\n",
      "Epoch 00078: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4838 - acc: 0.8472 - val_loss: 0.5651 - val_acc: 0.8272\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4816 - acc: 0.8492\n",
      "Epoch 00079: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4819 - acc: 0.8491 - val_loss: 0.5902 - val_acc: 0.8248\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.8498\n",
      "Epoch 00080: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.4829 - acc: 0.8497 - val_loss: 0.5619 - val_acc: 0.8283\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4789 - acc: 0.8503\n",
      "Epoch 00081: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4789 - acc: 0.8503 - val_loss: 0.6001 - val_acc: 0.8160\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8499\n",
      "Epoch 00082: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4763 - acc: 0.8499 - val_loss: 0.5706 - val_acc: 0.8253\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4756 - acc: 0.8498\n",
      "Epoch 00083: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.4757 - acc: 0.8497 - val_loss: 0.6298 - val_acc: 0.8062\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.8500\n",
      "Epoch 00084: val_loss did not improve from 0.56015\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.4736 - acc: 0.8500 - val_loss: 0.5841 - val_acc: 0.8176\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4679 - acc: 0.8525\n",
      "Epoch 00085: val_loss improved from 0.56015 to 0.54995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/085-0.5500.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4682 - acc: 0.8524 - val_loss: 0.5500 - val_acc: 0.8314\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8500\n",
      "Epoch 00086: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4715 - acc: 0.8501 - val_loss: 0.5980 - val_acc: 0.8192\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8544\n",
      "Epoch 00087: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.4634 - acc: 0.8544 - val_loss: 0.6523 - val_acc: 0.8006\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8550\n",
      "Epoch 00088: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4606 - acc: 0.8550 - val_loss: 0.5723 - val_acc: 0.8283\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8556\n",
      "Epoch 00089: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.4629 - acc: 0.8556 - val_loss: 0.6037 - val_acc: 0.8162\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8550\n",
      "Epoch 00090: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.4595 - acc: 0.8550 - val_loss: 0.5832 - val_acc: 0.8206\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8546\n",
      "Epoch 00091: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4592 - acc: 0.8547 - val_loss: 0.5888 - val_acc: 0.8230\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8560\n",
      "Epoch 00092: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4538 - acc: 0.8560 - val_loss: 0.5866 - val_acc: 0.8230\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.8555\n",
      "Epoch 00093: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4560 - acc: 0.8554 - val_loss: 0.5962 - val_acc: 0.8183\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4559 - acc: 0.8539\n",
      "Epoch 00094: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.4562 - acc: 0.8538 - val_loss: 0.5612 - val_acc: 0.8309\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8586\n",
      "Epoch 00095: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.4511 - acc: 0.8586 - val_loss: 0.6077 - val_acc: 0.8139\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.8588\n",
      "Epoch 00096: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4483 - acc: 0.8588 - val_loss: 0.5853 - val_acc: 0.8178\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.8587\n",
      "Epoch 00097: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.4436 - acc: 0.8586 - val_loss: 0.6017 - val_acc: 0.8218\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8569\n",
      "Epoch 00098: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.4475 - acc: 0.8569 - val_loss: 0.6117 - val_acc: 0.8146\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8592\n",
      "Epoch 00099: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4421 - acc: 0.8591 - val_loss: 0.5818 - val_acc: 0.8265\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8587\n",
      "Epoch 00100: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4413 - acc: 0.8586 - val_loss: 0.5861 - val_acc: 0.8192\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8608\n",
      "Epoch 00101: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4413 - acc: 0.8608 - val_loss: 0.5887 - val_acc: 0.8197\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8611\n",
      "Epoch 00102: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4400 - acc: 0.8610 - val_loss: 0.5692 - val_acc: 0.8314\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.8614\n",
      "Epoch 00103: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4381 - acc: 0.8614 - val_loss: 0.5580 - val_acc: 0.8358\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8625\n",
      "Epoch 00104: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4356 - acc: 0.8625 - val_loss: 0.5932 - val_acc: 0.8188\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8614\n",
      "Epoch 00105: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.4363 - acc: 0.8614 - val_loss: 0.5597 - val_acc: 0.8290\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8618\n",
      "Epoch 00106: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4350 - acc: 0.8618 - val_loss: 0.6336 - val_acc: 0.8064\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8614\n",
      "Epoch 00107: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4313 - acc: 0.8613 - val_loss: 0.5600 - val_acc: 0.8330\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.8641\n",
      "Epoch 00108: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4290 - acc: 0.8641 - val_loss: 0.5864 - val_acc: 0.8246\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8628\n",
      "Epoch 00109: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.4262 - acc: 0.8628 - val_loss: 0.5721 - val_acc: 0.8272\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.8636\n",
      "Epoch 00110: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.4296 - acc: 0.8636 - val_loss: 0.6618 - val_acc: 0.8004\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8660\n",
      "Epoch 00111: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4231 - acc: 0.8660 - val_loss: 0.6255 - val_acc: 0.8143\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4242 - acc: 0.8658\n",
      "Epoch 00112: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4244 - acc: 0.8658 - val_loss: 0.5806 - val_acc: 0.8276\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8640\n",
      "Epoch 00113: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.4264 - acc: 0.8640 - val_loss: 0.5749 - val_acc: 0.8274\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8659\n",
      "Epoch 00114: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.4216 - acc: 0.8659 - val_loss: 0.6347 - val_acc: 0.8074\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8650\n",
      "Epoch 00115: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4215 - acc: 0.8649 - val_loss: 0.6006 - val_acc: 0.8190\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8667\n",
      "Epoch 00116: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4188 - acc: 0.8666 - val_loss: 0.6150 - val_acc: 0.8164\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8672\n",
      "Epoch 00117: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.4177 - acc: 0.8671 - val_loss: 0.5771 - val_acc: 0.8279\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8661\n",
      "Epoch 00118: val_loss did not improve from 0.54995\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.4146 - acc: 0.8660 - val_loss: 0.5515 - val_acc: 0.8407\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8664\n",
      "Epoch 00119: val_loss improved from 0.54995 to 0.54641, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv_checkpoint/119-0.5464.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.4177 - acc: 0.8664 - val_loss: 0.5464 - val_acc: 0.8395\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8667\n",
      "Epoch 00120: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4176 - acc: 0.8667 - val_loss: 0.5923 - val_acc: 0.8255\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8690\n",
      "Epoch 00121: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4099 - acc: 0.8690 - val_loss: 0.6380 - val_acc: 0.8092\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8693\n",
      "Epoch 00122: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.4122 - acc: 0.8693 - val_loss: 0.5669 - val_acc: 0.8290\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8689\n",
      "Epoch 00123: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4106 - acc: 0.8688 - val_loss: 0.6059 - val_acc: 0.8227\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4092 - acc: 0.8693\n",
      "Epoch 00124: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.4092 - acc: 0.8694 - val_loss: 0.5928 - val_acc: 0.8251\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8684\n",
      "Epoch 00125: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.4109 - acc: 0.8684 - val_loss: 0.5652 - val_acc: 0.8351\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8708\n",
      "Epoch 00126: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4067 - acc: 0.8708 - val_loss: 0.5962 - val_acc: 0.8239\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8696\n",
      "Epoch 00127: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.4059 - acc: 0.8695 - val_loss: 0.5839 - val_acc: 0.8251\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8708\n",
      "Epoch 00128: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4070 - acc: 0.8708 - val_loss: 0.5703 - val_acc: 0.8279\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8724\n",
      "Epoch 00129: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4022 - acc: 0.8724 - val_loss: 0.5491 - val_acc: 0.8381\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8716\n",
      "Epoch 00130: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4016 - acc: 0.8716 - val_loss: 0.6048 - val_acc: 0.8197\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8715\n",
      "Epoch 00131: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.4016 - acc: 0.8716 - val_loss: 0.5742 - val_acc: 0.8355\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8731\n",
      "Epoch 00132: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4024 - acc: 0.8731 - val_loss: 0.5560 - val_acc: 0.8325\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8716\n",
      "Epoch 00133: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.3997 - acc: 0.8716 - val_loss: 0.5817 - val_acc: 0.8304\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8734\n",
      "Epoch 00134: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.3985 - acc: 0.8734 - val_loss: 0.5849 - val_acc: 0.8293\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8739\n",
      "Epoch 00135: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3959 - acc: 0.8738 - val_loss: 0.6422 - val_acc: 0.8113\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8733\n",
      "Epoch 00136: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.3969 - acc: 0.8733 - val_loss: 0.5601 - val_acc: 0.8330\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8727\n",
      "Epoch 00137: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3994 - acc: 0.8726 - val_loss: 0.5725 - val_acc: 0.8309\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8725\n",
      "Epoch 00138: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.3958 - acc: 0.8724 - val_loss: 0.5767 - val_acc: 0.8330\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8749\n",
      "Epoch 00139: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3938 - acc: 0.8749 - val_loss: 0.6094 - val_acc: 0.8192\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8749\n",
      "Epoch 00140: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.3934 - acc: 0.8749 - val_loss: 0.6009 - val_acc: 0.8199\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8750\n",
      "Epoch 00141: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.3924 - acc: 0.8750 - val_loss: 0.5678 - val_acc: 0.8346\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3909 - acc: 0.8757\n",
      "Epoch 00142: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.3908 - acc: 0.8758 - val_loss: 0.5685 - val_acc: 0.8351\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8750\n",
      "Epoch 00143: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3907 - acc: 0.8750 - val_loss: 0.5674 - val_acc: 0.8323\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8761\n",
      "Epoch 00144: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3880 - acc: 0.8760 - val_loss: 0.6906 - val_acc: 0.7892\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8758\n",
      "Epoch 00145: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3863 - acc: 0.8758 - val_loss: 0.5582 - val_acc: 0.8395\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8764\n",
      "Epoch 00146: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3871 - acc: 0.8765 - val_loss: 0.6495 - val_acc: 0.8022\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8751\n",
      "Epoch 00147: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3861 - acc: 0.8750 - val_loss: 0.5689 - val_acc: 0.8267\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8746\n",
      "Epoch 00148: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3869 - acc: 0.8747 - val_loss: 0.6125 - val_acc: 0.8139\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8773\n",
      "Epoch 00149: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.3859 - acc: 0.8773 - val_loss: 0.6370 - val_acc: 0.8102\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8774\n",
      "Epoch 00150: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3825 - acc: 0.8772 - val_loss: 0.5946 - val_acc: 0.8253\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8755\n",
      "Epoch 00151: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.3868 - acc: 0.8756 - val_loss: 0.5702 - val_acc: 0.8328\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8786\n",
      "Epoch 00152: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3797 - acc: 0.8785 - val_loss: 0.6561 - val_acc: 0.8015\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8791\n",
      "Epoch 00153: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3804 - acc: 0.8791 - val_loss: 0.6210 - val_acc: 0.8137\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8745\n",
      "Epoch 00154: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3846 - acc: 0.8745 - val_loss: 0.5924 - val_acc: 0.8251\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8789\n",
      "Epoch 00155: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3805 - acc: 0.8789 - val_loss: 0.6033 - val_acc: 0.8232\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8789\n",
      "Epoch 00156: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.3794 - acc: 0.8789 - val_loss: 0.6345 - val_acc: 0.8102\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8800\n",
      "Epoch 00157: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.3742 - acc: 0.8800 - val_loss: 0.5898 - val_acc: 0.8311\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8791\n",
      "Epoch 00158: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3775 - acc: 0.8790 - val_loss: 0.5772 - val_acc: 0.8334\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8801\n",
      "Epoch 00159: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.3724 - acc: 0.8800 - val_loss: 0.6013 - val_acc: 0.8199\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8792\n",
      "Epoch 00160: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.3775 - acc: 0.8791 - val_loss: 0.5908 - val_acc: 0.8325\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8813\n",
      "Epoch 00161: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.3702 - acc: 0.8813 - val_loss: 0.5803 - val_acc: 0.8318\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8813\n",
      "Epoch 00162: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.3697 - acc: 0.8813 - val_loss: 0.5927 - val_acc: 0.8297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8791\n",
      "Epoch 00163: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3715 - acc: 0.8791 - val_loss: 0.6223 - val_acc: 0.8146\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8809\n",
      "Epoch 00164: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3731 - acc: 0.8808 - val_loss: 0.5701 - val_acc: 0.8372\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8836\n",
      "Epoch 00165: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3683 - acc: 0.8834 - val_loss: 0.5574 - val_acc: 0.8388\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8827\n",
      "Epoch 00166: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3697 - acc: 0.8827 - val_loss: 0.6088 - val_acc: 0.8239\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8801\n",
      "Epoch 00167: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.3719 - acc: 0.8801 - val_loss: 0.5938 - val_acc: 0.8267\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8804\n",
      "Epoch 00168: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.3710 - acc: 0.8804 - val_loss: 0.6961 - val_acc: 0.8029\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8821\n",
      "Epoch 00169: val_loss did not improve from 0.54641\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3670 - acc: 0.8821 - val_loss: 0.5679 - val_acc: 0.8390\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 363us/sample - loss: 0.6524 - acc: 0.8077\n",
      "Loss: 0.6523727787679848 Accuracy: 0.8076843\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2901 - acc: 0.2738\n",
      "Epoch 00001: val_loss improved from inf to 2.15835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/001-2.1584.hdf5\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 2.2896 - acc: 0.2739 - val_loss: 2.1584 - val_acc: 0.3103\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7333 - acc: 0.4429\n",
      "Epoch 00002: val_loss improved from 2.15835 to 1.58225, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/002-1.5822.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.7332 - acc: 0.4430 - val_loss: 1.5822 - val_acc: 0.5029\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4930 - acc: 0.5268\n",
      "Epoch 00003: val_loss improved from 1.58225 to 1.35673, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/003-1.3567.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.4928 - acc: 0.5269 - val_loss: 1.3567 - val_acc: 0.5786\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3158 - acc: 0.5872\n",
      "Epoch 00004: val_loss improved from 1.35673 to 1.23594, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/004-1.2359.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.3159 - acc: 0.5872 - val_loss: 1.2359 - val_acc: 0.6089\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1684 - acc: 0.6391\n",
      "Epoch 00005: val_loss improved from 1.23594 to 1.06432, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/005-1.0643.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.1684 - acc: 0.6391 - val_loss: 1.0643 - val_acc: 0.6648\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0547 - acc: 0.6771\n",
      "Epoch 00006: val_loss improved from 1.06432 to 0.97428, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/006-0.9743.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.0546 - acc: 0.6772 - val_loss: 0.9743 - val_acc: 0.6960\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9646 - acc: 0.7063\n",
      "Epoch 00007: val_loss improved from 0.97428 to 0.88726, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/007-0.8873.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9648 - acc: 0.7062 - val_loss: 0.8873 - val_acc: 0.7237\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8896 - acc: 0.7340\n",
      "Epoch 00008: val_loss improved from 0.88726 to 0.80380, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/008-0.8038.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8897 - acc: 0.7339 - val_loss: 0.8038 - val_acc: 0.7575\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8300 - acc: 0.7509\n",
      "Epoch 00009: val_loss improved from 0.80380 to 0.77866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/009-0.7787.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8300 - acc: 0.7510 - val_loss: 0.7787 - val_acc: 0.7689\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7685\n",
      "Epoch 00010: val_loss improved from 0.77866 to 0.75161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/010-0.7516.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7759 - acc: 0.7685 - val_loss: 0.7516 - val_acc: 0.7680\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7324 - acc: 0.7803\n",
      "Epoch 00011: val_loss improved from 0.75161 to 0.67436, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/011-0.6744.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7323 - acc: 0.7803 - val_loss: 0.6744 - val_acc: 0.8022\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6969 - acc: 0.7910\n",
      "Epoch 00012: val_loss did not improve from 0.67436\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.6972 - acc: 0.7909 - val_loss: 0.7076 - val_acc: 0.7815\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.8038\n",
      "Epoch 00013: val_loss did not improve from 0.67436\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.6636 - acc: 0.8040 - val_loss: 0.6829 - val_acc: 0.7925\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.8116\n",
      "Epoch 00014: val_loss improved from 0.67436 to 0.59810, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/014-0.5981.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.6311 - acc: 0.8115 - val_loss: 0.5981 - val_acc: 0.8260\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6096 - acc: 0.8159\n",
      "Epoch 00015: val_loss improved from 0.59810 to 0.59003, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/015-0.5900.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.6097 - acc: 0.8159 - val_loss: 0.5900 - val_acc: 0.8244\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5858 - acc: 0.8251\n",
      "Epoch 00016: val_loss did not improve from 0.59003\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.5862 - acc: 0.8249 - val_loss: 0.5911 - val_acc: 0.8197\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.8316\n",
      "Epoch 00017: val_loss did not improve from 0.59003\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.5661 - acc: 0.8316 - val_loss: 0.5907 - val_acc: 0.8248\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.8341\n",
      "Epoch 00018: val_loss improved from 0.59003 to 0.56649, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/018-0.5665.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.5497 - acc: 0.8341 - val_loss: 0.5665 - val_acc: 0.8283\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8410\n",
      "Epoch 00019: val_loss improved from 0.56649 to 0.53014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/019-0.5301.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.5320 - acc: 0.8411 - val_loss: 0.5301 - val_acc: 0.8400\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8436\n",
      "Epoch 00020: val_loss improved from 0.53014 to 0.52149, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/020-0.5215.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.5172 - acc: 0.8436 - val_loss: 0.5215 - val_acc: 0.8411\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8479\n",
      "Epoch 00021: val_loss improved from 0.52149 to 0.51527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/021-0.5153.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.5070 - acc: 0.8479 - val_loss: 0.5153 - val_acc: 0.8388\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8522\n",
      "Epoch 00022: val_loss did not improve from 0.51527\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4913 - acc: 0.8522 - val_loss: 0.5222 - val_acc: 0.8353\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.8546\n",
      "Epoch 00023: val_loss did not improve from 0.51527\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.4838 - acc: 0.8545 - val_loss: 0.5201 - val_acc: 0.8421\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8574\n",
      "Epoch 00024: val_loss improved from 0.51527 to 0.49112, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/024-0.4911.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4720 - acc: 0.8573 - val_loss: 0.4911 - val_acc: 0.8502\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8610\n",
      "Epoch 00025: val_loss improved from 0.49112 to 0.48933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/025-0.4893.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4621 - acc: 0.8609 - val_loss: 0.4893 - val_acc: 0.8486\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.8623\n",
      "Epoch 00026: val_loss did not improve from 0.48933\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.4553 - acc: 0.8624 - val_loss: 0.5294 - val_acc: 0.8323\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8659\n",
      "Epoch 00027: val_loss did not improve from 0.48933\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4440 - acc: 0.8657 - val_loss: 0.5204 - val_acc: 0.8346\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4387 - acc: 0.8674\n",
      "Epoch 00028: val_loss improved from 0.48933 to 0.45466, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/028-0.4547.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.4388 - acc: 0.8674 - val_loss: 0.4547 - val_acc: 0.8581\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8702\n",
      "Epoch 00029: val_loss did not improve from 0.45466\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.4294 - acc: 0.8702 - val_loss: 0.4960 - val_acc: 0.8430\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8726\n",
      "Epoch 00030: val_loss improved from 0.45466 to 0.45300, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/030-0.4530.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.4234 - acc: 0.8726 - val_loss: 0.4530 - val_acc: 0.8642\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8754\n",
      "Epoch 00031: val_loss did not improve from 0.45300\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.4147 - acc: 0.8753 - val_loss: 0.4860 - val_acc: 0.8460\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8757\n",
      "Epoch 00032: val_loss did not improve from 0.45300\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4094 - acc: 0.8757 - val_loss: 0.4846 - val_acc: 0.8486\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8780\n",
      "Epoch 00033: val_loss improved from 0.45300 to 0.44155, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/033-0.4415.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.4042 - acc: 0.8779 - val_loss: 0.4415 - val_acc: 0.8658\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8790\n",
      "Epoch 00034: val_loss did not improve from 0.44155\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3995 - acc: 0.8790 - val_loss: 0.4431 - val_acc: 0.8630\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8812\n",
      "Epoch 00035: val_loss did not improve from 0.44155\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.3896 - acc: 0.8812 - val_loss: 0.4560 - val_acc: 0.8574\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8835\n",
      "Epoch 00036: val_loss improved from 0.44155 to 0.43293, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/036-0.4329.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3850 - acc: 0.8835 - val_loss: 0.4329 - val_acc: 0.8677\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8832\n",
      "Epoch 00037: val_loss did not improve from 0.43293\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.3814 - acc: 0.8833 - val_loss: 0.4493 - val_acc: 0.8640\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8865\n",
      "Epoch 00038: val_loss did not improve from 0.43293\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.3775 - acc: 0.8864 - val_loss: 0.4479 - val_acc: 0.8602\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8863\n",
      "Epoch 00039: val_loss improved from 0.43293 to 0.42123, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/039-0.4212.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3715 - acc: 0.8862 - val_loss: 0.4212 - val_acc: 0.8684\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8896\n",
      "Epoch 00040: val_loss improved from 0.42123 to 0.41893, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/040-0.4189.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3636 - acc: 0.8895 - val_loss: 0.4189 - val_acc: 0.8663\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8903\n",
      "Epoch 00041: val_loss did not improve from 0.41893\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.3594 - acc: 0.8903 - val_loss: 0.4390 - val_acc: 0.8633\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8911\n",
      "Epoch 00042: val_loss did not improve from 0.41893\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.3546 - acc: 0.8912 - val_loss: 0.4396 - val_acc: 0.8637\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8915\n",
      "Epoch 00043: val_loss did not improve from 0.41893\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.3518 - acc: 0.8915 - val_loss: 0.4289 - val_acc: 0.8670\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8912\n",
      "Epoch 00044: val_loss did not improve from 0.41893\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3507 - acc: 0.8913 - val_loss: 0.4322 - val_acc: 0.8675\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8943\n",
      "Epoch 00045: val_loss did not improve from 0.41893\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3464 - acc: 0.8943 - val_loss: 0.4589 - val_acc: 0.8556\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3421 - acc: 0.8957\n",
      "Epoch 00046: val_loss improved from 0.41893 to 0.41838, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/046-0.4184.hdf5\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3423 - acc: 0.8957 - val_loss: 0.4184 - val_acc: 0.8698\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8983\n",
      "Epoch 00047: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3330 - acc: 0.8983 - val_loss: 0.4394 - val_acc: 0.8588\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8983\n",
      "Epoch 00048: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.3311 - acc: 0.8982 - val_loss: 0.4690 - val_acc: 0.8486\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8997\n",
      "Epoch 00049: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.3268 - acc: 0.8997 - val_loss: 0.4374 - val_acc: 0.8619\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8993\n",
      "Epoch 00050: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3268 - acc: 0.8993 - val_loss: 0.4441 - val_acc: 0.8626\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8995\n",
      "Epoch 00051: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.3220 - acc: 0.8995 - val_loss: 0.4253 - val_acc: 0.8637\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.9036\n",
      "Epoch 00052: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3181 - acc: 0.9037 - val_loss: 0.4302 - val_acc: 0.8640\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9041\n",
      "Epoch 00053: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3134 - acc: 0.9041 - val_loss: 0.4577 - val_acc: 0.8553\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3117 - acc: 0.9039\n",
      "Epoch 00054: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.3119 - acc: 0.9038 - val_loss: 0.4471 - val_acc: 0.8605\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9055\n",
      "Epoch 00055: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.3081 - acc: 0.9055 - val_loss: 0.4291 - val_acc: 0.8651\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9055\n",
      "Epoch 00056: val_loss did not improve from 0.41838\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.3073 - acc: 0.9053 - val_loss: 0.4666 - val_acc: 0.8535\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9079\n",
      "Epoch 00057: val_loss improved from 0.41838 to 0.40262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/057-0.4026.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.2999 - acc: 0.9079 - val_loss: 0.4026 - val_acc: 0.8742\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9104\n",
      "Epoch 00058: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2949 - acc: 0.9104 - val_loss: 0.4797 - val_acc: 0.8474\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9094\n",
      "Epoch 00059: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2945 - acc: 0.9093 - val_loss: 0.4791 - val_acc: 0.8493\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9108\n",
      "Epoch 00060: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.2933 - acc: 0.9109 - val_loss: 0.4348 - val_acc: 0.8637\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9118\n",
      "Epoch 00061: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2859 - acc: 0.9117 - val_loss: 0.4249 - val_acc: 0.8677\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9113\n",
      "Epoch 00062: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2871 - acc: 0.9112 - val_loss: 0.4119 - val_acc: 0.8721\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9119\n",
      "Epoch 00063: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.2841 - acc: 0.9119 - val_loss: 0.4226 - val_acc: 0.8668\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9136\n",
      "Epoch 00064: val_loss did not improve from 0.40262\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2824 - acc: 0.9136 - val_loss: 0.4965 - val_acc: 0.8451\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9127\n",
      "Epoch 00065: val_loss improved from 0.40262 to 0.40039, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv_checkpoint/065-0.4004.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.2816 - acc: 0.9128 - val_loss: 0.4004 - val_acc: 0.8737\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9157\n",
      "Epoch 00066: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2753 - acc: 0.9157 - val_loss: 0.4243 - val_acc: 0.8693\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9157\n",
      "Epoch 00067: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.2740 - acc: 0.9157 - val_loss: 0.4658 - val_acc: 0.8523\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9160\n",
      "Epoch 00068: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2700 - acc: 0.9159 - val_loss: 0.4089 - val_acc: 0.8710\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9155\n",
      "Epoch 00069: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2715 - acc: 0.9153 - val_loss: 0.4419 - val_acc: 0.8605\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9177\n",
      "Epoch 00070: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2661 - acc: 0.9176 - val_loss: 0.4983 - val_acc: 0.8446\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9171\n",
      "Epoch 00071: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.2669 - acc: 0.9169 - val_loss: 0.4349 - val_acc: 0.8626\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9188\n",
      "Epoch 00072: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2621 - acc: 0.9188 - val_loss: 0.4522 - val_acc: 0.8626\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9187\n",
      "Epoch 00073: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.2611 - acc: 0.9186 - val_loss: 0.4113 - val_acc: 0.8700\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9195\n",
      "Epoch 00074: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.2559 - acc: 0.9195 - val_loss: 0.4668 - val_acc: 0.8502\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9200\n",
      "Epoch 00075: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2545 - acc: 0.9199 - val_loss: 0.4545 - val_acc: 0.8602\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9204\n",
      "Epoch 00076: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2542 - acc: 0.9204 - val_loss: 0.4273 - val_acc: 0.8651\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9195\n",
      "Epoch 00077: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.2507 - acc: 0.9195 - val_loss: 0.4406 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9230\n",
      "Epoch 00078: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.2460 - acc: 0.9230 - val_loss: 0.4224 - val_acc: 0.8717\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9223\n",
      "Epoch 00079: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2476 - acc: 0.9222 - val_loss: 0.4273 - val_acc: 0.8712\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9250\n",
      "Epoch 00080: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.2434 - acc: 0.9250 - val_loss: 0.4431 - val_acc: 0.8621\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9241\n",
      "Epoch 00081: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2427 - acc: 0.9241 - val_loss: 0.5020 - val_acc: 0.8451\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9234\n",
      "Epoch 00082: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2423 - acc: 0.9234 - val_loss: 0.4325 - val_acc: 0.8677\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9249\n",
      "Epoch 00083: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.2393 - acc: 0.9248 - val_loss: 0.4655 - val_acc: 0.8609\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9249\n",
      "Epoch 00084: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.2374 - acc: 0.9248 - val_loss: 0.4523 - val_acc: 0.8619\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9267\n",
      "Epoch 00085: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.2364 - acc: 0.9267 - val_loss: 0.4731 - val_acc: 0.8581\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9251\n",
      "Epoch 00086: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2354 - acc: 0.9251 - val_loss: 0.4151 - val_acc: 0.8714\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9286\n",
      "Epoch 00087: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.2305 - acc: 0.9284 - val_loss: 0.4464 - val_acc: 0.8644\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9273\n",
      "Epoch 00088: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.2317 - acc: 0.9273 - val_loss: 0.4156 - val_acc: 0.8717\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9287\n",
      "Epoch 00089: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2291 - acc: 0.9287 - val_loss: 0.4388 - val_acc: 0.8623\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9276\n",
      "Epoch 00090: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2290 - acc: 0.9277 - val_loss: 0.4352 - val_acc: 0.8684\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9287\n",
      "Epoch 00091: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2266 - acc: 0.9286 - val_loss: 0.4444 - val_acc: 0.8661\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9288\n",
      "Epoch 00092: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2244 - acc: 0.9288 - val_loss: 0.4295 - val_acc: 0.8717\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9316\n",
      "Epoch 00093: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2189 - acc: 0.9316 - val_loss: 0.4718 - val_acc: 0.8516\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9316\n",
      "Epoch 00094: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.2197 - acc: 0.9315 - val_loss: 0.4220 - val_acc: 0.8710\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9291\n",
      "Epoch 00095: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2265 - acc: 0.9291 - val_loss: 0.4489 - val_acc: 0.8661\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9318\n",
      "Epoch 00096: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2182 - acc: 0.9318 - val_loss: 0.4323 - val_acc: 0.8637\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9328\n",
      "Epoch 00097: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.2156 - acc: 0.9327 - val_loss: 0.4517 - val_acc: 0.8640\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9333\n",
      "Epoch 00098: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2115 - acc: 0.9333 - val_loss: 0.4318 - val_acc: 0.8651\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9333\n",
      "Epoch 00099: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2143 - acc: 0.9333 - val_loss: 0.4367 - val_acc: 0.8649\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9327\n",
      "Epoch 00100: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2119 - acc: 0.9326 - val_loss: 0.4459 - val_acc: 0.8635\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9329\n",
      "Epoch 00101: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.2126 - acc: 0.9328 - val_loss: 0.4649 - val_acc: 0.8621\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9349\n",
      "Epoch 00102: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.2064 - acc: 0.9350 - val_loss: 0.4474 - val_acc: 0.8651\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9346\n",
      "Epoch 00103: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.2046 - acc: 0.9346 - val_loss: 0.4811 - val_acc: 0.8570\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9347\n",
      "Epoch 00104: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.2060 - acc: 0.9347 - val_loss: 0.4441 - val_acc: 0.8668\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9345\n",
      "Epoch 00105: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.2064 - acc: 0.9344 - val_loss: 0.4764 - val_acc: 0.8579\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9348\n",
      "Epoch 00106: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.2022 - acc: 0.9347 - val_loss: 0.4452 - val_acc: 0.8665\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9367\n",
      "Epoch 00107: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.2007 - acc: 0.9366 - val_loss: 0.4902 - val_acc: 0.8549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9369\n",
      "Epoch 00108: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.2008 - acc: 0.9369 - val_loss: 0.5091 - val_acc: 0.8500\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9372\n",
      "Epoch 00109: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.1991 - acc: 0.9372 - val_loss: 0.4744 - val_acc: 0.8565\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9376\n",
      "Epoch 00110: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.1979 - acc: 0.9375 - val_loss: 0.4506 - val_acc: 0.8670\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9379\n",
      "Epoch 00111: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.1971 - acc: 0.9379 - val_loss: 0.4523 - val_acc: 0.8644\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9374\n",
      "Epoch 00112: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.1957 - acc: 0.9374 - val_loss: 0.4410 - val_acc: 0.8644\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9390\n",
      "Epoch 00113: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.1953 - acc: 0.9390 - val_loss: 0.4696 - val_acc: 0.8605\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9398\n",
      "Epoch 00114: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.1922 - acc: 0.9398 - val_loss: 0.4471 - val_acc: 0.8675\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9389\n",
      "Epoch 00115: val_loss did not improve from 0.40039\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.1919 - acc: 0.9389 - val_loss: 0.4658 - val_acc: 0.8588\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 347us/sample - loss: 0.4886 - acc: 0.8546\n",
      "Loss: 0.48855698931563807 Accuracy: 0.854621\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1756 - acc: 0.3158\n",
      "Epoch 00001: val_loss improved from inf to 1.90552, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/001-1.9055.hdf5\n",
      "36805/36805 [==============================] - 35s 939us/sample - loss: 2.1746 - acc: 0.3161 - val_loss: 1.9055 - val_acc: 0.3853\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4206 - acc: 0.5595\n",
      "Epoch 00002: val_loss improved from 1.90552 to 1.18108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/002-1.1811.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 1.4210 - acc: 0.5595 - val_loss: 1.1811 - val_acc: 0.6508\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0778 - acc: 0.6828\n",
      "Epoch 00003: val_loss improved from 1.18108 to 0.90031, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/003-0.9003.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 1.0777 - acc: 0.6828 - val_loss: 0.9003 - val_acc: 0.7407\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8528 - acc: 0.7532\n",
      "Epoch 00004: val_loss improved from 0.90031 to 0.73946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/004-0.7395.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.8528 - acc: 0.7532 - val_loss: 0.7395 - val_acc: 0.7901\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7113 - acc: 0.7951\n",
      "Epoch 00005: val_loss improved from 0.73946 to 0.62947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/005-0.6295.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7115 - acc: 0.7951 - val_loss: 0.6295 - val_acc: 0.8269\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6135 - acc: 0.8250\n",
      "Epoch 00006: val_loss improved from 0.62947 to 0.53591, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/006-0.5359.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6137 - acc: 0.8250 - val_loss: 0.5359 - val_acc: 0.8526\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.8462\n",
      "Epoch 00007: val_loss improved from 0.53591 to 0.48593, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/007-0.4859.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5387 - acc: 0.8462 - val_loss: 0.4859 - val_acc: 0.8553\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.8621\n",
      "Epoch 00008: val_loss improved from 0.48593 to 0.45307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/008-0.4531.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4863 - acc: 0.8621 - val_loss: 0.4531 - val_acc: 0.8721\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4439 - acc: 0.8730\n",
      "Epoch 00009: val_loss improved from 0.45307 to 0.40756, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/009-0.4076.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4438 - acc: 0.8730 - val_loss: 0.4076 - val_acc: 0.8838\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8839\n",
      "Epoch 00010: val_loss improved from 0.40756 to 0.39451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/010-0.3945.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4067 - acc: 0.8838 - val_loss: 0.3945 - val_acc: 0.8831\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8923\n",
      "Epoch 00011: val_loss improved from 0.39451 to 0.37771, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/011-0.3777.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3787 - acc: 0.8922 - val_loss: 0.3777 - val_acc: 0.8912\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8984\n",
      "Epoch 00012: val_loss improved from 0.37771 to 0.34514, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/012-0.3451.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3541 - acc: 0.8984 - val_loss: 0.3451 - val_acc: 0.8989\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.9049\n",
      "Epoch 00013: val_loss improved from 0.34514 to 0.32549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/013-0.3255.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3326 - acc: 0.9049 - val_loss: 0.3255 - val_acc: 0.9047\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.9088\n",
      "Epoch 00014: val_loss did not improve from 0.32549\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3158 - acc: 0.9088 - val_loss: 0.3599 - val_acc: 0.8973\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9121\n",
      "Epoch 00015: val_loss did not improve from 0.32549\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3000 - acc: 0.9120 - val_loss: 0.3263 - val_acc: 0.9010\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9165\n",
      "Epoch 00016: val_loss improved from 0.32549 to 0.31293, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/016-0.3129.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.2870 - acc: 0.9165 - val_loss: 0.3129 - val_acc: 0.9066\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9195\n",
      "Epoch 00017: val_loss improved from 0.31293 to 0.30857, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/017-0.3086.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.2761 - acc: 0.9194 - val_loss: 0.3086 - val_acc: 0.9047\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9232\n",
      "Epoch 00018: val_loss improved from 0.30857 to 0.30774, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/018-0.3077.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.2630 - acc: 0.9232 - val_loss: 0.3077 - val_acc: 0.9106\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9269\n",
      "Epoch 00019: val_loss improved from 0.30774 to 0.28387, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/019-0.2839.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.2521 - acc: 0.9269 - val_loss: 0.2839 - val_acc: 0.9161\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9306\n",
      "Epoch 00020: val_loss did not improve from 0.28387\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.2408 - acc: 0.9306 - val_loss: 0.3132 - val_acc: 0.9071\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9334\n",
      "Epoch 00021: val_loss did not improve from 0.28387\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.2343 - acc: 0.9334 - val_loss: 0.2867 - val_acc: 0.9145\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9337\n",
      "Epoch 00022: val_loss improved from 0.28387 to 0.26829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/022-0.2683.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.2260 - acc: 0.9337 - val_loss: 0.2683 - val_acc: 0.9178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9377\n",
      "Epoch 00023: val_loss did not improve from 0.26829\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.2180 - acc: 0.9376 - val_loss: 0.2771 - val_acc: 0.9182\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9386\n",
      "Epoch 00024: val_loss improved from 0.26829 to 0.26484, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/024-0.2648.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.2116 - acc: 0.9385 - val_loss: 0.2648 - val_acc: 0.9213\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9407\n",
      "Epoch 00025: val_loss did not improve from 0.26484\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.2034 - acc: 0.9406 - val_loss: 0.2768 - val_acc: 0.9136\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9433\n",
      "Epoch 00026: val_loss did not improve from 0.26484\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1968 - acc: 0.9434 - val_loss: 0.2657 - val_acc: 0.9210\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9449\n",
      "Epoch 00027: val_loss improved from 0.26484 to 0.25818, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/027-0.2582.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.1902 - acc: 0.9449 - val_loss: 0.2582 - val_acc: 0.9231\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9474\n",
      "Epoch 00028: val_loss improved from 0.25818 to 0.25526, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/028-0.2553.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.1838 - acc: 0.9474 - val_loss: 0.2553 - val_acc: 0.9248\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9495\n",
      "Epoch 00029: val_loss improved from 0.25526 to 0.25414, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/029-0.2541.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1793 - acc: 0.9494 - val_loss: 0.2541 - val_acc: 0.9222\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9487\n",
      "Epoch 00030: val_loss improved from 0.25414 to 0.25229, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/030-0.2523.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1795 - acc: 0.9486 - val_loss: 0.2523 - val_acc: 0.9255\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9511\n",
      "Epoch 00031: val_loss did not improve from 0.25229\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.1703 - acc: 0.9511 - val_loss: 0.2702 - val_acc: 0.9166\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9523\n",
      "Epoch 00032: val_loss did not improve from 0.25229\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1640 - acc: 0.9523 - val_loss: 0.3052 - val_acc: 0.9050\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9539\n",
      "Epoch 00033: val_loss improved from 0.25229 to 0.24211, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/033-0.2421.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1624 - acc: 0.9539 - val_loss: 0.2421 - val_acc: 0.9264\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9565\n",
      "Epoch 00034: val_loss did not improve from 0.24211\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.1560 - acc: 0.9564 - val_loss: 0.2485 - val_acc: 0.9262\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9573\n",
      "Epoch 00035: val_loss improved from 0.24211 to 0.24132, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/035-0.2413.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.1495 - acc: 0.9572 - val_loss: 0.2413 - val_acc: 0.9271\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9575\n",
      "Epoch 00036: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.1480 - acc: 0.9575 - val_loss: 0.2414 - val_acc: 0.9283\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9599\n",
      "Epoch 00037: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.1434 - acc: 0.9598 - val_loss: 0.2552 - val_acc: 0.9248\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9597\n",
      "Epoch 00038: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.1412 - acc: 0.9597 - val_loss: 0.2472 - val_acc: 0.9259\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9617\n",
      "Epoch 00039: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.1351 - acc: 0.9617 - val_loss: 0.2812 - val_acc: 0.9131\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9607\n",
      "Epoch 00040: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1356 - acc: 0.9607 - val_loss: 0.2531 - val_acc: 0.9278\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9633\n",
      "Epoch 00041: val_loss did not improve from 0.24132\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1293 - acc: 0.9632 - val_loss: 0.2544 - val_acc: 0.9227\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9645\n",
      "Epoch 00042: val_loss improved from 0.24132 to 0.23736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/042-0.2374.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1260 - acc: 0.9645 - val_loss: 0.2374 - val_acc: 0.9287\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1235 - acc: 0.9649\n",
      "Epoch 00043: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1235 - acc: 0.9650 - val_loss: 0.2518 - val_acc: 0.9245\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9665\n",
      "Epoch 00044: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.1194 - acc: 0.9664 - val_loss: 0.2392 - val_acc: 0.9283\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9670\n",
      "Epoch 00045: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1171 - acc: 0.9670 - val_loss: 0.2586 - val_acc: 0.9222\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9680\n",
      "Epoch 00046: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.1134 - acc: 0.9680 - val_loss: 0.2389 - val_acc: 0.9266\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9692\n",
      "Epoch 00047: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1110 - acc: 0.9692 - val_loss: 0.2468 - val_acc: 0.9269\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9704\n",
      "Epoch 00048: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1075 - acc: 0.9704 - val_loss: 0.2613 - val_acc: 0.9213\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9700\n",
      "Epoch 00049: val_loss did not improve from 0.23736\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.1063 - acc: 0.9699 - val_loss: 0.2487 - val_acc: 0.9266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9684\n",
      "Epoch 00050: val_loss improved from 0.23736 to 0.23683, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv_checkpoint/050-0.2368.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.1090 - acc: 0.9683 - val_loss: 0.2368 - val_acc: 0.9287\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9701\n",
      "Epoch 00051: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1037 - acc: 0.9701 - val_loss: 0.2423 - val_acc: 0.9292\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9724\n",
      "Epoch 00052: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0991 - acc: 0.9723 - val_loss: 0.2469 - val_acc: 0.9262\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9723\n",
      "Epoch 00053: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0990 - acc: 0.9723 - val_loss: 0.2413 - val_acc: 0.9259\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9742\n",
      "Epoch 00054: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0931 - acc: 0.9741 - val_loss: 0.2403 - val_acc: 0.9294\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9759\n",
      "Epoch 00055: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0895 - acc: 0.9759 - val_loss: 0.2440 - val_acc: 0.9285\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9760\n",
      "Epoch 00056: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.0892 - acc: 0.9760 - val_loss: 0.2635 - val_acc: 0.9241\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9761\n",
      "Epoch 00057: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0892 - acc: 0.9761 - val_loss: 0.2599 - val_acc: 0.9248\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9770\n",
      "Epoch 00058: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0851 - acc: 0.9770 - val_loss: 0.2444 - val_acc: 0.9276\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9778\n",
      "Epoch 00059: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0836 - acc: 0.9778 - val_loss: 0.2660 - val_acc: 0.9257\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9760\n",
      "Epoch 00060: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0853 - acc: 0.9760 - val_loss: 0.2786 - val_acc: 0.9189\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9783\n",
      "Epoch 00061: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0795 - acc: 0.9783 - val_loss: 0.2538 - val_acc: 0.9285\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9794- ETA: 1s - loss:\n",
      "Epoch 00062: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0777 - acc: 0.9794 - val_loss: 0.2615 - val_acc: 0.9257\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9798\n",
      "Epoch 00063: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0758 - acc: 0.9798 - val_loss: 0.2535 - val_acc: 0.9245\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9790\n",
      "Epoch 00064: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0771 - acc: 0.9789 - val_loss: 0.2560 - val_acc: 0.9245\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9801\n",
      "Epoch 00065: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0742 - acc: 0.9801 - val_loss: 0.2796 - val_acc: 0.9206\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9790\n",
      "Epoch 00066: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.0746 - acc: 0.9789 - val_loss: 0.2677 - val_acc: 0.9236\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9810\n",
      "Epoch 00067: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0703 - acc: 0.9810 - val_loss: 0.2487 - val_acc: 0.9304\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9828\n",
      "Epoch 00068: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0677 - acc: 0.9828 - val_loss: 0.2541 - val_acc: 0.9255\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9835\n",
      "Epoch 00069: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0657 - acc: 0.9834 - val_loss: 0.2779 - val_acc: 0.9213\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9829\n",
      "Epoch 00070: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0660 - acc: 0.9829 - val_loss: 0.2730 - val_acc: 0.9236\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9821\n",
      "Epoch 00071: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0665 - acc: 0.9821 - val_loss: 0.2790 - val_acc: 0.9227\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9840\n",
      "Epoch 00072: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0621 - acc: 0.9840 - val_loss: 0.2646 - val_acc: 0.9243\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9839\n",
      "Epoch 00073: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0625 - acc: 0.9838 - val_loss: 0.2790 - val_acc: 0.9215\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9838\n",
      "Epoch 00074: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0622 - acc: 0.9837 - val_loss: 0.2674 - val_acc: 0.9224\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9837\n",
      "Epoch 00075: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.0608 - acc: 0.9837 - val_loss: 0.2968 - val_acc: 0.9213\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9848\n",
      "Epoch 00076: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0602 - acc: 0.9848 - val_loss: 0.2914 - val_acc: 0.9189\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9845\n",
      "Epoch 00077: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0592 - acc: 0.9845 - val_loss: 0.2489 - val_acc: 0.9301\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9864\n",
      "Epoch 00078: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0546 - acc: 0.9864 - val_loss: 0.2662 - val_acc: 0.9271\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9863\n",
      "Epoch 00079: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0540 - acc: 0.9863 - val_loss: 0.2706 - val_acc: 0.9257\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9845\n",
      "Epoch 00080: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0573 - acc: 0.9844 - val_loss: 0.2782 - val_acc: 0.9220\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9861\n",
      "Epoch 00081: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0540 - acc: 0.9861 - val_loss: 0.2770 - val_acc: 0.9264\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9877\n",
      "Epoch 00082: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0506 - acc: 0.9876 - val_loss: 0.2731 - val_acc: 0.9292\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9850\n",
      "Epoch 00083: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0556 - acc: 0.9850 - val_loss: 0.2767 - val_acc: 0.9255\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9895\n",
      "Epoch 00084: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.0469 - acc: 0.9895 - val_loss: 0.2651 - val_acc: 0.9297\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9879\n",
      "Epoch 00085: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0483 - acc: 0.9879 - val_loss: 0.2979 - val_acc: 0.9217\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9855\n",
      "Epoch 00086: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.0539 - acc: 0.9855 - val_loss: 0.2962 - val_acc: 0.9201\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9867\n",
      "Epoch 00087: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0505 - acc: 0.9866 - val_loss: 0.2766 - val_acc: 0.9271\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9877\n",
      "Epoch 00088: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0482 - acc: 0.9877 - val_loss: 0.2896 - val_acc: 0.9259\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9890\n",
      "Epoch 00089: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0452 - acc: 0.9890 - val_loss: 0.2892 - val_acc: 0.9241\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9895\n",
      "Epoch 00090: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.0441 - acc: 0.9895 - val_loss: 0.3132 - val_acc: 0.9187\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9876\n",
      "Epoch 00091: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0480 - acc: 0.9876 - val_loss: 0.2683 - val_acc: 0.9292\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9886\n",
      "Epoch 00092: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0434 - acc: 0.9886 - val_loss: 0.2852 - val_acc: 0.9262\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9903\n",
      "Epoch 00093: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0407 - acc: 0.9902 - val_loss: 0.2790 - val_acc: 0.9271\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9891\n",
      "Epoch 00094: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.0434 - acc: 0.9891 - val_loss: 0.2724 - val_acc: 0.9315\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9885\n",
      "Epoch 00095: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.0437 - acc: 0.9885 - val_loss: 0.2788 - val_acc: 0.9266\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9894\n",
      "Epoch 00096: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0417 - acc: 0.9894 - val_loss: 0.3199 - val_acc: 0.9187\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9899\n",
      "Epoch 00097: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.0416 - acc: 0.9899 - val_loss: 0.3007 - val_acc: 0.9217\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9915\n",
      "Epoch 00098: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.0375 - acc: 0.9915 - val_loss: 0.2921 - val_acc: 0.9250\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9894\n",
      "Epoch 00099: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.0416 - acc: 0.9893 - val_loss: 0.2890 - val_acc: 0.9229\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9901\n",
      "Epoch 00100: val_loss did not improve from 0.23683\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.0412 - acc: 0.9900 - val_loss: 0.2944 - val_acc: 0.9255\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 388us/sample - loss: 0.2979 - acc: 0.9099\n",
      "Loss: 0.2979069372760915 Accuracy: 0.909865\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8505 - acc: 0.4247\n",
      "Epoch 00001: val_loss improved from inf to 1.61732, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/001-1.6173.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.8504 - acc: 0.4248 - val_loss: 1.6173 - val_acc: 0.4782\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9590 - acc: 0.7243\n",
      "Epoch 00002: val_loss improved from 1.61732 to 0.73319, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/002-0.7332.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.9589 - acc: 0.7244 - val_loss: 0.7332 - val_acc: 0.7950\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6717 - acc: 0.8133\n",
      "Epoch 00003: val_loss improved from 0.73319 to 0.56190, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/003-0.5619.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.6718 - acc: 0.8133 - val_loss: 0.5619 - val_acc: 0.8519\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.8509\n",
      "Epoch 00004: val_loss improved from 0.56190 to 0.44727, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/004-0.4473.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.5303 - acc: 0.8510 - val_loss: 0.4473 - val_acc: 0.8810\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8770\n",
      "Epoch 00005: val_loss improved from 0.44727 to 0.39099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/005-0.3910.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4426 - acc: 0.8771 - val_loss: 0.3910 - val_acc: 0.8910\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8942\n",
      "Epoch 00006: val_loss improved from 0.39099 to 0.34462, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/006-0.3446.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.3802 - acc: 0.8942 - val_loss: 0.3446 - val_acc: 0.8998\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.9052\n",
      "Epoch 00007: val_loss improved from 0.34462 to 0.32201, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/007-0.3220.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.3368 - acc: 0.9051 - val_loss: 0.3220 - val_acc: 0.9064\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.9136\n",
      "Epoch 00008: val_loss improved from 0.32201 to 0.30982, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/008-0.3098.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3035 - acc: 0.9137 - val_loss: 0.3098 - val_acc: 0.9080\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9218\n",
      "Epoch 00009: val_loss improved from 0.30982 to 0.26823, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/009-0.2682.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2760 - acc: 0.9217 - val_loss: 0.2682 - val_acc: 0.9224\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9274\n",
      "Epoch 00010: val_loss did not improve from 0.26823\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2550 - acc: 0.9274 - val_loss: 0.2704 - val_acc: 0.9201\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9335\n",
      "Epoch 00011: val_loss improved from 0.26823 to 0.23848, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/011-0.2385.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2365 - acc: 0.9335 - val_loss: 0.2385 - val_acc: 0.9287\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9371\n",
      "Epoch 00012: val_loss improved from 0.23848 to 0.23349, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/012-0.2335.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2212 - acc: 0.9370 - val_loss: 0.2335 - val_acc: 0.9317\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9416\n",
      "Epoch 00013: val_loss did not improve from 0.23349\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.2079 - acc: 0.9416 - val_loss: 0.2457 - val_acc: 0.9264\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9449\n",
      "Epoch 00014: val_loss improved from 0.23349 to 0.22204, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/014-0.2220.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1938 - acc: 0.9448 - val_loss: 0.2220 - val_acc: 0.9345\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9486\n",
      "Epoch 00015: val_loss did not improve from 0.22204\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1844 - acc: 0.9486 - val_loss: 0.2432 - val_acc: 0.9241\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9507\n",
      "Epoch 00016: val_loss improved from 0.22204 to 0.21444, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/016-0.2144.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1743 - acc: 0.9507 - val_loss: 0.2144 - val_acc: 0.9338\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9532\n",
      "Epoch 00017: val_loss did not improve from 0.21444\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1648 - acc: 0.9532 - val_loss: 0.2386 - val_acc: 0.9259\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9566\n",
      "Epoch 00018: val_loss did not improve from 0.21444\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1546 - acc: 0.9566 - val_loss: 0.2177 - val_acc: 0.9331\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9586\n",
      "Epoch 00019: val_loss improved from 0.21444 to 0.20819, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/019-0.2082.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1468 - acc: 0.9586 - val_loss: 0.2082 - val_acc: 0.9362\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9604\n",
      "Epoch 00020: val_loss did not improve from 0.20819\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1399 - acc: 0.9604 - val_loss: 0.2108 - val_acc: 0.9394\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9630\n",
      "Epoch 00021: val_loss improved from 0.20819 to 0.20158, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/021-0.2016.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1337 - acc: 0.9630 - val_loss: 0.2016 - val_acc: 0.9355\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9655\n",
      "Epoch 00022: val_loss did not improve from 0.20158\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1275 - acc: 0.9655 - val_loss: 0.2043 - val_acc: 0.9373\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9671\n",
      "Epoch 00023: val_loss did not improve from 0.20158\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1205 - acc: 0.9672 - val_loss: 0.2026 - val_acc: 0.9371\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9689\n",
      "Epoch 00024: val_loss improved from 0.20158 to 0.20023, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/024-0.2002.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1148 - acc: 0.9689 - val_loss: 0.2002 - val_acc: 0.9383\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9690\n",
      "Epoch 00025: val_loss did not improve from 0.20023\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1110 - acc: 0.9690 - val_loss: 0.2114 - val_acc: 0.9334\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9714\n",
      "Epoch 00026: val_loss improved from 0.20023 to 0.19807, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/026-0.1981.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1050 - acc: 0.9713 - val_loss: 0.1981 - val_acc: 0.9348\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9712\n",
      "Epoch 00027: val_loss did not improve from 0.19807\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1038 - acc: 0.9712 - val_loss: 0.2027 - val_acc: 0.9397\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9752\n",
      "Epoch 00028: val_loss improved from 0.19807 to 0.19094, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/028-0.1909.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0930 - acc: 0.9752 - val_loss: 0.1909 - val_acc: 0.9397\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9768\n",
      "Epoch 00029: val_loss did not improve from 0.19094\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0898 - acc: 0.9766 - val_loss: 0.2215 - val_acc: 0.9266\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9770\n",
      "Epoch 00030: val_loss improved from 0.19094 to 0.18583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv_checkpoint/030-0.1858.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0876 - acc: 0.9769 - val_loss: 0.1858 - val_acc: 0.9392\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9780\n",
      "Epoch 00031: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0834 - acc: 0.9780 - val_loss: 0.2060 - val_acc: 0.9369\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9782\n",
      "Epoch 00032: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0818 - acc: 0.9782 - val_loss: 0.2031 - val_acc: 0.9376\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9812\n",
      "Epoch 00033: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.0749 - acc: 0.9811 - val_loss: 0.1959 - val_acc: 0.9385\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9820\n",
      "Epoch 00034: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0729 - acc: 0.9820 - val_loss: 0.2541 - val_acc: 0.9276\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9827\n",
      "Epoch 00035: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0699 - acc: 0.9827 - val_loss: 0.2146 - val_acc: 0.9355\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9854\n",
      "Epoch 00036: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0634 - acc: 0.9854 - val_loss: 0.1888 - val_acc: 0.9408\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9846\n",
      "Epoch 00037: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0616 - acc: 0.9846 - val_loss: 0.2149 - val_acc: 0.9357\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9870\n",
      "Epoch 00038: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0582 - acc: 0.9870 - val_loss: 0.2075 - val_acc: 0.9394\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9868\n",
      "Epoch 00039: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0572 - acc: 0.9868 - val_loss: 0.2011 - val_acc: 0.9399\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9878\n",
      "Epoch 00040: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0540 - acc: 0.9878 - val_loss: 0.2169 - val_acc: 0.9341\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9868\n",
      "Epoch 00041: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0563 - acc: 0.9867 - val_loss: 0.2121 - val_acc: 0.9350\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9873\n",
      "Epoch 00042: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.0522 - acc: 0.9873 - val_loss: 0.2014 - val_acc: 0.9427\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9900\n",
      "Epoch 00043: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0460 - acc: 0.9900 - val_loss: 0.2238 - val_acc: 0.9364\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9887\n",
      "Epoch 00044: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.0495 - acc: 0.9887 - val_loss: 0.2018 - val_acc: 0.9385\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9909\n",
      "Epoch 00045: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.0424 - acc: 0.9909 - val_loss: 0.2107 - val_acc: 0.9392\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0401 - acc: 0.9920 - val_loss: 0.1968 - val_acc: 0.9415\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9922\n",
      "Epoch 00047: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0393 - acc: 0.9922 - val_loss: 0.1997 - val_acc: 0.9401\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9919- ETA: 1s - loss: 0.03\n",
      "Epoch 00048: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0372 - acc: 0.9919 - val_loss: 0.2205 - val_acc: 0.9362\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9905\n",
      "Epoch 00049: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0415 - acc: 0.9905 - val_loss: 0.2338 - val_acc: 0.9341\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9937- ETA: 2s  - ETA: 0s - loss: 0.0336\n",
      "Epoch 00050: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0337 - acc: 0.9938 - val_loss: 0.2175 - val_acc: 0.9397\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9935\n",
      "Epoch 00051: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0323 - acc: 0.9935 - val_loss: 0.2323 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9945\n",
      "Epoch 00052: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0306 - acc: 0.9945 - val_loss: 0.2142 - val_acc: 0.9364\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9937\n",
      "Epoch 00053: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.0314 - acc: 0.9938 - val_loss: 0.2132 - val_acc: 0.9392\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9951\n",
      "Epoch 00054: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0280 - acc: 0.9951 - val_loss: 0.2082 - val_acc: 0.9415\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9951\n",
      "Epoch 00055: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0275 - acc: 0.9950 - val_loss: 0.2297 - val_acc: 0.9357\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9919\n",
      "Epoch 00056: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0346 - acc: 0.9919 - val_loss: 0.2037 - val_acc: 0.9415\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9938\n",
      "Epoch 00057: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0294 - acc: 0.9938 - val_loss: 0.2377 - val_acc: 0.9343\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9929\n",
      "Epoch 00058: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0322 - acc: 0.9929 - val_loss: 0.2246 - val_acc: 0.9392\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9941\n",
      "Epoch 00059: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0281 - acc: 0.9941 - val_loss: 0.2346 - val_acc: 0.9348\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9966\n",
      "Epoch 00060: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.0213 - acc: 0.9966 - val_loss: 0.2222 - val_acc: 0.9408\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9946\n",
      "Epoch 00061: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0280 - acc: 0.9946 - val_loss: 0.2267 - val_acc: 0.9380\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9945\n",
      "Epoch 00062: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0262 - acc: 0.9945 - val_loss: 0.2162 - val_acc: 0.9432\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9971\n",
      "Epoch 00063: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0193 - acc: 0.9971 - val_loss: 0.2258 - val_acc: 0.9408\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9962\n",
      "Epoch 00064: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.0206 - acc: 0.9962 - val_loss: 0.2278 - val_acc: 0.9359\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9947\n",
      "Epoch 00065: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.0241 - acc: 0.9947 - val_loss: 0.2272 - val_acc: 0.9383\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9952\n",
      "Epoch 00066: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0235 - acc: 0.9952 - val_loss: 0.2352 - val_acc: 0.9392\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9973\n",
      "Epoch 00067: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0180 - acc: 0.9973 - val_loss: 0.2282 - val_acc: 0.9394\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9952\n",
      "Epoch 00068: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0235 - acc: 0.9951 - val_loss: 0.2523 - val_acc: 0.9369\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9947\n",
      "Epoch 00069: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0248 - acc: 0.9946 - val_loss: 0.2276 - val_acc: 0.9387\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9981\n",
      "Epoch 00070: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0158 - acc: 0.9981 - val_loss: 0.2681 - val_acc: 0.9334\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9951\n",
      "Epoch 00071: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.0221 - acc: 0.9951 - val_loss: 0.2724 - val_acc: 0.9336\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9973\n",
      "Epoch 00072: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0167 - acc: 0.9973 - val_loss: 0.2539 - val_acc: 0.9311\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9942\n",
      "Epoch 00073: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0252 - acc: 0.9942 - val_loss: 0.2596 - val_acc: 0.9299\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9924\n",
      "Epoch 00074: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0303 - acc: 0.9924 - val_loss: 0.2480 - val_acc: 0.9364\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9984\n",
      "Epoch 00075: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0137 - acc: 0.9984 - val_loss: 0.2305 - val_acc: 0.9429\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9982\n",
      "Epoch 00076: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0129 - acc: 0.9982 - val_loss: 0.2376 - val_acc: 0.9397\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9955\n",
      "Epoch 00077: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0213 - acc: 0.9955 - val_loss: 0.2341 - val_acc: 0.9383\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9981\n",
      "Epoch 00078: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0126 - acc: 0.9981 - val_loss: 0.2327 - val_acc: 0.9394\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9979\n",
      "Epoch 00079: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0135 - acc: 0.9979 - val_loss: 0.2493 - val_acc: 0.9373\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.18583\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0147 - acc: 0.9975 - val_loss: 0.2439 - val_acc: 0.9364\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 424us/sample - loss: 0.2293 - acc: 0.9306\n",
      "Loss: 0.22926596655033582 Accuracy: 0.9306334\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6214 - acc: 0.5101\n",
      "Epoch 00001: val_loss improved from inf to 1.46189, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/001-1.4619.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.6214 - acc: 0.5101 - val_loss: 1.4619 - val_acc: 0.5355\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.7904\n",
      "Epoch 00002: val_loss improved from 1.46189 to 0.54550, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/002-0.5455.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.7360 - acc: 0.7904 - val_loss: 0.5455 - val_acc: 0.8544\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8575\n",
      "Epoch 00003: val_loss improved from 0.54550 to 0.42303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/003-0.4230.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.5006 - acc: 0.8575 - val_loss: 0.4230 - val_acc: 0.8856\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8884\n",
      "Epoch 00004: val_loss improved from 0.42303 to 0.32250, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/004-0.3225.hdf5\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.3920 - acc: 0.8884 - val_loss: 0.3225 - val_acc: 0.9110\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9088\n",
      "Epoch 00005: val_loss improved from 0.32250 to 0.28366, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/005-0.2837.hdf5\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.3232 - acc: 0.9088 - val_loss: 0.2837 - val_acc: 0.9224\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9192\n",
      "Epoch 00006: val_loss improved from 0.28366 to 0.26586, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/006-0.2659.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.2804 - acc: 0.9191 - val_loss: 0.2659 - val_acc: 0.9238\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9313\n",
      "Epoch 00007: val_loss improved from 0.26586 to 0.22357, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/007-0.2236.hdf5\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.2445 - acc: 0.9313 - val_loss: 0.2236 - val_acc: 0.9369\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9374\n",
      "Epoch 00008: val_loss improved from 0.22357 to 0.22045, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/008-0.2205.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.2204 - acc: 0.9374 - val_loss: 0.2205 - val_acc: 0.9352\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9427\n",
      "Epoch 00009: val_loss improved from 0.22045 to 0.21301, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/009-0.2130.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.1989 - acc: 0.9426 - val_loss: 0.2130 - val_acc: 0.9399\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9485\n",
      "Epoch 00010: val_loss improved from 0.21301 to 0.19353, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/010-0.1935.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1811 - acc: 0.9485 - val_loss: 0.1935 - val_acc: 0.9427\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9519\n",
      "Epoch 00011: val_loss did not improve from 0.19353\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.1658 - acc: 0.9519 - val_loss: 0.1941 - val_acc: 0.9408\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9561\n",
      "Epoch 00012: val_loss improved from 0.19353 to 0.18082, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/012-0.1808.hdf5\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.1540 - acc: 0.9561 - val_loss: 0.1808 - val_acc: 0.9439\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9604\n",
      "Epoch 00013: val_loss did not improve from 0.18082\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1409 - acc: 0.9603 - val_loss: 0.1895 - val_acc: 0.9427\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9619\n",
      "Epoch 00014: val_loss improved from 0.18082 to 0.17417, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/014-0.1742.hdf5\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1353 - acc: 0.9619 - val_loss: 0.1742 - val_acc: 0.9453\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9660\n",
      "Epoch 00015: val_loss did not improve from 0.17417\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1222 - acc: 0.9659 - val_loss: 0.1838 - val_acc: 0.9415\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1165 - acc: 0.9671\n",
      "Epoch 00016: val_loss did not improve from 0.17417\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.1165 - acc: 0.9672 - val_loss: 0.1974 - val_acc: 0.9383\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9721\n",
      "Epoch 00017: val_loss did not improve from 0.17417\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 0.1033 - acc: 0.9721 - val_loss: 0.1775 - val_acc: 0.9474\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9728\n",
      "Epoch 00018: val_loss did not improve from 0.17417\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.1004 - acc: 0.9727 - val_loss: 0.1866 - val_acc: 0.9427\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9740\n",
      "Epoch 00019: val_loss improved from 0.17417 to 0.16702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/019-0.1670.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0967 - acc: 0.9740 - val_loss: 0.1670 - val_acc: 0.9502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9758\n",
      "Epoch 00020: val_loss improved from 0.16702 to 0.16592, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv_checkpoint/020-0.1659.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0891 - acc: 0.9758 - val_loss: 0.1659 - val_acc: 0.9467\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9783\n",
      "Epoch 00021: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0822 - acc: 0.9783 - val_loss: 0.1692 - val_acc: 0.9478\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9799\n",
      "Epoch 00022: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0771 - acc: 0.9799 - val_loss: 0.1696 - val_acc: 0.9471\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9824\n",
      "Epoch 00023: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0703 - acc: 0.9824 - val_loss: 0.1776 - val_acc: 0.9464\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9834\n",
      "Epoch 00024: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0660 - acc: 0.9833 - val_loss: 0.1805 - val_acc: 0.9432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9856\n",
      "Epoch 00025: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0611 - acc: 0.9856 - val_loss: 0.1710 - val_acc: 0.9488\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9856\n",
      "Epoch 00026: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0580 - acc: 0.9856 - val_loss: 0.1942 - val_acc: 0.9415\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9857\n",
      "Epoch 00027: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0584 - acc: 0.9857 - val_loss: 0.1756 - val_acc: 0.9448\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9892\n",
      "Epoch 00028: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0497 - acc: 0.9892 - val_loss: 0.1759 - val_acc: 0.9436\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9911\n",
      "Epoch 00029: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0466 - acc: 0.9911 - val_loss: 0.1686 - val_acc: 0.9506\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9896\n",
      "Epoch 00030: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0472 - acc: 0.9896 - val_loss: 0.1803 - val_acc: 0.9474\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9919\n",
      "Epoch 00031: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0399 - acc: 0.9919 - val_loss: 0.1727 - val_acc: 0.9464\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9913\n",
      "Epoch 00032: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0389 - acc: 0.9913 - val_loss: 0.1859 - val_acc: 0.9460\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9927\n",
      "Epoch 00033: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0360 - acc: 0.9927 - val_loss: 0.2073 - val_acc: 0.9385\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9939\n",
      "Epoch 00034: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0329 - acc: 0.9939 - val_loss: 0.1797 - val_acc: 0.9471\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9949\n",
      "Epoch 00035: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0299 - acc: 0.9949 - val_loss: 0.1824 - val_acc: 0.9460\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9951\n",
      "Epoch 00036: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0282 - acc: 0.9951 - val_loss: 0.1836 - val_acc: 0.9455\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9943\n",
      "Epoch 00037: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0292 - acc: 0.9943 - val_loss: 0.1936 - val_acc: 0.9427\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9955\n",
      "Epoch 00038: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0256 - acc: 0.9955 - val_loss: 0.1936 - val_acc: 0.9450\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9959\n",
      "Epoch 00039: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0241 - acc: 0.9959 - val_loss: 0.1975 - val_acc: 0.9427\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9965\n",
      "Epoch 00040: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0220 - acc: 0.9965 - val_loss: 0.1945 - val_acc: 0.9427\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9955\n",
      "Epoch 00041: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0241 - acc: 0.9955 - val_loss: 0.1970 - val_acc: 0.9432\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9972\n",
      "Epoch 00042: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0188 - acc: 0.9972 - val_loss: 0.1937 - val_acc: 0.9450\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9975\n",
      "Epoch 00043: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0173 - acc: 0.9974 - val_loss: 0.1886 - val_acc: 0.9455\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9953\n",
      "Epoch 00044: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0231 - acc: 0.9953 - val_loss: 0.1956 - val_acc: 0.9464\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9967\n",
      "Epoch 00045: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0190 - acc: 0.9967 - val_loss: 0.2018 - val_acc: 0.9441\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9964\n",
      "Epoch 00046: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0193 - acc: 0.9963 - val_loss: 0.1832 - val_acc: 0.9471\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9957\n",
      "Epoch 00047: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0200 - acc: 0.9956 - val_loss: 0.1987 - val_acc: 0.9481\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9969\n",
      "Epoch 00048: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0178 - acc: 0.9969 - val_loss: 0.1900 - val_acc: 0.9495\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9981\n",
      "Epoch 00049: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0134 - acc: 0.9981 - val_loss: 0.1858 - val_acc: 0.9490\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9985\n",
      "Epoch 00050: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0120 - acc: 0.9985 - val_loss: 0.2082 - val_acc: 0.9422\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0241 - acc: 0.9946 - val_loss: 0.1871 - val_acc: 0.9471\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9985\n",
      "Epoch 00052: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0111 - acc: 0.9985 - val_loss: 0.1927 - val_acc: 0.9439\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9990\n",
      "Epoch 00053: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0094 - acc: 0.9990 - val_loss: 0.2034 - val_acc: 0.9450\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9985\n",
      "Epoch 00054: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0113 - acc: 0.9985 - val_loss: 0.2056 - val_acc: 0.9439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9981\n",
      "Epoch 00055: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0128 - acc: 0.9981 - val_loss: 0.2099 - val_acc: 0.9455\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9956\n",
      "Epoch 00056: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0192 - acc: 0.9956 - val_loss: 0.2007 - val_acc: 0.9469\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9944\n",
      "Epoch 00057: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0219 - acc: 0.9944 - val_loss: 0.1852 - val_acc: 0.9485\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9984\n",
      "Epoch 00058: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0107 - acc: 0.9984 - val_loss: 0.1902 - val_acc: 0.9499\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9989\n",
      "Epoch 00059: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0088 - acc: 0.9988 - val_loss: 0.2055 - val_acc: 0.9450\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9973\n",
      "Epoch 00060: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0138 - acc: 0.9973 - val_loss: 0.2041 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9977\n",
      "Epoch 00061: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0128 - acc: 0.9977 - val_loss: 0.2324 - val_acc: 0.9415\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.0163 - acc: 0.9963 - val_loss: 0.1861 - val_acc: 0.9513\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9990\n",
      "Epoch 00063: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0078 - acc: 0.9989 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9968\n",
      "Epoch 00064: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0150 - acc: 0.9968 - val_loss: 0.1897 - val_acc: 0.9513\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9973\n",
      "Epoch 00065: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0130 - acc: 0.9973 - val_loss: 0.1841 - val_acc: 0.9509\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9994\n",
      "Epoch 00066: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0064 - acc: 0.9994 - val_loss: 0.1937 - val_acc: 0.9488\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9980\n",
      "Epoch 00067: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0114 - acc: 0.9980 - val_loss: 0.1909 - val_acc: 0.9509\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9990\n",
      "Epoch 00068: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0072 - acc: 0.9990 - val_loss: 0.2073 - val_acc: 0.9457\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9942\n",
      "Epoch 00069: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0211 - acc: 0.9942 - val_loss: 0.2023 - val_acc: 0.9469\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9986\n",
      "Epoch 00070: val_loss did not improve from 0.16592\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.2054 - val_acc: 0.9460\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 456us/sample - loss: 0.1987 - acc: 0.9364\n",
      "Loss: 0.19870965909116117 Accuracy: 0.9364486\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5697 - acc: 0.5165\n",
      "Epoch 00001: val_loss improved from inf to 1.23433, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/001-1.2343.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.5698 - acc: 0.5164 - val_loss: 1.2343 - val_acc: 0.6334\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7139 - acc: 0.7865\n",
      "Epoch 00002: val_loss improved from 1.23433 to 0.52511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/002-0.5251.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.7140 - acc: 0.7865 - val_loss: 0.5251 - val_acc: 0.8442\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8583\n",
      "Epoch 00003: val_loss improved from 0.52511 to 0.37848, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/003-0.3785.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.4770 - acc: 0.8583 - val_loss: 0.3785 - val_acc: 0.8915\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8912\n",
      "Epoch 00004: val_loss improved from 0.37848 to 0.31792, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/004-0.3179.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.3681 - acc: 0.8911 - val_loss: 0.3179 - val_acc: 0.9085\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.9113\n",
      "Epoch 00005: val_loss improved from 0.31792 to 0.28423, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/005-0.2842.hdf5\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.3014 - acc: 0.9113 - val_loss: 0.2842 - val_acc: 0.9154\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9225\n",
      "Epoch 00006: val_loss improved from 0.28423 to 0.24551, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/006-0.2455.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.2596 - acc: 0.9225 - val_loss: 0.2455 - val_acc: 0.9257\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9316\n",
      "Epoch 00007: val_loss did not improve from 0.24551\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.2296 - acc: 0.9316 - val_loss: 0.2539 - val_acc: 0.9262\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9385\n",
      "Epoch 00008: val_loss improved from 0.24551 to 0.23070, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/008-0.2307.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.2060 - acc: 0.9384 - val_loss: 0.2307 - val_acc: 0.9264\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9450\n",
      "Epoch 00009: val_loss did not improve from 0.23070\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.1849 - acc: 0.9450 - val_loss: 0.2467 - val_acc: 0.9259\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9501\n",
      "Epoch 00010: val_loss improved from 0.23070 to 0.20787, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/010-0.2079.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1684 - acc: 0.9500 - val_loss: 0.2079 - val_acc: 0.9359\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9526\n",
      "Epoch 00011: val_loss improved from 0.20787 to 0.20206, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/011-0.2021.hdf5\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.1540 - acc: 0.9526 - val_loss: 0.2021 - val_acc: 0.9352\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9583\n",
      "Epoch 00012: val_loss improved from 0.20206 to 0.19058, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/012-0.1906.hdf5\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1397 - acc: 0.9583 - val_loss: 0.1906 - val_acc: 0.9420\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9618\n",
      "Epoch 00013: val_loss improved from 0.19058 to 0.18445, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/013-0.1845.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1296 - acc: 0.9619 - val_loss: 0.1845 - val_acc: 0.9460\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9658\n",
      "Epoch 00014: val_loss improved from 0.18445 to 0.17602, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/014-0.1760.hdf5\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1177 - acc: 0.9658 - val_loss: 0.1760 - val_acc: 0.9462\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9692\n",
      "Epoch 00015: val_loss improved from 0.17602 to 0.17294, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/015-0.1729.hdf5\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.1083 - acc: 0.9692 - val_loss: 0.1729 - val_acc: 0.9450\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9730\n",
      "Epoch 00016: val_loss did not improve from 0.17294\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0979 - acc: 0.9730 - val_loss: 0.1904 - val_acc: 0.9397\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9751\n",
      "Epoch 00017: val_loss improved from 0.17294 to 0.16600, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv_checkpoint/017-0.1660.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0888 - acc: 0.9751 - val_loss: 0.1660 - val_acc: 0.9497\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9771\n",
      "Epoch 00018: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0846 - acc: 0.9771 - val_loss: 0.2037 - val_acc: 0.9385\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9785\n",
      "Epoch 00019: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0779 - acc: 0.9784 - val_loss: 0.1751 - val_acc: 0.9481\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9792\n",
      "Epoch 00020: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0767 - acc: 0.9792 - val_loss: 0.1680 - val_acc: 0.9478\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9823\n",
      "Epoch 00021: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0673 - acc: 0.9823 - val_loss: 0.1884 - val_acc: 0.9455\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9844\n",
      "Epoch 00022: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0607 - acc: 0.9843 - val_loss: 0.1758 - val_acc: 0.9462\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9861\n",
      "Epoch 00023: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0567 - acc: 0.9861 - val_loss: 0.1930 - val_acc: 0.9436\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9885\n",
      "Epoch 00024: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0493 - acc: 0.9885 - val_loss: 0.1670 - val_acc: 0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9898\n",
      "Epoch 00025: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0455 - acc: 0.9898 - val_loss: 0.1928 - val_acc: 0.9415\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9910\n",
      "Epoch 00026: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0423 - acc: 0.9910 - val_loss: 0.1859 - val_acc: 0.9485\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9905\n",
      "Epoch 00027: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0407 - acc: 0.9905 - val_loss: 0.1818 - val_acc: 0.9485\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9929\n",
      "Epoch 00028: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0355 - acc: 0.9929 - val_loss: 0.1988 - val_acc: 0.9436\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9933\n",
      "Epoch 00029: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0338 - acc: 0.9933 - val_loss: 0.1840 - val_acc: 0.9485\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0317 - acc: 0.9932 - val_loss: 0.2080 - val_acc: 0.9397\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9919\n",
      "Epoch 00031: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0357 - acc: 0.9919 - val_loss: 0.1826 - val_acc: 0.9504\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9928\n",
      "Epoch 00032: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0328 - acc: 0.9928 - val_loss: 0.1809 - val_acc: 0.9492\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9932\n",
      "Epoch 00033: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0290 - acc: 0.9932 - val_loss: 0.1853 - val_acc: 0.9513\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9954\n",
      "Epoch 00034: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0248 - acc: 0.9953 - val_loss: 0.2012 - val_acc: 0.9460\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9928\n",
      "Epoch 00035: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0305 - acc: 0.9927 - val_loss: 0.1974 - val_acc: 0.9460\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9948\n",
      "Epoch 00036: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0250 - acc: 0.9947 - val_loss: 0.1849 - val_acc: 0.9509\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9922\n",
      "Epoch 00037: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0315 - acc: 0.9921 - val_loss: 0.1828 - val_acc: 0.9490\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9964\n",
      "Epoch 00038: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0194 - acc: 0.9964 - val_loss: 0.1910 - val_acc: 0.9483\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9973\n",
      "Epoch 00039: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0164 - acc: 0.9973 - val_loss: 0.2159 - val_acc: 0.9441\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9921\n",
      "Epoch 00040: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0312 - acc: 0.9921 - val_loss: 0.1987 - val_acc: 0.9474\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9980\n",
      "Epoch 00041: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0144 - acc: 0.9980 - val_loss: 0.2090 - val_acc: 0.9481\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9975\n",
      "Epoch 00042: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0153 - acc: 0.9975 - val_loss: 0.1850 - val_acc: 0.9529\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9973\n",
      "Epoch 00043: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0155 - acc: 0.9973 - val_loss: 0.2337 - val_acc: 0.9387\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9953\n",
      "Epoch 00044: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0209 - acc: 0.9953 - val_loss: 0.2017 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9979\n",
      "Epoch 00045: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0132 - acc: 0.9979 - val_loss: 0.2178 - val_acc: 0.9415\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9980\n",
      "Epoch 00046: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0126 - acc: 0.9980 - val_loss: 0.2014 - val_acc: 0.9492\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9980\n",
      "Epoch 00047: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0120 - acc: 0.9979 - val_loss: 0.2228 - val_acc: 0.9469\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9931\n",
      "Epoch 00048: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0258 - acc: 0.9931 - val_loss: 0.1965 - val_acc: 0.9488\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9979\n",
      "Epoch 00049: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0112 - acc: 0.9979 - val_loss: 0.2134 - val_acc: 0.9439\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9975\n",
      "Epoch 00050: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0136 - acc: 0.9975 - val_loss: 0.2043 - val_acc: 0.9481\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9988\n",
      "Epoch 00051: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0092 - acc: 0.9988 - val_loss: 0.1942 - val_acc: 0.9527\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9986\n",
      "Epoch 00052: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0093 - acc: 0.9986 - val_loss: 0.1980 - val_acc: 0.9525\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9975\n",
      "Epoch 00053: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0122 - acc: 0.9975 - val_loss: 0.2306 - val_acc: 0.9448\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9977\n",
      "Epoch 00054: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0111 - acc: 0.9977 - val_loss: 0.2268 - val_acc: 0.9441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9967\n",
      "Epoch 00055: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0142 - acc: 0.9966 - val_loss: 0.1881 - val_acc: 0.9567\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9950\n",
      "Epoch 00056: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0189 - acc: 0.9950 - val_loss: 0.2119 - val_acc: 0.9476\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9965\n",
      "Epoch 00057: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0143 - acc: 0.9965 - val_loss: 0.1898 - val_acc: 0.9532\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9992\n",
      "Epoch 00058: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0067 - acc: 0.9992 - val_loss: 0.2116 - val_acc: 0.9511\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9987\n",
      "Epoch 00059: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0079 - acc: 0.9987 - val_loss: 0.2058 - val_acc: 0.9522\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9984\n",
      "Epoch 00060: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0086 - acc: 0.9984 - val_loss: 0.2147 - val_acc: 0.9502\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9958\n",
      "Epoch 00061: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0165 - acc: 0.9958 - val_loss: 0.2087 - val_acc: 0.9518\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9949\n",
      "Epoch 00062: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0192 - acc: 0.9949 - val_loss: 0.1963 - val_acc: 0.9550\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 00063: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0065 - acc: 0.9993 - val_loss: 0.1995 - val_acc: 0.9548\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9994\n",
      "Epoch 00064: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0053 - acc: 0.9994 - val_loss: 0.2255 - val_acc: 0.9460\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9952\n",
      "Epoch 00065: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0179 - acc: 0.9952 - val_loss: 0.2145 - val_acc: 0.9502\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 00066: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0062 - acc: 0.9992 - val_loss: 0.2061 - val_acc: 0.9497\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9949\n",
      "Epoch 00067: val_loss did not improve from 0.16600\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0178 - acc: 0.9949 - val_loss: 0.2150 - val_acc: 0.9467\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 506us/sample - loss: 0.2115 - acc: 0.9344\n",
      "Loss: 0.21154268181583963 Accuracy: 0.93437177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 64)           256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 448us/sample - loss: 0.6524 - acc: 0.8077\n",
      "Loss: 0.6523727787679848 Accuracy: 0.8076843\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 64)           256         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 486us/sample - loss: 0.4886 - acc: 0.8546\n",
      "Loss: 0.48855698931563807 Accuracy: 0.854621\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 96)           384         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 505us/sample - loss: 0.2979 - acc: 0.9099\n",
      "Loss: 0.2979069372760915 Accuracy: 0.909865\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 128)          512         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 551us/sample - loss: 0.2293 - acc: 0.9306\n",
      "Loss: 0.22926596655033582 Accuracy: 0.9306334\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 128)          512         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 566us/sample - loss: 0.1987 - acc: 0.9364\n",
      "Loss: 0.19870965909116117 Accuracy: 0.9364486\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 128)          512         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 584us/sample - loss: 0.2115 - acc: 0.9344\n",
      "Loss: 0.21154268181583963 Accuracy: 0.93437177\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 64)           256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 569us/sample - loss: 0.6770 - acc: 0.8046\n",
      "Loss: 0.6770068614901288 Accuracy: 0.80456907\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 64)           256         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 579us/sample - loss: 0.5606 - acc: 0.8428\n",
      "Loss: 0.5605787344315589 Accuracy: 0.842783\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 96)           384         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 608us/sample - loss: 0.3814 - acc: 0.9022\n",
      "Loss: 0.38142653022847317 Accuracy: 0.9021807\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 128)          512         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 671us/sample - loss: 0.2951 - acc: 0.9254\n",
      "Loss: 0.2950542231184176 Accuracy: 0.9254413\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 128)          512         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 673us/sample - loss: 0.2447 - acc: 0.9371\n",
      "Loss: 0.2446750401584035 Accuracy: 0.9370716\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 128)          512         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 689us/sample - loss: 0.2675 - acc: 0.9379\n",
      "Loss: 0.2674754717670621 Accuracy: 0.9379024\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
