{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 64)           256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1040        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 64)           256         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 96)           384         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1552        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 128)          512         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 128)          512         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 128)          512         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2345 - acc: 0.3168\n",
      "Epoch 00001: val_loss improved from inf to 2.26297, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/001-2.2630.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 2.2338 - acc: 0.3170 - val_loss: 2.2630 - val_acc: 0.2662\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7634 - acc: 0.4633\n",
      "Epoch 00002: val_loss improved from 2.26297 to 1.76292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/002-1.7629.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.7630 - acc: 0.4634 - val_loss: 1.7629 - val_acc: 0.4472\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5765 - acc: 0.5267\n",
      "Epoch 00003: val_loss improved from 1.76292 to 1.58082, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/003-1.5808.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.5765 - acc: 0.5266 - val_loss: 1.5808 - val_acc: 0.5306\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4642 - acc: 0.5660\n",
      "Epoch 00004: val_loss improved from 1.58082 to 1.56446, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/004-1.5645.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.4638 - acc: 0.5661 - val_loss: 1.5645 - val_acc: 0.5064\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3826 - acc: 0.5911\n",
      "Epoch 00005: val_loss did not improve from 1.56446\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.3827 - acc: 0.5911 - val_loss: 1.5907 - val_acc: 0.5092\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3163 - acc: 0.6126\n",
      "Epoch 00006: val_loss did not improve from 1.56446\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.3163 - acc: 0.6126 - val_loss: 1.7651 - val_acc: 0.4139\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2594 - acc: 0.6296\n",
      "Epoch 00007: val_loss improved from 1.56446 to 1.25308, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/007-1.2531.hdf5\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 1.2595 - acc: 0.6296 - val_loss: 1.2531 - val_acc: 0.6245\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2199 - acc: 0.6430\n",
      "Epoch 00008: val_loss did not improve from 1.25308\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.2199 - acc: 0.6430 - val_loss: 1.3323 - val_acc: 0.5754\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1851 - acc: 0.6506\n",
      "Epoch 00009: val_loss did not improve from 1.25308\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 1.1854 - acc: 0.6503 - val_loss: 1.4405 - val_acc: 0.5663\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1527 - acc: 0.6652\n",
      "Epoch 00010: val_loss improved from 1.25308 to 1.24720, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/010-1.2472.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 1.1531 - acc: 0.6649 - val_loss: 1.2472 - val_acc: 0.6122\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1268 - acc: 0.6702\n",
      "Epoch 00011: val_loss did not improve from 1.24720\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.1269 - acc: 0.6702 - val_loss: 1.4072 - val_acc: 0.5323\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1075 - acc: 0.6750\n",
      "Epoch 00012: val_loss did not improve from 1.24720\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 1.1075 - acc: 0.6750 - val_loss: 1.4565 - val_acc: 0.5798\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0818 - acc: 0.6822\n",
      "Epoch 00013: val_loss did not improve from 1.24720\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 1.0818 - acc: 0.6822 - val_loss: 1.8768 - val_acc: 0.4554\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.6867\n",
      "Epoch 00014: val_loss did not improve from 1.24720\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 1.0669 - acc: 0.6867 - val_loss: 2.1423 - val_acc: 0.3993\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0505 - acc: 0.6911\n",
      "Epoch 00015: val_loss did not improve from 1.24720\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.0505 - acc: 0.6911 - val_loss: 1.4242 - val_acc: 0.5563\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0338 - acc: 0.6966\n",
      "Epoch 00016: val_loss improved from 1.24720 to 1.23986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/016-1.2399.hdf5\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.0340 - acc: 0.6965 - val_loss: 1.2399 - val_acc: 0.5986\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0244 - acc: 0.6992\n",
      "Epoch 00017: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 1.0247 - acc: 0.6991 - val_loss: 1.4240 - val_acc: 0.5446\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0087 - acc: 0.7042\n",
      "Epoch 00018: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 1.0087 - acc: 0.7042 - val_loss: 1.3594 - val_acc: 0.5935\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9978 - acc: 0.7087\n",
      "Epoch 00019: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.9980 - acc: 0.7087 - val_loss: 2.0429 - val_acc: 0.4184\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9911 - acc: 0.7091\n",
      "Epoch 00020: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9909 - acc: 0.7092 - val_loss: 1.4899 - val_acc: 0.5737\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.7151\n",
      "Epoch 00021: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.9776 - acc: 0.7151 - val_loss: 1.5579 - val_acc: 0.5248\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9754 - acc: 0.7146\n",
      "Epoch 00022: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9755 - acc: 0.7146 - val_loss: 1.3944 - val_acc: 0.5935\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.7188\n",
      "Epoch 00023: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9614 - acc: 0.7187 - val_loss: 1.6666 - val_acc: 0.4822\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9540 - acc: 0.7199\n",
      "Epoch 00024: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9538 - acc: 0.7200 - val_loss: 1.8147 - val_acc: 0.4745\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9431 - acc: 0.7233\n",
      "Epoch 00025: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9431 - acc: 0.7234 - val_loss: 1.2573 - val_acc: 0.6226\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9383 - acc: 0.7271\n",
      "Epoch 00026: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.9384 - acc: 0.7270 - val_loss: 2.1456 - val_acc: 0.4009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7276\n",
      "Epoch 00027: val_loss did not improve from 1.23986\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.9318 - acc: 0.7275 - val_loss: 1.5456 - val_acc: 0.5129\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9208 - acc: 0.7293\n",
      "Epoch 00028: val_loss improved from 1.23986 to 1.18721, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/028-1.1872.hdf5\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.9209 - acc: 0.7293 - val_loss: 1.1872 - val_acc: 0.6261\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9158 - acc: 0.7325\n",
      "Epoch 00029: val_loss improved from 1.18721 to 1.18175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/029-1.1818.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.9154 - acc: 0.7326 - val_loss: 1.1818 - val_acc: 0.6406\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9097 - acc: 0.7337\n",
      "Epoch 00030: val_loss did not improve from 1.18175\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.9100 - acc: 0.7337 - val_loss: 1.3117 - val_acc: 0.5968\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7356\n",
      "Epoch 00031: val_loss did not improve from 1.18175\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.9022 - acc: 0.7356 - val_loss: 1.7045 - val_acc: 0.5183\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9038 - acc: 0.7367\n",
      "Epoch 00032: val_loss improved from 1.18175 to 1.02266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/032-1.0227.hdf5\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.9038 - acc: 0.7367 - val_loss: 1.0227 - val_acc: 0.6874\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8947 - acc: 0.7375\n",
      "Epoch 00033: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8947 - acc: 0.7375 - val_loss: 1.1097 - val_acc: 0.6413\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.7418\n",
      "Epoch 00034: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8875 - acc: 0.7417 - val_loss: 1.0642 - val_acc: 0.6704\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7426\n",
      "Epoch 00035: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8838 - acc: 0.7423 - val_loss: 1.8161 - val_acc: 0.5041\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8770 - acc: 0.7424\n",
      "Epoch 00036: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8772 - acc: 0.7424 - val_loss: 1.1285 - val_acc: 0.6497\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8748 - acc: 0.7447\n",
      "Epoch 00037: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8748 - acc: 0.7447 - val_loss: 1.5557 - val_acc: 0.5083\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.7471\n",
      "Epoch 00038: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8702 - acc: 0.7471 - val_loss: 1.0688 - val_acc: 0.6704\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8676 - acc: 0.7453\n",
      "Epoch 00039: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.8681 - acc: 0.7451 - val_loss: 1.4014 - val_acc: 0.5630\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8599 - acc: 0.7493\n",
      "Epoch 00040: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8599 - acc: 0.7493 - val_loss: 1.2752 - val_acc: 0.5809\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8529 - acc: 0.7507\n",
      "Epoch 00041: val_loss did not improve from 1.02266\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.8529 - acc: 0.7507 - val_loss: 1.1350 - val_acc: 0.6427\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8496 - acc: 0.7503\n",
      "Epoch 00042: val_loss improved from 1.02266 to 0.94246, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/042-0.9425.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8496 - acc: 0.7503 - val_loss: 0.9425 - val_acc: 0.7216\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8467 - acc: 0.7515\n",
      "Epoch 00043: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.8470 - acc: 0.7515 - val_loss: 1.4025 - val_acc: 0.5756\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7520\n",
      "Epoch 00044: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.8456 - acc: 0.7520 - val_loss: 1.6784 - val_acc: 0.5458\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8399 - acc: 0.7548\n",
      "Epoch 00045: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8394 - acc: 0.7550 - val_loss: 1.4418 - val_acc: 0.5595\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.7554\n",
      "Epoch 00046: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.8334 - acc: 0.7554 - val_loss: 1.3151 - val_acc: 0.6017\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8311 - acc: 0.7590\n",
      "Epoch 00047: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.8310 - acc: 0.7590 - val_loss: 1.1317 - val_acc: 0.6573\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8230 - acc: 0.7587\n",
      "Epoch 00048: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8229 - acc: 0.7587 - val_loss: 2.1114 - val_acc: 0.4913\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8258 - acc: 0.7574\n",
      "Epoch 00049: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 0.8257 - acc: 0.7575 - val_loss: 1.2627 - val_acc: 0.5714\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8157 - acc: 0.7604\n",
      "Epoch 00050: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 0.8158 - acc: 0.7603 - val_loss: 2.0212 - val_acc: 0.4729\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8135 - acc: 0.7619\n",
      "Epoch 00051: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 757us/sample - loss: 0.8133 - acc: 0.7619 - val_loss: 1.1933 - val_acc: 0.6336\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.7632\n",
      "Epoch 00052: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8125 - acc: 0.7631 - val_loss: 1.2289 - val_acc: 0.6292\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.7631\n",
      "Epoch 00053: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.8081 - acc: 0.7631 - val_loss: 1.3201 - val_acc: 0.5989\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.7645\n",
      "Epoch 00054: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.8032 - acc: 0.7645 - val_loss: 1.6501 - val_acc: 0.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.7654\n",
      "Epoch 00055: val_loss did not improve from 0.94246\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.8065 - acc: 0.7653 - val_loss: 0.9619 - val_acc: 0.7058\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7997 - acc: 0.7656\n",
      "Epoch 00056: val_loss improved from 0.94246 to 0.85816, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv_checkpoint/056-0.8582.hdf5\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7997 - acc: 0.7656 - val_loss: 0.8582 - val_acc: 0.7519\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7955 - acc: 0.7682\n",
      "Epoch 00057: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7959 - acc: 0.7681 - val_loss: 1.4942 - val_acc: 0.5705\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7923 - acc: 0.7685\n",
      "Epoch 00058: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7922 - acc: 0.7686 - val_loss: 1.4215 - val_acc: 0.6187\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.7681\n",
      "Epoch 00059: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7939 - acc: 0.7681 - val_loss: 1.0306 - val_acc: 0.6830\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7866 - acc: 0.7703\n",
      "Epoch 00060: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7867 - acc: 0.7703 - val_loss: 1.1157 - val_acc: 0.6450\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7815 - acc: 0.7704\n",
      "Epoch 00061: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7818 - acc: 0.7703 - val_loss: 1.9059 - val_acc: 0.4505\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7853 - acc: 0.7701\n",
      "Epoch 00062: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7854 - acc: 0.7701 - val_loss: 1.6412 - val_acc: 0.5542\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7724\n",
      "Epoch 00063: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7779 - acc: 0.7724 - val_loss: 1.5035 - val_acc: 0.5448\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7703\n",
      "Epoch 00064: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7777 - acc: 0.7703 - val_loss: 1.4564 - val_acc: 0.5635\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7705 - acc: 0.7742\n",
      "Epoch 00065: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7706 - acc: 0.7742 - val_loss: 1.5163 - val_acc: 0.5644\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7671 - acc: 0.7751\n",
      "Epoch 00066: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7672 - acc: 0.7751 - val_loss: 1.1804 - val_acc: 0.6334\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7664 - acc: 0.7758\n",
      "Epoch 00067: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7664 - acc: 0.7758 - val_loss: 2.4052 - val_acc: 0.4086\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7639 - acc: 0.7768\n",
      "Epoch 00068: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7639 - acc: 0.7767 - val_loss: 1.3742 - val_acc: 0.5940\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7613 - acc: 0.7754\n",
      "Epoch 00069: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7616 - acc: 0.7753 - val_loss: 1.4468 - val_acc: 0.5621\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7599 - acc: 0.7786\n",
      "Epoch 00070: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7599 - acc: 0.7786 - val_loss: 1.4346 - val_acc: 0.5877\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7549 - acc: 0.7801\n",
      "Epoch 00071: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7552 - acc: 0.7800 - val_loss: 2.1914 - val_acc: 0.3697\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7520 - acc: 0.7804\n",
      "Epoch 00072: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7528 - acc: 0.7804 - val_loss: 1.1273 - val_acc: 0.6601\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7536 - acc: 0.7775\n",
      "Epoch 00073: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7538 - acc: 0.7774 - val_loss: 1.4587 - val_acc: 0.5525\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.7777\n",
      "Epoch 00074: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7556 - acc: 0.7776 - val_loss: 1.1699 - val_acc: 0.6518\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7803\n",
      "Epoch 00075: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7487 - acc: 0.7801 - val_loss: 1.0219 - val_acc: 0.6571\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7479 - acc: 0.7818\n",
      "Epoch 00076: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.7478 - acc: 0.7817 - val_loss: 1.4334 - val_acc: 0.5826\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7422 - acc: 0.7831\n",
      "Epoch 00077: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7423 - acc: 0.7831 - val_loss: 1.1420 - val_acc: 0.6613\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7824\n",
      "Epoch 00078: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7429 - acc: 0.7823 - val_loss: 1.8471 - val_acc: 0.4980\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7401 - acc: 0.7836\n",
      "Epoch 00079: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7404 - acc: 0.7836 - val_loss: 1.2964 - val_acc: 0.6014\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7349 - acc: 0.7844\n",
      "Epoch 00080: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7355 - acc: 0.7843 - val_loss: 1.7188 - val_acc: 0.5178\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7365 - acc: 0.7848\n",
      "Epoch 00081: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7366 - acc: 0.7848 - val_loss: 1.0393 - val_acc: 0.6921\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7845\n",
      "Epoch 00082: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7362 - acc: 0.7845 - val_loss: 1.7835 - val_acc: 0.5264\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.7843\n",
      "Epoch 00083: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7343 - acc: 0.7843 - val_loss: 1.0507 - val_acc: 0.6788\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7869\n",
      "Epoch 00084: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7305 - acc: 0.7869 - val_loss: 1.1798 - val_acc: 0.6401\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7302 - acc: 0.7854\n",
      "Epoch 00085: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7303 - acc: 0.7854 - val_loss: 1.0718 - val_acc: 0.6834\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7241 - acc: 0.7860\n",
      "Epoch 00086: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7239 - acc: 0.7861 - val_loss: 1.5648 - val_acc: 0.5851\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7888\n",
      "Epoch 00087: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7232 - acc: 0.7888 - val_loss: 1.5217 - val_acc: 0.5758\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7239 - acc: 0.7886\n",
      "Epoch 00088: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7242 - acc: 0.7886 - val_loss: 1.0570 - val_acc: 0.6639\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7213 - acc: 0.7891\n",
      "Epoch 00089: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7213 - acc: 0.7891 - val_loss: 1.1674 - val_acc: 0.6359\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7171 - acc: 0.7895\n",
      "Epoch 00090: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7171 - acc: 0.7895 - val_loss: 0.9282 - val_acc: 0.7154\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7882\n",
      "Epoch 00091: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.7168 - acc: 0.7882 - val_loss: 1.2496 - val_acc: 0.6294\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7147 - acc: 0.7901\n",
      "Epoch 00092: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7148 - acc: 0.7901 - val_loss: 1.1848 - val_acc: 0.6261\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7904\n",
      "Epoch 00093: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7142 - acc: 0.7905 - val_loss: 2.6330 - val_acc: 0.4610\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7147 - acc: 0.7906\n",
      "Epoch 00094: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7147 - acc: 0.7906 - val_loss: 1.0458 - val_acc: 0.6935\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7103 - acc: 0.7935\n",
      "Epoch 00095: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 764us/sample - loss: 0.7101 - acc: 0.7936 - val_loss: 1.2647 - val_acc: 0.6205\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7095 - acc: 0.7928\n",
      "Epoch 00096: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7095 - acc: 0.7927 - val_loss: 1.0133 - val_acc: 0.6872\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7071 - acc: 0.7937\n",
      "Epoch 00097: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 755us/sample - loss: 0.7071 - acc: 0.7937 - val_loss: 2.2411 - val_acc: 0.4675\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7041 - acc: 0.7930\n",
      "Epoch 00098: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7042 - acc: 0.7930 - val_loss: 1.6051 - val_acc: 0.5590\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.7946\n",
      "Epoch 00099: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7049 - acc: 0.7943 - val_loss: 1.6931 - val_acc: 0.5497\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7010 - acc: 0.7959\n",
      "Epoch 00100: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.7009 - acc: 0.7959 - val_loss: 0.9207 - val_acc: 0.6986\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7020 - acc: 0.7943\n",
      "Epoch 00101: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7020 - acc: 0.7943 - val_loss: 2.9849 - val_acc: 0.3857\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7024 - acc: 0.7931\n",
      "Epoch 00102: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7021 - acc: 0.7932 - val_loss: 1.3474 - val_acc: 0.6175\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.7931\n",
      "Epoch 00103: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.6963 - acc: 0.7930 - val_loss: 1.5541 - val_acc: 0.5749\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6976 - acc: 0.7985\n",
      "Epoch 00104: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.6977 - acc: 0.7985 - val_loss: 1.0457 - val_acc: 0.6774\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6976 - acc: 0.7946\n",
      "Epoch 00105: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.6977 - acc: 0.7945 - val_loss: 1.5447 - val_acc: 0.5747\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7959\n",
      "Epoch 00106: val_loss did not improve from 0.85816\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.6948 - acc: 0.7958 - val_loss: 1.3755 - val_acc: 0.6087\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 367us/sample - loss: 0.9155 - acc: 0.7254\n",
      "Loss: 0.9154667171238615 Accuracy: 0.72544134\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0467 - acc: 0.3704\n",
      "Epoch 00001: val_loss improved from inf to 2.04292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/001-2.0429.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 2.0467 - acc: 0.3704 - val_loss: 2.0429 - val_acc: 0.3245\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5724 - acc: 0.5298\n",
      "Epoch 00002: val_loss improved from 2.04292 to 1.45724, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/002-1.4572.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 1.5723 - acc: 0.5298 - val_loss: 1.4572 - val_acc: 0.5730\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3941 - acc: 0.5857\n",
      "Epoch 00003: val_loss improved from 1.45724 to 1.34977, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/003-1.3498.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.3940 - acc: 0.5857 - val_loss: 1.3498 - val_acc: 0.6101\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2805 - acc: 0.6235\n",
      "Epoch 00004: val_loss improved from 1.34977 to 1.23947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/004-1.2395.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.2806 - acc: 0.6234 - val_loss: 1.2395 - val_acc: 0.6452\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1969 - acc: 0.6487\n",
      "Epoch 00005: val_loss improved from 1.23947 to 1.17732, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/005-1.1773.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.1970 - acc: 0.6487 - val_loss: 1.1773 - val_acc: 0.6562\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1390 - acc: 0.6661\n",
      "Epoch 00006: val_loss did not improve from 1.17732\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.1392 - acc: 0.6660 - val_loss: 1.2103 - val_acc: 0.6212\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0924 - acc: 0.6779\n",
      "Epoch 00007: val_loss improved from 1.17732 to 1.12627, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/007-1.1263.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.0924 - acc: 0.6780 - val_loss: 1.1263 - val_acc: 0.6795\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0507 - acc: 0.6906\n",
      "Epoch 00008: val_loss did not improve from 1.12627\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0508 - acc: 0.6906 - val_loss: 1.2749 - val_acc: 0.5935\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0204 - acc: 0.6993\n",
      "Epoch 00009: val_loss did not improve from 1.12627\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.0205 - acc: 0.6993 - val_loss: 1.5834 - val_acc: 0.4922\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9895 - acc: 0.7079\n",
      "Epoch 00010: val_loss improved from 1.12627 to 0.95751, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/010-0.9575.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9896 - acc: 0.7079 - val_loss: 0.9575 - val_acc: 0.7195\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9647 - acc: 0.7154\n",
      "Epoch 00011: val_loss did not improve from 0.95751\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.9647 - acc: 0.7153 - val_loss: 1.2083 - val_acc: 0.6264\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9422 - acc: 0.7234\n",
      "Epoch 00012: val_loss improved from 0.95751 to 0.95429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/012-0.9543.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.9422 - acc: 0.7234 - val_loss: 0.9543 - val_acc: 0.7205\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9210 - acc: 0.7275\n",
      "Epoch 00013: val_loss did not improve from 0.95429\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9210 - acc: 0.7275 - val_loss: 1.0133 - val_acc: 0.6958\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.7348\n",
      "Epoch 00014: val_loss improved from 0.95429 to 0.88465, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/014-0.8847.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.9018 - acc: 0.7348 - val_loss: 0.8847 - val_acc: 0.7435\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8837 - acc: 0.7392\n",
      "Epoch 00015: val_loss did not improve from 0.88465\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8838 - acc: 0.7391 - val_loss: 0.9021 - val_acc: 0.7324\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.7451\n",
      "Epoch 00016: val_loss did not improve from 0.88465\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8687 - acc: 0.7450 - val_loss: 0.9032 - val_acc: 0.7310\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8469 - acc: 0.7502\n",
      "Epoch 00017: val_loss did not improve from 0.88465\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.8468 - acc: 0.7502 - val_loss: 0.9774 - val_acc: 0.6962\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7553\n",
      "Epoch 00018: val_loss did not improve from 0.88465\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8341 - acc: 0.7553 - val_loss: 0.8912 - val_acc: 0.7400\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.7576\n",
      "Epoch 00019: val_loss improved from 0.88465 to 0.88202, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/019-0.8820.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.8226 - acc: 0.7576 - val_loss: 0.8820 - val_acc: 0.7379\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.7596\n",
      "Epoch 00020: val_loss improved from 0.88202 to 0.82165, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/020-0.8216.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.8150 - acc: 0.7595 - val_loss: 0.8216 - val_acc: 0.7608\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7995 - acc: 0.7664\n",
      "Epoch 00021: val_loss improved from 0.82165 to 0.80889, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/021-0.8089.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.7995 - acc: 0.7663 - val_loss: 0.8089 - val_acc: 0.7601\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7685\n",
      "Epoch 00022: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.7897 - acc: 0.7685 - val_loss: 0.9656 - val_acc: 0.6918\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7724\n",
      "Epoch 00023: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.7797 - acc: 0.7723 - val_loss: 0.8467 - val_acc: 0.7461\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7728 - acc: 0.7759\n",
      "Epoch 00024: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7727 - acc: 0.7758 - val_loss: 0.8404 - val_acc: 0.7619\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7790\n",
      "Epoch 00025: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.7593 - acc: 0.7790 - val_loss: 0.8338 - val_acc: 0.7431\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7807\n",
      "Epoch 00026: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.7559 - acc: 0.7806 - val_loss: 0.8311 - val_acc: 0.7666\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.7833\n",
      "Epoch 00027: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.7447 - acc: 0.7833 - val_loss: 0.9670 - val_acc: 0.7060\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7843\n",
      "Epoch 00028: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.7396 - acc: 0.7843 - val_loss: 0.9145 - val_acc: 0.7142\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7285 - acc: 0.7878\n",
      "Epoch 00029: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.7286 - acc: 0.7878 - val_loss: 0.9806 - val_acc: 0.6990\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.7900\n",
      "Epoch 00030: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.7254 - acc: 0.7900 - val_loss: 0.8946 - val_acc: 0.7310\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7935\n",
      "Epoch 00031: val_loss did not improve from 0.80889\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.7142 - acc: 0.7935 - val_loss: 0.8677 - val_acc: 0.7379\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7107 - acc: 0.7909\n",
      "Epoch 00032: val_loss improved from 0.80889 to 0.75473, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/032-0.7547.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.7108 - acc: 0.7909 - val_loss: 0.7547 - val_acc: 0.7780\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7041 - acc: 0.7960\n",
      "Epoch 00033: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.7043 - acc: 0.7960 - val_loss: 0.7806 - val_acc: 0.7682\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7010 - acc: 0.7955\n",
      "Epoch 00034: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.7010 - acc: 0.7955 - val_loss: 0.8238 - val_acc: 0.7533\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.7996\n",
      "Epoch 00035: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6909 - acc: 0.7996 - val_loss: 0.9328 - val_acc: 0.7049\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6876 - acc: 0.8010\n",
      "Epoch 00036: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6876 - acc: 0.8010 - val_loss: 0.8009 - val_acc: 0.7589\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8025\n",
      "Epoch 00037: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6810 - acc: 0.8025 - val_loss: 0.8772 - val_acc: 0.7340\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.8014\n",
      "Epoch 00038: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6776 - acc: 0.8013 - val_loss: 0.8622 - val_acc: 0.7358\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.8049\n",
      "Epoch 00039: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6680 - acc: 0.8048 - val_loss: 0.8550 - val_acc: 0.7382\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8051\n",
      "Epoch 00040: val_loss did not improve from 0.75473\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6660 - acc: 0.8051 - val_loss: 0.8447 - val_acc: 0.7363\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6590 - acc: 0.8090\n",
      "Epoch 00041: val_loss improved from 0.75473 to 0.72085, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/041-0.7209.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6590 - acc: 0.8090 - val_loss: 0.7209 - val_acc: 0.7925\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6518 - acc: 0.8122\n",
      "Epoch 00042: val_loss did not improve from 0.72085\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.6518 - acc: 0.8122 - val_loss: 0.7702 - val_acc: 0.7640\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.8101\n",
      "Epoch 00043: val_loss did not improve from 0.72085\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6504 - acc: 0.8102 - val_loss: 0.7212 - val_acc: 0.7848\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8109\n",
      "Epoch 00044: val_loss did not improve from 0.72085\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6475 - acc: 0.8109 - val_loss: 1.0228 - val_acc: 0.6872\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6409 - acc: 0.8150\n",
      "Epoch 00045: val_loss did not improve from 0.72085\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6411 - acc: 0.8150 - val_loss: 0.7463 - val_acc: 0.7745\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.8130\n",
      "Epoch 00046: val_loss improved from 0.72085 to 0.65161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/046-0.6516.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.6406 - acc: 0.8130 - val_loss: 0.6516 - val_acc: 0.8153\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.8149\n",
      "Epoch 00047: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6353 - acc: 0.8149 - val_loss: 0.7620 - val_acc: 0.7757\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6312 - acc: 0.8165\n",
      "Epoch 00048: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.6312 - acc: 0.8165 - val_loss: 0.7421 - val_acc: 0.7927\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8177\n",
      "Epoch 00049: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6258 - acc: 0.8177 - val_loss: 0.7787 - val_acc: 0.7678\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.8195\n",
      "Epoch 00050: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6197 - acc: 0.8195 - val_loss: 0.6686 - val_acc: 0.8055\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.8211\n",
      "Epoch 00051: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.6195 - acc: 0.8211 - val_loss: 0.7639 - val_acc: 0.7657\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6160 - acc: 0.8207\n",
      "Epoch 00052: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.6160 - acc: 0.8207 - val_loss: 0.7316 - val_acc: 0.7906\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6138 - acc: 0.8218\n",
      "Epoch 00053: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.6140 - acc: 0.8217 - val_loss: 0.7048 - val_acc: 0.7964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8235\n",
      "Epoch 00054: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.6064 - acc: 0.8235 - val_loss: 0.7067 - val_acc: 0.8134\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.8262\n",
      "Epoch 00055: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.6035 - acc: 0.8262 - val_loss: 0.8355 - val_acc: 0.7324\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5998 - acc: 0.8243\n",
      "Epoch 00056: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.6000 - acc: 0.8243 - val_loss: 0.8360 - val_acc: 0.7412\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.8278\n",
      "Epoch 00057: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5950 - acc: 0.8278 - val_loss: 0.6833 - val_acc: 0.8104\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.8259\n",
      "Epoch 00058: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5964 - acc: 0.8259 - val_loss: 0.6685 - val_acc: 0.8150\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5930 - acc: 0.8276\n",
      "Epoch 00059: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5930 - acc: 0.8276 - val_loss: 0.6730 - val_acc: 0.8020\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8281\n",
      "Epoch 00060: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.5890 - acc: 0.8281 - val_loss: 0.8772 - val_acc: 0.7419\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.8283\n",
      "Epoch 00061: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5886 - acc: 0.8283 - val_loss: 0.6975 - val_acc: 0.8083\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.8318\n",
      "Epoch 00062: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5810 - acc: 0.8318 - val_loss: 0.7991 - val_acc: 0.7612\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.8335\n",
      "Epoch 00063: val_loss did not improve from 0.65161\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5762 - acc: 0.8334 - val_loss: 0.8355 - val_acc: 0.7463\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.8332\n",
      "Epoch 00064: val_loss improved from 0.65161 to 0.64026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/064-0.6403.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5742 - acc: 0.8331 - val_loss: 0.6403 - val_acc: 0.8176\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8326\n",
      "Epoch 00065: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5732 - acc: 0.8326 - val_loss: 0.6945 - val_acc: 0.8006\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5684 - acc: 0.8342\n",
      "Epoch 00066: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5684 - acc: 0.8342 - val_loss: 1.0021 - val_acc: 0.6993\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.8346\n",
      "Epoch 00067: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5674 - acc: 0.8346 - val_loss: 1.1815 - val_acc: 0.6494\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.8376\n",
      "Epoch 00068: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5616 - acc: 0.8376 - val_loss: 0.6754 - val_acc: 0.8055\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.8381\n",
      "Epoch 00069: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5609 - acc: 0.8381 - val_loss: 0.6970 - val_acc: 0.8088\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8370\n",
      "Epoch 00070: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5597 - acc: 0.8369 - val_loss: 0.7277 - val_acc: 0.7962\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8401\n",
      "Epoch 00071: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5540 - acc: 0.8401 - val_loss: 0.7310 - val_acc: 0.7852\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5530 - acc: 0.8406\n",
      "Epoch 00072: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5529 - acc: 0.8406 - val_loss: 0.7596 - val_acc: 0.7824\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.8410\n",
      "Epoch 00073: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5503 - acc: 0.8411 - val_loss: 0.6977 - val_acc: 0.8088\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8416\n",
      "Epoch 00074: val_loss did not improve from 0.64026\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5452 - acc: 0.8416 - val_loss: 0.6714 - val_acc: 0.8055\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5438 - acc: 0.8418\n",
      "Epoch 00075: val_loss improved from 0.64026 to 0.59022, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/075-0.5902.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5439 - acc: 0.8418 - val_loss: 0.5902 - val_acc: 0.8390\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.8411\n",
      "Epoch 00076: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5428 - acc: 0.8410 - val_loss: 0.5926 - val_acc: 0.8283\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8456\n",
      "Epoch 00077: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5393 - acc: 0.8455 - val_loss: 0.7458 - val_acc: 0.7820\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.8453\n",
      "Epoch 00078: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5392 - acc: 0.8453 - val_loss: 0.7966 - val_acc: 0.7636\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.8469\n",
      "Epoch 00079: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5295 - acc: 0.8468 - val_loss: 0.8372 - val_acc: 0.7466\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8466\n",
      "Epoch 00080: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5335 - acc: 0.8466 - val_loss: 1.0006 - val_acc: 0.6890\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8460\n",
      "Epoch 00081: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5301 - acc: 0.8460 - val_loss: 0.6399 - val_acc: 0.8181\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8498\n",
      "Epoch 00082: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5300 - acc: 0.8498 - val_loss: 0.6522 - val_acc: 0.8185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5261 - acc: 0.8485\n",
      "Epoch 00083: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.5261 - acc: 0.8486 - val_loss: 0.7103 - val_acc: 0.7945\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8482\n",
      "Epoch 00084: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5243 - acc: 0.8482 - val_loss: 0.6974 - val_acc: 0.8050\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.8505\n",
      "Epoch 00085: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.5182 - acc: 0.8505 - val_loss: 0.7902 - val_acc: 0.7750\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8479\n",
      "Epoch 00086: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5188 - acc: 0.8478 - val_loss: 0.6698 - val_acc: 0.8071\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5159 - acc: 0.8508\n",
      "Epoch 00087: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5161 - acc: 0.8508 - val_loss: 0.6355 - val_acc: 0.8230\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8497\n",
      "Epoch 00088: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5166 - acc: 0.8496 - val_loss: 0.7904 - val_acc: 0.7764\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8507\n",
      "Epoch 00089: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5119 - acc: 0.8507 - val_loss: 0.7226 - val_acc: 0.7829\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8532\n",
      "Epoch 00090: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5115 - acc: 0.8531 - val_loss: 0.9280 - val_acc: 0.7261\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.8516\n",
      "Epoch 00091: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5112 - acc: 0.8515 - val_loss: 0.7257 - val_acc: 0.7873\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8518\n",
      "Epoch 00092: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5079 - acc: 0.8517 - val_loss: 0.6242 - val_acc: 0.8206\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8550\n",
      "Epoch 00093: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5045 - acc: 0.8549 - val_loss: 0.7501 - val_acc: 0.7713\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8531\n",
      "Epoch 00094: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5032 - acc: 0.8531 - val_loss: 0.6514 - val_acc: 0.8206\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.8563\n",
      "Epoch 00095: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.5003 - acc: 0.8563 - val_loss: 0.7139 - val_acc: 0.7952\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8582\n",
      "Epoch 00096: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4950 - acc: 0.8583 - val_loss: 0.7303 - val_acc: 0.7980\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8562\n",
      "Epoch 00097: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4925 - acc: 0.8562 - val_loss: 0.9268 - val_acc: 0.7174\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8562\n",
      "Epoch 00098: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4950 - acc: 0.8561 - val_loss: 0.7287 - val_acc: 0.8006\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8560\n",
      "Epoch 00099: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4961 - acc: 0.8560 - val_loss: 0.7914 - val_acc: 0.7650\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8590\n",
      "Epoch 00100: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4886 - acc: 0.8589 - val_loss: 0.7083 - val_acc: 0.7955\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8548\n",
      "Epoch 00101: val_loss did not improve from 0.59022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4977 - acc: 0.8547 - val_loss: 0.6231 - val_acc: 0.8295\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8615\n",
      "Epoch 00102: val_loss improved from 0.59022 to 0.58463, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/102-0.5846.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4865 - acc: 0.8614 - val_loss: 0.5846 - val_acc: 0.8334\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8604\n",
      "Epoch 00103: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4849 - acc: 0.8604 - val_loss: 0.6885 - val_acc: 0.7990\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8634\n",
      "Epoch 00104: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.4825 - acc: 0.8634 - val_loss: 0.6968 - val_acc: 0.7978\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8603\n",
      "Epoch 00105: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4801 - acc: 0.8603 - val_loss: 0.7434 - val_acc: 0.7922\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8606\n",
      "Epoch 00106: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4803 - acc: 0.8606 - val_loss: 1.1123 - val_acc: 0.6858\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8602\n",
      "Epoch 00107: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4802 - acc: 0.8602 - val_loss: 0.6179 - val_acc: 0.8262\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8638\n",
      "Epoch 00108: val_loss did not improve from 0.58463\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4716 - acc: 0.8637 - val_loss: 0.6871 - val_acc: 0.8088\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8620\n",
      "Epoch 00109: val_loss improved from 0.58463 to 0.57696, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/109-0.5770.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4739 - acc: 0.8619 - val_loss: 0.5770 - val_acc: 0.8383\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8630\n",
      "Epoch 00110: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4707 - acc: 0.8630 - val_loss: 0.9605 - val_acc: 0.7216\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4773 - acc: 0.8613\n",
      "Epoch 00111: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4773 - acc: 0.8613 - val_loss: 0.7148 - val_acc: 0.7950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.8623\n",
      "Epoch 00112: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4713 - acc: 0.8623 - val_loss: 0.6259 - val_acc: 0.8225\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4676 - acc: 0.8633\n",
      "Epoch 00113: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4679 - acc: 0.8632 - val_loss: 0.7813 - val_acc: 0.7736\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.8643\n",
      "Epoch 00114: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4641 - acc: 0.8643 - val_loss: 0.6637 - val_acc: 0.8167\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8642\n",
      "Epoch 00115: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4659 - acc: 0.8641 - val_loss: 0.7323 - val_acc: 0.7799\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8665\n",
      "Epoch 00116: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4632 - acc: 0.8664 - val_loss: 0.7915 - val_acc: 0.7678\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8661\n",
      "Epoch 00117: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4612 - acc: 0.8660 - val_loss: 1.2945 - val_acc: 0.6627\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8645\n",
      "Epoch 00118: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4601 - acc: 0.8645 - val_loss: 0.5838 - val_acc: 0.8353\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8662\n",
      "Epoch 00119: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4596 - acc: 0.8662 - val_loss: 1.0165 - val_acc: 0.7133\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4575 - acc: 0.8670\n",
      "Epoch 00120: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4574 - acc: 0.8670 - val_loss: 0.5774 - val_acc: 0.8402\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8694\n",
      "Epoch 00121: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4505 - acc: 0.8693 - val_loss: 1.0500 - val_acc: 0.6974\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8665\n",
      "Epoch 00122: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4574 - acc: 0.8665 - val_loss: 0.8638 - val_acc: 0.7503\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8680\n",
      "Epoch 00123: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4566 - acc: 0.8681 - val_loss: 0.7797 - val_acc: 0.7750\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.8696\n",
      "Epoch 00124: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4480 - acc: 0.8695 - val_loss: 0.6454 - val_acc: 0.8309\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.8699\n",
      "Epoch 00125: val_loss did not improve from 0.57696\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4493 - acc: 0.8699 - val_loss: 1.4541 - val_acc: 0.5924\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8703\n",
      "Epoch 00126: val_loss improved from 0.57696 to 0.56038, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/126-0.5604.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4495 - acc: 0.8703 - val_loss: 0.5604 - val_acc: 0.8463\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8727\n",
      "Epoch 00127: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4413 - acc: 0.8727 - val_loss: 1.0819 - val_acc: 0.6804\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8731\n",
      "Epoch 00128: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4420 - acc: 0.8730 - val_loss: 0.6203 - val_acc: 0.8404\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8703\n",
      "Epoch 00129: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4441 - acc: 0.8703 - val_loss: 0.6120 - val_acc: 0.8346\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8704\n",
      "Epoch 00130: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4428 - acc: 0.8703 - val_loss: 2.0118 - val_acc: 0.5386\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8714\n",
      "Epoch 00131: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4397 - acc: 0.8713 - val_loss: 0.9367 - val_acc: 0.7358\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4398 - acc: 0.8718\n",
      "Epoch 00132: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4398 - acc: 0.8718 - val_loss: 0.6162 - val_acc: 0.8241\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8738\n",
      "Epoch 00133: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4396 - acc: 0.8738 - val_loss: 0.7667 - val_acc: 0.7803\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8754\n",
      "Epoch 00134: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4348 - acc: 0.8753 - val_loss: 0.5679 - val_acc: 0.8386\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8737\n",
      "Epoch 00135: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4347 - acc: 0.8737 - val_loss: 0.6050 - val_acc: 0.8302\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8745\n",
      "Epoch 00136: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4324 - acc: 0.8745 - val_loss: 0.7775 - val_acc: 0.7687\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4328 - acc: 0.8756\n",
      "Epoch 00137: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4328 - acc: 0.8756 - val_loss: 0.6208 - val_acc: 0.8265\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.8769- \n",
      "Epoch 00138: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4258 - acc: 0.8768 - val_loss: 0.5942 - val_acc: 0.8388\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8737\n",
      "Epoch 00139: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4318 - acc: 0.8738 - val_loss: 0.8912 - val_acc: 0.7573\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8776\n",
      "Epoch 00140: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4286 - acc: 0.8777 - val_loss: 0.6939 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4225 - acc: 0.8774\n",
      "Epoch 00141: val_loss did not improve from 0.56038\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4225 - acc: 0.8774 - val_loss: 0.6819 - val_acc: 0.8006\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8766\n",
      "Epoch 00142: val_loss improved from 0.56038 to 0.55971, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/142-0.5597.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4256 - acc: 0.8766 - val_loss: 0.5597 - val_acc: 0.8498\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.8773\n",
      "Epoch 00143: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4240 - acc: 0.8773 - val_loss: 0.7753 - val_acc: 0.7757\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8752\n",
      "Epoch 00144: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4249 - acc: 0.8752 - val_loss: 0.6337 - val_acc: 0.8183\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8767\n",
      "Epoch 00145: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4193 - acc: 0.8766 - val_loss: 0.9903 - val_acc: 0.7081\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8776\n",
      "Epoch 00146: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4211 - acc: 0.8775 - val_loss: 0.6617 - val_acc: 0.8060\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8793\n",
      "Epoch 00147: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4163 - acc: 0.8792 - val_loss: 0.7706 - val_acc: 0.7743\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.8775\n",
      "Epoch 00148: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4210 - acc: 0.8775 - val_loss: 0.6179 - val_acc: 0.8302\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8785\n",
      "Epoch 00149: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4169 - acc: 0.8785 - val_loss: 0.6176 - val_acc: 0.8321\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8779\n",
      "Epoch 00150: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4150 - acc: 0.8778 - val_loss: 0.6609 - val_acc: 0.8227\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8788\n",
      "Epoch 00151: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4132 - acc: 0.8788 - val_loss: 0.6525 - val_acc: 0.8134\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8814\n",
      "Epoch 00152: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4111 - acc: 0.8813 - val_loss: 0.7372 - val_acc: 0.7980\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8809\n",
      "Epoch 00153: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4110 - acc: 0.8809 - val_loss: 0.6200 - val_acc: 0.8344\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8807\n",
      "Epoch 00154: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4128 - acc: 0.8806 - val_loss: 0.6096 - val_acc: 0.8358\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8800\n",
      "Epoch 00155: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4094 - acc: 0.8800 - val_loss: 0.7731 - val_acc: 0.7892\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8816\n",
      "Epoch 00156: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4072 - acc: 0.8815 - val_loss: 0.6159 - val_acc: 0.8365\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8818\n",
      "Epoch 00157: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4084 - acc: 0.8818 - val_loss: 0.6339 - val_acc: 0.8281\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8806\n",
      "Epoch 00158: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4063 - acc: 0.8806 - val_loss: 0.8482 - val_acc: 0.7589\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8814\n",
      "Epoch 00159: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4058 - acc: 0.8814 - val_loss: 0.6940 - val_acc: 0.7969\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8835\n",
      "Epoch 00160: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4028 - acc: 0.8834 - val_loss: 0.6074 - val_acc: 0.8316\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8830\n",
      "Epoch 00161: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4011 - acc: 0.8831 - val_loss: 0.7708 - val_acc: 0.7869\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8851\n",
      "Epoch 00162: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3963 - acc: 0.8851 - val_loss: 0.7165 - val_acc: 0.7887\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8840\n",
      "Epoch 00163: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3984 - acc: 0.8840 - val_loss: 0.7197 - val_acc: 0.8111\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8846\n",
      "Epoch 00164: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3974 - acc: 0.8846 - val_loss: 0.8512 - val_acc: 0.7594\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8862\n",
      "Epoch 00165: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3932 - acc: 0.8862 - val_loss: 0.8137 - val_acc: 0.7701\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8842\n",
      "Epoch 00166: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3999 - acc: 0.8841 - val_loss: 1.4623 - val_acc: 0.6334\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8846\n",
      "Epoch 00167: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3967 - acc: 0.8846 - val_loss: 1.2785 - val_acc: 0.6580\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8840\n",
      "Epoch 00168: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3950 - acc: 0.8840 - val_loss: 0.7002 - val_acc: 0.8102\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8870\n",
      "Epoch 00169: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3929 - acc: 0.8870 - val_loss: 0.6777 - val_acc: 0.8064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8852\n",
      "Epoch 00170: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3936 - acc: 0.8853 - val_loss: 0.6040 - val_acc: 0.8311\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8856\n",
      "Epoch 00171: val_loss did not improve from 0.55971\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3878 - acc: 0.8856 - val_loss: 0.7103 - val_acc: 0.8134\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8860\n",
      "Epoch 00172: val_loss improved from 0.55971 to 0.53670, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv_checkpoint/172-0.5367.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3892 - acc: 0.8859 - val_loss: 0.5367 - val_acc: 0.8519\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8882\n",
      "Epoch 00173: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3847 - acc: 0.8882 - val_loss: 0.6501 - val_acc: 0.8220\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8875\n",
      "Epoch 00174: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3886 - acc: 0.8874 - val_loss: 0.5800 - val_acc: 0.8423\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8876\n",
      "Epoch 00175: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3872 - acc: 0.8875 - val_loss: 0.9881 - val_acc: 0.7284\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8868\n",
      "Epoch 00176: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3859 - acc: 0.8869 - val_loss: 0.5780 - val_acc: 0.8463\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8895\n",
      "Epoch 00177: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3799 - acc: 0.8895 - val_loss: 1.7032 - val_acc: 0.5905\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8879\n",
      "Epoch 00178: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3847 - acc: 0.8879 - val_loss: 0.6750 - val_acc: 0.8188\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8881\n",
      "Epoch 00179: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3840 - acc: 0.8881 - val_loss: 0.6370 - val_acc: 0.8202\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8886\n",
      "Epoch 00180: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3801 - acc: 0.8886 - val_loss: 0.9612 - val_acc: 0.7324\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8917\n",
      "Epoch 00181: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3775 - acc: 0.8917 - val_loss: 0.9752 - val_acc: 0.7419\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8903\n",
      "Epoch 00182: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3788 - acc: 0.8903 - val_loss: 1.0715 - val_acc: 0.7191\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8906\n",
      "Epoch 00183: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3789 - acc: 0.8906 - val_loss: 0.6441 - val_acc: 0.8267\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8902\n",
      "Epoch 00184: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3750 - acc: 0.8903 - val_loss: 1.0858 - val_acc: 0.7126\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3777 - acc: 0.8895\n",
      "Epoch 00185: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3777 - acc: 0.8894 - val_loss: 0.7046 - val_acc: 0.8176\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8909\n",
      "Epoch 00186: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3743 - acc: 0.8909 - val_loss: 0.6071 - val_acc: 0.8360\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8917\n",
      "Epoch 00187: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3723 - acc: 0.8917 - val_loss: 0.5976 - val_acc: 0.8367\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8901\n",
      "Epoch 00188: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3725 - acc: 0.8901 - val_loss: 0.5976 - val_acc: 0.8355\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8901\n",
      "Epoch 00189: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3730 - acc: 0.8901 - val_loss: 0.5951 - val_acc: 0.8439\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8912\n",
      "Epoch 00190: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3710 - acc: 0.8912 - val_loss: 0.6300 - val_acc: 0.8276\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8929\n",
      "Epoch 00191: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3690 - acc: 0.8928 - val_loss: 0.6339 - val_acc: 0.8251\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8943\n",
      "Epoch 00192: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3635 - acc: 0.8943 - val_loss: 0.6997 - val_acc: 0.8057\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8930\n",
      "Epoch 00193: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3655 - acc: 0.8930 - val_loss: 0.5887 - val_acc: 0.8439\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8922\n",
      "Epoch 00194: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3645 - acc: 0.8922 - val_loss: 0.7279 - val_acc: 0.7948\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8940\n",
      "Epoch 00195: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3647 - acc: 0.8940 - val_loss: 0.6352 - val_acc: 0.8246\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8933\n",
      "Epoch 00196: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3627 - acc: 0.8933 - val_loss: 0.6494 - val_acc: 0.8255\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8942\n",
      "Epoch 00197: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3644 - acc: 0.8942 - val_loss: 0.7092 - val_acc: 0.8064\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8922\n",
      "Epoch 00198: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3645 - acc: 0.8922 - val_loss: 0.6525 - val_acc: 0.8237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8929\n",
      "Epoch 00199: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3625 - acc: 0.8929 - val_loss: 1.1717 - val_acc: 0.7030\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8950\n",
      "Epoch 00200: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3614 - acc: 0.8950 - val_loss: 0.6535 - val_acc: 0.8209\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8958\n",
      "Epoch 00201: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3605 - acc: 0.8957 - val_loss: 0.6651 - val_acc: 0.8130\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8941\n",
      "Epoch 00202: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3608 - acc: 0.8941 - val_loss: 0.6180 - val_acc: 0.8360\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8938\n",
      "Epoch 00203: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3601 - acc: 0.8938 - val_loss: 0.7772 - val_acc: 0.7734\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8947\n",
      "Epoch 00204: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3546 - acc: 0.8947 - val_loss: 0.6037 - val_acc: 0.8453\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8947\n",
      "Epoch 00205: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3586 - acc: 0.8947 - val_loss: 0.6683 - val_acc: 0.8171\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8946\n",
      "Epoch 00206: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3581 - acc: 0.8947 - val_loss: 0.8838 - val_acc: 0.7522\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8972\n",
      "Epoch 00207: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3523 - acc: 0.8972 - val_loss: 0.6074 - val_acc: 0.8386\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8964\n",
      "Epoch 00208: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3524 - acc: 0.8963 - val_loss: 0.5871 - val_acc: 0.8439\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8967\n",
      "Epoch 00209: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3522 - acc: 0.8966 - val_loss: 0.6579 - val_acc: 0.8269\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8966\n",
      "Epoch 00210: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3548 - acc: 0.8966 - val_loss: 0.5988 - val_acc: 0.8404\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8981\n",
      "Epoch 00211: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3475 - acc: 0.8981 - val_loss: 0.7714 - val_acc: 0.7878\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8985\n",
      "Epoch 00212: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3507 - acc: 0.8984 - val_loss: 0.6662 - val_acc: 0.8064\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8951\n",
      "Epoch 00213: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3568 - acc: 0.8952 - val_loss: 0.6079 - val_acc: 0.8421\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8970\n",
      "Epoch 00214: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3497 - acc: 0.8970 - val_loss: 1.8661 - val_acc: 0.5681\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8969\n",
      "Epoch 00215: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3489 - acc: 0.8968 - val_loss: 0.9882 - val_acc: 0.7419\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8978\n",
      "Epoch 00216: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3494 - acc: 0.8978 - val_loss: 0.5948 - val_acc: 0.8437\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8986- ETA: 0s - loss: 0.3456 - ac\n",
      "Epoch 00217: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3461 - acc: 0.8985 - val_loss: 0.6274 - val_acc: 0.8309\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.9000\n",
      "Epoch 00218: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3438 - acc: 0.9000 - val_loss: 1.1454 - val_acc: 0.7053\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8988\n",
      "Epoch 00219: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3409 - acc: 0.8988 - val_loss: 0.6834 - val_acc: 0.8169\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8984\n",
      "Epoch 00220: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3459 - acc: 0.8984 - val_loss: 0.9551 - val_acc: 0.7487\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8992\n",
      "Epoch 00221: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3397 - acc: 0.8991 - val_loss: 0.5973 - val_acc: 0.8442\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8988\n",
      "Epoch 00222: val_loss did not improve from 0.53670\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3460 - acc: 0.8988 - val_loss: 0.5842 - val_acc: 0.8439\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VFX6xz9nJpNJL6QQQoBQBUIgdBQFK0UUURfRFbHiumtj3R8ruhZcda27ulhWUVk7oIBrQ1GUoggqvUsJJQmkEdLbZOb8/jhzM5OQhBAyyWQ4n+eZ587ce+fec9v5nvd9z3mvkFKi0Wg0Gs3JMLV2ATQajUbTNtCCodFoNJpGoQVDo9FoNI1CC4ZGo9FoGoUWDI1Go9E0Ci0YGo1Go2kUWjA0Go1G0yi0YGg0Go2mUWjB0Gg0Gk2j8GvtAjQn0dHRMjExsbWLodFoNG2GDRs25EopYxqzrk8JRmJiIuvXr2/tYmg0Gk2bQQhxqLHrapeURqPRaBqFFgyNRqPRNAotGBqNRqNpFD4Vw6gLm81Geno65eXlrV2UNklAQAAJCQlYLJbWLopGo2llfF4w0tPTCQ0NJTExESFEaxenTSGl5NixY6Snp9O1a9fWLo5Go2llfN4lVV5eTlRUlBaLJiCEICoqSltnGo0GOAMEA9BicRroc6fRaAzOCME4GRWl6VRVHG/tYmg0Go1XowUD8N+VCUePemTb+fn5vPrqq03676WXXkp+fn6j1589ezbPP/98k/al0Wg0J0MLBoAApPTIphsSjKqqqgb/u3TpUiIiIjxRLI1GozllPCYYQohOQogVQoidQogdQoh761hHCCHmCCH2CSG2CiEGuS27UQix1/m50VPlBJAeFIxZs2axf/9+UlJSmDlzJitXruS8885j4sSJ9O3bF4BJkyYxePBgkpKSmDt3bvV/ExMTyc3N5eDBg/Tp04fp06eTlJTEmDFjKCsra3C/mzdvZsSIEfTv358rr7yS48eVy23OnDn07duX/v37c+211wKwatUqUlJSSElJYeDAgRQVFXnkXGg0mraNJ7vVVgF/kVJuFEKEAhuEEN9KKXe6rTMe6On8DAf+AwwXQrQDHgWGANL538+klKcVaNi7dwbFxZtPXFBchCw3IQqCT3mbISEp9Oz5Yr3Ln376abZv387mzWq/K1euZOPGjWzfvr26q+q8efNo164dZWVlDB06lKuvvpqoqKhaZd/L/PnzeeONN7jmmmtYvHgxU6dOrXe/06ZN46WXXmL06NE88sgjPPbYY7z44os8/fTTHDhwAKvVWu3uev7553nllVcYOXIkxcXFBAQEnPJ50LQylZUwfz5Mmwa6o4LGQ3jMwpBSHpVSbnR+LwJ2AR1rrXYF8K5UrAMihBAdgLHAt1LKPKdIfAuM81RZlUvKY1s/gWHDhtUY1zBnzhwGDBjAiBEjSEtLY+/evSf8p2vXrqSkpAAwePBgDh48WO/2CwoKyM/PZ/To0QDceOONrF69GoD+/ftz/fXX8/777+Pnp9oLI0eO5L777mPOnDnk5+dXz9e0Ib77Dm66CTbX0SDSaJqJFqkZhBCJwEDg51qLOgJpbr/TnfPqm39a1GcJOLZswBFkwa9n/9PdRaMIDnZZMitXrmT58uWsXbuWoKAgzj///DrHPVit1urvZrP5pC6p+vjyyy9ZvXo1n3/+OU8++STbtm1j1qxZTJgwgaVLlzJy5EiWLVtG7969m7R9TStRWammFRWtWw6NT+PxoLcQIgRYDMyQUhZ6YPu3CyHWCyHW5+TkNHEjeCyGERoa2mBMoKCggMjISIKCgti9ezfr1q077X2Gh4cTGRnJDz/8AMB7773H6NGjcTgcpKWlccEFF/DMM89QUFBAcXEx+/fvJzk5mfvvv5+hQ4eye/fu0y6DpoWx29XU4Wjdcmh8Go9aGEIIC0osPpBSLqljlQygk9vvBOe8DOD8WvNX1rUPKeVcYC7AkCFDmlbrC+ExwYiKimLkyJH069eP8ePHM2HChBrLx40bx2uvvUafPn0466yzGDFiRLPs95133uGOO+6gtLSUbt268d///he73c7UqVMpKChASsk999xDREQEDz/8MCtWrMBkMpGUlMT48eObpQyaFsQQCkM4NBoPIKSHKkqhhgi/A+RJKWfUs84E4C7gUlTQe46Ucpgz6L0BMHpNbQQGSynzGtrnkCFDZO0XKO3atYs+ffo0WFb79o1IPxN+vVNOfmBnII05h5pW5qOPYMoUWLECzj+/tUujaUMIITZIKYc0Zl1PWhgjgRuAbUIIIxL3INAZQEr5GrAUJRb7gFLgZueyPCHE48Cvzv/9/WRicVoIgfCQcGo0LYK2MDQtgMcEQ0r5Iyo60NA6EriznmXzgHkeKNqJCAFS+341bRgtGJoWQI/0hhbvVqvRNDuGUGjB0HgQLRjg0aC3RtMiaAtD0wJowQA9MlbT9jEEQ3er1XgQLRigXVKato+2MDQtgBYM8DqXVEhIyCnN12i0YGhaAi0Y4OxW29qF0GhOAy0YmhZACwY4LQzPbHrWrFm88sor1b+NlxwVFxdz0UUXMWjQIJKTk/n0008bvU0pJTNnzqRfv34kJyezcOFCAI4ePcqoUaNISUmhX79+/PDDD9jtdm666abqdV944YVmP0aNF6B7SWlagDMrLemMGXVm8zSVl0KVHRkS2vDAkbpISYEX609vPmXKFGbMmMGdd6rhJh999BHLli0jICCATz75hLCwMHJzcxkxYgQTJ05s1Du0lyxZwubNm9myZQu5ubkMHTqUUaNG8eGHHzJ27Fj+9re/YbfbKS0tZfPmzWRkZLB9+3aAU3qDn6YNoS0MTQtwZglGKzBw4ECys7M5cuQIOTk5REZG0qlTJ2w2Gw8++CCrV6/GZDKRkZFBVlYWcXFxJ93mjz/+yHXXXYfZbKZ9+/aMHj2aX3/9laFDh3LLLbdgs9mYNGkSKSkpdOvWjdTUVO6++24mTJjAmDFjWuCoNS2OFgxNC3BmCUY9loDj4G5MecUwcCAIc7PvdvLkySxatIjMzEymTJkCwAcffEBOTg4bNmzAYrGQmJhYZ1rzU2HUqFGsXr2aL7/8kptuuon77ruPadOmsWXLFpYtW8Zrr73GRx99xLx5LTOAXtOC6G61vsUXXyhvyEMPtXZJaqBjGADCBBKkh9KDTJkyhQULFrBo0SImT54MqLTmsbGxWCwWVqxYwaFDhxq9vfPOO4+FCxdit9vJyclh9erVDBs2jEOHDtG+fXumT5/ObbfdxsaNG8nNzcXhcHD11VfzxBNPsHHjRo8co6aV0RaGb7FkCbz6amuX4gTOLAujPqqD3p6JfCclJVFUVETHjh3p0KEDANdffz2XX345ycnJDBky5JReWHTllVeydu1aBgwYgBCCZ599lri4ON555x2ee+45LBYLISEhvPvuu2RkZHDzzTfjcFYoTz31lEeOUdPK6KC3b+FweOW11IIBYBLOdyh5zpzftm1bjd/R0dGsXbu2znWLi4sbnC+E4LnnnuO5556rsfzGG2/kxhtvPOF/2qo4A9AWhm9ht0NVVWuX4gS0SwqqU4N4UjA0Go+iBcO38FILQwsGqBgG6IChpu2iBcO30BaGF2OMfdAWhqatogXDt9AWhhdjWBhaMDRtFaNy0Vayb6AtDC/GiGHoh03TVtEWhm+hLQwvRlsYmraOFgzfwm5XGbS9rBGrBQMQJs8JRn5+Pq82cQDOpZdeqnM/aRqHFgzfwkuvp8cEQwgxTwiRLYTYXs/ymUKIzc7PdiGEXQjRzrnsoBBim3PZek+V0VWY1hGMqpP4KJcuXUpERESzl0njg3hpBaNpIsb19LI4hictjLeBcfUtlFI+J6VMkVKmAA8Aq6SUeW6rXOBcPsSDZVR4sJfUrFmz2L9/PykpKcycOZOVK1dy3nnnMXHiRPr27QvApEmTGDx4MElJScydO7f6v4mJieTm5nLw4EH69OnD9OnTSUpKYsyYMZSVlZ2wr88//5zhw4czcOBALr74YrKysgA14O/mm28mOTmZ/v37s3jxYgC+/vprBg0axIABA7joooua/dg1LYge6e1beOn19NhIbynlaiFEYiNXvw6Y76myGNST3RyqIqEsEBnojzjFM3KS7OY8/fTTbN++nc3OHa9cuZKNGzeyfft2unbtCsC8efNo164dZWVlDB06lKuvvpqoqKga29m7dy/z58/njTfe4JprrmHx4sVMnTq1xjrnnnsu69atQwjBm2++ybPPPss///lPHn/8ccLDw6tHmx8/fpycnBymT5/O6tWr6dq1K3l5eWjaMNrC8C281MJo9dQgQogglCVyl9tsCXwjhJDA61LKuXX+Wf3/duB2gM6dO59maVrmtXvDhg2rFguAOXPm8MknnwCQlpbG3r17TxCMrl27kpKSAsDgwYM5ePDgCdtNT09nypQpHD16lMrKyup9LF++nAULFlSvFxkZyeeff86oUaOq12nXrl2zHqOmhdHZan2LM83COAUuB9bUckedK6XMEELEAt8KIXZLKVfX9WenmMwFGDJkSIM1fn2WgCwsQezZh61bLJZ2pys6Jyc4OLj6+8qVK1m+fDlr164lKCiI888/v84051artfq72Wyu0yV19913c9999zFx4kRWrlzJ7NmzPVJ+jReiLQzfwkstDG/oJXUttdxRUsoM5zQb+AQY5tESeDDoHRoaSlFRUb3LCwoKiIyMJCgoiN27d7Nu3bom76ugoICOHTsC8M4771TPv+SSS2q8Jvb48eOMGDGC1atXc+DAAQDtkmrraMHwLbzUwmhVwRBChAOjgU/d5gULIUKN78AYoM6eVs1XEM/lkoqKimLkyJH069ePmTNnnrB83LhxVFVV0adPH2bNmsWIESOavK/Zs2czefJkBg8eTHR0dPX8hx56iOPHj9OvXz8GDBjAihUriImJYe7cuVx11VUMGDCg+sVOmjaKFgzfwkstDCGlZ/z2Qoj5wPlANJAFPApYAKSUrznXuQkYJ6W81u1/3VBWBSiX2YdSyicbs88hQ4bI9etr9sLdtWsXffr0afB/srQUsXMnts6RWGK7N2ZXZxSNOYeaVuaOO+D11+GPf/TKF+9oTpERI+DnnyE1FdzinZ5ACLGhsb1RPdlL6rpGrPM2qvut+7xUYIBnSlU3orpbbcsEvTWaZkdbGPDCC/Dee+AL73/xUgvDG2IYrY/OVqtp62jBgP37Yd++1i5F8+Cl11MLBrgJhrYwNG0UL61gWhS73XeO3zgObWF4IdVv3NOCoWmj6HEYqpL1leP30gaAFgyoFgyhBUPTVvHSbpgtipemBG8SXno9tWCAjmFo2j5e2iJtUXzJJaWD3l5MtWC0bjEMQkJCWrsImraGFgyXS8oXPAXawvBitIWhaetowfCt19RqC8OL8aCFMWvWrBppOWbPns3zzz9PcXExF110EYMGDSI5OZlPP/20ga0o6kuDXlea8vpSmmt8FC0YvnUOvNTC8Ibkgy3GjK9nsDmzrvzmQFER0mJCBATXvbweUuJSeHFc/fnNp0yZwowZM7jzzjsB+Oijj1i2bBkBAQF88sknhIWFkZuby4gRI5g4caJrEGEd1JUG3eFw1JmmvK6U5hofxksrmBbFl86Bl1oYZ5RgnJzmNzEGDhxIdnY2R44cIScnh8jISDp16oTNZuPBBx9k9erVmEwmMjIyyMrKIi4urt5t1ZUGPScnp8405XWlNNf4MLpbrW8KhpcdyxklGA1ZAnLjBqoizFi6pTT7fidPnsyiRYvIzMysTvL3wQcfkJOTw4YNG7BYLCQmJtaZ1tygsWnQNWcoXlrBtCi+dA70wD0vR+Cx3hVTpkxhwYIFLFq0iMmTJwMqFXlsbCwWi4UVK1Zw6NChBrdRXxr0+tKU15XSXOPD+FJl2VS0heFxtGA4kUI0TjDKy09Z9ZOSkigqKqJjx4506NABgOuvv57169eTnJzMu+++S+/evRvcRn1p0OtLU15XSnOND+OlFUyL4kuC4aUWxhnlkmoQQeNCGL/9BtHR4HxRUWMxgs8G0dHRrF27ts51i4uLT5hntVr56quv6lx//PjxjB8/vsa8kJCQGi9R0vg4vlRZNhVfOgde2gDQFoZBYywMKcFm8zrV12i8tYJpUXzpHHiphaEFw8ApGA0mIDSW+cJIUo1v4UuVZVPRFobHOSMEo1FZaIVwuqQa6JZoXMQzSDB0Bt82gu5W61uCoS2M1iEgIIBjx46dvOITAiFBygZutjNMMKSUHDt2jICAgNYuiuZkeGmLtEXxJcHw0uvp80HvhIQE0tPTycnJaXA9mZWNpAocuzCZ/OteyWaD3FwoLYWKCg+U1vsICAggISGhtYuhORm+VFk2FS+tZJvEmTbSWwgxD7gMyJZS9qtj+fnAp8AB56wlUsq/O5eNA/4NmIE3pZRPN7UcFoulehR0Q9huu5bi0q2IFauIiBhV90rbtsH48XDllbBkSVOLpNE0P75UWTYVXxJNLz0WT7qk3gbGnWSdH6SUKc6PIRZm4BVgPNAXuE4I0deD5QRA+AdgqgKbLa/+lUpL1dRm83RxNJpTQwuG11ayTcJLLQyPCYaUcjXQQO1bL8OAfVLKVCllJbAAuKJZC1cHwhKIqIKqqgZGRGvB0HgrWjB8RzCk9Nrr2dpB77OFEFuEEF8JIZKc8zoCaW7rpDvneRQRENR4wfAy1ddovLWCaVF85Ry4d6rxsrqmNYPeG4EuUspiIcSlwP+Anqe6ESHE7cDtAJ07d25yYYR/oHZJadouulut71gY7uX3smNpNQtDSlkopSx2fl8KWIQQ0UAG0Mlt1QTnvPq2M1dKOURKOSQmJqbJ5REWf4TdpF1SmraJr1SWp4OvnAN30fcyC6PVBEMIESecbwsSQgxzluUY8CvQUwjRVQjhD1wLfObxAlksmKq0YGjaKL7ijjkdfOUceLGF4clutfOB84FoIUQ68ChgAZBSvgb8DvijEKIKKAOulWp0XZUQ4i5gGapb7Twp5Q5PlbMaf39MdqEFQ9M28ZXKcs8e9XwlJZ183dpoC8PjeEwwpJTXnWT5y8DL9SxbCiz1RLnqxWJB2HQMQ+PFSAkffgiTJ4N/rcGlviIYf/mLGhxbTybnBvFFwfCyY2ntXlLeQ7t2+BVVUVWhBUPjpezYAVOnwrJlJy7zFcEoKYGTZGWoF18RDPfye5mFoQXDICEBUSUROVowNF6K8Ureul7N6yuVpc0GTX07pK+IprYw2gDOfEnmo/n1JyrUgqFpTYzWZl2ViK90q7XZID+/acfhK6KpLYw2QCfVk9ea7cBuL6p7HT1wT9OaGPddXfefr7SubTZ1LEX1PIMN4SuCoS2MNoDTwrDmNDDaW1sYmtakMRaGl1Uwp4xxjPn5p/5fXxEMbWG0AaKikFYL1hyw2bRgaLyQM8XCgKbFMXzlHGgLow0gBI74GKeFUU/gWwuGpjVpqAXtK63r0xEMXzkH2sJoG8iEjk4Lo55ufVowNK1JYywMh6NtvxHSeLbOZJeUtjDaBqZOXbHmQEVFPamrDMGw29v2Q6lpmzQmhlH7e1vDOEbtklJoC8N7EZ26Ys2FirL0ulcwBAO0laFpeRpjYUDbrjC1S8qrc0lpwXBDdOqEqQqqjqbWvYIWDE1rciZYGFowtIXRZnB2rSX9YN3LS0vBbFbftWBoWprGBL3rW95W0DEMr76WWjDccQ7eE+lHT1wmpRKMsDD1WwuGpqU5E1xSTY1huMcU2/Lxg7Yw2gxOC8N0JO/E9CBG/p7wcDX1sgupOQM4mUvKYql/eVuhqS4pL26VnzJefCxaMNyJjkb6m7HmVGGzHau5zIhfGIKhLQxNS3MyC8PP+bYCL6tkGo2UrrKfyYKhLYw2gsmEIy4KazZUVtbqWmsIhnZJaVqLk1kYxjsy2mqF6V45nmoMw5cEw4uPRQtGLWTHDnWPxdAWhqa1MSqP+iyMtu6Scn+mTtXC8JUYDmgLoy0hOieqsRhaMDQtRWMrx4YsDLvdJRhttVut8UxZLOqcnMrgWC9ulZ8yXix+WjBqYercQ1kY5bUG72nB0HiCAwcgOhp++eXk69YnGEbF6isWRkwMVFbW/aKo+vAlwTDK7+enLQxvR3TqgskG9qz9NRfoGIbGE2RmqhZlRj3paNypL+httEjbumAYxxUTo6an4pby4lb5KWMci7+/1x2LxwRDCDFPCJEthNhez/LrhRBbhRDbhBA/CSEGuC076Jy/WQix3lNlrBNn11rHob0152sLQ+MJKivVtDEtyfosDPcKpq7lbQXjmYqKUtOCgsb/1xctDH//M8rCeBsY18DyA8BoKWUy8Dgwt9byC6SUKVLKIR4qX904BUOmHaw5P8+Z8jw2Vk21YGiag4oKNW1MxVBf0NtXLAzjmQoOVlNDTBuDLwnGmWhhSClXA/W8WAKklD9JKQ2bcx2Q4KmynBLGaO+MHByOCtf83Fw17dBBTb1M+TVtFKNSbEwDpD4Lw93nXdfytkJtwTiVRpkvCcYZamGcCrcCX7n9lsA3QogNQojbW7QksbFIPzPWHElZmVsSwpwcdQHbtVO/tYWhaQ6a4pLyVQvDOK6gIDU9lWdMxzBahEYJhhDiXiFEmFC8JYTYKIQY0xwFEEJcgBKM+91mnyulHASMB+4UQoxq4P+3CyHWCyHW5+TU8+KjU8FkQsbHEJANZWVucYycHBWMMx5KLRia5qA5Yxi+0q3WEIwz1SXlAxbGLVLKQmAMEAncADx9ujsXQvQH3gSukFJW5+KQUmY4p9nAJ8Cw+rYhpZwrpRwipRwSY/SuOF3O6kPQoVqCkZurBUPT/DTFJVWfheErQe+mWBi+JBht3cIAhHN6KfCelHKH27wmIYToDCwBbpBS7nGbHyyECDW+o0Sqzp5WnsI0aBjBB6Cs4DfXzJwc1V9eC4amOTkVC6O+9N3aJeWbgmGxeJ2F4dfI9TYIIb4BugIPOCv0Bu1eIcR84HwgWgiRDjwKWACklK8BjwBRwKtCCIAqZ4+o9sAnznl+wIdSyq9P8bhOj4EDMVWB3L4FNr8PY8cqwejWraZg7N4NZ50F4rS0U3Mm0xwuKeN3WxeM03FJ+VIMw90l5WXH0ljBuBVIAVKllKVCiHbAzQ39QUp53UmW3wbcVsf8VGDAif9oQQYNAiBi/g747AZ46qkTXVK7dsGNN8K338LFF7diYTVtmuZ0SfmaYJzpFkYbjmGcDfwmpcwXQkwFHgJOYVRNG6N7dxwh/sR+UaJ+b98OhYU1XVJpaWp6tI6XLWk0jcUTQe+2WmFqwVB4sYXRWMH4D1DqHI39F2A/8K7HStXamExU9euBMKzcdevU1N3CMAbyFRe3ePHaNMeOed1D0KrobrUuascwztReUj5gYVRJ9Qq6K4CXpZSvAKGeK1brYxo0AoCq+AjY78wrFRPjGhxl5LkpKmqF0rVRSkogMREWLGjtkngPxkjvxrSmGxv09pVutWfqOAwfsDCKhBAPoLrTfimEMOEMYPsq5hv/QPb4AI5f1dk1090lZQiGtjAaT3a2Ol9HjrR2SbyH5rAwfDXo3VZcUrt3Q0iIyjzcHPiAhTEFqECNx8hEpfF4zmOl8gLEsGFkPXMJ+R1yXTPdXVJaME4d45ydiqvB19ExDBdtdeDe/v3Kej50qHm219YtDKdIfACECyEuA8qllL4bw3ASGjqMgli31rAWjNPDOGcVFQ2vdyahBcOFcXxNySXVmi4po5zNNTbL3cJwOE7tRVIeprGpQa4BfgEmA9cAPwshfufJgnkDYWFDKYt3/hBC5ZESAsxm182hBaPxaAvjRPRIbxfGOQgMrPm7MbSmhWFcw+a6r+u7niUl0LEjfPdd8+ynCTR2HMbfgKHOVB0IIWKA5cAiTxXMGwgNHYo9CKpiQvBzWJVQgGrJGRdRC0bjMXqWacFwoUd6u2irLqnmFgx3lxSoe8PPT40FO3JExUwuuqh59nWKNDaGYTLEwsmxU/hvm8ViaUdwcH/KE8yut4CpBa7vWjAaj7YwTqQ5u9W29fTmxnFZrWAyaQujtoVhbL8VXbqNrfS/FkIsE0LcJIS4CfgSWOq5YnkPEREXkHp9KY7HHnHN1ILRNHxVMBwOV9frU6U534fhK91qLRb1aSsxjJawMNy37+2CIaWciXojXn/nZ66U8v6G/+UbREZeSN5gG4Vj4l0ztWA0DV8VjK+/hl69mtZdWA/cc+EuGP7+TbMwzOa2LxgnszBa8flpbAwDKeViYLEHy+KVhIePAkwcP/49ERGj1UwtGE3DVwUjM1M95Hl5EB9/8vXdOZVXtJ4p7/T281PPWFNiGO7xxZZCWxgKIUSREKKwjk+REKKwpQrZmlgsEYSGDuL48e/cZ7q+a8FoPL4qGGVlatqU4zoVl5Svv9PbOK6muKROZexCXl7zVrpnkIXRoGBIKUOllGF1fEKllGEtVcjWJjJyLIWF67DZ8tUMPzfDTAtG4/FVwSgvV9PTEYxTtTC2b4fbb1eVi68Ihs2mXEpCND2GYYxdaIgRI1QG6uZCWxgad6KixgN2jh//Vs1wtzBsNt+rAD2Fr3arNQSjKQ9yUwXjm2/gjTdqJnP0BcEwjsHfv2kuqfosjHXrVKwJICOjedPTeMrCqH09tWC0DUJDh+PnF0lenrNjmHEhjam2MhqHr1oYLeWScg96u1cevmRhGNZ7U11S9cUwHn4Y/vpX9b2iwnXNmgNPu6RqWxje6pLSKEwmP9q1G8uxY18hpcP1YLZvr6ZaME6OwwEFzleo+JpgeMIlJeWJFZ+7heGLglFVVbMx1lSXVF3Hf/CgEgm7XX2Ma9YcGOX0lEtKWxhtj3btLsVmy6KwcJ3rpu7QQU21YJycggJXThwtGC7qE4y77oIrrqg5zz3o7V5J+VJ6c0+4pBwOOHxYVbRGZducguFpl5SOYbQ9oqMnYTIFc/ToPC0YTcFwR4HvJR803BunE8Oo3ZpOTYWdO2vO83ULw10wmtMllZmpzpenBaO5kg/a7Wqku5GKqK30ktK48PMLJTb2WrKzF+Dwc562uDg11S9ROjmGYFit2sJwpz4Lo6IC8vNrzqsvhuErQW8jZxI0b7dFmJH/AAAgAElEQVTagwfVtC1ZGGaz61ycKRaGEGKeECJbCLG9nuVCCDFHCLFPCLFVCDHIbdmNQoi9zs+NnixnY+nQ4TYcjhIqZY6aYQhGW7Iw7r8f/vznlt+vIRjt22vBcMddMGw25Tox5ufn13Qv1WVh1OWSaquCcTouqYZiGG1NME5mYfiqYABvA+MaWD4e6On83I56dzhCiHbAo8BwYBjwqBAi0qMlbQRhYcMJCkqivCpDzTAE48sv4cIL20ZFuGYN/Phjy+/XlwXjdHpJub+i9d13oU8ftb2KChXzcbdeT+aS8oWR3qfrkmpIMMrL24Zg1LYwjONp7uB6E/CoYEgpVwN5DaxyBfCuVKwDIoQQHYCxwLdSyjwp5XHgWxoWnhZBCEF8/HRsOF0FRgzj/fdhxQpIT2+9wjWWkpLWsYiMMRjNJRiff+49sZDmGoeRlQWlper6GNtyj/0YFYfD4VquYxiKhmIYhmCA6973ZsGobWGcKS6pRtARSHP7ne6cV9/8ExBC3C6EWC+EWJ+Tk+Oxghq0bz8VWTuGYbQwMzM9vv/TprRUiUZLY7SUY2JO/8HavRsmTlSi4Q001SVlt7sq+6qqmiJgbMs9juEe53C3ajyV3ryoCFrgmarGPYbRnL2k3AWj0JnRyJsFoz4LQwe9Tx8p5Vwp5RAp5ZAY93dWeAiLJQr/kE4AVEUF1VyYldW8O5Oy+a2B1hKM0lI1jYg4/RveEOZCL0ln1lSXlPv6tQWjLgujLsFwD3obaTWaq1vtAw/A+PHNs63G0BQLY+tWSEyEbOfrenxFMNwtjNquqDPYwsgAOrn9TnDOq2++VxAUlgxAZtli1+sk4UQL48EH4auvmr6jJUuUC+fYsaZvozatKRhWqzpfp/tgGeejOUfrng5NtTDc17fZarq26rMwrFb13V0wDIEwKpnmsjCOHlWflqIpgrFjBxw6BAcOqN+1BcNuV8sjItTvtiAYhkvKuNa1t38GC8ZnwDRnb6kRQIGU8iiwDBgjhIh0BrvHOOd5Bf5BKnZxOO8/yJAQ1wJ3wSgvh2eegXfeqXsjK1ee2G0S1MP//POqZblqlapod+1qvsKXltZslbYUpaXq1Zv+/qriO51WsCEYzfnQnw7NIRgnszAcDmVxBgSo34bF5u6Sam7BKC937aclcE8N0liXlHGejEZQ7RjGgw+q7Yx2vprAE4LR3MFowyVlXGvjGH3dJSWEmA+sBc4SQqQLIW4VQtwhhLjDucpSIBXYB7wB/AlASpkHPA786vz83TnPO7BYkP4WKsnBHmBXrZeYmJqCsWePuvD79p34/+JiuPhieO21E5dt3w4zZ8J//wtbtqh5dW2jKbinRGhpK8NdMOD0Bjl5m4VR38C9Rx6Bt9+u/3/Ggx8UdPIYhlEJNmRhmM3NKxhlZS17jpuSGsS4nw1hc7cwvvoKnn0W/vhHuO46Nc8QjKqqxiV8bAyesjAMwajdIDF60LWk9eek0S9QagpSyutOslwCd9azbB4wzxPlOm1SUhDnnke7dgGUBS8juM9oTJm5NQXDsAr27VMXVwjXspwcdVMYfe7dMeIga9Yo/6yxjebA/eEvKYGwFsxQX1swKitdld+pYvS48nYL48MPISkJbrqp7v+5C4Zh+UHdFob7+66hZVxSRvdeu93lT/ckTXFJGeepLsHYtk1Nn30Wli9X393jXhUVNV9V0FQ8FfRuSDC++QYuu0zVIUZvzRagtV1SbZPp0+G77+jW7Rl2Pujg8EPdVI8pd8Ew0joUFLgqOAOjhVxXimVDML7+2tW6bOr7omvj7l5obQvjdB4ub7Mw6hOM8vKGOy0Y6wcHn9zCqC0Y7i4po4L0hEvKfeppag/ca4qF4e6SMuYFBbnOm5EA0/2/p4unYxi176/KSiUUVVWuYH8LoQXjNAgJ6Uf40Fs45HiHqpjgmr2k3OMOtS2E3Fw1rcukNLbh3quoPgvDboeHHmp8d15fE4xTfeClbP7EfO5jIuoSjIbSxtQnGO4i09oWhvvU09RODdKUGIafn+v4y8pUK9298nW3MJpbMJorl9TJLAz3zM8t3GjSgnGadO36d4SwkG/dpSpuIyPrzp3Qo4f6XttCMASjLgujdovh8sth717Xdt3Zvh2efBI++aRxhfUGwTAe3NPp6dFUC+Pbb5UA19XZwJ1Zs1QcqTG4H0ftYzqZhWGsHxSkKgHjeNwrtdoWRu2gd23BCAhovoqwduvd0zTFJeVextoxnNJSVy9G47x5UjA8FcOoq0HSSm5ZLRinidUaT6dOfyHfultd2IIC9XDv2QMTJqjYRW0Lwajwjh49scWblQUdO6r4QrdukJJSt1sLXCPLGzv+w10kWnq0tzdYGDt3qhb/yd629t138P33jdumexlOx8IA1/Vx/49hYdQX9HbvJWU2u+IhzUFLWxi1XVJ1vROkNu4xjNqCUVamzge0jIXR3DGM+lxS4KoPWtjC8GjQ+0yhc+dZpLZ/HcjGfmgv5sw8dfMPHAgJCfW7pOx2FQA3XsQEqvLv0AGuvlpVJO5WSlRUze2cqmB4g4XRHILR1IfFMONPZmEUFDTedeVehtrdZO32xsUwjErNuCYNWRi1OwrUtjACA5uvEmlNwTCmlZU1xzrVxr3XX0MWRlsTDJNJNTatVi0YvobZHERcyl+B/0OOuwgynS3ElBRV4dcnGKBau+6CkZ2tAuj//rf6bQTP9+2DYcNqbifNmT3lTIphSNl0c9wQDPfAZ13UzhLbEPVZGMaDXFR0Yi+52usbgmGIi2FhmM31xzDct+EuGM1pYbS0S6p2DAOUiDQkGO7umuBgdc6kVJ+6LAz3a99AZSulasulpamixMYqb2ZamkuHCgpU2y62QpJLB3aWDiXzfXU5EhMhNNTlaSwtVd+PHlWe5JAQV4cum029ZvzYMbWvc/aOp+r4EHJmAvJZjn0+nKxtkL1uDrlYCaSMyG8cBHMbxbMGwvOqCmmsZ/p00ILRTIT2HA/8H36ZRVTMmo716tthwAAlGAsXqjhEz55qZfeR20ePKkvEICtL/c+gWzd1F61dC7//fc2dnokWRmGhq/JsqoXRkGBI6bIw6qvo3XEXDPcYhjHfblfzDX+0O7VdUoZgGK3g2Nj6Yxju+3TvJRUU1Pjz8oc/wHnnwdSpJy4zajLwSCu2vFwNwA4NVXX8sWOQXjqSDoVdiMkEW0k7Aohm23eSo6XQpYu6FOXlat2dO1UR/X8dRwCRBFPCPltfsj4ZTQKB8FdJxcZpHC0O5adOEOCXSDvWIQ6ZEdgIpgS/P/cmq0q1t6xW5QwQQj1OaWmn0h5xnp9S4IaTrx0Q4BpKYRAfr4ZyFRXB4lT1NofAV4DK24g6VE5sALQ359Gb/ZQTwHFHZwoJI9SvEoJPfps2F1owmov4eKTJxPFzLOybtIbBA1/ADHD77SrFx+DB8PPPKn11bi507qy6xrn706VUFoa7xREQAFOmqAFgjz/uSnEANQXDZlNxk6Sk+svoScHIc7rh3Mtee9/NIRjuYusJC6O83FW20lJXZV4f9bmk3MtWVNSwYNR2SRkWRvv2qkFRUXFqLik3l5vNpioTPz91e+Xnq5ZzcTEkLfwfGVnBbAqcihCq4jY0p1fHcirpRjoJpH/VjvT16lTExCijx2pVRUpLg40b1aEEBalPYKC6HTIz1TpRUarNc/Soyhtps9U32H8xLEB9uFl9rqr7tAthpM36HfA7AAIqy4hLqyCDP2N6SWCtuoRISzHnjQVHqZ38g8eRdjMSSTEh2PL9SOitHs3ycvUoSqnab1dcoR7RTp2UMGVlqWNKSFAiV16upmmpNo7/+TEiRQF92EWn35YjpXq0i4td5zMwEMzvvU3k7rX0/P716vCMcc7dK/wjY28mKGMvEdt/hC591KsT/vtfGHovrF+vVmrXBYoOwZ0vw511DmXzCFowmouICMTKldDlOKWpV7J796307TsfMWSIEooePeCzz1yCkZR0omDk56unKTa25rbvu0+lUH/zTfi//3PNdxeMBQtg2jR47726W4xQUySaWzD+9CdVQ6xceeIywy5vTsHw8/OMheEe3ygoOLlgGMJQ+02C7oJRXKxq2lrIikpsWLAEBiEASkspx8qxLD+2MwZpGkJ/ssj+sZiDO4LI5A+U7z+PMrqyjx7soRcV38RxVqqDrjxG6d/DKNl1DyXHysgZr4xaI+9eVJSq6N07HllIw/apP3xa14EFA87eff+s//BNJnVLh4a6srOXlqp2TXy8ukzHjqn9xsTALbeo2yAgALp3V6dGSoiMhPibLiHjkpspmPB7LOt+oPTdj+k+7yESh8WSlubqHRseDr17q21UTbiC8qXfUUQo0VFgmTlD9XLLK4FR5yvR/fBLKKiAiPHg3tnwqY/hd7+r/+AaQ1E5/PlJCHf2vuumVKBXrzrW/dfnsHsNmF4H6h8LGW89Bv7Oxp17r7fKSnUSHA4dw/AJzjuPdkBX+SQHDjxATs5VxMZeo56MxETVFAP1BA0bpp4gd8EwXEu1W+kDB8KoUaqVYQiGlEowzGZV+a9Zo+ZPn66e1AsvPLF8nrQw0tPr731k3PDNIRjGg9KhQ9MtjHqC3lJC/uEiwhGYcDbH4+MpKFAN+eho+N//VEvZZlONvZ7mDvTmGoQ1DHtOBGvvUa3uLsGRFPMGwZQQ+69AjllUyz472xWWSN01iWNUYn7RTkfuoQIrWcSpt78AbAR4Ei4GiAJeA2cDM4pc+rGdcFMxy3/rRSaPEPSOJNgxgmBbAe2yVcv52mtd/vioKNUWiY0Ff7OdX677Nx26BjL647uqhy84HMrA2b02j6C//ZkE0kl46i463nUlxcXq1o2MVOfDbFbbqst4ahLTVkL/ofBHIPgAvPsSjLoXusfWazj72coIoYQQSsAcW/MtdXXFMNxpjqC3cR+HhKj7paEgfUFB4547o1stnCgYoaFqO4YVqgWj7dO580yysz8kNfUBoqMnYTL5q6d3wwb19Obmqqc3Pr5xggEwZAj85z8uv7px8yUnqxQIK1eqeIfJBBddBLfdBi+/XPNBMQQjNLT5BaOgoGYqbnfcR9w2l4WRkADp6Tgc6hSGhqqeyCUlqlXdo4cq0qFDqqLPz4ddaaM4yCUk/HwW1gWqH8GGDeoyHDqkwkR5eb0IpJg4MjGP7YDDX21PSrVLo58BqL4JH2Z1RbIQCoFCCHxT+du/PRRJGJdSRCjFL4cSEqLaBzEx0K6d+v+AgQfp8sP7lI6eQNrKffhTSVcOENU5mN6Hv0FOncaO9zfS4ZHbSezhR4dpFxN02/UEvPkSAZQrqyTlIuTlE5EzZmBKP6aS7S1ZAhtOEtcqKmUyf4WQZBh8FwA2uw0hBH4mPy7omAt/e1etGzEZQlSdaLwCplkpK3ONXHbvVgsn3CfFlcVsy9rG2Z3OVjPcK32jlxSoSte9l5SxPXCN8WhOwTAs0YYEo7BQ3aAni40Z3WrhRMEICfHMaPVGogXDAwhhplu3Z9m2bTyHDz9DYuLDMGgQLF6sWuKVlaoW69BB5Yvat0/VcMagvdouKVBjM8rKVKXcrp3LHTVkiBKM335TsY633oK//13lz9m9W+WcMW7g0lL1sEREeEYw8vPrzjvUCMEoL3flujM+FRXqL9u2qUwpVVWQu+Ec8lhLwN5QAgqy2NbJpblmsytW7edXV26559Xke+cH1Q/h++/VKb/qKugld3Pkra/IJRpHj4twxAVyww3q+V6zBv7xD5U30uFQQnP87c84evMDyL79IC+PbqnL1en+4ScYNQoJVH62DOvlY048Z698Bz88CVe1h5X3uOYH9AL2wGV3cOH7L0OHJOg1EDgCEZI3B5XTrgyu2qXOo3DYEUhXDKMxvZqM6+9mbU1aOIlwazgfXv1hzYrodHtJPfKIupjPPFP38tdeg7/8RX2v3a221uC9p354in/8+A923bmL2OBYKmURccCT58Ho0irOrW1hGPe+yeQSivBw1WirVdlWOapYtHMRF3a9kNhg1zN4pOgI27O3M6Z7HdfQ3cJw/+3237c2vsWsc2dhKSwEKaksKeSzw9/SL7YfvaN745AOTMI1JC7DXEK0GawAVis/+WcRfWwPvQwLw43Ftq3s+eEpwgPC+dPQP9V9fpsRLRgeol27scTETOHgwUfw84skYfBgteBbp78hKkq5j6ZNU07g2293tVLqsjASEtQ0I6OmYAwerFxVoBy7wcHqwezWDe64Q6VIH+d8u60ReA4ObtrAvd9+Uwn1hFCuMffU7m7xgbLAduxflU7etgwCRg8nMNPOcc4jY1Nvio5E8zNvsu+R0dheVKdh16460mX5lcO1V8DeCfDzPURFqd1FVfoTRSEV/tHk2cMYMQIuuEDVR0YwtmtX1YsmIkKdkrw8VUd0v3YoPSp3cOSCqVS9OpeYmBOHtrBwC7x1n/r+h/nKp+PGvrx9BAXHEmZViRsj/YqIZBfEdYC8nRAIR4uOMn3TTF4PhY5FYK2sZ/CeW9D7l44QVgFd8qHfhP3MkTChWzfVwty3T1mSAFYrL4yA+CKYsAd6nvMzXYsz+FcHGOzerdbZin181eMcKTrC0xc/TXhAOACrDq7igaV/ZkkIxDmvW5mtjG/3f0uHUGciO3dXh9t3KSVlVWUEWWq+PCyjMIPPfvuMwopC7h1xL+mF6SzeuZg7h91JyLffqjLVJxipqa4uQycRjCW7lwDwn1//w8pDKwkZuI9VP8EjF8Afd5fVFAzjfjewWmsIRkVZMf/64SlSj6fib/ZnU+Ym1qav5eyEs1l10yosZlWGWctn8eG2D8m7P6/6up9wDd0Eo8pRxau/vsrU/lP5YOsHPLLyEXpF9WJKYSGZITD89X4cLk6nc3hnXrn0Fa5fcj0fXvUhE3pNoKC8gN4Df+KR1ARmAgQE8Ps+v1D01tmsDoYk4/oANhNMCfoS+/dfEBcSpwWjLSOEoE+fd5Gygn377sYc/wwdQLX4QVkYEyfCOeeo3k9z56omsdlcRy2GsjBACUZyck3BMOjd2/V9jLM15O7yKilxCcYpWBhG560N93xN7jcHaE8WP+1Ko6pHH2Ji4PAhSWHhvyjHSublAfywHiorE1DvvQLoCqyGl9WvSK4kWVYQFKRcQX36wM03q2cuIEA1CrfYvuRf6d9Aj2+Y9ofjzJv2qDJcLv+DUoNrr1VW1OKG00dUt94qK6FSBQB6VO2G3vX8wT2+4fY9sziTce+PY0vWFq7uczWLrlmkFhiVaVgY38eW0Lc4k/+s/w9fHv+ZH7rAtdupf7S3mzvjliugRx48+y2khtv5tSNMMCLDe/fyVtpnfPJ7+MJqpdgf8gIhKwTSAitJc6Tyh8thvdkMgYHYcFBcmE1EWCwv/vwieWV5LD+wnK13bGVr1lYmfDiBElsJP3WCq3YVgt3OuvR12Bw2DhccpriymBD31rfzGD/Z9Qm3fX4beWV53DrwVl4a/xKBFtWC/+OXf+TzPeqVuZ3CO7HiwAre3PQmb256k++qBJ1zS6i0V9Lv1X7MGDGjZuXm3i3c/X0YznO0K2cXVy68knuG38Pu3N2E+Icw55c5AMQGmckOBocJiv05MYbh5h6qDLLycddifm8JQwCPl33Nk9+vIi4kjoqqCswmM3cNvYuXf32Zaf+bxq0Db2V0l9F8vudz7NLOT2k/MTBuIJsyN+Fv9ufCrhdSVV5KlR8EuAnG25vf5t6v7wVgR84OAF765SWmFBaysiscLk5n9ujZPL76cS6ffzkAS/cuZUKvCXz626cU+9k5GqDMY1uAP2kBFTjKKrh0giB1UzCG/Z4WDnYheW3Ca1zf//q677FmRguGBzGZ/OnbdyE7dkzmt2P3Ex0fit83y5TvOTparRQXB6+8AvffDytWqAq9ru4T7oIBqkluMqkxG0KoWr1PH9f6Rspj9wSHzhaXDAomM8+KtTQfqwghI82P/fvVJlNT1fTgQbXZ0lL3Pun3uo7tYweY7Tj8iggyhRPJWAIoJ+yY5K7bKxj62m1EVR3h+7dvp9+hXsQ8ej+d3pxNUP8eJAzrgHnGy+o9BfVw5cL3iQuJY0TCCJakPs+bPIi5rEql7bjtNggI4If4Kr5Z/jfCgyK57+z7apj1APd/ez8Ldyxk3z378HP3+xYUMH/bfHbm7OTxCx/nwe8epF9sP36f/PsTe0k5mbdpHluytlRXIPnl+UQERFS7NRxhoVw6sYjk+ZeTUaiu0RGn9yCn8Cgj5nTnpfEvMbb7WNYfWc/whOE1LIycIAiogkxnvZMThGoR9+gBe/cyPyud1d3VvGJ/EMAxZ10YipVjgRXVfTifORdentuflTevIq8sjwsSL2DFwRVsztzM31f/nVBrKCW2EvYY7ZKiIlYfWl19rLtzd5OavoyRTgvJcEkt2LEAszBz+6DbmbtxLhuPbmTxNYtpF9iOZfuXcefQO3l/6/usOLCCFQdX0L99f7ZmbWV+bBj3by3nm33L2Ju3l4U7FtYvGO4vQgKw2fj76r/z27HfuHOp6j760viXuPnTmzEJE9mBdg46e5oXWySVJsnsi2D31zdzTe8qrnWzMD7rJZl6MfT4xYE13sTTjtVMS5nGO5PUS86klAghCPAL4IV1L7Bg+wKmDZhGfrm6J1YdXMVfv/0r27JV2vTPrv2M//z4DMdvhLVFwTx4EZg2PMu7aV8CsClzEztydmASJtakrWFTKOxzxq9mjpxJqDWUuRvmEuAXwJo01Wnlox0fqWPxczjvIYlDoM4lW8mN9KcyDKpMcMB53L2iehHi72btexAtGB7GZPInKekj9u//Pw5e+TI9X3EuqG1FdO4MN95Y/4bi49XUEIzvvoMRI1QLKipKBYONgYEAAQEUhnfimx/j+PJm1cWy++HbqCr8PT9mDOVwVSg81gl+vhdWPVz9t6Ag5c3q2tXVjXHcFQV8Fj6G61YHclNhFzJ+TmP/w/E8HrGanJIcvrj4bS44+1qqTLDmiecZXdkBXn6fNZ3g6YPfc1vsON7gW+gzG7r74RAONpWmcnj3p0zoNQE/k+s23Hh0I6nHU1m6dyl3Dr2Tczqdw/92/4/1R9Zz9rbjqtV42WWwdSszxsHGNf8AoEt4FyYnTa7ezpJdS3j2p2cB2HBkA8PLnefbzw+Zf5wHv3+Q9MJ07hp2F8+ueZbIwEgm9Z5EUEGBauUKUS0eUko+2PYB53Y+l2cveZbhbw5nya4l3DLwlmrByIuwUuEH64+sry6DIRivF35Pqj2VRTsXkVOSw02f3sSaW9ZwjrObpPT3Jy8QzNIlGNnBqFZ2z544vv6KX4v8sJmh0t9MsT/YzJDrrAs7i3ByLNnVgrGxA2SVZvPSzy8BMGPEDFYcXMGGoxv4JeMXrux9JV9sX8Leds5OCvn5rD68msiASI6XH+ervV/xyL6neGgIPL4CbGXFWICf03/mgq4X8Prlr3NZr8u44ZMbGPLGEP405E9U2iu5Pvl60gvTWbJ7CXllebww9gX+tfZfbAs5CpVVzN/8HgBr09YqK8Y/hH+v+zc9TPuYEB5es+OExcLbKZBz4H0WHlzIZb0u46u9X5ESl8KNA26ksKKQ42XHmb1qNhucbaNif8kW+xGeOg84vIzcoXCtm4VxyFnBpoXaWTXEhBV4YewL1cuFMxD93JjneOyCx7jkvUt4d8u7BPoFclb0WczdOJe8sjyeuOAJ5m+fzzWLrqG8qpzQWJAymDk9oGSHeilah5AObDiygdTjqUztP5X52+bzUZKNzBCIt8YQZAnivrPv476z7+PRFY/yxA9PcLjgMN/sVx6IEqdgHA5WlsbZCWezNWsrmeF+zLpcWZi3b1DlToxIpKXQyQdbAJPJSs+eLxHx6GIO3gD2cCvSEIDG4u+vIrMZGSpgt349BaMnsnAhPGl6mNuDP+DqG4K45Rbo31810MILDjP561v59FNV/32f3Y915QNIjkjjtpQ/QdAxYs5fyLvvwo8/KmNkzg/zKLilMwsWl/K//6lB6vnn3M0h+y8cjv2Fs5L8uDDoZx7mE0KtoSRGJnLFd7eyKxoW9YXzd/wf65bOBWCbMxTzZvbXfHoW1dlq7x0Hg4qfZ9LCSdzy6S04pHo4Sm2ljH1/LJM/nlxdAZ2feD4A3x/4Hr74QrnTRo+GgACOhMLNva+jT3QfHlv1GNd8fA2XfXgZlfZK7vjiDpJjk13/NayFjh1ZE5zHwfyDVDmq+Nfaf2GXdnJLc5m7YS6HCtNwRIRTFB3KBxW/IqVka9ZWdubs5Prk6xkaP5Tukd2Zt2ke27K24ShVLeLsUPUo9WjXg/7t+5NojiIjFCrN8ErVWgBWHVrFl3tV6/PjHR8rC8Pfn2JTFVVmyAqGDMMqCabawtgdVkmhXbXy8/ztVPpBkRUy2ym3TScZTok/1UHv/ZFqG29tegur2cq4HuOICYph8a7F5JXlMTR+KL2s8dUWRsmxTNamreX3yb/HYrLwyq+qVZMWDttiITT2Lb7Y8wWHCg4xvONwAC4/63J+nf4rJmHiiR+eICEsgeEJw7mw64XklamuzxckXkC/2CS2t6ui1AKf7vuSPtF9sDlsrDq4ik1HNzFj2Qye7JUFN9ygxhA9rBovDosft02Evx58gwC/AN6a+BaLrlnEy5e+jBCCe4bfo6w04Fen8V3sJykUymrrHthRnQc3CyMtTMVJMoLsHIoQdLeH0y6wXZ2PW5AliNcmvIafyY9xPcYxrvs48spUDGPGiBm8ftnrlFeVYzX5U2SFo6GCEn+Y3OFiXhr/Ejel3MS27G2U2EoY2WkkfcN7sDlOWRg9Amq+8Ghk55E4pIM7vrgDm8NGgF1QYnYKRqByuQ6PHwrA0VBJaiRscW7LJCEhLIGWQgtGCxITcxWO2Q/ww+IKdqffjcNx8hTfBQXKU/Xgg3CH/RUeWzGK3gP8aSdziXluJtdeCw9l38P/7Jexa6DVv48AACAASURBVBcsXaoMjr/8BZ7o+hZPXjKer7Zs4IcfIG3IlewfeSNfjH0J/+7KB58jdtD/ki2sN/+b4Mgi/rt5HmmFaXyX+h2px1O5bvF1vLf1PSwmC3uCyyAujpyESLJMpdw28Da+mfoNpfZyPkyGjc7nYHnmTxAfz44YCDEH0c0Sy2tDqO4ltT0WBhDHw6Me5r2t73HrZ7dS5aji3S3vkluay5xxc/jodx8xOH4w0UHRpMSl8N2B79TBXXwxWK3YA6zkBEO8NZpHRj/CjpwdfLzzY77c+yWPfXwnOaU5PH3x0/SL7ceKbZ+5Wq5duvB+9xIC/QIxCROvrn8VszAzJH4If172ZxJj3mdxfwsfpJiZGv4dKz/9Nx/88iZ+Jj9+1/d3CCG4of8NrElbQ//X+pPkeJkV3U1kB6j0HK+PfZmNt2+ks4jgSCgs6QOZooRxPcaRejy1WjAW71qMrKwAf3/yUGLgMMFWp8i6Wxg/d3TdD0f9XAHovXHKbdOZMEotIIVABgay31kHVtgrSIlLwd/sz6AOg1h5cCUAQzsOpZc5lj1RqtIZ9v11lFWVcXWfq+kZ1ZOsEuUiSg+DjV38qTA5qt1BhmAA9IzqyYKrF2ASJqYkTcEkTFzYVY3/iQqMIrl9Mv3Ce7ErGj49C0qqSvnnmH8S4BfAsv3LuH/5/QBsiLVT3j5KDTh19jk+Thl2E9wdM4G1t64lNjiWSb0nMSJhRPX+O4aqE/Ors+1VbJEUop6pgYHdOBIGZVaXezc9WFXCGYFVpIVBJ1sDOaqA5PbJLL9hOS+Oe5FRXUYBcEP/Gwj2D2Zk55Fs+sMmXuyq3KobQlScalL0udw17C4GxrnS/STFJDEgtAdb4mB/O+juV7MX5PCOwxEIvtr3FZf3upyUwmCXYFjV9R4aq7Z3NMhBehhU+MHybpBQYa0OzrcE2iXVwnTt+gQmk5WDB2dTVraHpKQlWK1x1a/o3b4dfv1VfV+3To0TABXWCBHjKDgWwsjYPYyxriTontuYdJUKYwQGhp6wL3vaciITl7Hgiwx+mf4LjyTu5c6yfnQODuLLqFIGtB/AlqwtjPtgHJnFmezI2cFPaT8B8Olvn3L/8vs5XHCYB859gOPHj/Bh6TvI9u3Z3kW12pLbJ9MpvBN9AzqxPv4gDmfX8hUdbTwUNpwdYZ+QFNSFeFsAuyOylWD4+ZEbBD1lBI+d/xhmYWb2qtnsy9vH4YLDDI0fyl3D7qp2DwBcmHghr/z6CmUZFQQ6u18es9iwmyDOL4LJfSez/sh6hnUcxs1LbuQfv71JnDWKMd3HsGzTIt7Y+l+W/TKb+Fjo2zmBjzrCpO4T2FN4gA1HNzCs4zBev+x1/rvpv/xn3Uts6gBlZrX/D96+jy9Sgri096VEB6m404PnPciY7mPYlbuLR/93L38ZAw/4VYAdYv0jMZvMxMsQ1ofCT939CXE4ePLCJ/l639eU2kq5tOelLN27lF+LfmNYeDh5Dle31U1O0XWPYfzs1oA8KlydFfZGqTImyFCkgAppo8BSQYk/xFmjyKw4xpD4IQAM6jCIZfuXYTVbSY5NpqeIIjsE7hkP6eXZfDP1Gy7oegF9Y/qyM0clvEwPg4NxAUAlhwsO42fyY1CHQTXusYu6XcSuO3dVu0WSYpKID43n3M7nYhImkoO7UukHz46EGL9wxnQfw3mdz+OlX5S77MIO5/D90Z9YH1nGuW7bzXWo4xxu7c6AuAHURcdQpRS/OcOBxX4OioQSjBRLAouAVEsRxpi/NKd7J8NaSZrVzojyhgUDYHTiaABig2O5e9jdzDzH9Z6UlLgUcp2V/3rrMXBAvFn5vQZ2cAlG35i+pFi78K7zEe0harqjwwPCSW6fzIHjB3h1wqvcvKYvxc4GyCH/MqJLoVuQEsfdAcUYd8v6jjDqmD8tiUctDCHEOCHEb0KIfUKIWXUsf0EIsdn52SOEyHdbZndb9pkny9mSCGEiMfFR+vb9mLS0DGbPfooJEw4SFSXp2BHGjlUv0fvgA9XAfOwx+PJLFaLYMP0PDJ0ewAOdhzJn0vc8/aypOoxRF791DKDIX7ItexvXfHwNz/XMZknsMXaGlHMozMGdQ++kb0xfMosziQyI5I2NbyCR9Ivtx9ub32ZX7i7emvgW/7joH/T160BhAGTHBLKtg7pt+sX2A2CwX2c2xLtaxz91gorkPuyIgSRLPF1kGIfDQQYGghDkBkO03YoQgkfPf5S5l80lvTCdwwWHeWjUQzXEAmBsj7FU2CuYOxiVLA/IdLa048zhmE1mnh/zPNckXcO1FvWg3hB8Dn4mPy70P4syC4zruoZbroDfOgdzPBDGxZ7D6C6qMhjVeRQpcSn8e/y/6VYWwJ5IO3vD1QM7L0WSJUqYPmh6dXksZgtndzqbWwbewoTyzhwOk2SbVCwj1qy6Xcbbg8kIgx1xZvqWhzGg/QDC7RZMEuaMm4PFZOEx8w9UDuzPcekSjB3ODCLHgsDuZ4aEBH5OEPg78wseFa7u0Hsi7ISVQ7hUgzNLKktIdT5Cf4i7DFC+b4BB7VMAGBjSA8v/t3fmYVJVZ/7/nNqrq7qrl+p9BbobkFVoQEFR3DfUiPsejTqJJuPyMxOTOKNOYoyZaGImJtG4JTqJ0YkZjVvELaCigICAIHtDb/S+VXXXen5/nHtr64VGbRrhfp6Hh65bt27dOvfe8z3ve97zvmYr1VHlt3q1Ci5zHcXJE04GYHJaOQDjRbYSDK8ZkxbxOj1/eiwqKpHqnGpsZtVxCSF494Q/8ut596h7xKrUbm0hnO2cidlk5j8X/Sffnf9d/vi1P/KnycoFtdyenHG5Nap+Zy5Dp2bJMrtxhEBqt0uvJUqPVC6pI6VaYbid+GLSOs29s83uo80RpbR/5J2tw+LgodMfotRTmrS9UKjrvdqszr/IpEKXx2eNJ104KDR5yHJmMcMUd0FPkJmk8ujpv+G1i/9OSUYJrrAJn1ld8N3mXso6IS1qJqMfVpmTF2RW9B6AWusJjJqFIYQwA78GTgbqgJVCiBellJ/q+0gpb0nY/9tAQtpW+qSUM0fr/MaK1atVSqjVq89n1aolSCnw5u7mxBPXMPFYO/6c97lr8XVkZAz87EMFTayUARYXBXi4qpB/2cd3fZjthwDYsMRCHmvTgryX1gLASeWLAHh922vcPu5yjnr1PCr7nNw+/3au+ttVVGdXc/4RKtdOdVCd0BZ3gA05EXL6TeS7lELMlgU8qT3Xi+qsvF0S4u+lQZr3wBSZiwj34LNBhylAlpS0OiEnHH9Yr5t9HdfNvg5f0IfLNrCDOHn8yZzRX8YdJ+3m9CI71UCTSRMMkRwd8q/tVfyz/wOuc6v5i5PkOM7fCI3p8GExvJ/VB90wy1ZOVnElD6x4IDZPAlDdZWFLVoCAKYqnH7ocUOxXfuzBKA3YacuS7BKdCAk5JnU+xWEnfVZY6Q2wpCMbs8nMmU3ptEgfE7In8Ivj7+PGt27j2qI9nBONWw1h7fmXAtrC3eztamZtgeTMrYKXqyQNxEN0t6QHyeuGNIsZTGoOaHtEpc6/yH0Up9hdzCs7A4BZzvEAzOlI065n3CK9WMbzbizebua9HbAwu4y7Mtv5JDvEvE43EbOJM0wDO7rBqLz0JliwAB59lMkyB1NUudu+FlZBGfP++BbzXvwnfPBTeOklJrbC0+7l/O6X43h08aOcNP4kWiMqY683OnTeEREMUtQDOzQXXK8lSjdKuGeG1Ch+u1SZAUKREI02JSZr7GqOpdT/xbu/ItT1XhVRYe6F2muTMHF8gw27UBd0RsQb+0xlaKA3YO4d/63WiDy3EFdYxAXD1ENVF+D3U9gLq5zqe4RU98i4rgOUplZjNC2MucA2KeUOKWUQlYPynGH2vwT40yiez5jQ2qryAn7/+1By4zXUPFPGo4ETcbgC3HWX4Ocvv4C8dSILL5/N/3E8D267nqht8FxHL1l3MqsBzmj2cNP2X/HmjjcB+OO6P/Lk2icH7P+hvRVPP9y8MR1HCPJ7YZe9n23WHuxhKLflct0rTTx/z2bmvb6B7y2DO9dlcmbWXAp74Ef+ozCb1A1f7VOj2C3Wbta7epnWImKWwOxAfOLwpvdCmKLwE99rAEwJeCgLqlFpbV8TPcEewmbwhlNGdx0duC7/OuzcOeB3CCF49A0HDmHl6peuJRKN0IQagRbIZMGYvr2Xrb+CqmblfnB1+njuObjtfdUZPxFdjSMEk6LZnFV9Fq9f/jpnVJ0R+3x1q2Sr3cdOm5+r1qo2u+kjsAzxqJT2qd+xJlJPjh/MYfWgFwVVe/VYoxzRpXzMTy1N5+UngxCJ8C1Zww2r4FnxKa3h7qRjerQlEC3+Vu5dfi/usInbl6thfmMkHurba4mQ0weusLoO/pCf7aFmhIRxa3cx/46HMb/yKgDjwunc+S7c0D4OgAn9ToSE4m44xhd3kczxZ/HmH2BSoxqNr3X3UtFr4cNf9fEfazyDtsEA9uyJ5VBx+AJUtYM7ACd2affJxx8rf2tTEzQ1ccxu2Ni7g12du1hZvxIg1ibDCQb9/SrsV8NnidIV7cMagUKfCU8/bA+r7AmNvY1IARn9EBJqfqDsSxidZ4at2MOwN9pNegDSI3EReu61DJ55R7Vtjk9SrF3mCf1pAw+0aVMs15w7ZMJniiClpJYuyruAnh4KeqFbqpujRlteVTFENp7RYjQFoxhIyLxDnbZtAEKIctTqrsS6mA4hxCohxAohxLmjd5qjw8aNKjNnSQlccgnc/+dl1Oc9QU66m0j5Wzz4zAYuvWkbd39yNW2Bfu781M6GDjU6fP2THww4XmNPIyuDOzlvE/xp0a+Z6J3IpX+9lHA0zJ1v38m3Xv4Wzb7keuAfRnYzpx5+9L8dbH9I3WS1Fh/bTV2M6wCTzw8ffaQWwt19Nz95E658r5ec3a00/Bwu+MOq2ArcstYwtjB8Fmlmg6WdqQ2R2DqCGT0utDk6jq2Fb64xs7p9AwBT/S7K+1Xnubt7D61+9Ru9wZSJukcegeeeU9n9Umlvp2j1Fv7bdi4f1H3Azz/4OXulEoz8aIqLRBccfcGiVqxqrhaN/H7vJmbsBUuPD9HbyykPvYxIWPVe3Rik3xQhJKJM3wu1D8K/vROOhzOnoI9SPw7tJs9HrE2KAnFBPKJFdeiW1nasYX0V5Gqm7YWQDLPZVwtAmrYsY5rmdXh/z/s8u+FZbmwdT5mmE42R5MFEjh/S+lXj+0I+tgcaKe4GR612vlposOjs5J63YcpedT0d/hCn7DTx7TVWTF0JgqXV4ijZrT4XFVDe2KdGv4mFv4YiGCSWpVA73k0fwb+/C45W7dz191asgL17ufEjuG7GNaRZ02L3cEtI/eBhBSMQiHXC6Vr8SFOki4wACJ+P8R2wPaBcRXu6VFc0L+EylnZ/8dG5CIUo1ESruJuk1CD2lnZsrVqP3t3NjCZ1vTL9gxTnam9Xi3GjUWVhmCJ0BbroJaCufU9P7HsATtVqslW07aOE7ZfMwRIldTHwvJQy8deXSylrgEuBXwghJgz2QSHE9ZqwrGppaTkQ5zoo/eF+Fj5xHDc8+QBnnQVTp6qQ1K9d9xnf+tNPmPlvt1LoLuTNbyojan3zen741g8RCH575m/pCgZik3jvfvYwa9YcT0fHO7Hj69E1i3/yPOkXXMZdx91Fs6+Zl7e8TG1XLX3hPn6x4hf0BHpYUbeC17a9xnr/TubVgzWqUkmUd8IuUzfbZDsTOlAhWPqK8UhELSLs6VH5rUAJiTbrbt7bzIROwcu1b9BLkGnNxB78tE4fR7SbKQzayfXDA9srObbsWPJ9gqLOCGU+1anu7tqdIBgJo7tIRCVWhHhG30S0GgCXLPgXzpl4Dne/eze7w62kBcEdTHnodcHQFyxqnVxxDxT51O0+u0H77e+8Aw89pMLQAMJhqhvjD3xVO9hnzlYLLbduHeSqQ6nWkXdEfUmCUdwXH2kesVcroqSv+K6vh1WrKLUpN8W67i04QjBe61uma4Lx+NrHkUhuZA7p2mk1hpKHlDl94ArEw5K3++rUtd29W+2gL0TUs/zqnb7Px2t/z+TfPstNTman/V2yLf4sVewNJH82lZaWeAoPXQz0fbu6uOkjuH1DRvy9FME4sj+TR859jEJ3YSxCqzXYiTMEacMt5E+wMKq0QzaGO5V49PQwoR3W92zn6r9dzd82q4HIvLr4x4u7RlhVcTg0txioZywmGP39asFjR0esKNfd7woe+Yd98CwL7e3qsy0tuELgM4Wp7VQDibIuoLubAm1ck+8XnP0ZVAXTmVY3fKaDL5vRFIx6IHGGqETbNhgXk+KOklLWa//vAN4heX4jcb9HpJQ1Usqa3EFqDhwIpIT7nl7Bst3/5JHa23gjewmn3P0ztu8MIY6/m4c/+z6rm1Zx1/F3MTVvKk6Lk/V71/PP2n9yZvWZ3FBzA4+c9QgvXPQ3SjNKaeBI+vt38sknJ9PY+AQPfvAAN792M5XZlUybryrK6GF+9713HwATcyby0/d+SsZ9GRz92NGc/szpRGSEo3Ub79RTqeiETtHP5lAjE9pRboO6Ojj3XBXS+G8qzJHly9X/drsq3ATQ1ES138mnLZ+SZXZxynaSOoXbN+dwR5eaN7CVlPP65a/z0UsFiM4ucn0SR1gkCUZOwBz7LA8+qHKEeDywZs3ABtZK0Iryci6Zegn+kJ+lrSsp6AWRmMKiszPeQSZaGFqOrrma/35Wo/a9emnb2trYuVQn1GeqbgMuvFC9SC2zCxCNUvLhptjLPB+xqnuFmjg5pUWN0BMLPzU0KMEoV+21rmMzWf1QqHUI0zRDcUXdCioyKyj1ToiPoIOq4xfaRLSyMNQ4yx/ys9NXp4RHFwxdDFIFQ8+zlJmZvLpdszAK24Ox7yjX9WQwwQiFYOJElRkZBgqGXj1wwoT4Nn2fDz5Q10BLgZvnyotZGK3BTnJ9DMgllUSChaFft4ZQOxkBoLeXCR1Q37eXp9Y9xX99oBJPHqUJRkHIgd2/nxmTB0vCGAzGrluSYOi/MRJRA4Xubmp8Hs5rzByYxy0Uim/bswdXSCAF7Orcpc61F+jujn1Psd/CnAbY4ruGnM6U0n2jzGgKxkqgSggxTghhQ4nCgGgnIcQkIAv4IGFblhDCrv3tBRYAn6Z+dqwIRUJ0B7oJheCHP61nwtQ27n7qbZAmTs2+kcxpy/mH/C7/bHmBN3a8wUVTLmLTjZu4btZ1mE1mjsg9gle2vUJjb2MsiuW62dcxp3gOswpnsanLx5w56/F4FvL6x9dw6z9uY05+OW9c8UZs3iDfnc8k7yRW1K3AZrbxwkUvcPWMq/nxCT/mxYtf5K0r3+Iv5z/LaXV2lW/7iiuo0PqFoAypUejmzeohnj1bLZqqUSGYLFumHuIlS9T27m7Yu5fL2ktYMnkJa2seU8dKEIwr2kv4tklLOV1WhtPqpMzqha4uhL+PMr+V2q5a2vzqQfL2a/mdjjwSbr9drTb85jeVLzf1wdS/x+tlbrGqa765Z4d6kBIFQ7cuCgqSLYzKSigoYJ5PRQYd2YTqJPV9dMF45hkKe8BlcuA2Ocj3C5XC1m4f3MJYvx5Hczu5JjWJmWhhpPWFyQyamBzNxtTrS+5st2yBLVsom6jWNHQEu8juI+ZymNIcF4T5pfMhLw97BKwRaOxXI399VJvTB2n9ar7GF/TR1t+uzkO3HFMtDL0j06sJ6iusdbQO3haB/H4l6vp9Q1vbwDJ5u3apUbRee17/nt5eJZ7d3WrVaEVFvA30fVauVOepJdvMd+fHLYxAB14/wwtGfz9V2qF0q6wx1K6ssZ4eztsEp5WdwA+OVS7e9KiVydoplEZc+1dLorFRFQJ5883k7UNZGInXu71dtUNGxuB53BLLAuzejVs7xJ5uNdrL8ZNkYZT0ae7cnBx1PYZroy+ZURMMKWUYuAl4HdgE/EVKuVEIcY8Q4uyEXS8G/ixlkkxOBlYJIdYBbwP3JUZXjTU/fe+nTP7lDGpq4Mfbv0braWdQferb1BTN4rVv/zcNtzWQ78rnzrfvpNXfyuLqxUzyTop19tPyp7G5dTMQD3vUmV04my1tW3hi3bM0pN3M6vBiLELwndJPCbU8SE9P3GWjh4XOKpzF5NzJPHbOY3z/2O+zeOJiFo1bxAVTLsR8xFQ44ww44QTKA3F/cGWnKW5J6Jlw9f/r6tQDfsst6kZ/7DFoauICMYXnL3yeshItqiZBMPB41AMFKs0JqG2dneD3UxawJ7uk+oSqQLhzpzr+6tWqqFQ0qvKZJ9LaqtJ1ZGRQkVlBjlNNJBb0En/o/f64YCxYoL63ry9ee+SCC7i29BwePPVBjvRnqA4g0cJYtw5uvx1x1llU502mKm8yYtNmJTbjxw9uYbylptxKs1QoaqJg0N/PnC4Xx5nHq3NrTphf0jIWZ0+bi9Oi5mCy++IWRkk3eDULbH7J/FiHmh6AYFR1DvqchtcPrj5lYbT1tRGMBPEEiOd2H8zCkDKeiHIICwOgRAs71b+LaHRg8Sm9XXSLLtGSamtT35+errIUtLbGc9fPnKn+X7FC1UEF8tLiFkZLX6sSjMR04eFwvH45QCDAmVtg7caFsUngtrCafKa3l7n18OpFL3H38Xcz2TuZcdIT69xLo+n7V0ti2zZ1LitXJm8PBmNCX9TDQNccKEEYTjD0awOahaH9qc27ZPWjLAx9riRgU7H0enbrA1gTY1QX7kkpXwFeSdn27ymv7xrkc+8D00bz3D4voRA8t3QHDeZdRLs7cJRuoifaS08A/l+FqoZnNpm5cMqFscVJJ40/KekYesoKp8XJ9PzpSe/pC6Ou//v12Mw2XFYXp1edyeSyfOrrf0V9/UNkZp5IaemtLCw7mt+t/t0A0Uni3XdVnhCbjYoVm+HXFQBMcBYNFIzEdCXjximL49hj4Wc/Uw++tgYiljhRnzPq6lIda6pgZGbGJovLQmm8qgmGOQoef0TFF5eUqBxaZrOqGQJqHmNefEUxra3qO4VAAHOL5/LqtlfJ9xEvpFFcHE8DsWCBqj3S2Kge3Bkz4KGHyAVuBih7XLls9PUetbXw+ONKlJ54ggd7NyKRUKHV2ayqGtzCeOstqKqixDuej9s2DBCMf6ydjrz4YmBFfAQO8E+V6E9Mn05pfSlb2raQ3afcJeMiGRT1dJMbtNDiiCgLo0914ulBaE8DZwjlrkFzSflUD9PQo3rNzIgV0HodvYPXR7HBoOqwdJeUx5P82xKsjZKAg1pMpIUSOrjW1ngFKNi3YOgdpderPqsPMq64QuVCmzhRRYWgLIxWfyuRaITWvjYqUy2MU09VpY0feijWxgKYYSvlgwRdydDmMABwODCbTLx62av47/8RjvDvmWwuYJbMh/7B56UGRRd8fVASCsUGOEXabTSoSwqUIHR1qXbQxTqRVMEIqrGzbmFk9RGLkgIoCTrBZYmXOuzrY9A4/FHgYJn0/kqwfTsccwx8slndjL/5v1X0R+P+yMR4/kumqodgZsFM8t3J9S10wagpqhmwrP+YsmM4cdyJ/NfJ/0VRehEd/R18feY1TJr0e+bPb2LChAfx+daxfv2ZZLTdSkl6PudMHCZa2eWKpYrOzSnDaXEiEFR4q+IPuS4YNlu8FkdFhfr/Bz9QnU1xMSxWqZhjiROHsjDK1Yg7Nnr1+ymPuGnsbaS+p56ckBWxdZtK9X7NNfHsvGVlqjPSl7fr6IKhMUfLqxOzMNasUd/T0KDO44gj1I4NDQM+G/ue2tpkC+Pjj9Wo1+vluIrjkq4lVVXq4ie6Yzo7lRifcAKlGWqEnOsnNodBfz84nYgJWqzGRx+p/4uL1XtpaTBuXOyz2X1w7mbYEbkJewTygmqwMC1/WpKFAeAOKlcUaC6pXvWGLhgeEqosploYenvqLqnMzEFdUgC3NZTzoPNr6oUurqnzGLrYDCYYra3qeB6PugaRiEqHDOoe+c1v4OabY78vz5VHVEZp62uj1d9KbsCcbNGsWqVcljp6W3s8MTcOKGGlt1e5ErVSp+WZ5Uy2q/t8XdE93GE+Xp1bqostlSuvhD/8YaBgbNwIa9fCJ58wqV21zcQ2hndJeTwjszASXFIusxN7BOjuZkIHzMqYyHFdmaoWgL5i9wCWaTUEY4QsXarc7Vu2wIy56qH6qEVF15w76VzyXHkcW35sbP+jSo5iQekCrpx+5YBjTctXgjGYZeBxeFh65VJum38bL1/6MncuvJOzqtWqXZstj9LSmznqqD1Mm/Yqea48np7dRk7vL2lufg65j8kvIQTlmeWUekqxl4+Pv6GnToe4eOiCceqpqnPZsgVOP11t06v2pQrGsceqOhxHavEJemfU2so0qTqFN3a8odZg7NihRvTXXpt4gqoW+QsvJD9UqYJRnCAY/f1xN8ENN8Bll8VTu9fXq4dxMMHYvTs+h6GFucYsnFQqK9X36KG1mzerORe/Hy66KNbp5/lQHeiyZWp/h0O5syAuGHohpClTwGSizKOssSz9mddGiktavHxn3ndUJl+tQ9U7RXdQCQwoC8PlU2/ELAwSQo1T5zD09tQtjMJC9foltbAzUTCOCRZwmUdL2KFnQk6NRNQtjOZmNeoeysLQA1J0S2uQmi96lbu67jq6A914S6rhqafiwtPdnfw7dFdMimDok95JxZMgVq7Y6nBhKq9QLq7E9P+p9PSoOby//32gYKyKZyWe12Kn7uY9ah5lMAtjXy6phDxn7NmDKyBj7ZBl1Rb5dXeTFoLVx/0PC/xedRxdMA6gS8oQjBHw0N+WcebNr1BRoVzdaVnKwnhrp/Jh33XcXTTd1pRUjUsIwfJrlnPL0bcMOF6Bu4Bnz3+WW4++ddjvPSL3CO5ZdM8AK8RsdpCTcxpHHvkBRUX/Qk/PKj799EJWr55Nbe19BIPNQxxRzXucMO4E5XIC1dEnln1MFQz1rjSUEwAAIABJREFUYwYeKDdXjbDCYfVgeTyqc3z9dfW3fuyODti+nWMq1aryhp4Gcro1N8O3vx13X+nccovqFB5/PL4tRTCOKz+OsyeezQmNDjW6WrlSdXy//a2qLaK71jZuVC6AVMEoL1ffUV8fF5e+vrjQpaJ3lnrn+OSTqqN5/31YtIhp+dOwCAvjOlArNM87Ly4YFRWq/bZuVR2G3q6acCRaGECscttNzRXce6JK3U5mJthssdBad1CbCEVZGFZ/P+ZogoUhEtYuJFoYegfa1qY6LZcLvvMd5XpcskRdzwTBwOGIf2bmzPi1SGTbtvj90dSkvkcvgtTaGnfF6Peb3tEmurU09MwBei4r75kXqPP8yU/ik/iJE8RDWRi6Syo1Z45e395uj1vBesDDYOhzafX1ccGorVWWUqIVbLNR7ClRA6lEwdCLQO1r0lsXwenTYc+e2G+p764n266trtevi80WD1ZIdEkdIAzB2AfLl8Mtr96OPP3bLF2q+rfugLp4eu2DcVnjBuQ/2hcXTrlwgKtqf7FaM6mq+hVHHbWTSZOeBCQ7d97BqlWzaGt7jXB4YKW33571W54454l4x5VoXUBcMPQHfCj+9V/VSHrJEtUp6yKRSGZmLOQvd/7JTPaqAk+xVd4/GLhAkfnz4eij4YEH4hOcKYKRbk/n/y7+Pyr701THvGoVzJkTP0ZOjnp49fUkg1kYoMTuqHj20yEFo7JS/a+7X3bvVhO1c1XE1umVp1O/5D21JkCfaG9rUw+03R6b1MXrjYuZLhieBMGw2eKdgN7ZgOqQ8/KSXFKnbYNLC06mqAdEfwBXWMQtDHNCepVEC0MXvkQLIzMTnn9eWQdvv606Yd0acDrjna7eNomCEQ6rEbduNTU0qN+t3zu6heHxqLBaiFtaw1gYG5tVlTrvhGmqsuLvfx8PEx6phdHZOaSFkSQYu3YNOI8Y69ap/+vq4oIRCsXComMDLf1a2WzJglFUpL5rJIIhhGrHhgZcfq3aXjREtl1z8yYKxv33qzBmw8I4uNi+HU4/K4jMX0s0vZasHDUy7tFqNEdkhGxn9sA6vwcYIcwUFFxFTc0aZs9eg8lkZf3601m+PIutW79DKDRIqhH9odYFQmf6dDX6Sx35p/Ktbyl30osvqk73rLMG7pOpjY7MZqipia0d8Z64WKXlHWSUCajj7tql3FbRqHr4Ujt9UA/M3r2q1niiYAihOqhly9TroQQD4pPrVqtyEw1Gaal68HULY8+epGMIIcjLSK5xwN698c5fd0sNJhi6hdEv1HckdmqJ5OeTHlKDEndIcGSblWfm3IclCvT1kRYWsZDUmGDk5ioBCwaTBaOtLbnedXGx8vV/9pl6rQukXi8XoLpa/a0Lxvr1SmjCYVWfBNRIXO8o3e64KykjQ0VJuVzK6oNBBUMfQOllTb1pXhXA0N0NH36odurqig8kEgTDESaWJDFd2JXwfVELQxeMhgZlPelzbVu2qMHIZZepey1RMP7yF+W6bW5WvzE7W7WLz6fm+BIFQ0rVfu3t6lmpqFArvdviVl6WUxMMfRLfZlP3zpFHGhbGwcQvPniI2b88CVm4CmkOEJERdnepUY5uYcCBrXY1EtLTZ1JTs46pU/9GYeG11Nf/mo8+qmLPngdob19KKKSN0HQLI1Uwrr1WjRpTO6xUhFDpPBoa1AKsiRMH7qNbHdOng8sVE4yc3PKhO2eIl5rdskW5IKLRwQXD4YhHeiUKBijLRx+NDlbdUGfWLOVCmTIleVSfiMmkOv1UCyORwT6rP9D66NrrVZ3JN76hLClUxNepE07l6CZL3CIZ7Hh5eaSH1ePqDmmdlL6P309a2BQrROXR/d769+qV7MaPV7+ltTXukgLVEebnq7kZiAuG06naZeJE1b56pBPARRfFoptigqFbGDk58SqQuktKCHVcKZVQOQam/Mh0ZGIxWVjZoOak8lx58QCG11+P79jZqTpwvaP0eBCAO6pctxlTtLmoVAtD/067Xf12r3d4C0O3UMNhNcCZqrIz89JLSoSPP16dn34drFZ1b7z2mnomvF4lGO++q96fOjUuGFIqF2pZmRpcZGXFBhGuBGspVuAp0cLQMSyMg4Mf/fNH3PKPf6Ur503GX3NPbPu29m1IKekJxF094zL34boZAyyWDLzec5g48XfMnr2KtLRJbN9+G598cjLvvZfD2rUn0GHfSLS4EKn7pnXM5pGH6JlM8TmAwdAtjKPV5P7C8oUIBEXp+6g2WK2Fs27ZkrRobwBOp+o4XK6YeyjGpZfG/079bGFhfLRYUqLCbk84YfhzqqpSFkYkokaMqRbYcIKhWxg5OWr0/eijsYc9y5nFa5e/RpnfOgILQxOMsEl9n75Pfz+uiHrPLMy47FpCRr3jr6tTnV5uruqYGhvV68QOtbBwcAujokIJSWlpXDB271bRSjNmKFFatEiJbqJgeL2qI/T54gMHXcCGsCxNwkSeK4+m3iam509nkndSfPCgWxigLM/x4+F3v1OvteO7pZo7SZ+nBZ8MZ2GA+m1DWRjRqBIM3RLp7FRzPULA00+rbXPmqPkq3V130kkq6g9UB5+To9pbD5aYMUNZXtGo6uTffVddi2XLVJvMmAEWS5J7LdvljR8PBhcMw8IYO3xBH/cuuxfHzq9hDeax3q+KzoASjL5wHxEZiZVFPBgFI5H09COZOfOfzJu3kxkzllJRcRc+36esW38Ky55s5KM5v2TPngcIBIbK2vIF0H3h2mi6JKOED679QNXCHg79Qdu6dXjBcLvVw/+3v8VDenWOOEI9gIN91mKJW1aFhcpK+elPhz+nykolGA0NSjSGEgxTwiM1mIUxFNYUwUgVoCVLSJ+mVuK7w6b4/gBSkqYJhsfhQaS54ucM8VDW7Gx1Dlom2QGCoYcZJ1oYieiCoY/2n3lGtUl2tvq8HpWmWxj66F0fgOjHHcQdpaPPY/zg2B9gEiblysrOTg5/XbNGdZK6wOmCoVsYRy9S7ZdqYUybpgYjuoCXlw9tYaxfr8TujHgmY0pLlfuurU1lJRg/Hu65B/7613h7PPZY/HrrLin9HMvL41adzxd3zzU1qf2cTpg6FVsEzFqRjyy39gwNJhiGS2rseWXrK/SF++h/9zucWqpqQSwsX0iaNY3tHdtj1sWMfNUZHWwuqcEQQuB0VpCVdSIVFf/BvHnbmDLlr1ROfhirLY/t22/jgw9K2bjxYnp7P/nyvnjGDDXHcdFFsU3zSuYNWvMi5YTVg70vC+PBB9Xo7KSTBr4HcOutauFMascBqsO32+PRJnpkz1BUValR4YoV6nWqS0rvvKur453EYHMYQ2GxDG9hnHUW6ecrqyk9bE52SUFMMDIdmfGOXu+g9VBQXTD0CWRXwnVIXLQ5frzq7FJFMVEwiovj7iL981u2qElh/Xv0OR/92Ikd6RBUZVcxNW8qSyYvURuEiH+PfpzETAAWS+z6uqUSjPTsAlUbXM8DpjNxohIZfSCjWxip4ej336+sBqtVTbrr5OfDiSfCOefAL3855G9gkYoIjLmkQD0LQsTbvLMzLngQ32/OHATgQv2WbLdasHqwuKSMEq0pPPfpc1j685madSy3n2bm7089zNziuTT1NrGtfVts/uK48uNY27SWBWULxviM9x+LxU1urlqQVVz8Tfz+z2hsfIKGhodpaXmWjIwFWK3ZpKfPJSvrBFyuqVgsn2NiX4j4Yr/9pbpaZZMdTjASI5wG48or1b/BmDw5Hp0yEvQJ41dVfYkBnanZHI90+ewzdWxdMCZNUiNl3eIZjFTBGMTF5bYpV5M7kuKSAlza/IbH7okLpN5BJwpGTk48wWOqhaGTk6OsEleKsHu98RDT889PbruioljKk5iFEY0qN855KmHmSCyMx895nHA0HKvDAqhrtXy5ar+GhmTBSJj3iQmGLV2VrdwX5eWqs21uji9YDYVUZoOFC+Hhh9W1s1rV9ry8eDLO4Vi0SEV25eTEo9T0a6+36bp16pgmk2on3UKuqYFHH8UVNtFthWxnjvqNfX2q/Q0L4+DBF/Tx0md/J7z+PK75upljyhdw/0n3c/3s66nMrmRb+7ZYhFR1TjV1t9Yxs+CrXxQwLW0iEybcx1FH1VJRcQ8Qoa9vB7t23cmaNQt47z0vO3b8kHBKoZ9RpbpauU700fBwo/PPw89+Fu/gRsJRR6kH9Nln1etUC0MIFdFz5pnxUGX9gU5PV/78M88c+vipLqlBgg7SbWoy2x3RLIzMTDhbpWVLiyZYGGVl6vx0EUgUjAUL4skdEwUhUTA8HuVGMid02qDmifTUIqmj9wsuiEf/5OSoKDy7HZ54Im697WMOA5QoZjpSKvvpFsZ0LY1OomDY7bF2dmuj8hFHLeqBH4luqbfeUoOUm29W32syxa9nXt7IjnvKKcqaqamJ/1Z9rlBvcz28+GRVGjfRwgBwa/nBspxZ8fvokkuSXZ7GpPfY8srWV+iP9MHGCzj3XDUJd/uC2ynzlFGZXcmOjh109qsRw1iH0o4GVmsWFRV3MmvWB8ydu4H585uYOvUlcnMvZPfuH7N8eTYff3wMu3bdQ1/fTqSURKOjlClTn/hesSJ5AdmXRUbG8BP2qbhc6uH2+9VnB1t3smyZyoulz48MEgk0JCOwMNLtumBY1PtCqFXxv/kNrmoVweNxeJQrbv36+Dnqro/sbLj++tjiwCEtjKGCHi6/XI2Kg0GVASCRSy5RBbCqq5WVddNNqiPWI4tAtUteXtxaGympgpG4FiPJwlBtprfTPtEn1O+4Qy3+/Owz+J//Ue2mZzWA/RcMr1cFCsybl+ySgrg77PnnVed/+eXqtb6f1l4ul7I4sp3Z8Xsi1VoeAwvDcEkl8Nynz2EJ5DEzd+GAAWRldiWBSIBNLSqXzYhvyq8wNls+Xu9ZeL1nUVLybVpbX6Sj4w127bqLXbvuwmRyImWYrKwT8XrPJSNjHlarF7t90MKK+4cuGHp44n4ujBwVzj1XhVTua43KlyEYw1kYUXPyJPu//Atpr2yAle+r0bnVqjq9aFS1244dyv1SWKheX3ut8sEPJhiJk+mDIcTQ12LJEvVPR6tzEcNsVh2pLlgjZdEiNa9w7rnKWuvpURaMHv4dszBsWE3WWJDKPqmsVC6mG25IjrK7+urkNtCv50gFI5HzzlNuKd3CmDtXhSG/+64SzkWL1Hfp7jqrFfx+XH86GfY0K8FwOJS4zZ6dfGyzWbXlsmUq6m1f83BfAoZgaPhDfv6+5WXCn1zFkvMG1vrVo6HWNytz+FC0MIYjI2MeGRnzgB/T319HU9MThMMdSBmlre0ltmy5IbZvQcG1jB9/Lzbb53jAdKqq1MPg9ydPPI4lixerDjp1NJHKKFkYk7yTmFM0h9mdzeBIfj/Nqjr/THuCO8dkUh1sd7fquPSO/nvfU6PSxNxZumDoayZGi9RotpFgtaqaKfrne3qUFaOnX7FYwGSiSmZRlVO1f1kXrrpKuek2bFBrRjZsgOuuS96nsnJoq3JfFBSodDE6Qqhsu7NmqTUuxcVqXiaxXZzOWGBIliNLRWKVlAx+Xe6/Xy2iveEGNW8yygMrQzA0VHSUHzZewCn3DHy/PFPFY29oVrWq9dHe4YjDUUJFxZ2x15WVD+LzbaSv7zO6uj6gru4X7N37B7KyTsTprMLhqMDjWUB6+tyRP8wul3JpOJ1fvjvq85Kbq6oSJrpZBkMXjH0tfkxk8WJluSQuLkshy5nFR9d9BA9MhoxkwXBZVQfjcaR0apmZSjBSR/76GgYdfdL3AKXJ/txkZ6t5reJiFd6s58pKS+O74XncesN9+3/Mysr4CH8wvvvd+KruL4Pp01Uorp5tYZA5HZfVhVmY1cD0iiuGPtY3v6nCcpcuVXNI+2u97SeGYGi8s+sdbDIdU/PCmKs0ET2Fgy4Yh5uFMRxCCNzuqbjdU8nNXUJR0XXU1/+Gzs636Op6j0hEBQo4HBNwOsfh9S6hoOAqzGbn8AceJppmzLj33n3vc8wxKqnfscfue1+d++9X/0sJ990XjywaDItFjboTiFkYqRPGHo+yHo4epmYKKIsmN/fzjaIPJHrnWlysQmf13E1PPIFp+nRs5iFW638RMjK+fCE9++xh39Yn/0c0wLrrLmXF7M8A5XNiCIZGV6AL0eelZpZ5UFeg0+okNy2XFn8LJmGKPaAGA0lLm0hV1S8AkFISCrXR2vpX2ttfpa9vG1u3fpOtW7+J2ezBavWSlXUSRUU34HSOx2I5yDuskZCWNnyc/nAIEa+tPhSDzDPo96PHntJ+3/mOslpMI4hvKSxMzlx8MKK7boqLkyfqzz9/bM5nlPj6zK/HyhHvEyEOiFiAIRgxuvt7CfakJxV6S6U8s5wWfwvptvT9zk57uCKEwGbzUlR0PUVF1yOlpLPzbbq6lhMKtREIqPmQxkblInE4KsjOPoOCgq+TllaF2ZxhtHUq9947YMSr+7wHWBjf+MbIj/vv/z50Pq2DhUQL4xBm0bhFLBq3aKxPYwCGYGg0tvUgA27mHj/0PmWeMlY1rDosIqRGCyEEWVknkJUVz90UCDTS2fkOgcBuurs/pLHxMRoaHgbAZHKRljaJrKwTsFg85OScjdt9UFbvPXCcdtqATTELI3UOY39InOc4WDlMBONgxRAMjZauXgh6hrUwyjJUOKUxf/HlYrcXkp9/Sex1MNhKR8dSAoE6gsF6enpWsWfPz4EotbU/oqDgGnp71+JyHUFa2hQsFg/Z2adht+/HuopDjOqcauxmO5XZw0zeHgqUlCTXGDE4oIyqYAghTgN+CZiB30sp70t5/2rgZ4Ce+e6/pZS/1967CtDX9/9ISvnUaJ5rV18vVkqGDbHXI6UO5wipA4HN5iU/PzmUVsoooVALGzdeREPDb0hPn01z81+IRPTV5wKHoxyP5xhKSm7FYsnCZsvDbD485ppmFsyk7wd9h7777hvfULnDDva5lkOUURMMIYQZ+DVwMlAHrBRCvCil/DRl12ellDelfDYb+A+gBpDAau2zHYwSAdlDmtk9bOScXn/ZsDAOPEKYsNnymTnzbaJRP2azi2g0TCTSQyBQR1vbS/T2fkJr69/Yu/dp7TMWHI4KpJRkZ59MZuYiTCYnJpMDt3s6NtsXq3h4sHHIiwWoCfxJk8b6LA5bRtPCmAtsk1LuABBC/Bk4B0gVjME4FXhDStmuffYN4DTgT6N0rgRFLx7L8DHMumAYcxhjhxACs1ZRzmSyYDJlYbVmxeY1gsFWWlv/hhAm+vq209e3HSkDNDU9SUPDb5OO5XbPIivrJJzOSlyuaaSnz8Zksg74TgMDA8VoCkYxsCfhdR0w2AzBEiHEQmALcIuUcs8Qnx3VWa6IqRf3PlxNhoVx8KMisgZGBoXD3QQCe4hG+wmHe+jufp/29te0uRGV6M1iyaKw8Bvk5JwFCHy+9YRCLWRnn4HJ5GDPnp9TVvY9XC5jhGtweDLWk94vAX+SUgaEEDcATwH7KH2WjBDieuB6gLJ95fgZgmAkiDQHybAPb2HkpuWSbksnx3kQLigzGBaLJQOLJV4WNivreMrLv080GiIQUBPrLS3PsmfPz9mz52dJn921626EsCFlgPb2l5kw4UHc7pmkpVVjMh3kYagGBl8ioykY9UBiKEMJ8cltAKSUbQkvfw/cn/DZ41M++85gXyKlfAR4BKCmpkYOts++6An0AuBJG14whBAsvXIp5Z7yz/M1BgchJpMVp7MCp7OCvLzzCQZb6On5CCkl6elHYjKlUVt7D/39eygpuZnNm69k82aVqkEIC2lpU3A4SpFaEZ68vIvJz78sNp8QCDTh93+Kx7MQk2msx2cGBl+M0byDVwJVQohxKAG4GLg0cQchRKGUslF7eTawSfv7deBeIYSekesU4I7ROtG9nSp1RY5733MTI159afCVxGbLJScnuW5FZeWDsb/nzt2M378Jn28jPt8Geno+JhCoA8yEw51s3nwFtbV3EwzuxeWaSm/vOqJRP05nNQUFV5OTsxiXawpSBhHCdnhMVBscMoyaYEgpw0KIm1Cdvxl4XEq5UQhxD7BKSvki8B0hxNlAGGgHrtY+2y6E+E+U6ADco0+AjwZ1e5WFkZM+uom7DL76mEw23O4ZuN0Dq+dJGWHPnp/T2fkOmZkn0tu7Fq/3XLKzT6G+/mF27vw+O3d+H4slk3C4E7PZQ3r6kbjdM3E4KrDZirDbi7HbS7HbSxBCEA53YTI5MJkOTOoHA4PhEDK1nu1XmJqaGrlq1ar9/txTSz/k6veO4ofjX+Y/rzhj3x8wMPgcBAINtLW9TE/PKmy2QkKhvfT0rMHnW0c0mlw1LTNzERZLFq2tfwXA6z2PysoHiUb7iUYDOJ3jY9FiBgZfBCHEaillzUj2NZyqQGO7ckkVZBrhsgajh91eRFHRdUByvQU9QWMw2EAgUI/P9wm7d99PNNpPael3kTJCff1DMfEAsNkKGT/+p5hMTvr6thCJ+HA4ysnImI/DUQoIhLDuOyOwgcF+YAgG0NypXFKFXsMlZXDg0RM02mxe3O7p5OScTnHxTUgZxWJRg5iCgivp6FiK1ZqHECZqa+9l8+bEkp1m9PDgRDIzF5GZeRwmkwuXayomkwOHoxyncxzhcDdCWA6b1fAGXxxDMIDWbiUYxYZgGBwkpLqb3O7puN3xQi25uUvo6VmN2ezC4RiH2eyiv7+Wrq5lhEJtgCQU6qC5+X/Ytevt1KOTnX0qHR1vApKsrBMpKbmVcLidSMSHzZZPJOLD45n/5ZTbNThkMAQDaOvtAQHFuYZLyuCrgclkx+OZn7TN6RyP0zk+adu4cf8JSMLhTny+DUgZoqXlf2lu/hP5+ZdjsWSyd+/TfPLJyQO+QwgrLtdUQqF2cnLOIBhsoq9vKzk5Z5GRcTRpaZNxOMYZ4cKHEcaVBjr9veCCzH2swzAw+KqhwnYFVms2mZkLAcjKOpHq6odj+4wbdzetrS/hdI7Has0hGGxBCDN79z6N378Zh6OcxsbHsVgycLmmsHv3z9DdX0LYSEurxmzOIBzuIhhsID19LpmZx2K3lxAKtWO3l+JwlGGxZON0TjBCib/CGIKBylSLC6OKnsFhidnsSsoO7HROACAjY05sWyTiRwgrJpOVcLhbW4uyCb9/E37/ZiIRH05nLh7P0XR1LWfnztcH/S6rNU9zr80mL+9iwuEOpAzhcFTgdFbR3v4awWADmZnH43CMQ4gRVAo0OGAYggH0BHowhd2YjJvTwGBQEifGLZYMMjLmkZExdPGYcLibYHAvVms2/f27CAabCAQa6epaht+/ibq6n7Nnz0+TPmO15hMK7Y29NpnSSEubhM2WRzQaJC/vEjIy5iFlGLd7JuFwJ6FQC3Z7iTFxf4AwBAPwhXqxRg13lIHBl4XK3aWSdFqt8dxremLIQKCJ9vbXsNuLMZmc+HzraG9/g+zsU8nMXEhX13v4fJ/i928iGGwhGvWzZUs8HNlmKyIY3Eti4sicnMV4PPNpbX0JIQQu1zS83nNxu48EIBLpQQhrLPLMYP8xFu4BrqsuwVyyiu4fbx2FszIwMPiiqFrw7xAKNROJ+Glre4m0tEmkpU0iEKinr28rzc1/Jhrtw+GYgNnsxufbAEQQwoqUodix0tPnImWQcLiTzMwTSE+fjcWSTTTaj5Qh7PYSXK4jsNvLDov5FmPh3n4SiPaSYzZGHQYGByuqFvyi2OvCwq8P2Gf8+J8QCDTgds9ECEEo1EZHx1J6ej7GZHJitWYRCnXQ3v4aFksODkcFra1/panp8UG/02LJpKDgWhyOUoLB5pilZLFkY7cXEQjsIRLp00KbK3A4KrDbizCZ7EgpD0mxOewFQ0qomtKLw4iQMjD4SmOz5SdVUbRac8jLu4i8vIuS9hs37q7Y31JKAoF6IpEeTCYHQljo76/F7/+Uzs63qav7BcrtJVDFP/eNbtGYTC7S02fj9Z5LV9cyIhE/aWnVlJTcjN2uEnmHQm34/ZvIyJiL2ewiFOqkufkZ3O6ZeDwLvmCLfPkYLimg5pEa8lx5vHLZK6NwVgYGBl9V1DyJiu7S68cHg3sJBhux28sxm91EIl3099fS37+TYHCvNldiIRLppaXlBQKBWuz2Mmy2Anp71yJlcMD32O1leDzH0Nb2MpFIF6BW6VuteUgZxG4vweNZgMdzDHZ7MZGID59vE+npswgGm+jtXTMgy/JIMVxS+0lvsJcJ2RPG+jQMDAwOMhItFovFE/s/La06YS9vLBQ5lfHjf0YgUIvDMR4hBIFAvTbXopJNmkwu7PYidu/+CR0dS/F6F1NUdCPt7a/Q1vYygUAdQlhpb3+d+vpfAeBwjCMUaiMS6cbpnEggsBuTycHRR9ePeu4wQzBQguG2Gi4pAwODLxeTyZIkJnZ7MaWltw3YLy/vwqTXHs9RjBt3T+x1NBqit3cdXV3L6OpajtnsIiNjHk1NfyAj43wqKu46IIkmDcEAeoI9uG2GYBgYGBycmExWMjJqyMioobT0ltj24uIbD+x5HNBvO0hZXL2YmqIRufAMDAwMDlsMCwN4+rynx/oUDAwMDA56DAvDwMDAwGBEGIJhYGBgYDAiDMEwMDAwMBgRoyoYQojThBCfCSG2CSG+N8j7twohPhVCfCKEeFMIUZ7wXkQIsVb79+JonqeBgYGBwb4ZtUlvIYQZ+DVwMlAHrBRCvCil/DRhtzVAjZTSL4T4JnA/oK/j75NSzhyt8zMwMDAw2D9G08KYC2yTUu6Qai38n4FzEneQUr4tpfRrL1cAJaN4PgYGBgYGX4DRFIxiYE/C6zpt21BcC7ya8NohhFglhFghhDh3qA8JIa7X9lvV0tLyxc7YwMDAwGBIDop1GEKIy4Ea4LiEzeVSynohxHjgLSHEeinl9tTPSikfAR4BlXzwgJywgYGBwWHIaApGPVCa8LpE25aEEOIk4AfAcVJP/Y5VAAAF6UlEQVTKgL5dSlmv/b9DCPEOcCQwQDASWb16dasQovZznq8XaP2cnz1UMdpkIEabJGO0x0C+am1Svu9dFKOW3lwIYQG2ACeihGIlcKmUcmPCPkcCzwOnSSm3JmzPAvxSyoAQwgt8AJyTMmH+ZZ/vqpGm+D1cMNpkIEabJGO0x0AO5TYZNQtDShkWQtwEvA6YgcellBuFEPcAq6SULwI/A9zAc1p1qt1SyrOBycDvhBBR1DzLfaMpFgYGBgYG+2ZU5zCklK8Ar6Rs+/eEv08a4nPvA9NG89wMDAwMDPYPY6V3nEfG+gQOQow2GYjRJskY7TGQQ7ZNDqkSrQYGBgYGo4dhYRgYGBgYjIjDXjD2le/qcEEIsUsIsV7L3bVK25YthHhDCLFV+z9rrM9zNBFCPC6EaBZCbEjYNmgbCMVD2n3ziRBi1tid+egxRJvcJYSoT8j1dkbCe3dobfKZEOLUsTnr0UUIUSqEeFvLg7dRCPGv2vZD/l45rAUjId/V6cARwCVCiCPG9qzGlEVSypkJIYHfA96UUlYBb2qvD2WeBE5L2TZUG5wOVGn/rgd+c4DO8UDzJAPbBOBB7V6ZqQW3oD07FwNTtM88rD1jhxph4DYp5RHAUcCN2m8/5O+Vw1owGEG+q8Occ4CntL+fAoZM0XIoIKX8J9CesnmoNjgH+INUrAAyhRCFB+ZMDxxDtMlQnAP8WUoZkFLuBLahnrFDCillo5TyY+3vHmATKu3RIX+vHO6Csb/5rg5lJPAPIcRqIcT12rZ8KWWj9ncTkD82pzamDNUGh/u9c5PmXnk8wVV52LWJEKIClYXiQw6De+VwFwyDOMdIKWehzOcbhRALE9+UKpzusA6pM9ogxm+ACcBMoBH4+dieztgghHAD/wvcLKXsTnzvUL1XDnfBGFG+q8OBhNxdzcALKFfCXt101v5vHrszHDOGaoPD9t6RUu6VUkaklFHgUeJup8OmTYQQVpRYPCOl/Ku2+ZC/Vw53wVgJVAkhxgkhbKgJu8Ouup8QwiWESNf/Bk4BNqDa4iptt6uA/xubMxxThmqDF4ErtQiYo4CuBHfEIU2K//1rqHsFVJtcLISwCyHGoSZ5PzrQ5zfaCJXH6DFgk5TygYS3Dvl75aBIbz5WDJXvaoxPayzIB17Q8nlZgP+RUr4mhFgJ/EUIcS1QC1w4huc46ggh/gQcD3iFEHXAfwD3MXgbvAKcgZrY9QNfP+AnfAAYok2OF0LMRLlcdgE3AGi54v4CfIqKJLpRShkZi/MeZRYAVwDrhRBrtW3f5zC4V4yV3gYGBgYGI+Jwd0kZGBgYGIwQQzAMDAwMDEaEIRgGBgYGBiPCEAwDAwMDgxFhCIaBgYGBwYgwBMPA4CBACHG8EOLvY30eBgbDYQiGgYGBgcGIMATDwGA/EEJcLoT4SKsD8TshhFkI0SuEeFCrjfCmECJX23emEGKFlqTvhYT6CJVCiKVCiHVCiI+FEBO0w7uFEM8LITYLIZ7RVhQbGBw0GIJhYDBChBCTgYuABVLKmUAEuAxwAauklFOAd1GroQH+APyblHI6sD5h+zPAr6WUM4D5qAR+oLKe3oyqzTIetaLYwOCg4bBODWJgsJ+cCMwGVmqDfycqwVwUeFbb52ngr0IID5AppXxX2/4U8JyWs6tYSvkCgJSyH0A73kdSyjrt9VqgAlg++j/LwGBkGIJhYDByBPCUlPKOpI1C3Jmy3+fNtxNI+DuC8XwaHGQYLikDg5HzJnC+ECIPYjWcy1HP0fnaPpcCy6WUXUCHEOJYbfsVwLtahbY6IcS52jHsQoi0A/orDAw+J8YIxsBghEgpPxVC/BBVmdAEhIAbAR8wV3uvGTXPASrF9W81QdhBPEvpFcDvhBD3aMe44AD+DAODz42RrdbA4AsihOiVUrrH+jwMDEYbwyVlYGBgYDAiDAvDwMDAwGBEGBaGgYGBgcGIMATDwMDAwGBEGIJhYGBgYDAiDMEwMDAwMBgRhmAYGBgYGIwIQzAMDAwMDEbE/wd0UCIXa+eXewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 397us/sample - loss: 0.6321 - acc: 0.8216\n",
      "Loss: 0.6321213007097056 Accuracy: 0.8215992\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9732 - acc: 0.3945\n",
      "Epoch 00001: val_loss improved from inf to 1.97618, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/001-1.9762.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.9731 - acc: 0.3945 - val_loss: 1.9762 - val_acc: 0.3235\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3951 - acc: 0.5886\n",
      "Epoch 00002: val_loss improved from 1.97618 to 1.31451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/002-1.3145.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.3952 - acc: 0.5887 - val_loss: 1.3145 - val_acc: 0.6082\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1831 - acc: 0.6592\n",
      "Epoch 00003: val_loss did not improve from 1.31451\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 1.1829 - acc: 0.6592 - val_loss: 1.3597 - val_acc: 0.5702\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0488 - acc: 0.7017\n",
      "Epoch 00004: val_loss improved from 1.31451 to 0.98267, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/004-0.9827.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.0487 - acc: 0.7017 - val_loss: 0.9827 - val_acc: 0.7268\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9498 - acc: 0.7305\n",
      "Epoch 00005: val_loss improved from 0.98267 to 0.93680, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/005-0.9368.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.9499 - acc: 0.7304 - val_loss: 0.9368 - val_acc: 0.7181\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8710 - acc: 0.7538\n",
      "Epoch 00006: val_loss improved from 0.93680 to 0.84922, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/006-0.8492.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.8710 - acc: 0.7538 - val_loss: 0.8492 - val_acc: 0.7682\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8084 - acc: 0.7752\n",
      "Epoch 00007: val_loss improved from 0.84922 to 0.84158, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/007-0.8416.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.8085 - acc: 0.7752 - val_loss: 0.8416 - val_acc: 0.7519\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7603 - acc: 0.7897\n",
      "Epoch 00008: val_loss improved from 0.84158 to 0.77536, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/008-0.7754.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7609 - acc: 0.7895 - val_loss: 0.7754 - val_acc: 0.7682\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.8006\n",
      "Epoch 00009: val_loss improved from 0.77536 to 0.76066, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/009-0.7607.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.7180 - acc: 0.8006 - val_loss: 0.7607 - val_acc: 0.7878\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6796 - acc: 0.8116\n",
      "Epoch 00010: val_loss improved from 0.76066 to 0.66894, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/010-0.6689.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.6795 - acc: 0.8116 - val_loss: 0.6689 - val_acc: 0.8183\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8196\n",
      "Epoch 00011: val_loss improved from 0.66894 to 0.65904, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/011-0.6590.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.6510 - acc: 0.8196 - val_loss: 0.6590 - val_acc: 0.8281\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6212 - acc: 0.8295\n",
      "Epoch 00012: val_loss improved from 0.65904 to 0.65184, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/012-0.6518.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.6212 - acc: 0.8295 - val_loss: 0.6518 - val_acc: 0.8202\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8353\n",
      "Epoch 00013: val_loss did not improve from 0.65184\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.6002 - acc: 0.8353 - val_loss: 0.6822 - val_acc: 0.7985\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8410\n",
      "Epoch 00014: val_loss improved from 0.65184 to 0.61777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/014-0.6178.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.5735 - acc: 0.8410 - val_loss: 0.6178 - val_acc: 0.8300\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5538 - acc: 0.8463\n",
      "Epoch 00015: val_loss improved from 0.61777 to 0.60357, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/015-0.6036.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.5539 - acc: 0.8462 - val_loss: 0.6036 - val_acc: 0.8304\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8516\n",
      "Epoch 00016: val_loss improved from 0.60357 to 0.54753, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/016-0.5475.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.5382 - acc: 0.8516 - val_loss: 0.5475 - val_acc: 0.8498\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8555\n",
      "Epoch 00017: val_loss improved from 0.54753 to 0.51408, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/017-0.5141.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.5197 - acc: 0.8555 - val_loss: 0.5141 - val_acc: 0.8595\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8583\n",
      "Epoch 00018: val_loss did not improve from 0.51408\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5095 - acc: 0.8583 - val_loss: 0.5244 - val_acc: 0.8588\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8633\n",
      "Epoch 00019: val_loss did not improve from 0.51408\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.4909 - acc: 0.8633 - val_loss: 0.5360 - val_acc: 0.8546\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8678\n",
      "Epoch 00020: val_loss did not improve from 0.51408\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.4779 - acc: 0.8677 - val_loss: 0.6114 - val_acc: 0.8157\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.8713\n",
      "Epoch 00021: val_loss improved from 0.51408 to 0.50855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/021-0.5085.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.4623 - acc: 0.8712 - val_loss: 0.5085 - val_acc: 0.8644\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8735\n",
      "Epoch 00022: val_loss did not improve from 0.50855\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4551 - acc: 0.8734 - val_loss: 0.5160 - val_acc: 0.8574\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8767\n",
      "Epoch 00023: val_loss improved from 0.50855 to 0.46063, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/023-0.4606.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.4441 - acc: 0.8766 - val_loss: 0.4606 - val_acc: 0.8775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.8793\n",
      "Epoch 00024: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.4367 - acc: 0.8793 - val_loss: 0.5105 - val_acc: 0.8623\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8844\n",
      "Epoch 00025: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4230 - acc: 0.8843 - val_loss: 0.5984 - val_acc: 0.8199\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8856\n",
      "Epoch 00026: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4127 - acc: 0.8856 - val_loss: 0.4856 - val_acc: 0.8717\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8872\n",
      "Epoch 00027: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4059 - acc: 0.8872 - val_loss: 0.4930 - val_acc: 0.8672\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8903\n",
      "Epoch 00028: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3991 - acc: 0.8903 - val_loss: 0.5424 - val_acc: 0.8425\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.8906\n",
      "Epoch 00029: val_loss did not improve from 0.46063\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3892 - acc: 0.8906 - val_loss: 0.4923 - val_acc: 0.8607\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8942\n",
      "Epoch 00030: val_loss improved from 0.46063 to 0.45938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/030-0.4594.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3804 - acc: 0.8941 - val_loss: 0.4594 - val_acc: 0.8691\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8956\n",
      "Epoch 00031: val_loss improved from 0.45938 to 0.45506, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/031-0.4551.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3771 - acc: 0.8956 - val_loss: 0.4551 - val_acc: 0.8710\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8973\n",
      "Epoch 00032: val_loss did not improve from 0.45506\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.3667 - acc: 0.8973 - val_loss: 0.5534 - val_acc: 0.8330\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8979\n",
      "Epoch 00033: val_loss did not improve from 0.45506\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.3622 - acc: 0.8978 - val_loss: 0.4774 - val_acc: 0.8672\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.9001\n",
      "Epoch 00034: val_loss did not improve from 0.45506\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3556 - acc: 0.9001 - val_loss: 0.4665 - val_acc: 0.8640\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.9026\n",
      "Epoch 00035: val_loss improved from 0.45506 to 0.41670, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/035-0.4167.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3503 - acc: 0.9026 - val_loss: 0.4167 - val_acc: 0.8870\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.9036\n",
      "Epoch 00036: val_loss did not improve from 0.41670\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.3427 - acc: 0.9036 - val_loss: 0.4214 - val_acc: 0.8889\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.9061\n",
      "Epoch 00037: val_loss improved from 0.41670 to 0.39439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/037-0.3944.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3378 - acc: 0.9061 - val_loss: 0.3944 - val_acc: 0.8859\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.9052\n",
      "Epoch 00038: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3341 - acc: 0.9052 - val_loss: 0.4805 - val_acc: 0.8656\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.9074\n",
      "Epoch 00039: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3275 - acc: 0.9074 - val_loss: 0.4026 - val_acc: 0.8940\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.9093\n",
      "Epoch 00040: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3197 - acc: 0.9093 - val_loss: 0.4227 - val_acc: 0.8826\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.9100\n",
      "Epoch 00041: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.3186 - acc: 0.9100 - val_loss: 0.4629 - val_acc: 0.8707\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9125\n",
      "Epoch 00042: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3114 - acc: 0.9125 - val_loss: 0.4415 - val_acc: 0.8735\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9110\n",
      "Epoch 00043: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3083 - acc: 0.9109 - val_loss: 0.4286 - val_acc: 0.8805\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9149\n",
      "Epoch 00044: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3015 - acc: 0.9149 - val_loss: 0.4377 - val_acc: 0.8719\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.9146\n",
      "Epoch 00045: val_loss did not improve from 0.39439\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2987 - acc: 0.9145 - val_loss: 0.4359 - val_acc: 0.8789\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9167\n",
      "Epoch 00046: val_loss improved from 0.39439 to 0.38684, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/046-0.3868.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2955 - acc: 0.9166 - val_loss: 0.3868 - val_acc: 0.8952\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9176\n",
      "Epoch 00047: val_loss improved from 0.38684 to 0.38486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/047-0.3849.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.2912 - acc: 0.9175 - val_loss: 0.3849 - val_acc: 0.8963\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9178\n",
      "Epoch 00048: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2875 - acc: 0.9178 - val_loss: 0.4425 - val_acc: 0.8817\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9191\n",
      "Epoch 00049: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.2843 - acc: 0.9191 - val_loss: 0.4031 - val_acc: 0.8917\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9202\n",
      "Epoch 00050: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2807 - acc: 0.9202 - val_loss: 0.4405 - val_acc: 0.8775\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9219\n",
      "Epoch 00051: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2748 - acc: 0.9219 - val_loss: 0.4662 - val_acc: 0.8647\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9212\n",
      "Epoch 00052: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2726 - acc: 0.9212 - val_loss: 0.4845 - val_acc: 0.8633\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9243\n",
      "Epoch 00053: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2670 - acc: 0.9243 - val_loss: 0.4049 - val_acc: 0.8940\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9252\n",
      "Epoch 00054: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2617 - acc: 0.9252 - val_loss: 0.4932 - val_acc: 0.8598\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9250\n",
      "Epoch 00055: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2623 - acc: 0.9250 - val_loss: 0.3949 - val_acc: 0.8928\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9256\n",
      "Epoch 00056: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2570 - acc: 0.9256 - val_loss: 0.4015 - val_acc: 0.8831\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9270\n",
      "Epoch 00057: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.2549 - acc: 0.9270 - val_loss: 0.4402 - val_acc: 0.8756\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9266\n",
      "Epoch 00058: val_loss did not improve from 0.38486\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2539 - acc: 0.9267 - val_loss: 0.3910 - val_acc: 0.8917\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9292\n",
      "Epoch 00059: val_loss improved from 0.38486 to 0.36755, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/059-0.3676.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2463 - acc: 0.9291 - val_loss: 0.3676 - val_acc: 0.9033\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9310\n",
      "Epoch 00060: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2459 - acc: 0.9310 - val_loss: 0.4091 - val_acc: 0.8877\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9288\n",
      "Epoch 00061: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2462 - acc: 0.9288 - val_loss: 0.3716 - val_acc: 0.9017\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9302\n",
      "Epoch 00062: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2421 - acc: 0.9301 - val_loss: 0.3680 - val_acc: 0.8970\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9311\n",
      "Epoch 00063: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2383 - acc: 0.9311 - val_loss: 0.4087 - val_acc: 0.8814\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9329\n",
      "Epoch 00064: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2336 - acc: 0.9329 - val_loss: 0.3960 - val_acc: 0.8931\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9345\n",
      "Epoch 00065: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2306 - acc: 0.9345 - val_loss: 0.4377 - val_acc: 0.8835\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9340\n",
      "Epoch 00066: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2292 - acc: 0.9340 - val_loss: 0.3888 - val_acc: 0.8977\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9359\n",
      "Epoch 00067: val_loss did not improve from 0.36755\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2265 - acc: 0.9359 - val_loss: 0.3716 - val_acc: 0.8949\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9358\n",
      "Epoch 00068: val_loss improved from 0.36755 to 0.35529, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/068-0.3553.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2255 - acc: 0.9358 - val_loss: 0.3553 - val_acc: 0.9017\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9360\n",
      "Epoch 00069: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2211 - acc: 0.9359 - val_loss: 0.4006 - val_acc: 0.8926\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9375\n",
      "Epoch 00070: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2196 - acc: 0.9375 - val_loss: 0.4266 - val_acc: 0.8796\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9383\n",
      "Epoch 00071: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2143 - acc: 0.9383 - val_loss: 0.3762 - val_acc: 0.8970\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9379\n",
      "Epoch 00072: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2154 - acc: 0.9379 - val_loss: 0.3800 - val_acc: 0.8984\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9388\n",
      "Epoch 00073: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2111 - acc: 0.9388 - val_loss: 0.3880 - val_acc: 0.8991\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9399\n",
      "Epoch 00074: val_loss did not improve from 0.35529\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2087 - acc: 0.9399 - val_loss: 0.3800 - val_acc: 0.8991\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9414\n",
      "Epoch 00075: val_loss improved from 0.35529 to 0.34330, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/075-0.3433.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2066 - acc: 0.9414 - val_loss: 0.3433 - val_acc: 0.9126\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9416\n",
      "Epoch 00076: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2037 - acc: 0.9416 - val_loss: 0.3834 - val_acc: 0.9003\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9418\n",
      "Epoch 00077: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.2000 - acc: 0.9418 - val_loss: 0.3861 - val_acc: 0.9003\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9428\n",
      "Epoch 00078: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1985 - acc: 0.9429 - val_loss: 0.3730 - val_acc: 0.8968\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9419\n",
      "Epoch 00079: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1976 - acc: 0.9419 - val_loss: 0.3609 - val_acc: 0.9019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9435\n",
      "Epoch 00080: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1961 - acc: 0.9435 - val_loss: 0.4036 - val_acc: 0.8945\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9446\n",
      "Epoch 00081: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1929 - acc: 0.9447 - val_loss: 0.3565 - val_acc: 0.9024\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9451\n",
      "Epoch 00082: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1904 - acc: 0.9450 - val_loss: 0.3654 - val_acc: 0.9038\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9460\n",
      "Epoch 00083: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1906 - acc: 0.9460 - val_loss: 0.3982 - val_acc: 0.8977\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9469\n",
      "Epoch 00084: val_loss did not improve from 0.34330\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1823 - acc: 0.9469 - val_loss: 0.3830 - val_acc: 0.8947\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9454\n",
      "Epoch 00085: val_loss improved from 0.34330 to 0.33942, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv_checkpoint/085-0.3394.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1880 - acc: 0.9454 - val_loss: 0.3394 - val_acc: 0.9110\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9487\n",
      "Epoch 00086: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1802 - acc: 0.9487 - val_loss: 0.4175 - val_acc: 0.8861\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9480\n",
      "Epoch 00087: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1795 - acc: 0.9480 - val_loss: 0.3916 - val_acc: 0.9012\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9472\n",
      "Epoch 00088: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1790 - acc: 0.9472 - val_loss: 0.5248 - val_acc: 0.8588\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9491\n",
      "Epoch 00089: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1771 - acc: 0.9491 - val_loss: 0.4408 - val_acc: 0.8835\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9496\n",
      "Epoch 00090: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1739 - acc: 0.9496 - val_loss: 0.3946 - val_acc: 0.9015\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9504\n",
      "Epoch 00091: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1722 - acc: 0.9504 - val_loss: 0.3753 - val_acc: 0.9012\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9506\n",
      "Epoch 00092: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1719 - acc: 0.9506 - val_loss: 0.3655 - val_acc: 0.9059\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9502\n",
      "Epoch 00093: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1684 - acc: 0.9501 - val_loss: 0.3838 - val_acc: 0.8977\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9492\n",
      "Epoch 00094: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1704 - acc: 0.9492 - val_loss: 0.3932 - val_acc: 0.8947\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9518\n",
      "Epoch 00095: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1644 - acc: 0.9518 - val_loss: 0.3548 - val_acc: 0.9075\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9518\n",
      "Epoch 00096: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1643 - acc: 0.9518 - val_loss: 0.4898 - val_acc: 0.8644\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9529\n",
      "Epoch 00097: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1640 - acc: 0.9529 - val_loss: 0.3641 - val_acc: 0.8968\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9543\n",
      "Epoch 00098: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1611 - acc: 0.9543 - val_loss: 0.3870 - val_acc: 0.9017\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9545\n",
      "Epoch 00099: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1590 - acc: 0.9545 - val_loss: 0.3775 - val_acc: 0.9001\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9562\n",
      "Epoch 00100: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1543 - acc: 0.9561 - val_loss: 0.3813 - val_acc: 0.9059\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9540\n",
      "Epoch 00101: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1607 - acc: 0.9539 - val_loss: 0.4464 - val_acc: 0.8845\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9556\n",
      "Epoch 00102: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1534 - acc: 0.9556 - val_loss: 0.3959 - val_acc: 0.8949\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9554\n",
      "Epoch 00103: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1545 - acc: 0.9554 - val_loss: 0.3551 - val_acc: 0.9096\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9561\n",
      "Epoch 00104: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1507 - acc: 0.9561 - val_loss: 0.4122 - val_acc: 0.8954\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9565\n",
      "Epoch 00105: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1502 - acc: 0.9565 - val_loss: 0.3828 - val_acc: 0.9010\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9559\n",
      "Epoch 00106: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1485 - acc: 0.9559 - val_loss: 0.3514 - val_acc: 0.9085\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9559\n",
      "Epoch 00107: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1515 - acc: 0.9559 - val_loss: 0.3734 - val_acc: 0.9040\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9579\n",
      "Epoch 00108: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1413 - acc: 0.9579 - val_loss: 0.4389 - val_acc: 0.8880\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9573\n",
      "Epoch 00109: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1448 - acc: 0.9572 - val_loss: 0.5460 - val_acc: 0.8544\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9570\n",
      "Epoch 00110: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1439 - acc: 0.9571 - val_loss: 0.4234 - val_acc: 0.8945\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9575\n",
      "Epoch 00111: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1469 - acc: 0.9575 - val_loss: 0.4099 - val_acc: 0.8952\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9603\n",
      "Epoch 00112: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1399 - acc: 0.9602 - val_loss: 0.4010 - val_acc: 0.8938\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9596\n",
      "Epoch 00113: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1376 - acc: 0.9596 - val_loss: 0.3947 - val_acc: 0.8966\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9607\n",
      "Epoch 00114: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1357 - acc: 0.9607 - val_loss: 0.3479 - val_acc: 0.9138\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9611\n",
      "Epoch 00115: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1352 - acc: 0.9610 - val_loss: 0.4583 - val_acc: 0.8812\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9600\n",
      "Epoch 00116: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1360 - acc: 0.9600 - val_loss: 0.4498 - val_acc: 0.8826\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.9606\n",
      "Epoch 00117: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1365 - acc: 0.9606 - val_loss: 0.3881 - val_acc: 0.9064\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9635\n",
      "Epoch 00118: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1280 - acc: 0.9634 - val_loss: 0.3852 - val_acc: 0.9052\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9603\n",
      "Epoch 00119: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1343 - acc: 0.9602 - val_loss: 0.3742 - val_acc: 0.9073\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9618\n",
      "Epoch 00120: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1308 - acc: 0.9618 - val_loss: 0.3837 - val_acc: 0.9026\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9642\n",
      "Epoch 00121: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1256 - acc: 0.9642 - val_loss: 0.3536 - val_acc: 0.9103\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9636\n",
      "Epoch 00122: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1251 - acc: 0.9636 - val_loss: 0.4237 - val_acc: 0.8931\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9646\n",
      "Epoch 00123: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1227 - acc: 0.9646 - val_loss: 0.3767 - val_acc: 0.9071\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9636\n",
      "Epoch 00124: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1231 - acc: 0.9635 - val_loss: 0.3991 - val_acc: 0.9005\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9637\n",
      "Epoch 00125: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1249 - acc: 0.9637 - val_loss: 0.4148 - val_acc: 0.8963\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9645\n",
      "Epoch 00126: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1237 - acc: 0.9645 - val_loss: 0.3512 - val_acc: 0.9103\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9661\n",
      "Epoch 00127: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1177 - acc: 0.9660 - val_loss: 0.4329 - val_acc: 0.8938\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9651\n",
      "Epoch 00128: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1208 - acc: 0.9651 - val_loss: 0.4320 - val_acc: 0.8928\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9671\n",
      "Epoch 00129: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1148 - acc: 0.9670 - val_loss: 0.3948 - val_acc: 0.9005\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9656\n",
      "Epoch 00130: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1186 - acc: 0.9656 - val_loss: 0.3919 - val_acc: 0.9036\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9661\n",
      "Epoch 00131: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1177 - acc: 0.9661 - val_loss: 0.3801 - val_acc: 0.9047\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9683\n",
      "Epoch 00132: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1121 - acc: 0.9683 - val_loss: 0.3863 - val_acc: 0.9047\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9656\n",
      "Epoch 00133: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1193 - acc: 0.9656 - val_loss: 0.3649 - val_acc: 0.9136\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9670\n",
      "Epoch 00134: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1145 - acc: 0.9670 - val_loss: 0.4049 - val_acc: 0.9036\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9682\n",
      "Epoch 00135: val_loss did not improve from 0.33942\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1093 - acc: 0.9682 - val_loss: 0.3740 - val_acc: 0.9082\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 450us/sample - loss: 0.4086 - acc: 0.8837\n",
      "Loss: 0.408611616414903 Accuracy: 0.8836968\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7259 - acc: 0.4746\n",
      "Epoch 00001: val_loss improved from inf to 1.59963, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/001-1.5996.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.7258 - acc: 0.4746 - val_loss: 1.5996 - val_acc: 0.5127\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0789 - acc: 0.6960\n",
      "Epoch 00002: val_loss improved from 1.59963 to 0.92916, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/002-0.9292.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 1.0788 - acc: 0.6960 - val_loss: 0.9292 - val_acc: 0.7421\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8362 - acc: 0.7730\n",
      "Epoch 00003: val_loss improved from 0.92916 to 0.79499, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/003-0.7950.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.8363 - acc: 0.7729 - val_loss: 0.7950 - val_acc: 0.7724\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.8133\n",
      "Epoch 00004: val_loss improved from 0.79499 to 0.62945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/004-0.6294.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.6931 - acc: 0.8133 - val_loss: 0.6294 - val_acc: 0.8346\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8376\n",
      "Epoch 00005: val_loss improved from 0.62945 to 0.55552, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/005-0.5555.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.6022 - acc: 0.8376 - val_loss: 0.5555 - val_acc: 0.8458\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.8547\n",
      "Epoch 00006: val_loss improved from 0.55552 to 0.52628, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/006-0.5263.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.5342 - acc: 0.8547 - val_loss: 0.5263 - val_acc: 0.8491\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8672\n",
      "Epoch 00007: val_loss did not improve from 0.52628\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.4859 - acc: 0.8672 - val_loss: 0.6718 - val_acc: 0.7992\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4414 - acc: 0.8798\n",
      "Epoch 00008: val_loss improved from 0.52628 to 0.41834, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/008-0.4183.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.4414 - acc: 0.8797 - val_loss: 0.4183 - val_acc: 0.8796\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8864\n",
      "Epoch 00009: val_loss improved from 0.41834 to 0.39882, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/009-0.3988.hdf5\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.4138 - acc: 0.8863 - val_loss: 0.3988 - val_acc: 0.8928\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8940\n",
      "Epoch 00010: val_loss did not improve from 0.39882\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.3850 - acc: 0.8940 - val_loss: 0.4090 - val_acc: 0.8812\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.9004\n",
      "Epoch 00011: val_loss improved from 0.39882 to 0.33388, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/011-0.3339.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.3625 - acc: 0.9004 - val_loss: 0.3339 - val_acc: 0.9054\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.9078\n",
      "Epoch 00012: val_loss did not improve from 0.33388\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.3390 - acc: 0.9078 - val_loss: 0.3636 - val_acc: 0.8991\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.9115\n",
      "Epoch 00013: val_loss improved from 0.33388 to 0.32687, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/013-0.3269.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.3215 - acc: 0.9115 - val_loss: 0.3269 - val_acc: 0.9045\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9160\n",
      "Epoch 00014: val_loss improved from 0.32687 to 0.32379, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/014-0.3238.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.3069 - acc: 0.9160 - val_loss: 0.3238 - val_acc: 0.9085\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9196\n",
      "Epoch 00015: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.2901 - acc: 0.9195 - val_loss: 0.3528 - val_acc: 0.8987\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9203\n",
      "Epoch 00016: val_loss improved from 0.32379 to 0.31172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/016-0.3117.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2848 - acc: 0.9203 - val_loss: 0.3117 - val_acc: 0.9154\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9276\n",
      "Epoch 00017: val_loss improved from 0.31172 to 0.30659, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/017-0.3066.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.2622 - acc: 0.9276 - val_loss: 0.3066 - val_acc: 0.9108\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9289\n",
      "Epoch 00018: val_loss did not improve from 0.30659\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.2587 - acc: 0.9289 - val_loss: 0.3264 - val_acc: 0.9064\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9301\n",
      "Epoch 00019: val_loss did not improve from 0.30659\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.2478 - acc: 0.9300 - val_loss: 0.3334 - val_acc: 0.9038\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9335\n",
      "Epoch 00020: val_loss improved from 0.30659 to 0.28005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/020-0.2800.hdf5\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.2387 - acc: 0.9335 - val_loss: 0.2800 - val_acc: 0.9180\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9359\n",
      "Epoch 00021: val_loss did not improve from 0.28005\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2294 - acc: 0.9359 - val_loss: 0.2874 - val_acc: 0.9201\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9373\n",
      "Epoch 00022: val_loss did not improve from 0.28005\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.2238 - acc: 0.9373 - val_loss: 0.3489 - val_acc: 0.8940\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9405\n",
      "Epoch 00023: val_loss improved from 0.28005 to 0.26356, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/023-0.2636.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.2112 - acc: 0.9405 - val_loss: 0.2636 - val_acc: 0.9241\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9428\n",
      "Epoch 00024: val_loss improved from 0.26356 to 0.26205, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/024-0.2621.hdf5\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.2050 - acc: 0.9428 - val_loss: 0.2621 - val_acc: 0.9269\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9424\n",
      "Epoch 00025: val_loss improved from 0.26205 to 0.25660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/025-0.2566.hdf5\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1992 - acc: 0.9424 - val_loss: 0.2566 - val_acc: 0.9273\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9454\n",
      "Epoch 00026: val_loss did not improve from 0.25660\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1918 - acc: 0.9454 - val_loss: 0.2648 - val_acc: 0.9257\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9479\n",
      "Epoch 00027: val_loss did not improve from 0.25660\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1863 - acc: 0.9479 - val_loss: 0.2676 - val_acc: 0.9220\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9476\n",
      "Epoch 00028: val_loss did not improve from 0.25660\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1829 - acc: 0.9476 - val_loss: 0.2815 - val_acc: 0.9157\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9494\n",
      "Epoch 00029: val_loss improved from 0.25660 to 0.24598, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/029-0.2460.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1776 - acc: 0.9494 - val_loss: 0.2460 - val_acc: 0.9269\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9529\n",
      "Epoch 00030: val_loss did not improve from 0.24598\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1698 - acc: 0.9528 - val_loss: 0.3041 - val_acc: 0.9078\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9543\n",
      "Epoch 00031: val_loss did not improve from 0.24598\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1630 - acc: 0.9543 - val_loss: 0.2737 - val_acc: 0.9194\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9549\n",
      "Epoch 00032: val_loss improved from 0.24598 to 0.24472, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/032-0.2447.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1610 - acc: 0.9549 - val_loss: 0.2447 - val_acc: 0.9271\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9562\n",
      "Epoch 00033: val_loss did not improve from 0.24472\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1545 - acc: 0.9562 - val_loss: 0.2509 - val_acc: 0.9248\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9585\n",
      "Epoch 00034: val_loss did not improve from 0.24472\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1502 - acc: 0.9585 - val_loss: 0.2611 - val_acc: 0.9250\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9583\n",
      "Epoch 00035: val_loss did not improve from 0.24472\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1476 - acc: 0.9583 - val_loss: 0.2472 - val_acc: 0.9271\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9601\n",
      "Epoch 00036: val_loss improved from 0.24472 to 0.24197, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/036-0.2420.hdf5\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1398 - acc: 0.9601 - val_loss: 0.2420 - val_acc: 0.9271\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9608\n",
      "Epoch 00037: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1374 - acc: 0.9608 - val_loss: 0.2427 - val_acc: 0.9308\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9614\n",
      "Epoch 00038: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1354 - acc: 0.9614 - val_loss: 0.2789 - val_acc: 0.9171\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9614\n",
      "Epoch 00039: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1315 - acc: 0.9614 - val_loss: 0.2648 - val_acc: 0.9238\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9625\n",
      "Epoch 00040: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1318 - acc: 0.9625 - val_loss: 0.2520 - val_acc: 0.9331\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9652\n",
      "Epoch 00041: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1234 - acc: 0.9652 - val_loss: 0.2516 - val_acc: 0.9285\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9672\n",
      "Epoch 00042: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1185 - acc: 0.9672 - val_loss: 0.2461 - val_acc: 0.9290\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9670\n",
      "Epoch 00043: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1161 - acc: 0.9670 - val_loss: 0.2476 - val_acc: 0.9264\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9674\n",
      "Epoch 00044: val_loss did not improve from 0.24197\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1138 - acc: 0.9673 - val_loss: 0.2550 - val_acc: 0.9287\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9662\n",
      "Epoch 00045: val_loss improved from 0.24197 to 0.24030, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/045-0.2403.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1166 - acc: 0.9662 - val_loss: 0.2403 - val_acc: 0.9343\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9696\n",
      "Epoch 00046: val_loss improved from 0.24030 to 0.22109, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv_checkpoint/046-0.2211.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1058 - acc: 0.9696 - val_loss: 0.2211 - val_acc: 0.9348\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9704\n",
      "Epoch 00047: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1042 - acc: 0.9704 - val_loss: 0.2388 - val_acc: 0.9285\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9710\n",
      "Epoch 00048: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1015 - acc: 0.9710 - val_loss: 0.2583 - val_acc: 0.9304\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9715\n",
      "Epoch 00049: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1011 - acc: 0.9716 - val_loss: 0.2315 - val_acc: 0.9364\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9730\n",
      "Epoch 00050: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0954 - acc: 0.9730 - val_loss: 0.2282 - val_acc: 0.9376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9752\n",
      "Epoch 00051: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0927 - acc: 0.9752 - val_loss: 0.2652 - val_acc: 0.9266\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9737\n",
      "Epoch 00052: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0923 - acc: 0.9737 - val_loss: 0.2846 - val_acc: 0.9210\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9751\n",
      "Epoch 00053: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0906 - acc: 0.9750 - val_loss: 0.2469 - val_acc: 0.9283\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9751\n",
      "Epoch 00054: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0878 - acc: 0.9751 - val_loss: 0.2239 - val_acc: 0.9373\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9775\n",
      "Epoch 00055: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0834 - acc: 0.9774 - val_loss: 0.2319 - val_acc: 0.9385\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9765\n",
      "Epoch 00056: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0839 - acc: 0.9764 - val_loss: 0.2452 - val_acc: 0.9341\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9768\n",
      "Epoch 00057: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0809 - acc: 0.9768 - val_loss: 0.2546 - val_acc: 0.9280\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9781\n",
      "Epoch 00058: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0790 - acc: 0.9781 - val_loss: 0.2559 - val_acc: 0.9294\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9789\n",
      "Epoch 00059: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0767 - acc: 0.9789 - val_loss: 0.2273 - val_acc: 0.9362\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9799\n",
      "Epoch 00060: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0735 - acc: 0.9799 - val_loss: 0.3228 - val_acc: 0.9161\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9793\n",
      "Epoch 00061: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0750 - acc: 0.9793 - val_loss: 0.2693 - val_acc: 0.9206\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9818\n",
      "Epoch 00062: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0688 - acc: 0.9818 - val_loss: 0.2510 - val_acc: 0.9331\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9814\n",
      "Epoch 00063: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0698 - acc: 0.9813 - val_loss: 0.2357 - val_acc: 0.9322\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9814\n",
      "Epoch 00064: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0682 - acc: 0.9814 - val_loss: 0.2811 - val_acc: 0.9243\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9831\n",
      "Epoch 00065: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0638 - acc: 0.9831 - val_loss: 0.2327 - val_acc: 0.9366\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9819\n",
      "Epoch 00066: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0654 - acc: 0.9819 - val_loss: 0.2489 - val_acc: 0.9338\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9837\n",
      "Epoch 00067: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0616 - acc: 0.9837 - val_loss: 0.2573 - val_acc: 0.9313\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9836\n",
      "Epoch 00068: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0608 - acc: 0.9836 - val_loss: 0.2611 - val_acc: 0.9306\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9839\n",
      "Epoch 00069: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0607 - acc: 0.9839 - val_loss: 0.2419 - val_acc: 0.9317\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9837\n",
      "Epoch 00070: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0602 - acc: 0.9836 - val_loss: 0.2405 - val_acc: 0.9345\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9859\n",
      "Epoch 00071: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0557 - acc: 0.9859 - val_loss: 0.2454 - val_acc: 0.9301\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9849\n",
      "Epoch 00072: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0562 - acc: 0.9849 - val_loss: 0.2813 - val_acc: 0.9245\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9862\n",
      "Epoch 00073: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0543 - acc: 0.9862 - val_loss: 0.2600 - val_acc: 0.9324\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9847\n",
      "Epoch 00074: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0559 - acc: 0.9847 - val_loss: 0.2445 - val_acc: 0.9366\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9871\n",
      "Epoch 00075: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0498 - acc: 0.9871 - val_loss: 0.2619 - val_acc: 0.9299\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9875\n",
      "Epoch 00076: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0494 - acc: 0.9874 - val_loss: 0.2586 - val_acc: 0.9301\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9837\n",
      "Epoch 00077: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0597 - acc: 0.9837 - val_loss: 0.2527 - val_acc: 0.9324\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9882\n",
      "Epoch 00078: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0464 - acc: 0.9882 - val_loss: 0.2620 - val_acc: 0.9327\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9895\n",
      "Epoch 00079: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0445 - acc: 0.9895 - val_loss: 0.2598 - val_acc: 0.9320\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9896\n",
      "Epoch 00080: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0431 - acc: 0.9896 - val_loss: 0.2615 - val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9878\n",
      "Epoch 00081: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0475 - acc: 0.9878 - val_loss: 0.2581 - val_acc: 0.9341\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9881\n",
      "Epoch 00082: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0476 - acc: 0.9881 - val_loss: 0.2693 - val_acc: 0.9329\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9886\n",
      "Epoch 00083: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0438 - acc: 0.9886 - val_loss: 0.2518 - val_acc: 0.9359\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9887\n",
      "Epoch 00084: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0423 - acc: 0.9887 - val_loss: 0.2505 - val_acc: 0.9343\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9889\n",
      "Epoch 00085: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0433 - acc: 0.9889 - val_loss: 0.2540 - val_acc: 0.9408\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9895\n",
      "Epoch 00086: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0412 - acc: 0.9894 - val_loss: 0.2632 - val_acc: 0.9329\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9893\n",
      "Epoch 00087: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0405 - acc: 0.9893 - val_loss: 0.3007 - val_acc: 0.9238\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9911\n",
      "Epoch 00088: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0379 - acc: 0.9911 - val_loss: 0.2828 - val_acc: 0.9294\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9902\n",
      "Epoch 00089: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0397 - acc: 0.9902 - val_loss: 0.2630 - val_acc: 0.9311\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9868\n",
      "Epoch 00090: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0501 - acc: 0.9868 - val_loss: 0.2562 - val_acc: 0.9362\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9921\n",
      "Epoch 00091: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0336 - acc: 0.9921 - val_loss: 0.3617 - val_acc: 0.9092\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9909\n",
      "Epoch 00092: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0374 - acc: 0.9909 - val_loss: 0.2756 - val_acc: 0.9366\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9926\n",
      "Epoch 00093: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0321 - acc: 0.9926 - val_loss: 0.2931 - val_acc: 0.9276\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9906\n",
      "Epoch 00094: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0360 - acc: 0.9906 - val_loss: 0.2773 - val_acc: 0.9341\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9903\n",
      "Epoch 00095: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0378 - acc: 0.9902 - val_loss: 0.2684 - val_acc: 0.9308\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9912\n",
      "Epoch 00096: val_loss did not improve from 0.22109\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0350 - acc: 0.9913 - val_loss: 0.2529 - val_acc: 0.9371\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 458us/sample - loss: 0.2878 - acc: 0.9182\n",
      "Loss: 0.2878276407471079 Accuracy: 0.91817236\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6040 - acc: 0.5142\n",
      "Epoch 00001: val_loss improved from inf to 1.67429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/001-1.6743.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.6040 - acc: 0.5142 - val_loss: 1.6743 - val_acc: 0.4472\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7674\n",
      "Epoch 00002: val_loss improved from 1.67429 to 0.65864, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/002-0.6586.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.8271 - acc: 0.7674 - val_loss: 0.6586 - val_acc: 0.8204\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8374\n",
      "Epoch 00003: val_loss improved from 0.65864 to 0.48227, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/003-0.4823.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.5899 - acc: 0.8374 - val_loss: 0.4823 - val_acc: 0.8700\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4700 - acc: 0.8704\n",
      "Epoch 00004: val_loss improved from 0.48227 to 0.43698, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/004-0.4370.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.4700 - acc: 0.8703 - val_loss: 0.4370 - val_acc: 0.8800\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8915\n",
      "Epoch 00005: val_loss improved from 0.43698 to 0.38687, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/005-0.3869.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.3934 - acc: 0.8915 - val_loss: 0.3869 - val_acc: 0.8884\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.9033\n",
      "Epoch 00006: val_loss improved from 0.38687 to 0.33306, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/006-0.3331.hdf5\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.3447 - acc: 0.9032 - val_loss: 0.3331 - val_acc: 0.9066\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.9144\n",
      "Epoch 00007: val_loss improved from 0.33306 to 0.30825, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/007-0.3083.hdf5\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3079 - acc: 0.9144 - val_loss: 0.3083 - val_acc: 0.9138\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9216\n",
      "Epoch 00008: val_loss improved from 0.30825 to 0.28013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/008-0.2801.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2801 - val_acc: 0.9206\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9274\n",
      "Epoch 00009: val_loss improved from 0.28013 to 0.26251, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/009-0.2625.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.2555 - acc: 0.9275 - val_loss: 0.2625 - val_acc: 0.9259\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9330\n",
      "Epoch 00010: val_loss improved from 0.26251 to 0.25235, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/010-0.2524.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.2365 - acc: 0.9330 - val_loss: 0.2524 - val_acc: 0.9273\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9379\n",
      "Epoch 00011: val_loss did not improve from 0.25235\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.2192 - acc: 0.9379 - val_loss: 0.2795 - val_acc: 0.9147\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9428\n",
      "Epoch 00012: val_loss improved from 0.25235 to 0.21903, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/012-0.2190.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.2034 - acc: 0.9428 - val_loss: 0.2190 - val_acc: 0.9345\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9453\n",
      "Epoch 00013: val_loss improved from 0.21903 to 0.20705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/013-0.2071.hdf5\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1922 - acc: 0.9453 - val_loss: 0.2071 - val_acc: 0.9401\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9511\n",
      "Epoch 00014: val_loss did not improve from 0.20705\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1762 - acc: 0.9511 - val_loss: 0.2427 - val_acc: 0.9301\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9521\n",
      "Epoch 00015: val_loss did not improve from 0.20705\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.1685 - acc: 0.9521 - val_loss: 0.2180 - val_acc: 0.9380\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9555\n",
      "Epoch 00016: val_loss improved from 0.20705 to 0.18737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/016-0.1874.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.1568 - acc: 0.9555 - val_loss: 0.1874 - val_acc: 0.9443\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9581\n",
      "Epoch 00017: val_loss did not improve from 0.18737\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.1487 - acc: 0.9580 - val_loss: 0.2200 - val_acc: 0.9313\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9604\n",
      "Epoch 00018: val_loss did not improve from 0.18737\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1410 - acc: 0.9604 - val_loss: 0.1875 - val_acc: 0.9460\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9620\n",
      "Epoch 00019: val_loss improved from 0.18737 to 0.18298, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/019-0.1830.hdf5\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1333 - acc: 0.9620 - val_loss: 0.1830 - val_acc: 0.9460\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9656\n",
      "Epoch 00020: val_loss improved from 0.18298 to 0.17383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/020-0.1738.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.1254 - acc: 0.9656 - val_loss: 0.1738 - val_acc: 0.9462\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9667\n",
      "Epoch 00021: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1191 - acc: 0.9667 - val_loss: 0.2135 - val_acc: 0.9413\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9692\n",
      "Epoch 00022: val_loss did not improve from 0.17383\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1107 - acc: 0.9692 - val_loss: 0.1859 - val_acc: 0.9453\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9708\n",
      "Epoch 00023: val_loss improved from 0.17383 to 0.17117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv_checkpoint/023-0.1712.hdf5\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.1063 - acc: 0.9707 - val_loss: 0.1712 - val_acc: 0.9485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9683\n",
      "Epoch 00024: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.1099 - acc: 0.9683 - val_loss: 0.1991 - val_acc: 0.9373\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9743\n",
      "Epoch 00025: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0949 - acc: 0.9743 - val_loss: 0.1729 - val_acc: 0.9502\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9742\n",
      "Epoch 00026: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.0925 - acc: 0.9742 - val_loss: 0.2066 - val_acc: 0.9413\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9747\n",
      "Epoch 00027: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0901 - acc: 0.9747 - val_loss: 0.1819 - val_acc: 0.9457\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9776\n",
      "Epoch 00028: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0840 - acc: 0.9776 - val_loss: 0.1854 - val_acc: 0.9441\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9784\n",
      "Epoch 00029: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0788 - acc: 0.9784 - val_loss: 0.1734 - val_acc: 0.9495\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9802\n",
      "Epoch 00030: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0728 - acc: 0.9802 - val_loss: 0.1749 - val_acc: 0.9455\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9806\n",
      "Epoch 00031: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0728 - acc: 0.9806 - val_loss: 0.1860 - val_acc: 0.9415\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9822\n",
      "Epoch 00032: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0682 - acc: 0.9822 - val_loss: 0.1918 - val_acc: 0.9436\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9836\n",
      "Epoch 00033: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0635 - acc: 0.9836 - val_loss: 0.1934 - val_acc: 0.9425\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9836\n",
      "Epoch 00034: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.0625 - acc: 0.9836 - val_loss: 0.1851 - val_acc: 0.9455\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9839\n",
      "Epoch 00035: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0594 - acc: 0.9839 - val_loss: 0.1742 - val_acc: 0.9462\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9863\n",
      "Epoch 00036: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0554 - acc: 0.9863 - val_loss: 0.1925 - val_acc: 0.9411\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9867\n",
      "Epoch 00037: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0519 - acc: 0.9867 - val_loss: 0.1869 - val_acc: 0.9457\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9865\n",
      "Epoch 00038: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0493 - acc: 0.9865 - val_loss: 0.1926 - val_acc: 0.9457\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9883\n",
      "Epoch 00039: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0481 - acc: 0.9883 - val_loss: 0.1757 - val_acc: 0.9471\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9890\n",
      "Epoch 00040: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 910us/sample - loss: 0.0446 - acc: 0.9890 - val_loss: 0.1999 - val_acc: 0.9432\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9900\n",
      "Epoch 00041: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0434 - acc: 0.9900 - val_loss: 0.1982 - val_acc: 0.9422\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9902\n",
      "Epoch 00042: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0398 - acc: 0.9902 - val_loss: 0.1945 - val_acc: 0.9443\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9907\n",
      "Epoch 00043: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 910us/sample - loss: 0.0388 - acc: 0.9907 - val_loss: 0.1966 - val_acc: 0.9446\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9906\n",
      "Epoch 00044: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0382 - acc: 0.9906 - val_loss: 0.1978 - val_acc: 0.9462\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9917\n",
      "Epoch 00045: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0360 - acc: 0.9917 - val_loss: 0.2236 - val_acc: 0.9404\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9921\n",
      "Epoch 00046: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0341 - acc: 0.9921 - val_loss: 0.1928 - val_acc: 0.9450\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9932\n",
      "Epoch 00047: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0317 - acc: 0.9932 - val_loss: 0.1868 - val_acc: 0.9474\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9932\n",
      "Epoch 00048: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0298 - acc: 0.9932 - val_loss: 0.2069 - val_acc: 0.9436\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0304 - acc: 0.9932 - val_loss: 0.2084 - val_acc: 0.9450\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9927\n",
      "Epoch 00050: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0311 - acc: 0.9927 - val_loss: 0.2063 - val_acc: 0.9415\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9940\n",
      "Epoch 00051: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0272 - acc: 0.9939 - val_loss: 0.2049 - val_acc: 0.9474\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9925\n",
      "Epoch 00052: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0307 - acc: 0.9924 - val_loss: 0.2050 - val_acc: 0.9474\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9940\n",
      "Epoch 00053: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0285 - acc: 0.9940 - val_loss: 0.1929 - val_acc: 0.9495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0265 - acc: 0.9938 - val_loss: 0.1999 - val_acc: 0.9471\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0264 - acc: 0.9940 - val_loss: 0.1885 - val_acc: 0.9499\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9958\n",
      "Epoch 00056: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0213 - acc: 0.9958 - val_loss: 0.2092 - val_acc: 0.9462\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9963\n",
      "Epoch 00057: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0201 - acc: 0.9962 - val_loss: 0.2514 - val_acc: 0.9364\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9916\n",
      "Epoch 00058: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0328 - acc: 0.9916 - val_loss: 0.1938 - val_acc: 0.9504\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9964\n",
      "Epoch 00059: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0192 - acc: 0.9964 - val_loss: 0.1840 - val_acc: 0.9513\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9969\n",
      "Epoch 00060: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0174 - acc: 0.9969 - val_loss: 0.2074 - val_acc: 0.9462\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9952\n",
      "Epoch 00061: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0220 - acc: 0.9952 - val_loss: 0.1864 - val_acc: 0.9497\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9968\n",
      "Epoch 00062: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0174 - acc: 0.9968 - val_loss: 0.2276 - val_acc: 0.9464\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9961\n",
      "Epoch 00063: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0191 - acc: 0.9961 - val_loss: 0.2172 - val_acc: 0.9439\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9949\n",
      "Epoch 00064: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0220 - acc: 0.9949 - val_loss: 0.2552 - val_acc: 0.9413\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9968\n",
      "Epoch 00065: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0166 - acc: 0.9968 - val_loss: 0.2431 - val_acc: 0.9425\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9967\n",
      "Epoch 00066: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.0175 - acc: 0.9967 - val_loss: 0.2143 - val_acc: 0.9464\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9940\n",
      "Epoch 00067: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0239 - acc: 0.9940 - val_loss: 0.1953 - val_acc: 0.9483\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9969\n",
      "Epoch 00068: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 910us/sample - loss: 0.0144 - acc: 0.9969 - val_loss: 0.1917 - val_acc: 0.9525\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9972\n",
      "Epoch 00069: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0147 - acc: 0.9972 - val_loss: 0.2162 - val_acc: 0.9462\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9937\n",
      "Epoch 00070: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0263 - acc: 0.9937 - val_loss: 0.2299 - val_acc: 0.9455\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9961\n",
      "Epoch 00071: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.2810 - val_acc: 0.9320\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9927\n",
      "Epoch 00072: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0283 - acc: 0.9927 - val_loss: 0.2075 - val_acc: 0.9474\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9988\n",
      "Epoch 00073: val_loss did not improve from 0.17117\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0098 - acc: 0.9988 - val_loss: 0.1992 - val_acc: 0.9513\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 519us/sample - loss: 0.2410 - acc: 0.9248\n",
      "Loss: 0.24100409129698327 Accuracy: 0.9248183\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5845 - acc: 0.5216\n",
      "Epoch 00001: val_loss improved from inf to 1.35203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/001-1.3520.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.5844 - acc: 0.5217 - val_loss: 1.3520 - val_acc: 0.5966\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7422 - acc: 0.7881\n",
      "Epoch 00002: val_loss improved from 1.35203 to 0.68749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/002-0.6875.hdf5\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.7424 - acc: 0.7880 - val_loss: 0.6875 - val_acc: 0.7876\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8493\n",
      "Epoch 00003: val_loss improved from 0.68749 to 0.43654, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/003-0.4365.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.5184 - acc: 0.8492 - val_loss: 0.4365 - val_acc: 0.8751\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8851\n",
      "Epoch 00004: val_loss improved from 0.43654 to 0.34662, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/004-0.3466.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.4052 - acc: 0.8851 - val_loss: 0.3466 - val_acc: 0.9061\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.9036\n",
      "Epoch 00005: val_loss improved from 0.34662 to 0.32265, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/005-0.3227.hdf5\n",
      "36805/36805 [==============================] - 35s 948us/sample - loss: 0.3361 - acc: 0.9036 - val_loss: 0.3227 - val_acc: 0.9085\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9161\n",
      "Epoch 00006: val_loss improved from 0.32265 to 0.30509, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/006-0.3051.hdf5\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.2921 - acc: 0.9160 - val_loss: 0.3051 - val_acc: 0.9064\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9269\n",
      "Epoch 00007: val_loss improved from 0.30509 to 0.27027, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/007-0.2703.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.2567 - acc: 0.9269 - val_loss: 0.2703 - val_acc: 0.9208\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9330\n",
      "Epoch 00008: val_loss improved from 0.27027 to 0.23937, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/008-0.2394.hdf5\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.2317 - acc: 0.9331 - val_loss: 0.2394 - val_acc: 0.9285\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9391\n",
      "Epoch 00009: val_loss improved from 0.23937 to 0.23137, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/009-0.2314.hdf5\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.2073 - acc: 0.9391 - val_loss: 0.2314 - val_acc: 0.9327\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9451\n",
      "Epoch 00010: val_loss did not improve from 0.23137\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.1915 - acc: 0.9451 - val_loss: 0.2372 - val_acc: 0.9292\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9489\n",
      "Epoch 00011: val_loss improved from 0.23137 to 0.22328, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/011-0.2233.hdf5\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.1780 - acc: 0.9489 - val_loss: 0.2233 - val_acc: 0.9317\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9526\n",
      "Epoch 00012: val_loss improved from 0.22328 to 0.20305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/012-0.2030.hdf5\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.1602 - acc: 0.9525 - val_loss: 0.2030 - val_acc: 0.9397\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9565\n",
      "Epoch 00013: val_loss did not improve from 0.20305\n",
      "36805/36805 [==============================] - 35s 954us/sample - loss: 0.1510 - acc: 0.9564 - val_loss: 0.2181 - val_acc: 0.9327\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9605\n",
      "Epoch 00014: val_loss did not improve from 0.20305\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.1385 - acc: 0.9605 - val_loss: 0.2070 - val_acc: 0.9352\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9644\n",
      "Epoch 00015: val_loss improved from 0.20305 to 0.19290, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/015-0.1929.hdf5\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.1270 - acc: 0.9644 - val_loss: 0.1929 - val_acc: 0.9413\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9656\n",
      "Epoch 00016: val_loss did not improve from 0.19290\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.1215 - acc: 0.9656 - val_loss: 0.2090 - val_acc: 0.9336\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9677\n",
      "Epoch 00017: val_loss improved from 0.19290 to 0.18852, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/017-0.1885.hdf5\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.1126 - acc: 0.9677 - val_loss: 0.1885 - val_acc: 0.9394\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9717\n",
      "Epoch 00018: val_loss did not improve from 0.18852\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.1018 - acc: 0.9717 - val_loss: 0.1909 - val_acc: 0.9404\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9739\n",
      "Epoch 00019: val_loss did not improve from 0.18852\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0917 - acc: 0.9739 - val_loss: 0.2028 - val_acc: 0.9348\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9763\n",
      "Epoch 00020: val_loss improved from 0.18852 to 0.18148, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/020-0.1815.hdf5\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0855 - acc: 0.9763 - val_loss: 0.1815 - val_acc: 0.9425\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9784\n",
      "Epoch 00021: val_loss did not improve from 0.18148\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0803 - acc: 0.9784 - val_loss: 0.1987 - val_acc: 0.9401\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9767\n",
      "Epoch 00022: val_loss did not improve from 0.18148\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0836 - acc: 0.9766 - val_loss: 0.1833 - val_acc: 0.9418\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9797\n",
      "Epoch 00023: val_loss did not improve from 0.18148\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0714 - acc: 0.9797 - val_loss: 0.1936 - val_acc: 0.9397\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9840\n",
      "Epoch 00024: val_loss did not improve from 0.18148\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0641 - acc: 0.9840 - val_loss: 0.2109 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9828\n",
      "Epoch 00025: val_loss did not improve from 0.18148\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0648 - acc: 0.9828 - val_loss: 0.1961 - val_acc: 0.9373\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9858\n",
      "Epoch 00026: val_loss improved from 0.18148 to 0.17459, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv_checkpoint/026-0.1746.hdf5\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0562 - acc: 0.9858 - val_loss: 0.1746 - val_acc: 0.9443\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9875\n",
      "Epoch 00027: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.0515 - acc: 0.9875 - val_loss: 0.2332 - val_acc: 0.9297\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9874\n",
      "Epoch 00028: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 952us/sample - loss: 0.0497 - acc: 0.9874 - val_loss: 0.1847 - val_acc: 0.9441\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9881\n",
      "Epoch 00029: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0463 - acc: 0.9881 - val_loss: 0.2013 - val_acc: 0.9390\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9877\n",
      "Epoch 00030: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0477 - acc: 0.9876 - val_loss: 0.2326 - val_acc: 0.9324\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9888\n",
      "Epoch 00031: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0447 - acc: 0.9888 - val_loss: 0.1822 - val_acc: 0.9457\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9921\n",
      "Epoch 00032: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0353 - acc: 0.9921 - val_loss: 0.1911 - val_acc: 0.9455\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9924\n",
      "Epoch 00033: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 947us/sample - loss: 0.0347 - acc: 0.9924 - val_loss: 0.2342 - val_acc: 0.9355\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9927\n",
      "Epoch 00034: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0316 - acc: 0.9927 - val_loss: 0.1982 - val_acc: 0.9411\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9934\n",
      "Epoch 00035: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 942us/sample - loss: 0.0304 - acc: 0.9934 - val_loss: 0.1862 - val_acc: 0.9453\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9948\n",
      "Epoch 00036: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0266 - acc: 0.9947 - val_loss: 0.2621 - val_acc: 0.9306\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9911\n",
      "Epoch 00037: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0350 - acc: 0.9911 - val_loss: 0.1892 - val_acc: 0.9429\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9944\n",
      "Epoch 00038: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.0263 - acc: 0.9944 - val_loss: 0.2026 - val_acc: 0.9439\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9943\n",
      "Epoch 00039: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.0251 - acc: 0.9943 - val_loss: 0.1867 - val_acc: 0.9457\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9919\n",
      "Epoch 00040: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.0325 - acc: 0.9919 - val_loss: 0.1933 - val_acc: 0.9415\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9926\n",
      "Epoch 00041: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.0308 - acc: 0.9926 - val_loss: 0.1860 - val_acc: 0.9474\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9958\n",
      "Epoch 00042: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.0220 - acc: 0.9957 - val_loss: 0.2148 - val_acc: 0.9406\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9904\n",
      "Epoch 00043: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.0352 - acc: 0.9904 - val_loss: 0.1976 - val_acc: 0.9443\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9977\n",
      "Epoch 00044: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.0162 - acc: 0.9976 - val_loss: 0.1930 - val_acc: 0.9462\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9932\n",
      "Epoch 00045: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 940us/sample - loss: 0.0268 - acc: 0.9932 - val_loss: 0.1904 - val_acc: 0.9462\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9969\n",
      "Epoch 00046: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 0.0163 - acc: 0.9968 - val_loss: 0.2611 - val_acc: 0.9290\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9906\n",
      "Epoch 00047: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 940us/sample - loss: 0.0346 - acc: 0.9906 - val_loss: 0.2029 - val_acc: 0.9450\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9974\n",
      "Epoch 00048: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.0151 - acc: 0.9974 - val_loss: 0.2108 - val_acc: 0.9429\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9966\n",
      "Epoch 00049: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.0165 - acc: 0.9966 - val_loss: 0.2037 - val_acc: 0.9460\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9982\n",
      "Epoch 00050: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.0116 - acc: 0.9982 - val_loss: 0.2225 - val_acc: 0.9429\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.0325 - acc: 0.9908 - val_loss: 0.1998 - val_acc: 0.9469\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9973\n",
      "Epoch 00052: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.0143 - acc: 0.9973 - val_loss: 0.2160 - val_acc: 0.9450\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9956\n",
      "Epoch 00053: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.0199 - acc: 0.9956 - val_loss: 0.1954 - val_acc: 0.9513\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9974\n",
      "Epoch 00054: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 940us/sample - loss: 0.0127 - acc: 0.9974 - val_loss: 0.2021 - val_acc: 0.9490\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9978\n",
      "Epoch 00055: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0123 - acc: 0.9978 - val_loss: 0.2028 - val_acc: 0.9499\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9969\n",
      "Epoch 00056: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.0137 - acc: 0.9969 - val_loss: 0.2117 - val_acc: 0.9460\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0133 - acc: 0.9974 - val_loss: 0.2169 - val_acc: 0.9441\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9974\n",
      "Epoch 00058: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.0125 - acc: 0.9974 - val_loss: 0.2187 - val_acc: 0.9408\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.2863 - val_acc: 0.9245\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 00060: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.2363 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9980\n",
      "Epoch 00061: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0107 - acc: 0.9980 - val_loss: 0.2505 - val_acc: 0.9387\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9963\n",
      "Epoch 00062: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 0.0146 - acc: 0.9963 - val_loss: 0.2359 - val_acc: 0.9415\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9976\n",
      "Epoch 00063: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0120 - acc: 0.9975 - val_loss: 0.3122 - val_acc: 0.9227\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9911\n",
      "Epoch 00064: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0315 - acc: 0.9911 - val_loss: 0.2206 - val_acc: 0.9471\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9956\n",
      "Epoch 00065: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 0.0177 - acc: 0.9955 - val_loss: 0.2130 - val_acc: 0.9460\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9956\n",
      "Epoch 00066: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 35s 938us/sample - loss: 0.0169 - acc: 0.9956 - val_loss: 0.1928 - val_acc: 0.9527\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9990\n",
      "Epoch 00067: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0073 - acc: 0.9990 - val_loss: 0.2002 - val_acc: 0.9518\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9991\n",
      "Epoch 00068: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0065 - acc: 0.9991 - val_loss: 0.2064 - val_acc: 0.9478\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9951\n",
      "Epoch 00069: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0176 - acc: 0.9950 - val_loss: 0.2133 - val_acc: 0.9425\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9935\n",
      "Epoch 00070: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0225 - acc: 0.9935 - val_loss: 0.2062 - val_acc: 0.9506\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9986\n",
      "Epoch 00071: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 0.0081 - acc: 0.9986 - val_loss: 0.2172 - val_acc: 0.9460\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9953\n",
      "Epoch 00072: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 0.0184 - acc: 0.9953 - val_loss: 0.1915 - val_acc: 0.9522\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9987\n",
      "Epoch 00073: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2180 - val_acc: 0.9469\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9989\n",
      "Epoch 00074: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0072 - acc: 0.9989 - val_loss: 0.2530 - val_acc: 0.9425\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9959\n",
      "Epoch 00075: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.0155 - acc: 0.9958 - val_loss: 0.2012 - val_acc: 0.9488\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9946\n",
      "Epoch 00076: val_loss did not improve from 0.17459\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0188 - acc: 0.9946 - val_loss: 0.2155 - val_acc: 0.9476\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 519us/sample - loss: 0.2577 - acc: 0.9281\n",
      "Loss: 0.2576975212042701 Accuracy: 0.92814124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 64)           256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 463us/sample - loss: 0.9155 - acc: 0.7254\n",
      "Loss: 0.9154667171238615 Accuracy: 0.72544134\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 64)           256         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 545us/sample - loss: 0.6321 - acc: 0.8216\n",
      "Loss: 0.6321213007097056 Accuracy: 0.8215992\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 96)           384         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 526us/sample - loss: 0.4086 - acc: 0.8837\n",
      "Loss: 0.408611616414903 Accuracy: 0.8836968\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 128)          512         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 575us/sample - loss: 0.2878 - acc: 0.9182\n",
      "Loss: 0.2878276407471079 Accuracy: 0.91817236\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 128)          512         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 558us/sample - loss: 0.2410 - acc: 0.9248\n",
      "Loss: 0.24100409129698327 Accuracy: 0.9248183\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 128)          512         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 630us/sample - loss: 0.2577 - acc: 0.9281\n",
      "Loss: 0.2576975212042701 Accuracy: 0.92814124\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 64)           256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,176\n",
      "Trainable params: 11,856\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 558us/sample - loss: 1.4429 - acc: 0.5786\n",
      "Loss: 1.4428583533840635 Accuracy: 0.5786085\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 64)           256         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 17,456\n",
      "Trainable params: 17,072\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 626us/sample - loss: 0.6571 - acc: 0.8085\n",
      "Loss: 0.6571419659928494 Accuracy: 0.8085151\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 96)           384         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 28,656\n",
      "Trainable params: 28,080\n",
      "Non-trainable params: 576\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 650us/sample - loss: 0.4297 - acc: 0.8837\n",
      "Loss: 0.42969588146526616 Accuracy: 0.8836968\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 128)          512         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 50,096\n",
      "Trainable params: 49,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 669us/sample - loss: 0.3010 - acc: 0.9232\n",
      "Loss: 0.3010020256661428 Accuracy: 0.9231568\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 128)          512         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 70,896\n",
      "Trainable params: 70,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 677us/sample - loss: 0.2606 - acc: 0.9331\n",
      "Loss: 0.2605644866004045 Accuracy: 0.9331257\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 128)          512         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 91,696\n",
      "Trainable params: 90,672\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 711us/sample - loss: 0.3027 - acc: 0.9331\n",
      "Loss: 0.30269156234379496 Accuracy: 0.9331257\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
