{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 341312)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 492928)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 492928)       1971712     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           7886864     batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 9,900,816\n",
      "Trainable params: 8,914,576\n",
      "Non-trainable params: 986,240\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 113728)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 164224)       0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 164224)       656896      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2627600     batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 3,347,536\n",
      "Trainable params: 3,018,576\n",
      "Non-trainable params: 328,960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 37888)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 58816)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 58816)        235264      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           941072      batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,280,976\n",
      "Trainable params: 1,162,576\n",
      "Non-trainable params: 118,400\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 12608)        0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 23616)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 23616)        94464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           377872      batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 659,536\n",
      "Trainable params: 611,280\n",
      "Non-trainable params: 48,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 8320)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 11904)        0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 11904)        47616       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           190480      batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 507,856\n",
      "Trainable params: 482,768\n",
      "Non-trainable params: 25,088\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 2688)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 3840)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 3840)         15360       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           61456       batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 429,136\n",
      "Trainable params: 419,920\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 6.6511 - acc: 0.3697\n",
      "Epoch 00001: val_loss improved from inf to 5.58645, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/001-5.5865.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 6.6515 - acc: 0.3697 - val_loss: 5.5865 - val_acc: 0.4284\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.8794 - acc: 0.5726\n",
      "Epoch 00002: val_loss did not improve from 5.58645\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 4.8796 - acc: 0.5726 - val_loss: 5.6055 - val_acc: 0.5143\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.1778 - acc: 0.6433\n",
      "Epoch 00003: val_loss did not improve from 5.58645\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 4.1786 - acc: 0.6432 - val_loss: 5.7384 - val_acc: 0.5299\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.7065 - acc: 0.6876\n",
      "Epoch 00004: val_loss did not improve from 5.58645\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 3.7071 - acc: 0.6875 - val_loss: 5.8862 - val_acc: 0.5171\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4071 - acc: 0.7179\n",
      "Epoch 00005: val_loss did not improve from 5.58645\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 3.4075 - acc: 0.7178 - val_loss: 5.6835 - val_acc: 0.5486\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1429 - acc: 0.7439\n",
      "Epoch 00006: val_loss improved from 5.58645 to 5.46297, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/006-5.4630.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 3.1433 - acc: 0.7439 - val_loss: 5.4630 - val_acc: 0.5688\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0451 - acc: 0.7579\n",
      "Epoch 00007: val_loss did not improve from 5.46297\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 3.0456 - acc: 0.7579 - val_loss: 5.5600 - val_acc: 0.5719\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6787 - acc: 0.7858\n",
      "Epoch 00008: val_loss did not improve from 5.46297\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.6792 - acc: 0.7858 - val_loss: 5.6672 - val_acc: 0.5698\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5244 - acc: 0.8007\n",
      "Epoch 00009: val_loss did not improve from 5.46297\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.5250 - acc: 0.8006 - val_loss: 5.6945 - val_acc: 0.5707\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5215 - acc: 0.8032\n",
      "Epoch 00010: val_loss did not improve from 5.46297\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.5211 - acc: 0.8032 - val_loss: 6.5401 - val_acc: 0.5311\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3819 - acc: 0.8160\n",
      "Epoch 00011: val_loss improved from 5.46297 to 5.26823, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/011-5.2682.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.3821 - acc: 0.8160 - val_loss: 5.2682 - val_acc: 0.6075\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3868 - acc: 0.8157\n",
      "Epoch 00012: val_loss did not improve from 5.26823\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.3870 - acc: 0.8157 - val_loss: 6.1902 - val_acc: 0.5479\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2494 - acc: 0.8277\n",
      "Epoch 00013: val_loss improved from 5.26823 to 5.18992, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/013-5.1899.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.2498 - acc: 0.8277 - val_loss: 5.1899 - val_acc: 0.6138\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1634 - acc: 0.8353\n",
      "Epoch 00014: val_loss did not improve from 5.18992\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.1634 - acc: 0.8353 - val_loss: 5.2801 - val_acc: 0.6052\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0428 - acc: 0.8460\n",
      "Epoch 00015: val_loss did not improve from 5.18992\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.0425 - acc: 0.8461 - val_loss: 5.2914 - val_acc: 0.6138\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9472 - acc: 0.8531\n",
      "Epoch 00016: val_loss did not improve from 5.18992\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.9478 - acc: 0.8530 - val_loss: 5.3214 - val_acc: 0.6103\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9386 - acc: 0.8557\n",
      "Epoch 00017: val_loss did not improve from 5.18992\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.9388 - acc: 0.8557 - val_loss: 5.2085 - val_acc: 0.6261\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9126 - acc: 0.8577\n",
      "Epoch 00018: val_loss did not improve from 5.18992\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.9123 - acc: 0.8577 - val_loss: 5.2372 - val_acc: 0.6164\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8834 - acc: 0.8601\n",
      "Epoch 00019: val_loss improved from 5.18992 to 5.13962, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/019-5.1396.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.8831 - acc: 0.8601 - val_loss: 5.1396 - val_acc: 0.6292\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8348 - acc: 0.8643\n",
      "Epoch 00020: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.8354 - acc: 0.8642 - val_loss: 5.2588 - val_acc: 0.6196\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7554 - acc: 0.8718\n",
      "Epoch 00021: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.7565 - acc: 0.8717 - val_loss: 5.5881 - val_acc: 0.5998\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7777 - acc: 0.8696\n",
      "Epoch 00022: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.7783 - acc: 0.8696 - val_loss: 5.6504 - val_acc: 0.5996\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7454 - acc: 0.8720\n",
      "Epoch 00023: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.7456 - acc: 0.8720 - val_loss: 5.6177 - val_acc: 0.6063\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6794 - acc: 0.8777\n",
      "Epoch 00024: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.6801 - acc: 0.8777 - val_loss: 5.2717 - val_acc: 0.6254\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6265 - acc: 0.8823\n",
      "Epoch 00025: val_loss did not improve from 5.13962\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.6263 - acc: 0.8823 - val_loss: 5.2715 - val_acc: 0.6219\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5933 - acc: 0.8833\n",
      "Epoch 00026: val_loss improved from 5.13962 to 5.13491, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/026-5.1349.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5935 - acc: 0.8833 - val_loss: 5.1349 - val_acc: 0.6289\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5691 - acc: 0.8879\n",
      "Epoch 00027: val_loss did not improve from 5.13491\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5704 - acc: 0.8878 - val_loss: 5.8718 - val_acc: 0.5940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6661 - acc: 0.8803\n",
      "Epoch 00028: val_loss did not improve from 5.13491\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.6668 - acc: 0.8802 - val_loss: 5.3866 - val_acc: 0.6196\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5164 - acc: 0.8913\n",
      "Epoch 00029: val_loss did not improve from 5.13491\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5167 - acc: 0.8913 - val_loss: 5.1409 - val_acc: 0.6352\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5098 - acc: 0.8919\n",
      "Epoch 00030: val_loss did not improve from 5.13491\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5105 - acc: 0.8918 - val_loss: 5.3811 - val_acc: 0.6233\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4853 - acc: 0.8938\n",
      "Epoch 00031: val_loss did not improve from 5.13491\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.4858 - acc: 0.8937 - val_loss: 5.6613 - val_acc: 0.6068\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4955 - acc: 0.8939\n",
      "Epoch 00032: val_loss improved from 5.13491 to 5.08392, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/032-5.0839.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.4953 - acc: 0.8939 - val_loss: 5.0839 - val_acc: 0.6438\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4470 - acc: 0.8967\n",
      "Epoch 00033: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.4479 - acc: 0.8966 - val_loss: 5.2940 - val_acc: 0.6308\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4527 - acc: 0.8963\n",
      "Epoch 00034: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.4525 - acc: 0.8963 - val_loss: 5.4606 - val_acc: 0.6196\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3932 - acc: 0.9005\n",
      "Epoch 00035: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.3939 - acc: 0.9005 - val_loss: 6.8027 - val_acc: 0.5355\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3842 - acc: 0.9021\n",
      "Epoch 00036: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.3845 - acc: 0.9021 - val_loss: 5.3434 - val_acc: 0.6268\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3435 - acc: 0.9053\n",
      "Epoch 00037: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.3440 - acc: 0.9052 - val_loss: 5.2124 - val_acc: 0.6364\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5427 - acc: 0.8904\n",
      "Epoch 00038: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5430 - acc: 0.8904 - val_loss: 5.3793 - val_acc: 0.6266\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3919 - acc: 0.9023\n",
      "Epoch 00039: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.3921 - acc: 0.9023 - val_loss: 5.3428 - val_acc: 0.6315\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3272 - acc: 0.9079\n",
      "Epoch 00040: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.3279 - acc: 0.9078 - val_loss: 5.3974 - val_acc: 0.6275\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2734 - acc: 0.9115\n",
      "Epoch 00041: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2741 - acc: 0.9114 - val_loss: 5.4758 - val_acc: 0.6254\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2945 - acc: 0.9099\n",
      "Epoch 00042: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2948 - acc: 0.9098 - val_loss: 6.4101 - val_acc: 0.5677\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2827 - acc: 0.9105\n",
      "Epoch 00043: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2831 - acc: 0.9104 - val_loss: 5.1685 - val_acc: 0.6452\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3516 - acc: 0.9054\n",
      "Epoch 00044: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.3519 - acc: 0.9054 - val_loss: 5.7894 - val_acc: 0.6105\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2402 - acc: 0.9135\n",
      "Epoch 00045: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2401 - acc: 0.9135 - val_loss: 5.7639 - val_acc: 0.6047\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2678 - acc: 0.9111\n",
      "Epoch 00046: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2676 - acc: 0.9112 - val_loss: 5.6667 - val_acc: 0.6126\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2064 - acc: 0.9166\n",
      "Epoch 00047: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2075 - acc: 0.9166 - val_loss: 5.3884 - val_acc: 0.6287\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2173 - acc: 0.9158\n",
      "Epoch 00048: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2171 - acc: 0.9158 - val_loss: 5.3736 - val_acc: 0.6327\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.9166\n",
      "Epoch 00049: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1973 - acc: 0.9166 - val_loss: 5.3081 - val_acc: 0.6364\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1863 - acc: 0.9174\n",
      "Epoch 00050: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1866 - acc: 0.9174 - val_loss: 6.2227 - val_acc: 0.5786\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1958 - acc: 0.9169\n",
      "Epoch 00051: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1965 - acc: 0.9169 - val_loss: 5.4086 - val_acc: 0.6271\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1296 - acc: 0.9228\n",
      "Epoch 00052: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1299 - acc: 0.9228 - val_loss: 5.4843 - val_acc: 0.6236\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1562 - acc: 0.9207\n",
      "Epoch 00053: val_loss did not improve from 5.08392\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1577 - acc: 0.9206 - val_loss: 5.2541 - val_acc: 0.6429\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1814 - acc: 0.9182\n",
      "Epoch 00054: val_loss improved from 5.08392 to 5.07245, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/054-5.0724.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1816 - acc: 0.9182 - val_loss: 5.0724 - val_acc: 0.6529\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1403 - acc: 0.9214\n",
      "Epoch 00055: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1408 - acc: 0.9214 - val_loss: 5.2767 - val_acc: 0.6371\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1519 - acc: 0.9206\n",
      "Epoch 00056: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1527 - acc: 0.9206 - val_loss: 5.6695 - val_acc: 0.6191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1112 - acc: 0.9230\n",
      "Epoch 00057: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1110 - acc: 0.9230 - val_loss: 5.2548 - val_acc: 0.6403\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.9274\n",
      "Epoch 00058: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0713 - acc: 0.9274 - val_loss: 5.2509 - val_acc: 0.6420\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1179 - acc: 0.9229\n",
      "Epoch 00059: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1182 - acc: 0.9228 - val_loss: 5.1759 - val_acc: 0.6527\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0901 - acc: 0.9251\n",
      "Epoch 00060: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0904 - acc: 0.9251 - val_loss: 7.1867 - val_acc: 0.5201\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.9253\n",
      "Epoch 00061: val_loss did not improve from 5.07245\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1037 - acc: 0.9253 - val_loss: 5.4515 - val_acc: 0.6324\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0634 - acc: 0.9270\n",
      "Epoch 00062: val_loss improved from 5.07245 to 4.95556, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/062-4.9556.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0632 - acc: 0.9270 - val_loss: 4.9556 - val_acc: 0.6620\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0448 - acc: 0.9291\n",
      "Epoch 00063: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0451 - acc: 0.9291 - val_loss: 5.0141 - val_acc: 0.6601\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1574 - acc: 0.9216\n",
      "Epoch 00064: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1577 - acc: 0.9216 - val_loss: 5.2641 - val_acc: 0.6387\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0604 - acc: 0.9276\n",
      "Epoch 00065: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0607 - acc: 0.9275 - val_loss: 5.1757 - val_acc: 0.6490\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0343 - acc: 0.9300\n",
      "Epoch 00066: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0354 - acc: 0.9299 - val_loss: 5.6756 - val_acc: 0.6177\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0390 - acc: 0.9290\n",
      "Epoch 00067: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0393 - acc: 0.9289 - val_loss: 5.6974 - val_acc: 0.6168\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0596 - acc: 0.9273\n",
      "Epoch 00068: val_loss did not improve from 4.95556\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0607 - acc: 0.9272 - val_loss: 5.8762 - val_acc: 0.6077\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0387 - acc: 0.9298\n",
      "Epoch 00069: val_loss improved from 4.95556 to 4.91546, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_3_conv_checkpoint/069-4.9155.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0387 - acc: 0.9298 - val_loss: 4.9155 - val_acc: 0.6683\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0996 - acc: 0.9260\n",
      "Epoch 00070: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1012 - acc: 0.9259 - val_loss: 5.4303 - val_acc: 0.6369\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0004 - acc: 0.9326\n",
      "Epoch 00071: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0011 - acc: 0.9325 - val_loss: 5.0508 - val_acc: 0.6573\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0232 - acc: 0.9310\n",
      "Epoch 00072: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0231 - acc: 0.9310 - val_loss: 5.1935 - val_acc: 0.6501\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9889 - acc: 0.9331\n",
      "Epoch 00073: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9901 - acc: 0.9330 - val_loss: 5.4445 - val_acc: 0.6315\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.9263\n",
      "Epoch 00074: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0849 - acc: 0.9263 - val_loss: 6.2099 - val_acc: 0.5868\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.9324\n",
      "Epoch 00075: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0076 - acc: 0.9324 - val_loss: 5.8902 - val_acc: 0.6084\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0174 - acc: 0.9305\n",
      "Epoch 00076: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0179 - acc: 0.9305 - val_loss: 5.4388 - val_acc: 0.6350\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9908 - acc: 0.9327\n",
      "Epoch 00077: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9915 - acc: 0.9326 - val_loss: 5.0836 - val_acc: 0.6562\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9822 - acc: 0.9339\n",
      "Epoch 00078: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9829 - acc: 0.9339 - val_loss: 5.6024 - val_acc: 0.6182\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.9343\n",
      "Epoch 00079: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9819 - acc: 0.9343 - val_loss: 5.3023 - val_acc: 0.6424\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.9334\n",
      "Epoch 00080: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9837 - acc: 0.9334 - val_loss: 5.0342 - val_acc: 0.6583\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9536 - acc: 0.9360\n",
      "Epoch 00081: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9539 - acc: 0.9360 - val_loss: 5.1103 - val_acc: 0.6553\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9538 - acc: 0.9353\n",
      "Epoch 00082: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9541 - acc: 0.9353 - val_loss: 5.0989 - val_acc: 0.6511\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9302 - acc: 0.9375\n",
      "Epoch 00083: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9310 - acc: 0.9374 - val_loss: 5.9644 - val_acc: 0.5984\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9725 - acc: 0.9343\n",
      "Epoch 00084: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9728 - acc: 0.9343 - val_loss: 5.2430 - val_acc: 0.6487\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0022 - acc: 0.9320\n",
      "Epoch 00085: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.0020 - acc: 0.9320 - val_loss: 6.0539 - val_acc: 0.5975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9200 - acc: 0.9386\n",
      "Epoch 00086: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9207 - acc: 0.9385 - val_loss: 5.9387 - val_acc: 0.6028\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9184 - acc: 0.9379\n",
      "Epoch 00087: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9191 - acc: 0.9379 - val_loss: 5.2387 - val_acc: 0.6501\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9253 - acc: 0.9375\n",
      "Epoch 00088: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9256 - acc: 0.9375 - val_loss: 5.5810 - val_acc: 0.6287\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.9383\n",
      "Epoch 00089: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9199 - acc: 0.9383 - val_loss: 5.4419 - val_acc: 0.6403\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9524 - acc: 0.9357\n",
      "Epoch 00090: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9527 - acc: 0.9357 - val_loss: 5.3272 - val_acc: 0.6438\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9060 - acc: 0.9400\n",
      "Epoch 00091: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9063 - acc: 0.9400 - val_loss: 5.2600 - val_acc: 0.6443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.9403\n",
      "Epoch 00092: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8921 - acc: 0.9403 - val_loss: 5.4604 - val_acc: 0.6350\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.9378\n",
      "Epoch 00093: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9261 - acc: 0.9378 - val_loss: 5.4004 - val_acc: 0.6366\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9763 - acc: 0.9349\n",
      "Epoch 00094: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9770 - acc: 0.9348 - val_loss: 5.3981 - val_acc: 0.6378\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8637 - acc: 0.9428\n",
      "Epoch 00095: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8641 - acc: 0.9428 - val_loss: 5.0428 - val_acc: 0.6641\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8819 - acc: 0.9417\n",
      "Epoch 00096: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8822 - acc: 0.9417 - val_loss: 4.9731 - val_acc: 0.6629\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8886 - acc: 0.9405\n",
      "Epoch 00097: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8893 - acc: 0.9404 - val_loss: 5.2615 - val_acc: 0.6494\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8668 - acc: 0.9423\n",
      "Epoch 00098: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8671 - acc: 0.9423 - val_loss: 5.1019 - val_acc: 0.6546\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8923 - acc: 0.9402\n",
      "Epoch 00099: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8929 - acc: 0.9401 - val_loss: 5.0183 - val_acc: 0.6639\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.9410\n",
      "Epoch 00100: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8818 - acc: 0.9409 - val_loss: 5.6949 - val_acc: 0.6177\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8491 - acc: 0.9434\n",
      "Epoch 00101: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8493 - acc: 0.9434 - val_loss: 5.3723 - val_acc: 0.6387\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.9382\n",
      "Epoch 00102: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9289 - acc: 0.9381 - val_loss: 6.1797 - val_acc: 0.5884\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.9412\n",
      "Epoch 00103: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8882 - acc: 0.9411 - val_loss: 5.2219 - val_acc: 0.6511\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8525 - acc: 0.9439\n",
      "Epoch 00104: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8528 - acc: 0.9439 - val_loss: 5.7699 - val_acc: 0.6198\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8818 - acc: 0.9416\n",
      "Epoch 00105: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8834 - acc: 0.9415 - val_loss: 5.2159 - val_acc: 0.6513\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8494 - acc: 0.9438\n",
      "Epoch 00106: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8502 - acc: 0.9438 - val_loss: 5.8924 - val_acc: 0.6129\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8568 - acc: 0.9430\n",
      "Epoch 00107: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8572 - acc: 0.9430 - val_loss: 5.1836 - val_acc: 0.6515\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8598 - acc: 0.9430\n",
      "Epoch 00108: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8601 - acc: 0.9430 - val_loss: 5.3466 - val_acc: 0.6459\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8253 - acc: 0.9454\n",
      "Epoch 00109: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8252 - acc: 0.9454 - val_loss: 5.2704 - val_acc: 0.6480\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.9424\n",
      "Epoch 00110: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8630 - acc: 0.9424 - val_loss: 5.1151 - val_acc: 0.6576\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.9443\n",
      "Epoch 00111: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8416 - acc: 0.9443 - val_loss: 5.4299 - val_acc: 0.6382\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.9439\n",
      "Epoch 00112: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8479 - acc: 0.9439 - val_loss: 5.6214 - val_acc: 0.6280\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.9418\n",
      "Epoch 00113: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8770 - acc: 0.9418 - val_loss: 5.2208 - val_acc: 0.6508\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8482 - acc: 0.9430\n",
      "Epoch 00114: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8486 - acc: 0.9430 - val_loss: 5.3213 - val_acc: 0.6445\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8082 - acc: 0.9470\n",
      "Epoch 00115: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8086 - acc: 0.9470 - val_loss: 5.3134 - val_acc: 0.6452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8450 - acc: 0.9435\n",
      "Epoch 00116: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8449 - acc: 0.9435 - val_loss: 5.1265 - val_acc: 0.6594\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.9469\n",
      "Epoch 00117: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8120 - acc: 0.9469 - val_loss: 5.2971 - val_acc: 0.6452\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8046 - acc: 0.9464\n",
      "Epoch 00118: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8045 - acc: 0.9464 - val_loss: 5.1843 - val_acc: 0.6553\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.9457\n",
      "Epoch 00119: val_loss did not improve from 4.91546\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8171 - acc: 0.9456 - val_loss: 5.2770 - val_acc: 0.6480\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXl8lNX1/z93tmQm+x5IgCSyLyFAUHABBEUQxAXXulu31p/Wr9VKXVrX1rrUpa22aLGuuCFaRaGiULTswbDJDoEsZE8mmSSz398fJ09mksyEmWQmM0nO+/Wa18w8y33Os32e85x77r1CSgmGYRim/6MKtQEMwzBM78CCzzAMM0BgwWcYhhkgsOAzDMMMEFjwGYZhBggs+AzDMAMEFnyGYZgBAgs+wzDMAIEFn2EYZoCgCbUB7iQnJ8usrKxQm8EwDNNnKCgoqJZSpviybFgJflZWFrZv3x5qMxiGYfoMQojjvi7LIR2GYZgBAgs+wzDMAIEFn2EYZoAQVjF8T9hsNpSUlMBsNofalD5JZGQkMjMzodVqQ20KwzAhJuwFv6SkBDExMcjKyoIQItTm9CmklKipqUFJSQmys7NDbQ7DMCEm7EM6ZrMZSUlJLPbdQAiBpKQkfjtiGAZAHxB8ACz2PYCPHcMwCn1C8BkmpGzZAuzYEWorGKbHsOCfgvr6erz66qvdWvfCCy9EfX29z8s/9thjeP7557u1LSaI3HsvsGRJqK1gmB7Dgn8KuhJ8u93e5bpfffUV4uPjg2EW05s0NAB+PLgZJlwJmuALIUYJIQrdPg1CiHuDtb1gsWTJEhw5cgR5eXl44IEHsH79epxzzjlYtGgRxo4dCwC45JJLMGXKFIwbNw5Lly5tWzcrKwvV1dUoKirCmDFjcNttt2HcuHGYO3cuWlpautxuYWEhpk2bhtzcXFx66aWoq6sDALzyyisYO3YscnNzcfXVVwMA/vvf/yIvLw95eXmYNGkSGhsbg3Q0BigmE8DHlOkHBC0tU0p5AEAeAAgh1ABKAazsSZmHDt0Lk6kwANa5iI7Ow4gRL3md/8wzz2DPnj0oLKTtrl+/Hjt27MCePXvaUh2XLVuGxMREtLS0YOrUqVi8eDGSkpI62H4Iy5cvx+uvv44rr7wSK1aswHXXXed1uzfccAP+8pe/YObMmfjd736Hxx9/HC+99BKeeeYZHDt2DBEREW3houeffx5/+9vfcNZZZ8FkMiEyMrKnh4Vxp6kJcDhCbQXD9JjeCunMAXBESulzJz/hzOmnn94ur/2VV17BxIkTMW3aNBQXF+PQoUOd1snOzkZeXh4AYMqUKSgqKvJavtFoRH19PWbOnAkAuPHGG7FhwwYAQG5uLq699lq8++670GjoeX3WWWfhvvvuwyuvvIL6+vq26UyAaGpiD5/pF/SWMlwNYHlPC+nKE+9NoqKi2n6vX78ea9euxaZNm2AwGDBr1iyPee8RERFtv9Vq9SlDOt5YtWoVNmzYgC+++AJPP/00du/ejSVLlmDBggX46quvcNZZZ2HNmjUYPXp0t8pnOuBwAGYzYLEAUgKc5sr0YYLu4QshdAAWAfjYy/zbhRDbhRDbq6qqgm2O38TExHQZEzcajUhISIDBYMD+/fuxefPmHm8zLi4OCQkJ+P777wEA77zzDmbOnAmn04ni4mKce+65+NOf/gSj0QiTyYQjR45gwoQJePDBBzF16lTs37+/xzYwrTQ10beUrt8M00fpDQ9/PoAdUsoKTzOllEsBLAWA/Px82Qv2+EVSUhLOOussjB8/HvPnz8eCBQvazZ83bx7+/ve/Y8yYMRg1ahSmTZsWkO2+9dZbuPPOO9Hc3IycnBy8+eabcDgcuO6662A0GiGlxD333IP4+Hg8+uijWLduHVQqFcaNG4f58+cHxAYG7UW+sRGIjg6dLQzTQ4SUwdVYIcQHANZIKd881bL5+fmy4wAo+/btw5gxY4Jl3oCAj2EPOHQIGDmSfh844PrNMGGCEKJASpnvy7JBDekIIaIAnA/g02Buh2GCRkcPn2H6MEEVfCllk5QySUppDOZ2mCAiJbB+PX0PRFjwmX4Et7Rluub774Fzz6X+ZAYiJpPrNws+08dhwWe65uRJ+q7wWOfe/2EPn+lHsOAzXdPapcOA7UuGBZ/pR/QLwZdSItjZRgMWRfCNA7Qaxj2k09AQOjsYJgD0C8E3mX6ExVIaajPaiPaSq+1teljDHr7rN3v4TB+nXwg+7QZ3bhUUBrqHrwh+VBQLPtPn6ReCL4QKUjqDUvaSJUvwt7/9re2/MkiJyWTCnDlzMHnyZEyYMAGff/65z2VKKfHAAw9g/PjxmDBhAj788EMAwMmTJzFjxgzk5eVh/Pjx+P777+FwOHDTTTe1Lfviiy8GfB+7ZKB7+CYToNcDcXEs+Eyfp291q3jvvUBh5+6R9Y4mQKgAld7/MvPygJe8d8p21dy5uPeRR3DXXXcBAD766COsWbMGkZGRWLlyJWJjY1FdXY1p06Zh0aJFPo0h++mnn6KwsBA7d+5EdXU1pk6dihkzZuD999/HBRdcgIcffhgOhwPNzc0oLCxEaWkp9uzZAwB+jaAVEAa64Dc1kXcfE8OCz/R5+pbgeyV4PRhOSk9HZUUFysrKUFVVhYSEBAwZMgQ2mw0PPfQQNmzYAJVKhdLSUlRUVCA9Pf2UZf7www+45pproFarkZaWhpkzZ2Lbtm2YOnUqbrnlFthsNlxyySXIy8tDTk4Ojh49irvvvhsLFizA3Llzg7avHuGQDgs+02/oW4LvxRO3NB+ElA5ERQWhvxirFVecfz4++eQTlJeX46qrrgIAvPfee6iqqkJBQQG0Wi2ysrI8dovsDzNmzMCGDRuwatUq3HTTTbjvvvtwww03YOfOnVizZg3+/ve/46OPPsKyZcsCsWe+oXj2A9XDN5mowzQWfKYf0E9i+GoEpdLW6QTsdlx13nn44IMP8Mknn+CKK64AQN0ip6amQqvVYt26dTh+3PexXc455xx8+OGHcDgcqKqqwoYNG3D66afj+PHjSEtLw2233YZbb70VO3bsQHV1NZxOJxYvXoynnnoKO3bsCPx+dgV7+OThx8ZyWibT5+lbHr5XglRpa7MBAMZlZaGxsREZGRkYNGgQAODaa6/FRRddhAkTJiA/P9+vAUcuvfRSbNq0CRMnToQQAs8++yzS09Px1ltv4bnnnoNWq0V0dDTefvttlJaW4uabb4bTSfv3xz/+MfD76Q2n0yX0PfXwT54Ezj4bWLUK6EuDs3BIh+lHBL17ZH/obvfIZvMJ2Gw1iImZFFiDTCZAGUwkLw/oo0MHdrt75Lo6IDGRslRsNsBq7f6IT2vWAPPmAe+/D1xzTffKCAWTJgFDhgCZmcDHHwNhOEgPM7AJm+6RewshVACcgW9ta7W6ftvtgS27L6CEc7KyaP+7OSwjAFefPDU1PTarV2EPn+lH9AvBB9QAZOsngLSGdADQ2KYDDUXwlQHbexLWKS+n79rantnU27gLvsXS/ppgmD5GvxB8qrQFpAywKLvf3APdwwd6VnHbVwXfPUsHYC+f6dP0E8FXdiPAFbcc0qHvQHj4wQjp7N8PrFgRuPI6ogxcrnj4QHgIvtkcHnYwfY5+IfgU0gmShx8ZSb/dQzrNzX0vFt0dOnr44RbS+fOfgeuuC95oXFYrnfdwE/y77wZmzgy1FeGLxQL89FOorQhL+oXgBy2kY7VShgrQ3sOvqAD8yLvvs3T08HsS0lE8/EAKflkZebvBChMpHae5h3TCIRd/927gxx+BoqJQWxKevPEGZVeFw7kKM/qF4Lt2I4AhHSkBmw31LS14dcWK9oJvs1GO+ikqci+88MLe7/smkNTVAVotMHgw/Q+Ehx/INyPlIVJSErgy3VH6wlcaXgHh4eGfOEHfa9aE1o5w5eBBctbKykJtSdgRVMEXQsQLIT4RQuwXQuwTQkwPznaC4OE7HIDTSYL/8cftxb21Mtd+iq4UvvrqK8THxwfOpt6mrg5ISACUfeiuh9/U5BLKQHrjwRZ8966RwyWkY7W6Hp4s+J5RrgflODFtBNvDfxnAainlaAATAewLxkaUStuAtrZtFfUlf/wjjpSUIO/CC/HAAw9g/fr1OOf667HovvswduJEAMAll1yCKVOmYNy4cVi6dGlbEVlZWaiurkZRURHGjBmD2267DePGjcPcuXPR4iGn/YsvvsAZZ5yBSZMm4bzzzkNF6ziyJpMJN998MyZMmIDc3FysaK2oXL16NSZPnoyJEydizpw5gdt3BUXwIyPJ0++uh6/ceEOGUJnOAJwnh8M1zm6wBT+csnRKS+ntMyYG+Pbb3kkTrakB7ruv79RbseB7JWhNR4UQcQBmALgJAKSUVgDWrtY5FV56Rwagg8MxCipVhN8NQb32jtyaofPME09gz549KPz0U2DMGKz/7jvs2LcPez74ANmtFWfLli1DYmIiWlpaMHXqVCxevBhJSUntijt06BCWL1+O119/HVdeeSVWrFiB6667rt0yZ599NjZv3gwhBN544w08++yzeOGFF/Dkk08iLi4Ou3fvBgDU1dWhqqoKt912GzZs2IDs7GzUBiOOrQi+EOTl91Twx40DiovpTSEhoWe2VVW5Hhy9EdIJF8FXwjnXXgv8/e/Ali3UZUUwWbsWePFFYN8+6hpDFeaRYBZ8rwTzzGUDqALwphDiRyHEG0KIqCBuL7AtbRXPSaulbyWk43Dg9HHjkJ2R0bbMK6+8gokTJ2LatGkoLi7GoUOHOhWXnZ2NvLw8AMCUKVNQ5KHCraSkBBdccAEmTJiA5557Dnv37gUArF27tq0/fgBISEjA5s2bMWPGDGS3VqgmJiYGYq/bowg+QAOAdDeko9x4Y8fSdyAeTko4BxhYIR1F8G+6CVCrgdWrg79N5fiuXg089VTwt9cTbDbXtaG8ATJtBLNzGA2AyQDullJuEUK8DGAJgEfdFxJC3A7gdgAYOnRolwV6H6dEoLHxILTaNERGZvbUbkLJwddqycNVKm3tdkQpmTs2G9avX4+1a9di06ZNMBgMmDVrlsdukiMiItp+q9VqjyGdu+++G/fddx8WLVqE9evX47HHHgvMvnSXujpg1Cj63RMPX7kBx42j75oa4LTTemab8hDRansnpKPT0SfUgl9cTN8TJgBnnEFx/ECK8McfA//3f8Dhw66U5OJiOgaXXQY89hht94ILArfNQHLypCtNlz38TgTTwy8BUCKl3NL6/xPQA6AdUsqlUsp8KWV+SkpKtzcW8C6SbTZAo0FMXBwam5rIw5eyfeWt3Q6j0YiEhAQYDAbs378fmzdv7vYmjUYjMjIyAABvvfVW2/Tzzz+/3TCLdXV1mDZtGjZs2IBjx44BQHBDOgAJfk88fLXa9fAIpIefm9s7IR0gPPrTOXECSE4GDAbqjK6gAKiuDlz527ZRPYF72nFJCdW/vPYaMHIk8LvfBW57gcb9WmDB70TQBF9KWQ6gWAjRepdjDoAgtoZQec7SsVqBQ4eoMYY/WK2AVoukpCScdfrpGH/llXjg/vtdnr5KBdhsmDdvHux2O8aMGYMlS5Zg2rRp3d6Dxx57DFdccQWmTJmC5OTktumPPPII6urqMH78eEycOBHr1q1DSkoKli5dissuuwwTJ05sG5gFzc2BqchzOsmjdw/p9MTDT0sjoQICK/hTp5IHGozGV+4hHYAEPxC53Q4HsGxZ9x6gJ04AypvwBRfQfq9d23ObFJRURveQY0kJ9RZqMACzZ9P9FK4ogj9s2KkF32ikMOOmTcG3K0wIdn+/dwN4TwihA3AUwM3B2hB5+B6yP06coBNbXk4Xga/YbPQKD+D9pUvpBpgwAaipwawXX6QGWXY7IiIi8PXXX3ssQonTJycnt41JCwD333+/x+UvvvhiXHzxxZ2mR0dHt/P4FebPn4/58+e3n3joEOWMK42luktDA4mJu4ffk0rb9HRAqcgORLbHyZNk0/DhJMwNDfRQCiTuIR0gcB7+8uXAz39OocKb/bwlTpygfQaAyZPpOtyyBbj66p7bBZB3D3QWfGVozexsevMzGgN/vAOBEvLKzwf+97+ul927lyqi168HpgclYzzsCGp1u5SysDVckyulvERKWRe8rak7e/hGI4mURkMi409/ODabq8JWTXn+cDjaQj3Q6cKv50TFvubmnpeltLINVKVterornz9QHv6gQeR5AsEJ65hM9Can1L/ExvZc8B0OV8zdveLZV9w9fI0GmDgRCOQoaIqHr4R07HayUznOSjcb4drKt6SE3shGj6ZMrq4aRx49St/hui9BIMzzq3xHiA6jXjmddHNERgIjRtB/XwevcDrbefhtA5/Y7a4HgVYbfoKvVDSbzT3Pde8o+PHx5PF2Z58VcdZo6MGhCL7VSq/UU6dSXHjnTv/LDKbgKx2nKbm+gfDwP/wQOHCAfvsbYzYaafvuyQ1TplA3C4Fo2wB0DumUl1PZynFW3hzDVSSV8NOgQST2ytukwwF8/nn70B8Lft+lU6VtRQXF7YcMcaXVVVb6dmN0TMn0JPgaTVtr3LBBEXwpSfR7ghK+cRd8wP8YtsNBxz09nf4nJbluwiNH6JW6uhp4+mkKUaxc6Vu5vSX4SjgH6LngOxzAE08A48eTE+Jv2qCSkuku+JMnk02HD3ffLoWGBldFteLhK8e1o+C3JguEHYrgp6XRf+Wh+sUXwCWXAN9/71pW2QcW/L6I2uXht7SQp5KQ4IozpqWRWPsSh1YEX/HwO4Z0FA8fCK9uk927c+5pWMdTSAfwP45fU0PHTRH8xESXh69U/n34IYnf1Kk0/OH69V2XKaVL8AcNIg88WCGdKLemIz0RfJsN+Mc/yLv//e/Jbn89fE+CP2UKfRcUdM8udxTv3mBwiWBHwU9MpIdguIhkU1P746gIvnK9KfNaGy3CrS6tzcM/fjy8HLcg0m8En0I6ramTRUUk0u43RlwcxWK78qrq68lTOniQ/ncV0lGmhVNYRxlzVqU69XCEJlPXPX56CukA/gu+EqduHfy9neArXunw4ZTBs2oVkJMDLFrkrUk1YTTS29ugQXSO0tL8F3wpgbfe6ro+QQnpKHRH8MvKgPnz6TjedRf14njZZSRI3RX8IUNc08aOpes6EHF8RfDPOIN+WyydBV8I8vLDxcN/6CGqoHU66f4sK6Pjowi+cr/v29f+GyDBV6loPwdII61+JPgU0pHl5XSjDh3q8sJpASA1leZ58n4VsW9qorDDyJGurpFVKteFIWXvePhmM3mDP/1EF6MvDxaLhezS60/t4f/f/wHTpnlPZ/Tm4ftbcauImqeQzqFD9ABQWgknJQH/+Q95mA895L1M5SGilJmZ6b/g791LrVVfe837Mt5COv6kgK5aRS1Ur72WGjWtX0/XUncEv7iYHA1lvwE637m5gfXwzzzTtb3iYrqe3LvCyMoKHw9/927KLPrxx/b1DR09fKV/fEXwLRZaL7917O/u7I/d3v6tug/QPwTfboeq0QZdNVyhHE99tSQlkfB3rLxtbqanvcFAqZfDhrm6w1VQq11es7vgexDiaHeR8BcpKeb9008u0S4uBnbtOrWIW63k7en1ZKs3YbLbgU8/pZuhstLzMnV1JC6Kh9tdD7+j4HcM6YwY0X75zEwa1OTbb70/XDq+NXRH8DdupO8tW7wv4ymko4yC5Su7d1MZr70GXH6567pKS6OYuT8Dw584QfuqhBgVJk8mD7/j+a6ro0FifHVKlJRMJUWxqMjV6Mq9kyrFww/WwDP+oLxprFnT/m0kOpru5/JyCikqFeWK4B8/TvbPnt2+HH+45RbgnHN6Zn8v0/cF3+kEdu6EtqgauhoABj159556UdNoSHCUuDJAgn34MN1Ew4d77xhKo2kv+O5hnkBSV0c3dnQ0dUUwdix9pHR53d6wWinEYTC4wk+e2LDBJbrKjeDJDqXjNKD7Hn5Hbzwxkcp2OOi4Kznl7lx2Ge3LV191XWagBN+bcHkK6QD+hXV27SInouN11THk4AvuKZnuTJlC50WJSSssXw78+tf08PSFsjJ6ICldYBw/7oqJu5OVRcdAuYbeeIM6cPPn4RUIbLb2YwN0DD8pb1FFRfTGPGIE7aP7sTr3XPr218M/dAh4911g69Y+NU5z3xd8lQoYOhT2nHSYhgNyVE77UE5HUlLoIVFb6xIdm42ER4nZu7FkyRLq1kCtBpxOPLZ0KZ7/619hamnBnF/+EpPPPx8TJkzA559/fkpTvXWj3K6b44ULAY0GpkGDcPMdd1CXyNOmYcX//te10DidLsFXQlHe3gg+/dQlQPv3e17GvVsFoGcefkyMSziTkkhgKyroZu3o4QMUakpPJzs94Unw6+tdGSa+sHEjndPKSu91GZ5COoDvgi8lefgTJnSe1zHk4AveBH9ya48lHeP4SgXll1/6Vn5ZGQ12o7xFKB5+R8HvmJr51lvUyOnxx33bTqA4cYKu+8GD6XwqYRuljkMRfMWrX7yYvvftc3n048dTqNdfwX/+eZejsHVrj3ajNwl2S9uAcu/qe1FY7rkyT0o7nM4WqFRRboOae6GpCdgqACGQZ8jBSxe/1t6Tc+Oqq67Cvffei7taO4v6aO1arPnuO0RGRmLlyy8jNi0N1bGxmDZtGhYtWgTRRf/MnrpRdjqdrm6Os7JQu2EDEBuLJ596qn2XyHv2kN0OR+dXeqB9ZpEi+C0tLqFWcDqBzz4DFi4Evvnm1B6+QmwsefvdqbRVhBlwxeu3b6cbxpPgq1SUQvf227QPyv64l6nXu8IjiiCVllJ/PR09845UVZGHdtVVlCG0ZYurQZE7HUM6/o56VVZGjkVubud5/gq+w0H7515hqzB+PDk5BQXAFVe4prsL/iuveH7rdae0FMjIoLfXjAzygsvKPHv4AInm6NHA5s30YHz+eTqmkyb5tk89RRHt226jh82777avb0hPd9WDAfTm+MwzJPhHj1L4Mz3d/zqJ8nJ6yF17Lb1FbdpE/Rr1Afq+h9+GcjH7EFfU6egGstvpJu5iVKpJkyahsrISZTU12HnwIBJiYzEkKwtSSjz0178id/58nHfeeSgtLW0bsMQbnrpRbtfNcUsLEluH0+vUJfKQISSQ3rxYpfIoIsLVEtiTh799O93YixdTxbSvHr5KRR6uPyGdxkZqTOVeyagIvhI79yT4AN2czc1UidsR5SGiCJgiSIqIp6e7mth7Quk75c47qWGeNw+tpyEdJRXQk4ev5In7GtIpL6fr1ZOHHxFBou/u4UtJgh8XR2Lmy6DeiocPkAhu3Urb9Cb4RUWU1263U1gnOZnE11uY024HPvigZ2Mju6OEZa67js7ToUNkq3JdpKW5PPxBg+hBpNO5BD87m65rfwX/5ZfJwXrsMTq3fagvnj7l4b80z2v/yLDbTWhp2Q+9fgQ0mlP08aFU4iQluW68LrjiiivwyerVKC8qwlWtT/L33nsPVfX1KPjwQ2jz8pCVleWxW2QFn7pRVho1dawwBsiDEoLExlMfJkrncEpYymDwHFNduZIeCAsXUgaJp+yOkycpi+WGG9pP96c/nZMngQULKGT2xBOu6Up/Oorge4rhA8CsWbS9lSuBjv0LlZe3f2tQBOm661x9AP3rX8Cjj8IjGzeSRzxtGoVDPFXcOp30wHEP6SjH3deY7a5d9O1J8FNTXfuisGwZxcJHjuy8/JEj9O2tC/EpUygEJiVdJydP0kN7yRLyar/80hWb94SU7QV/2DCq6wE6C358PH2OHaOQmE4HXHQRieeVVwLPPus5y2rZMuCOO+gt5Z//BM4/37s9vnD0KJ3H7Gy6Xlatam9rejrV1+3cSfVgGg0d2337yOnJyaHlsrLordfpPPXgLg0NVAG/eDFdu9OnA++/79u6YUD4W+gjfo1rq1bTBeCD2AMU1vngiy/wyXff4YoLLwRAXRmnJidDKwTWrVuH413ltLcu76kb5XbdHDc0oNZiAXS6zl0iNzSQF+Otpavi4bsLvtncuS+RlSvp5khMpNfxY8c69yT65JNU3q9/3X66p/50pOy8/ubNdCMcPEgtHK+80jVP8fC3bSPx9zbylVZL+fj//jc9lNzfVjqGiVq7lIbVCqxYAcyZQ+LirTHNxo0kkJGRlHNeUNC5glvZnruHrwixL94yQIKfmena5477l5zsEvyGBupQ7ZlnPJf12Weuh5Qnpk2jB5HyxtY6eA4uuIA821PF8aur6Ri4e/gKHQUfIJEtKqIK4enT6Xq7/HJ6w3r0UWDduvbLO500oMXo0XRM586l87R4MTkW3emB89gxslOtdvXP7x7yUt4sCwuBMWPo95gxJPhHjrQXfPexgrti1Sq6B371K/o/fTqdO1+viRDTjwRfGdc2gH3itzJu3Dg0NjcjIyUFg1rF5dprr8X2PXswYfFivP322xg9enSXZXjrRrldN8cLF+KqJUsAeO4SGbGxJESeXplbu3Nu8zIMBvp2TyHcs4febC69lP6PGkU3onuz/MOHgddfp1fzjt63Jw//0UfpQXD99dRN7x13UB63wwH897/U6MgdRfwaGryHcxSuvpq81Px88rTnziXPrKPgR0ZSSGHjRtq3n/+cxKij6CjHads2V675GWfQg1EJvyh07BoZIBtyclye+6nwVmGrkJbmCukoAv3DD52XczgoFHLhhZ4fHgA9xAFXK2Ulfj9uHL1pbdzY9ZuJkoOvPDx9EfzCQsp/V1IbhaBrZ+RIOndKmidAWTT79gGPPEKhpyVL6NweOEAeslsSg88oYRnA1ZtnRw8fIKdEGW1tzBgS+4YG17odO4Qzmbxnbn39NT2oldRV5buvhHWklGHzmTJliuzITz/91GmaJxwOm2xo2CYtlnKflvebmhopt22T8vhx17SKCppmtXpep6WF1nM4Tl2+0Uhl1dV5X6ahwfsyBw5I6X6s7HYpCwqkPHbMdQwfeEBKjUbKykr6X1AgJSDlJ5+41rv6aikNBinLyjpv4+KLpRwyREqzmf4XF0sZESHluHFSxsZSWWq1lPfdR7Z6wm6n5QApr7vO+74qHDxI9j38sJRRUVImJdG6f/iD93Wam6WMj5fyZz/rPG/Llvb7fOwY/X+axZ5HAAAgAElEQVT11fbLHTlC0996q/30Sy6RctSoU9tttUqp1Ur54IPel5kzR8rp0+n30qWu41Le4Rr+5hua/tFH3styOqXMzJTyyivp/y23SJmWRr83b6b133vP+/pffUXLbNxI/9eupf86HZXdkfvuc9n7ww/t5/30E52r6dOlrK+naeedJ+XgwVJaLJ3LmjFDyvx877Z5IzFRyjvvpN9Op5Qvvyzl4cOu+cp+A1KuW0fTli93TVu50mUvIOW770pZVSVlcrKUM2fSveuOwyFlamr768rppGvy5pvpf2WllP/6l5SffSblpk10vQcZANuljxrbjzx8P0I63UHJu3dP+XTPxZeS4uvV1eTZ/PQTeVlHj/qWI97QQB6SUjHoCaXnRk9hHSUlU0GtpnBJba2r2fm775KXqIwspoQolEydH38kT/Lee9t70Aq33UaVoUr63eOP036vWkX7/MEH5PW98IL3/VCrXZXkp/LwlWUWL6YuhQsKXB6cJ69TQa+nDIoVKzq3XVDy7xXPbNgwiqd3jON3HO1KITeXwg+nyjk/cIBCJF15+O6tbd3fMDr24/7++3Q8Fy70XpYQ5OWvX++qsB0/nuZNnUrn/IsvvK+veOMdQzrulaDuKN5xVBSV786YMVSHsnUrhc7eeYfe/u6+22PqM2bNIq/fn8pco5GubSUsIwRwzz3th850TxZw9/AV3EM6AHn4Tz1F5W7aRNeI+9tvYSHVWbhn5AhB4bRNm+hamzWLWnBfcgmtf/vt7e1etqzz24zJ5D1bLsD0I8EXAET7LpIDiSfBV35brfSaeOAAXTQnT9KFkJlJN1plZdeDfkhJF3B0tOeUSwUlU6ayksRvxw4qV4mjd7yZkpJI7Fta6IY7eRK48UbX/OhoeoVX4r7PP0/lP/CA5+0vWECtC//0J7qhly0DfvlLEs3oaIrfKiLTFUpYwhfBd2fUKKofWL68fb2AJ37+czom77/ffvratXSDK8ImBIV1vv2WYsxvvEHhDU8hHYAE3+n0HLMtKKDt1tW5wj6eUjIVlCwSRaAnTaKMG/ewjtlMD67Fizunp3Zk1ixXK+29e13nQqWi+pBVq7z3oqqEdJQHvdK61tuDVRHJGTM8i/jll9PDx2ymGL3B0Fn83O12Oj2Hs0pK6EHRMTtNScnsaqAfpY4uMbG9k6M8wJR19Xpa9rvvgFdfpXP47bd0b02f7jo2ykBHSvhIYfp0uocWLCBn4LPPKBvu5pspfdO9k7Y776Sw54oVNK2lhZISZs7sneEzfX0V6I2Pt5CO09MrpQcaG3+ULS1FPi3rN06nlKWlUtpsrmnNzRRi2bmTvsvKKIzjHsJxOKTcv5/CJxUVFCr48UdXWEVK+r1tG80/FU1NFEopLpZy714q12Si9TuGApxO6SwslD+tW0ehmsREVzhGYc4cKU8/XcqTJykEcc89XW/faJRy6FB6BY6Obr8fvjJ1Kq2/dav/6/rDlClSjhzpeq0+fFhKISg85I57OAWg1/Ynn6Tf33/fftmDB2n6smWdt3fxxTRvyhQp77iDjqenEIbCs8/S8g0NUqakSHnrrRTemDrVtczHH9My//nPqff38GFa9v776XvpUte81atp2mefeV73jjvIBndyclyhio4oYZDnnuvapspKKa+5puvlmpspdPTrX3ee9+ijtJ1HHmk/fcUKml5Q0PX24+OlPPvs9tNOO43CNu6ccQaVZzDQfS4l3V86nZQ33kj/zz6bzm1Hvv3Wde24h81KS2l9Jex0yy30f9IkuncKC6WcP5+uyXfe6Xo/ugB+hHRCLvLuH0+Cf/ToUVlVVeWT6Dc27pLNzUd8PEwBwGYjofUktu5YrXRyt20jsd+zh37X1JCAb99OMXgfH2xtmM10we/a5TG273Q6ZdX+/fLoK69QrP2uuzqX8ctfUvz98cfpcjhw4NTb/fZbukiffNI/exUuuIC21VV9RSD45BPazvvv0/9776U6DOWGdsdikbK2lo5jTo7rBt6xo/1ydjuJwr33tp9eU0MCP3Mm3dSAlBMmdG3f22/Lthg4IOVLL0n50ENUD2Iy0TKLFkmZnu5bLNjppDqWhATZLh4vJV2DiYlSXnut53UvukjKiRPbT9u/37sT4nSSuCl29pQZMzyLaW4u7YteT06OwnPP+XYNXXMNPVjdueEGKefNaz/tqqs8P1h+8xua/s03dF46OgtS0jHIzpbyhRc6z7vjDroe1q2TUqWi66akhJwKrbbzg7kb+CP4YZ+Hn5mZiZKSElT5MFqVxVIJITTQ6fwcsLwnmEz0Slhb23UWhEpF4RqdzhXv37rVFcKJiPDeCKorbDZXB2habadh8yJtNmT+/vcU3uiYVw9QmlxDA/Dii5Ta5ikHvCOzZ1MsXwmL+EtKCn26aPAWEC69lLJUnnqK6i7++U8KBXmyW6ejT34+xfMvu4waFSntBhTUagqVdMzU+eQTOhd//jNl3lx6qavLA28oMWZlEPLx4yk8+Ic/kA0qFaWlPvpo16E+BSWO/8479N89716rpbjyxx9TmCUysv26paWdj8uoUV1v62c/O7VNvjJrFp2n+nrXdXHsGB3nu++msQQeeYRCico8pT1AV3QM6QEUtpMdsnBOP53i8B3DmQ8/TNu8/HLKlvLUojYqqnM/Rgq/+Q1lLi1YQDrx299SndFHH9E18vjjVDfWW/j6ZOiNjycP3x8KCs6SP/44u0dl9Bq1tVKOH09P/fXru1+OwyHlmWd27e1Mnizl2LGe3yDWrHF5s1980X07/OGnn8hj6g2UrIw5c+h7yxbf1rNYvIcLbr2VMjPcj+eMGVKOHu2aduCAlNXVXW9j506y6eyzZVt2Tl0dvT099BBlA+Xk0Fugr/zzn1TW0KGd5339Nc37/HP6X1ND196RI5TRc+utvm8n0Hz3Xedr8KWXaNrhw5RhJoTrjWvePLquA4m3t6jXXyc74uLah3R95Wc/8/z24Ev2ng8gXEI6AIoA7AZQ6ItRPRX8nTvnye3bp556wXChtrZzyKA7lJZK+eGH3ueXlLR/HXanqIgug5ycXkkh63XsdhJOwJUC2VNeeYXKU1JXlWPob4irooLW02gopqw8LHJzXa/7q1f7V6aSTnrhhZ3nWa0U7rn+egoxZma6HvaAlL/7nX/bCiSe4vjnnkspv1LSgzApicJOlZVUN3P55b1jm91O187Pf9699Y8coTCSkqIaYPwR/N4I6Zwrpazuhe1ArY6B2VzUG5sKDN767feXwYO7zlpRGtN4YsgQep39xS98Cxv0NdRqCgVcfz2lmwYCJfNm1y7Kalm+nP77G+JISiL77HYK5yjZI2efTWVffbWrBamvZGdTC1ZPKZxKWOejj6jFdXw8haKMRspEcc/g6m30ekpvVBrL1dZS1w4PPkj/4+MprfjSSymjpaioc5cbwUKtpgyiU3U+542cHMrWCQPCPobvD2p1DOz2Xkht6k+oVF0PAtIfuPZaqqtQxn/tKUpu/a5drpj59OmuvG5fUaupLqO8vH2+/hVXUPrpiy/6b5sQrjoBT1x5JfDmm1S/8MUX3a+HCQbz51OM+1e/AvLyKGbuLurz5tHoYQsXUl2Hv8e7J/SBfnJ8IdiCLwH8RwghAfxDStmN9tO+o1ZHw+FgwWc6IIRrKLtAkJhIb02ffUbiuW+fq6LUX5TGV+7tF2bNCsyQhZ644AISzbPP7roL6VDw619ThfdLL1G7l0GDOp+3mTPpLeCee+g34xfBFvyzpZSlQohUAN8IIfZLKTe4LyCEuB3A7QAw1FtPgD6iVsfA4TBBStllv/QM02Nyc6khTmYmfXe3P3QlU8eXBmuBQAj/w0S9hVZLbzVnnEGNn66+2rNnnZ/vajHN+EVQBV9KWdr6XSmEWAngdAAbOiyzFMBSAMjPz/fSY5FvaDQxAJxwOpuhVoeZ98L0L+6/n0JEDzzguTtrX1Fag3bVdfFA4+qrKY2xY+oo02OCJvhCiCgAKillY+vvuQCeOMVqPUKtpv5bHA4TCz4TXGbPdvUS2RMuvpjCF57GOBjIdNWnFNNtgunhpwFY2Rpa0QB4X0q5Oojbg1ZLjWSs1krodL71dc8wIeXSS13dVTNMkAma4EspjwKYGKzyPREZmQUAMJuLEB3dRS+FDMMwA5D+kWvUSmQk9X7Xp3LxGYZheol+JfhabQpUKgPM5mOhNoVhGCbs6FeCL4RAZGQWe/gMwzAe6FeCD6BV8NnDZxiG6Ug/FPxs9vAZhmE80A8FPwt2ez1stvpQm8IwDBNW9DvB1+s5U4dhGMYT/U7wXbn4HMdnGIZxpx8KPnv4DMMwnuh3gq/RJPS9gVAYhmF6gX4n+JSLn80hHYZhmA70O8EHwI2vGIZhPNBPBZ88fBrfl2EYhgH6reBnweEwwW6vDbUpDMMwYUO/FXwAaGnhOD7DMIxCvxR8bnzFMAzTmX4p+Nz4imEYpjP9UvA1mjhoNAns4TMMw7jRLwUfACIjc9DScjjUZjAMw4QN/Vbwo6MnwGTaGWozGIZhwoagC74QQi2E+FEI8WWwt+VOdPQk2GwVsFhO9uZmGYZhwpbe8PB/BWBfL2ynHdHReQAAk6mwtzfNMAwTlgRV8IUQmQAWAHgjmNvxRHT0RAAs+AzDMAo+Cb4Q4ldCiFhB/FMIsUMIMdeHVV8C8BsAzh5Z2Q00mjhERubAZPqxtzfNMAwTlvjq4d8ipWwAMBdAAoDrATzT1QpCiIUAKqWUBadY7nYhxHYhxPaqqiofzfGN6Og89vAZhmFa8VXwRev3hQDekVLudZvmjbMALBJCFAH4AMBsIcS7HReSUi6VUuZLKfNTUlJ8NMc3oqPz0NJyGHZ7Y0DLZRiG6Yv4KvgFQoj/gAR/jRAiBqcI00gpfyulzJRSZgG4GsB3UsrremStn1DFrURT067e3CzDMExY4qvg/xzAEgBTpZTNALQAbg6aVQEiOnoSAK64ZRiGAXwX/OkADkgp64UQ1wF4BIDR141IKddLKRd2x8CeEBGRAY0miQWfYRgGvgv+awCahRATAfwawBEAbwfNqgAhhEB0dB4aGzlTh2EYxlfBt0saPupiAH+VUv4NQEzwzAoc0dF5aGraA6fTFmpTGIZhQoqvgt8ohPgtKB1zlRBCBYrjhz0xMZMgpQXNzftDbQrDMExI8VXwrwJgAeXjlwPIBPBc0KwKIDExpwMAjMYNIbaEYRgmtPgk+K0i/x6AuNYGVWYpZdjH8AHAYBgBvX4Eamp6te82hmGYsMPXrhWuBLAVwBUArgSwRQhxeTANCyRJSQtRV/cd7HZTqE1hGIYJGb6GdB4G5eDfKKW8AcDpAB4NnlmBJSlpIaS0or7+21CbwjAMEzJ8FXyVlLLS7X+NH+uGnLi4s6FWx3JYh2GYAY3Gx+VWCyHWAFje+v8qAF8Fx6TAo1LpkJh4AWpqVkFKJyjJiGEYZmDha6XtAwCWAsht/SyVUj4YTMMCTVLSQlitJ7m7ZIZhBiy+eviQUq4AsCKItgSVxMT5AARqar5ETMyUUJvDMAzT63Tp4QshGoUQDR4+jUKIht4yMhDodCmIjZ2G6uovQm0KwzBMSOhS8KWUMVLKWA+fGCllbG8ZGSiSky+ByVSAlpZjoTaFYRim1xlQtZcpKVcCAKqqPg6xJQzDML3PgBJ8vT4LMTGno7Lyo1CbwjAM0+sMKMEHgNTUK2EyFaC5+XCoTWEYhulVBpzgp6RcAYDDOgzDDDwGnOBHRg5FbOx0VFVxWIdhmIHFgBN8gCpvTaZCNDcfDLUpDMMwvcaAFPzU1CsAqHH48H08EhbDMAOGoAm+ECJSCLFVCLFTCLFXCPF4sLblLxERGRgx4q+orV2FgwdvB43eyDAM07/xuWuFbmABMFtKaRJCaAH8IIT4Wkq5OYjb9JmMjDths1WgqOgx6HTpyMn5Y6hNYhiGCSpB8/AloYw4om39hJUrPWzY75CefjNOnHgGVmtFqM1hGIYJKkGN4Qsh1EKIQgCVAL6RUm4J5vb8RQiBQYNuBwAYjT+E2BqGYZjgElTBl1I6pJR5oEHPTxdCjO+4jBDidiHEdiHE9qqqqmCa45GYmMlQqQyor/9vr2+bYRimN+mVLB0pZT2AdQDmeZi3VEqZL6XMT0lJ6Q1z2qFS6RAbOx319Rt6fdsMwzC9STCzdFKEEPGtv/UAzgewP1jb6wnx8TPQ1LQLNltdqE1hGIYJGsH08AcBWCeE2AVgGyiGH5aDysbHzwQgYTT+L9SmMAzDBI2gpWVKKXcBmBSs8gNJTMzpEEIHo3EDkpMXhtochmGYoDAgW9p2RK3WIzb2dI7jMwzTr2HBbyUubgZMpgLY7aZTL8wwDNMHYcFvJT5+BqS0o6EhLBoCMwzDBBwW/FZiY88EoEZd3dpQm8IwDBMUWPBb0WhikJS0EGVlr3I3CwzD9EtY8N047bRn4XSacfTow6E2hWEYJuCw4LthMIxERsY9KC9fhsbGHaE2h2EYJqCw4HcgK+tRaLXJOHToLtTUfIX6+v/Cbm8MtVkMwzA9Jpj94fdJNJo45OQ8iwMHbsbu3QsAAHr9KOTnF0KtjgyxdQzDMN2HPXwPDBp0E8444wgmT96MkSNfR0vLARw//mSozWIYhukR7OF7Qa/PgV6fg9jYM2A0/oDi4meRmnoVoqNzQ20awzBMt2AP3weGD38BGk0CDhy4FfX1G1Bd/SWamw+G2iyGYRi/YA/fB7TaJAwf/gr27bsGhYUzAQAqVSTy83fCYBgZYusYhmF8gwXfR9LSroZefxocjgYAauzdeykOHLgNeXnrIAS/KDEME/6wUvlBbOxUJCTMQULCLJx22gswGjegrGxpqM1iGIbxCRb8bpKefjPi4+fg6NHfwGwuDrU5DMMwp4QFv5sIITBq1FJI6cSuXfNgsZSH2iSGYZguYcHvAXp9DiZM+BJm83EUFs6E2VwSapMYhmG8woLfQxISZmHixDWwWk+isHAmbLaaUJvEMAzjERb8ABAXdxZyc/8Di6UY+/ffAillqE1iGIbpRNAEXwgxRAixTgjxkxBirxDiV8HaVjgQFzcNp532HGpq/o3S0r+G2hyGYZhOBNPDtwP4tZRyLIBpAO4SQowN4vZCTkbGPUhKWogjR+7n7pUZhgk7gib4UsqTUsodrb8bAewDkBGs7YUDQgiMHv0v6HSp2L17EVpaikJtEsMwTBu9EsMXQmQBmARgS29sL5RotUmYMOErOJ3N2LnzPFgsJ0NtEsMwDIBeEHwhRDSAFQDulVI2eJh/uxBiuxBie1VVVbDN6RWioycgN3c1bLYK7Nw5G0VFj6Os7HUYjf+D02kPtXkMwwxQRDAzSoQQWgBfAlgjpfzzqZbPz8+X27dvD5o9vU1d3Trs338TLJYTbdM0mkQkJs5HSsqlSEycD7XaEEILGYbp6wghCqSU+b4sG7TO04QQAsA/AezzRez7IwkJ52L69ONwOq2wWsvR0LAVNTVfoKZmFSor34NKZUBy8iXIyvodDIZRoTaXYZh+TjB7yzwLwPUAdgshClunPSSl/CqI2wxLVCodIiOHIjJyKFJTL4fTaYfR+D2qqj5GRcU7qKz8EIMG3YLs7Keg06WG2lyGYfopQQ3p+Et/C+n4gtVaiePHn0ZZ2WvQ6QYjN3c1oqJGh9oshmH6CP6EdFjww4SGhu3YvXsBpLRj9Oi34HA0wmj8ARpNHNLSrkNUVL9uwsAwTDdhwe+jtLQcxa5d89DScggAoFbHwOFoBuBATEw+hg9/GXFxZ4bWSIZhwoqwqLRl/Eevz8HkyZtQU/MloqLGIzo6DzZbDSoq3kdJyUv48ccZyM5+CkOH/oZH2WIYxm9YNcIMrTYJ6ek3IiZmCoRQQ6dLxZAh92Lq1J1ISbkMx479Fjt3nn/KQdTtdhOqqj6FlI5espxhmHCHBb+PoNHEYezYDzFy5OtobCzAtm0TcPToI7DbjZ2WdTjM2LNnEfbuXYzS0tdCYC3DMOEIC34fQgiBwYNvxRlnHEBq6lU4ceJp/O9/adi792pUV38Bu90Ep9OOffuuQX39Ouj1w1FU9Cis1spQm84wTBjAlbZ9mMbGHSgvfxMVFctht9dACA0iIobBbD6C4cNfRkLC+di+PRdpaTdi9Og3Qm0uwzBBwJ9KW/bw+zAxMZMxYsRfcOaZZZg4cS2GDPkNIiIycdppLyAz8x5ERY1BZua9KC9fhvr6HyClM+A2VFZ+gt27L4bTaQ142QzDBBb28Ps5dnsDtm4dDav1JAAVtNokxMZOQ2LiPBgMY2G1lsNqLUdCwmxER+f6VbaUDmzZMgpm8xHk5DyLoUMfCM5OMAzjFU7LZNrQaGKRl7ceNTWrYLfXwmIpQ339OtTUfNFhSYHU1GtaUz61cDhM0OtHQquN91p2dfUXMJuPICIiE8ePP4G0tGsRETE4uDvEMEy3YcEfABgMI2EwjGz7L6VES8thmM1FiIjIgFodi7KyV1FS8hIqK99vW06rTcGIEX9FSsoVoL7w2lNS8mdERmYhN3c1tm3LxdGjD2LMmHd6ZZ8YhvEfFvwBiBACBsMIGAwj2qbl5PwBGRn/D7W1a6BS6aFSaXHixDP46aerkJT0HhISZkOny0BMzGTo9TloaNgGo/F7nHbaizAYRmHIkPtx4sQfkJ5+CxISzg3h3jEM4w2O4TNecTrtKCn5M44ffxIOh6l1qkBy8sWw2xvR2LgN06cXQ6OJhcPRhG3bcmGxlCAn50/IzPyVx7eCjjQ27oDBMAZqtT64O8Mw/RTO0mECgkqlwdChv8HZZzfgzDOrMGXKjxg27BHU129Aff23GDToNmg0sQAAtToKkydvQWLiBThy5P+we/eFMJtPeC3b6bTg4MG7UFAwBXv3Xh6UDCKGYdrDHj7jNw5HE2pqvkZi4jxoNNHt5kkpUVb2Ko4ceQCAwLBhjyIhYTaMxo0wmQqh06UgMjIb5eVvobFxKxIS5qKu7j8YNuxRZGc/EZodYpg+DGfpMEFFrY5CaurlHucJIZCRcReSkhbi8OH/w7Fjv8WxYzRPp0uHzVYHKS1Qq2MwbtwKJCdfigMHbsXx409CpxsEm60GtbWrER2dh5ycP0KjienFPWOY/g17+ExQqa//L6zWCsTFnYWIiAxI6YTVehIqVVRbyqfDYUZh4Qw0Nm4DAERFTURT0y5ERAzFiBF/gVodA6u1FCqVHrGx0zj1k2HcYA+fCRvi42e2+y+EChERGe2mqdWRmDDhS9TVfYP4+DmIiEiH0bgR+/ffhD17FnUqMyJiCPT6kYiMHAa1OgoWSyms1nKkpFzhsbK4peUITKbdSEiY3VbnwDADEfbwmbDF4WhGbe1qaDRx0OkyYLfXo6FhExobt6Kl5RgsluNwOJoQEZEJlSoCJlMhkpIWYeTIf6CpaSeqqz9Hbe1qmM0UU1KrYzF48J1ISDgPZvNxWK2lSEiYi7i46QGxt6FhGyorP0R29pOcdcT0GjziFTPgkFKitPQvOHLkfkhpAwCoVFFISJiDxMS50OtH4eTJN1BV9TGA9hlBcXEzMHTog0hMnNc2sIzJtBu1tV8hLu5sxMZOgxBqSOmE3V4HrTap0/YtllJs3z4FNlsFkpIWYdy4FVCp/HuBltLJA9swfhMWgi+EWAZgIYBKKeV4X9ZhwWd6SkPDdlRVfYL4+HMQHz8HanVku/ktLUUwm49Br8+BRpOA8vI3UVz8AiyWYhgMY5CR8f9gNG5sbXFM94ZWmwqdLg0tLYfgdJoRG3sWhg59AElJF0EIFZxOCwoLZ8Fk2o2MjLtQXPws0tNvwqhRy3xqiwAAVVUrcPDgL5CaejWGD3+5y/Xq6r6DxVKMtLQbfC6f6b+Ei+DPAGAC8DYLPhPOOJ1WVFZ+hJKSF2AyFUKl0iMj4x5kZPwCDQ2bUV39eVvfQhpNLMrL34TZXASdbhBiY8+A02lGbe1qjB37MVJTL0dR0eMoKnoMcXEzkZx8MeLjZwEQcDqb0Ny8H/X1G2Ay7YBePxLx8TNhMv2I8vJ/QacbBKv1JLKzn8awYQ95tLW2dg12774IUtowePCdGD78L36/STD9i7AQ/FZDsgB8yYLP9AWklDCZfoRONxgREelel3M67aiuXoHq6n+jsXE7WloOYujQ3yIn5w9t5RQXP4/y8jfR3Lyv0/pabTJiYvLR3LwfZnMRABWGDXsYw4Y9ggMHfo6KineRk/MnACoYjd9Dq01GWtr1EEKDXbvmQq8fiYSEOSgp+TMSE+cjMfECmM0nIKUDiYnnIz5+dkDrECiU1dBlR3pM6GDBZ5hexOFogUoV6TG80tJShMbGrRBCC7U6ChERQ2AwjG5b1mw+Dimd0OuzAdDbxu7dC1BXtxYAoNePgNVaDoejse3/pEnfQ6dLQ1nZP3Dw4F0AHFCp9K3rt0Cl0sNgGAWdbhC02lSoVFoAagDO1voNgfj4WUhOvhgaTZzX/Wpo2IKKivdQVbUCVmsZsrKexLBhDwc0jOR0WlBV9SkSE+d6rBthTk2fEnwhxO0AbgeAoUOHTjl+/HjQ7GGYUyEl4KueOZ2AzUYfgwFQudW3SglYLIDVSh+VClCr6aPV0sdmA8xmWjYqiqZZrUBlZTNKSzcjJmY8oqNTYbO1oKJiLerrt2LMmDuQkZEJtRqorwcqK2ugUqmg18dDCCtqaragouIHNDUVw2yug8XSCKfTAcAJIQRUKhUAC1paTLDbo6HTjYIQgFotodEMh06XDyASDQ0fwG7fgogIIDl5KlQqieLi7dBqb0RCwu1Qq1VQqehYKR+Hg46JlFVobl4Lm+0o9PqLIeV4WK20nwCg0wERERJW6ybU1LwGKYvgdJ4Jg+FpmEwaqFR0vDQa+qhUruMsBK2v1VJ5Tidgt7uOtV4PxMUBer0TFosKLS10jM1mmq/VAhERVI5ybgA6L8p+SEnbslio7MhI+gjhsgOg/0UzK/kAABisSURBVBpNe3scDiqzqQkwmahcg4G2aTYDzc1UpnLslH2NiwMeeaR712yfEnx32MPvHZQL026n38oFrtwUAF2EFgvQ2EgXr83murFaWuh76FBg3Di6WMvKgKNHgaoqEiKjkdZrbqabUrl5Afqv1QKxsUB0NC1bWUnfyg1pNtP/xkbXjaGs63S6i4vrRjWZgLo6+lbKIXGh/xaL64ZTbnD3fWpspHU1GiAmhm5UZb7D4TpWdjsdD4fDdUzVaiApybU/9fXt5/uCRkNlM+FDd8+JSkXXgsNB15yUNM1goDKVh5XySU0Fioq6ZyM3vOoDOBwkJGYziYzJBNTUuARToyGhcjpdyylCYjQCDQ0kUIrw2GwuwdLrgZQUEq3SUrqQjEbXxaV4KIGiq5tCqyUxdBdIxYtyR6cD4uNdnlVkJD1IYlp7VlDEXRFqxVsGXEIcFQXk5NCN5u71WSxUbkwMkJZG9irHQnkwREbS/KgoWrexkW5UnY4+ysNKCFpHo6FvZV5DA507k4n2IyGBylIeNsr2lIeFzeZ6GAlBD8emJhKEpCTad+W8qtW0rEpF57+6mspJTKRtAbSfTqfL21Ts6ui5Kg/KiAj6uIuPWu2A3X4AdnsNIiKmwWLRwmJxPfDi4iSczpWwWNZBSjscDkCIaKjVcRBCBykbIWUDIiLGIypqAYRIQnPzRzAa/waVqrztXNtsOmg0uYiLuxZRURfBatWgoeEl2GwvYvz4ewFEoKFhF1paTsJsNsJutyE+fjySkqbB6bSgsnI9amt/hFYbC71+MGJiRiMj42IkJuZh5847ceLE/6DVzoDNtgVabRPS02dg2LDrkZZ2LmpqtqC09N9wOu3IzFyM5OQzIYRoOz9SOuB0WhARoYdOJ9reXEymJgCAXh/Vdi2QoyRRV1eIysr1MBiGIyVlNiIjo9rOq2s5Oi+hTqoKZpbOcgCzACQDqADweynlP7tap695+FKSZ1peTt8nTwLHjwMnTtC06mqgttblEbu/4vvr/SlotSQGines07nCBDExNK2lhexpbAQyMoCsLBIg5TVSEQR3MdDpSPS0WtqOIgqKCCoCotPRA0WrBY4dA/bupQdVdjZ90tNJhGJjXWEK9+OlXPCKqDY20rJxcaG/GZjg4HC0oKlpD5qb98FiKUVi4jzExExqt4zTacPOnefBaNwAAIiIyERk5GnQapMhhApG4/9gtZYBAAyGMYiPnw2nsxlmcxEaGrbC6WyCSqWH02nG8OEvITPzHlgsZSgt/StOnvwnbLZKCBEBKS1QqQwQQgWHw4SIiKFQqw2w2apht9dDSnvrNsZh0KBbERs7DeXl/0JFxTtwOi2IiZmM2NgzIKUdNlstGhu3tla8EyqVHklJC5CRcQ/i4s4G4ERNzdcwGv+LhIQLkJBwLoRQB/T4hk1Ix1/CQfDNZmD/fuDIEfLwLBYS70OHyFNW4oiNjRTCaGrqXEZqKjB4MJCcTN6aXk9i6h5eUDysyEgS6eho8thSU0kwlTcAIVzLxsdTWSyMTH/EbjeioWELoqJyO2VJSSlhNh8FoIZen9VunsPRhKqqlait/QopKVciJeWSdvOdTiuqq/+Nurq1rZXVFwEAqqo+RXX1SgihgVabDI0mvrXyW6Cm5ks0Nm4BAKhUkUhN/Rl0unQYjd+jsXEH1GoDNJpEGAwjkJx8GZKSLkRT00+oqlqBysrlsNtrER09BTZbJSyWYgACgERERCbi4+dAp0uFRhMPi6UULS2HATgxceI33TpuLPh+4HAAhYXAypXAv/9NHqvTQ9fsGRkULoiIoP8GA/3PzqZ5qank3WZmkigzDNO3MZl2obFxB5KTL/Irg8jhaEJ5+dsoK/sHdLoUDB78CyQknI/a2q9RUfEOTKadsForIKUVanVc6+hzYzB69FvdyoBiwe+Cykrg66+BNWuA3bvJc7dYKLwxYwZwzjlUETliBIUzIiLISzcYgmoWwzADCCllWwptT9NcudK2A+XlwMcfAx9+CGzcSLHk9HQgPx+YPx/IzaXvJE4DZhimFxBCQK3ufS+yXwv+oUPAU08B771HoZvx44Hf/x646CIgL6993jTTv7E6rNCpdX6t43A6oFZ5rmA7XHsY+6r2QavWQqfWYWzKWKRHe2+d643C8kLsOLkDC0YsQFp0GgDAbDfjeP1xJOgTkKRP8mqDN2pbahGji4FWre00r9nWjMO1h6ESKug1eui1ekTrohGljfJ7O4FESoldFbuws2In1EINjUqD9Oh0jEwaifTodDikAw2WBlQ1VaG0sRR1LXU4/7TzERsRuO6upZSeG8/ZWlDaWIq4iDikRKX4XeZJ00mUNZYhOz4bSQbyKu1OO6qbqxGji0GULiog9vtCvxR8sxm47z7gH/+gkMzddwO33QaMHRsaeyx2C/ZX70eTrQmZsZlIi0pDuakch2sPo95cj0Exg5AZm4nM2EyouugtUUqJo3VHUdZYBoPWAL1Wj0ZLI6qbq6FRaTA7e3a7m7y0oRS7KnZhT+Ue2Jw25CTkICchB2NTxiJaFw2bw4bPD3yOt3a+BY1KgxGJIzAicQSyE7KRFZ+FnIScdvZUNlXiu2PfobShFOWmcsRHxiMzNhPxkfEwWU1osDRArVIjWhcNg9aACHUEdGodJqRNQGpUKgDAKZ144r9PYGvpVtww8QZcMvoSRGoiO+2rxW7Bvup9OFB9AA2WBjTZmtBkbUKTrQkOpwM35d2EMSlj2pYvKCtAbUstDFoDzHYztpVtw5bSLThUcwjFDcVosDRgSOwQ5KblYmLaREwZPAUTUifgYM1BfH/iexyoOQCrwwqbw4ZyUzmOG4/D7rRj4y0bMTF9IgDA5rDhxc0v4v3d72Nnxc5ONo9MGokxyWNQbipHSUMJchJy8LMJP8NlYy5Doj4RKqGCgIAQAjaHDU9teApPf/80HNIBlVDh3Kxz0WxrRsHJAlgd1CBCQCA7IRtTB0/FtMxpuGHiDUjUJ7bbbr25HmsOr8E3R7/BhuMbcKj2ENKj0/GL/F9g8ZjFKDhZgP8c+Q+2lW3DoZpDkOgcxhUQuGr8VXju/OeQGZuJ7459h8fWP4a06DQ8M+cZnJZ4WtuyTdYmvLLlFby6/VVkxmZixtAZyEnIwXHjcRyrPwaj2YhmWzNa7C1wSieklIiJiEFGTEbbQ9HmsMHutMMhHWiyNWF90XqUNJR4vO61Ki1szs65xIn6RDx41oNYNGoRtpZuRUFZAWZnz8aiUYsghMC20m2448s7YHfacc34azAraxbWHFmD5XuWQ0qJX+T/AjdMvAHfHvsWL295GVtKtiAuMg5xEXEQQsDqsKLJ2oQ6c13bNnMScnB6xunIic/B0LihaLQ24rtj32Fj8UYIIZCoT0SMLgYSEg6no+3aU0gxpCBCE4GyxjI4W8dx1mv0GJ44HLt+scvj/geSfhfDP3ECWLwY2L4duOce4KGHKPe6pxyrO4ZH1z2KGybegLmnzQUAbC/bjju/vBMmqwmDYgYhRheDyqZKlJvKYXfaYdAaIITA0bqjsDtP3XojNiIW/7+9ew+Pqj4TOP59ZyaZXBhIQi6QGyBEQAIx6JJg1EDVB6QKlVYXVsF1ae0ftV66291SUZ+1z/p0lae626da70qFKghWtMVKg8WHa7iUcknEBAO5EAghCeQ2MJP89o9zmA03TTAymcz7eZ48mXPmzJnfm9+Zd855z8nv5KXlkTskF7fLjSC0+lo53n6c2uZadtTuoL6t/qKvT45N5t6ce/F3+vlT2Z/Yf3z/BZcThFEJo2g53UJtSy0ZAzPwuD2UN5QHEg3ADZk3sHruauKi4vii8QsK3ygMfCjdTjenOk5162830D2QZ255hvk587nv/ft4e+/bJMYkUt9WT3xUPGOTxhIfFY/b5aautY6jLUepaKq44N8swhGBweAQB4/f+DjTRk1j0bpF/PnAn89bNishi3HJ48gYmEFCdALlDeXsPrqb0vrSs9btcrgYkziGKFcUEY4IEmMSGTZoGMtLlpPqSWXr97cS6YzkwTUP8uviXzM5fTJ3jbuLgowCOkwHbb42dtbu5NNDn3Kg8QCpnlRSPalsq9lGaf3ZY+nERMSQ6kmlo7ODiqYK5ufM54F/eID397/PytKVxEfFc33m9WQnZwf2aEvqSyiuKabyRCWeSA8P5j3IlOFT2Fi5kU8OfsLGqo34O/2B1+an57OhcgNrytcE3jcpJomCzAJyUnIYmzgWhzho97fT5muj9XQrlScqeWnnSzjFybWp17L+0HoyB2VyvO04vk4f90+8n4ToBBraG3i39F2OtBzh5itups3Xxraabfg6fbgcLjIHZTI4erD1he9y4xQnIsIJ7wlqmms42nIUhzhwOVy4HC6cDicRjggmpU3ititv47qM63CIA3+nn+qT1eyv30/VySpiI2IZFDWIxJhE0jzWDXSe3vQ0H5V/dFY/+jv9TBk+hfy0fBZvXsyQAUPIHJTJpqpNgW1/6oipnO44zYbKDYHXjowfyR1j7sDr99J0qgmASEck0RHRgf481nqMLTVb2HF4B9Unq+kw1rXVYxLHUDiskEhnJA3tDTSfbg58uQ8dMJSxSWNJ86RR0VRB6bFSfJ0+MgZmMGTAEFpOt1DfVo+I8PQtT3fr83SusD1pW1IChYXWSdi33oKZ59wsyRjDwaaDbKzaSHFNMY3eRtp97fg6fUQ6I4lwRCAidHR2EB0RzbwJ85g6fCrFNcXMfHsmda11APxg4g8YPXg0C4sWkjIghcnpk6ltqeXkqZMkxyYzdMBQIp2RtPna8HX6uDLhSsanjGeQexDVJ6s50nKElAEpZCVkkRCdQG1LLZUnKtlZu5PN1ZvZV7cvsDFFu6JJjEkkKTaJnJQc8tLyGBE/Aq/fS5uvDU+kh6TYJI62HOW1Xa/xwf4PcDlcTBk+hWkjpzFx6ETGp4zH7XRzsOkgZQ1l7Dm6h11Hd9HR2cGC3AXMyJqB0+EM7JEcajrE9sPbWVi0kOzkbF6+/WW+u/y7NJ9uZuVdK8kdkstA90C8fi+Hmw/T5G3C4/bgifRYe2z2nrivw0fL6Rae2vAU6yrWBRLGL2/6JT8t+CnrKtaxdM9Sqk5U0eht5JT/FEmxSSTHJpOVkMWElAlclXQVcVFxgaOGSGckda11/HjNj1m+bzkA8VHxPHrDo+Sn59Pma0NEyB2SGzh8Ple7rz1w5HNF/BXkpecRE3F+PfW90veYvXw2TxQ+wcj4kcz/w3weznuYZ6c/263t8UyZYu0Xa/H6vXR0WmWJwy2HaWxv5IfX/JA7xt7R3c2bvXV7eXL9k6woWQFYyStnSA7TR07n9tG3k5eWd1ZZ5vPjn/PXg39lUtokJqRM+NKjR7B2an7y8U/YXLWZR/If4aH8h2hob+DnRT9nyd+XAOBxe7hm6DX8YuovKMgsAKwyUX1bPameVFyXeeTOTVWb2Fe3j/z0fEYnjuaVna/w+CePc7z9OHOy5/D8jOeJj47nYNNBNlVtonBYIWkDrS+MHYd3sKJkBQUZBYHPQHf5O/3UNtcS4Yy4pFJebwrbhD93LvyxqJEXVu7lZNReALIGW0l19f7VLNuzjLKGMgBiI2JJjk0mOiI6cMh4ym/tsTodTo61HqPR28j45PGUNZSR6kll1V2rWLZnGYs3L6bTdDJz9Exen/X6eYfYwdTY3ojb5b5gAuupNWVrmL18Nl6/l7ioOIrmFzFx6MQer8cYw6t/e5XFmxbzROETzB0/92u3DeDDzz/ks/rPWJC7gPjo+F5Z57nuWXUP7+x7B5fDRV5aHmvnrb1gbfxyKjlWQuWJSvLT84mLujwjWHr9XiKdkV/5pdEXNHmbKD1WyuSM3rmTWV8Xlgm/uhoy734KM3URyIVrlFNHTGX2mNmBQ+Yv+0b3+r0s27OM57Y8R0J0AivuXBE4YVNcU0x5Qzlzs+f2+xtQfHroUxatW8QztzxDXnpesJtz2TW0N5D9vLWt7Lh/R+BchFJ9RdglfGMMBY89xuaI/2LGsDt5oOA+spOzcYiDsoYyaptruXHYjYFDOaV64nDzYVwOlyZ71SeF1XX4xhge+uO/sTniVwxv+D4fPP7iWYedmuTV15XqSQ12E5TqFX2/IPcVGr2NLNv1Hmx9gCV3vhgSNUallAqGkN/Dj3MnELe8mOHuwVxf0L/r6Uop9XWEfMJvbYWpeYnccouOIqmUUl8m5BO+xwMvvxzsViilVN+nBW+llAoTmvCVUipMaMJXSqkwoQlfKaXChCZ8pZQKE5rwlVIqTGjCV0qpMKEJXymlwkSfGi1TRI4Bhy7x5YnAxW8HFVr6Uyyg8fRl/SkW6F/xdDeWYcaYbt1st08l/K9DRLZ3d4jQvq4/xQIaT1/Wn2KB/hXPNxGLlnSUUipMaMJXSqkw0Z8S/kvBbkAv6k+xgMbTl/WnWKB/xdPrsfSbGr5SSqkv15/28JVSSn2JkE/4IjJdRPaLSLmI/CzY7ekpEckQkU9EpERE9onIQ/b8BBFZKyJl9u/4YLe1u0TEKSJ/E5EP7ekRIrLV7qN3RCQy2G3sLhGJE5F3ReQzESkVkckh3jeP2NvZXhH5vYhEhUr/iMhrIlInInu7zLtgX4jlf+2YdovIxOC1/MIuEs8z9ra2W0TeE5G4Ls8ttOPZLyLTLuU9Qzrhi4gT+A1wK3AVMFdErgpuq3rMD/yrMeYqIB/4kR3Dz4AiY0wWUGRPh4qHgNIu0/8NPGuMGQU0AguC0qpL8z/AR8aYMUAOVlwh2TcikgY8CFxrjMkGnMAcQqd/3gCmnzPvYn1xK5Bl/9wPvHCZ2tgTb3B+PGuBbGPMBOBzYCGAnRPmAOPs1zxv578eCemED0wCyo0xXxhjTgNvA7OC3KYeMcbUGmN22o+bsRJKGlYcb9qLvQl8Jzgt7BkRSQe+DbxiTwvwLeBde5FQimUQcCPwKoAx5rQxpokQ7RubC4gWERcQA9QSIv1jjPkUaDhn9sX6YhawxFi2AHEiMvTytLR7LhSPMeZjY4zfntwCpNuPZwFvG2NOGWMqgHKs/NcjoZ7w04CqLtPV9ryQJCLDgVxgK5BijKm1nzoCpASpWT31HPDvQKc9PRho6rIRh1IfjQCOAa/bJapXRCSWEO0bY0wNsBioxEr0J4AdhG7/wMX7oj/khn8B1tiPeyWeUE/4/YaIDABWAg8bY052fc5Yl1L1+cupROQ2oM4YsyPYbeklLmAi8IIxJhdo5ZzyTaj0DYBd356F9UWWCsRyfkkhZIVSX3wVEXkUq9y7tDfXG+oJvwbI6DKdbs8LKSISgZXslxpjVtmzj545BLV/1wWrfT1QAMwUkYNY5bVvYdXA4+wSAoRWH1UD1caYrfb0u1hfAKHYNwA3AxXGmGPGGB+wCqvPQrV/4OJ9EbK5QUT+GbgNuNv8/3XzvRJPqCf8bUCWfZVBJNZJjdVBblOP2DXuV4FSY8yvujy1GrjXfnwv8P7lbltPGWMWGmPSjTHDsfpinTHmbuAT4Hv2YiERC4Ax5ghQJSKj7Vk3ASWEYN/YKoF8EYmxt7sz8YRk/9gu1hergfn21Tr5wIkupZ8+S0SmY5VEZxpj2ro8tRqYIyJuERmBdTK6uMdvYIwJ6R9gBtbZ7APAo8FuzyW0/3qsw9DdwC77ZwZW7bsIKAP+AiQEu609jGsK8KH9+Ap74ywHVgDuYLevB3FcDWy3++cPQHwo9w3wn8BnwF7gd4A7VPoH+D3WuQcf1tHXgov1BSBYV/AdAPZgXZkU9Bi6EU85Vq3+TC74bZflH7Xj2Q/ceinvqf9pq5RSYSLUSzpKKaW6SRO+UkqFCU34SikVJjThK6VUmNCEr5RSYUITvlK9QESmnBkdVKm+ShO+UkqFCU34KqyIyD0iUiwiu0TkRXvs/hYRedYeJ75IRJLsZa8WkS1dxiY/M9b6KBH5i4j8XUR2ishIe/UDuoydv9T+b1al+gxN+CpsiMhY4B+BAmPM1UAHcDfWIGLbjTHjgPXAE/ZLlgD/Yayxyfd0mb8U+I0xJge4Duu/JcEa6fRhrHszXIE1To1SfYbrqxdRqt+4CbgG2GbvfEdjDbbVCbxjL/MWsMoeCz/OGLPenv8msEJEPECaMeY9AGOMF8BeX7Exptqe3gUMBzZ882Ep1T2a8FU4EeBNY8zCs2aKPHbOcpc63sipLo870M+X6mO0pKPCSRHwPRFJhsD9UIdhfQ7OjBb5T8AGY8wJoFFEbrDnzwPWG+uuZNUi8h17HW4RibmsUSh1iXQPRIUNY0yJiCwCPhYRB9YohT/CurHJJPu5Oqw6P1jD7f7WTuhfAPfZ8+cBL4rIk/Y67ryMYSh1yXS0TBX2RKTFGDMg2O1Q6pumJR2llAoTuoevlFJhQvfwlVIqTGjCV0qpMKEJXymlwoQmfKWUChOa8JVSKkxowldKqTDxf9uVPJvuaLo6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 686us/sample - loss: 5.5172 - acc: 0.6268\n",
      "Loss: 5.517237427078675 Accuracy: 0.6267913\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5853 - acc: 0.4571\n",
      "Epoch 00001: val_loss improved from inf to 2.18174, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_4_conv_checkpoint/001-2.1817.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.5854 - acc: 0.4571 - val_loss: 2.1817 - val_acc: 0.4647\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2000 - acc: 0.7256\n",
      "Epoch 00002: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1999 - acc: 0.7256 - val_loss: 2.5820 - val_acc: 0.5304\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6715 - acc: 0.8406\n",
      "Epoch 00003: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.6717 - acc: 0.8406 - val_loss: 2.3238 - val_acc: 0.5807\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8926\n",
      "Epoch 00004: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4514 - acc: 0.8926 - val_loss: 2.3224 - val_acc: 0.6028\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.9127\n",
      "Epoch 00005: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3718 - acc: 0.9126 - val_loss: 2.7640 - val_acc: 0.5772\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.9257\n",
      "Epoch 00006: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3336 - acc: 0.9256 - val_loss: 3.1320 - val_acc: 0.5255\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9417\n",
      "Epoch 00007: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2655 - acc: 0.9417 - val_loss: 2.4503 - val_acc: 0.6198\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9453\n",
      "Epoch 00008: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2530 - acc: 0.9453 - val_loss: 2.8480 - val_acc: 0.5788\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9473\n",
      "Epoch 00009: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2429 - acc: 0.9473 - val_loss: 2.9481 - val_acc: 0.5942\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9510\n",
      "Epoch 00010: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2292 - acc: 0.9509 - val_loss: 2.6544 - val_acc: 0.6231\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9545\n",
      "Epoch 00011: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2213 - acc: 0.9545 - val_loss: 2.9787 - val_acc: 0.6138\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9608\n",
      "Epoch 00012: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1942 - acc: 0.9607 - val_loss: 2.8084 - val_acc: 0.6233\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9571\n",
      "Epoch 00013: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2177 - acc: 0.9571 - val_loss: 3.0856 - val_acc: 0.6101\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9658\n",
      "Epoch 00014: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1772 - acc: 0.9658 - val_loss: 3.1124 - val_acc: 0.6184\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9640\n",
      "Epoch 00015: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1897 - acc: 0.9639 - val_loss: 3.2216 - val_acc: 0.6194\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9670\n",
      "Epoch 00016: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1733 - acc: 0.9669 - val_loss: 3.3618 - val_acc: 0.6045\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9593\n",
      "Epoch 00017: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2204 - acc: 0.9592 - val_loss: 3.4764 - val_acc: 0.6075\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9723\n",
      "Epoch 00018: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1524 - acc: 0.9723 - val_loss: 3.1479 - val_acc: 0.6387\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9730\n",
      "Epoch 00019: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1507 - acc: 0.9729 - val_loss: 3.3652 - val_acc: 0.6217\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9707\n",
      "Epoch 00020: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1709 - acc: 0.9706 - val_loss: 3.5280 - val_acc: 0.6045\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9709\n",
      "Epoch 00021: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1629 - acc: 0.9709 - val_loss: 3.5690 - val_acc: 0.6031\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9787\n",
      "Epoch 00022: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1292 - acc: 0.9787 - val_loss: 3.5654 - val_acc: 0.6066\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9749\n",
      "Epoch 00023: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1474 - acc: 0.9748 - val_loss: 3.4665 - val_acc: 0.6278\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9741\n",
      "Epoch 00024: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1526 - acc: 0.9741 - val_loss: 4.0344 - val_acc: 0.5851\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9777\n",
      "Epoch 00025: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1334 - acc: 0.9777 - val_loss: 3.7076 - val_acc: 0.6089\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9792\n",
      "Epoch 00026: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1264 - acc: 0.9792 - val_loss: 3.7523 - val_acc: 0.6117\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.9770\n",
      "Epoch 00027: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1388 - acc: 0.9770 - val_loss: 3.7365 - val_acc: 0.6229\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9756\n",
      "Epoch 00028: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1520 - acc: 0.9755 - val_loss: 4.9972 - val_acc: 0.5399\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9796\n",
      "Epoch 00029: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1242 - acc: 0.9796 - val_loss: 3.9921 - val_acc: 0.6045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9796\n",
      "Epoch 00030: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1245 - acc: 0.9796 - val_loss: 3.7880 - val_acc: 0.6177\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9829\n",
      "Epoch 00031: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1068 - acc: 0.9829 - val_loss: 3.7424 - val_acc: 0.6259\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9812\n",
      "Epoch 00032: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1203 - acc: 0.9812 - val_loss: 4.0043 - val_acc: 0.6000\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9818\n",
      "Epoch 00033: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1168 - acc: 0.9818 - val_loss: 3.6569 - val_acc: 0.6424\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9803\n",
      "Epoch 00034: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1293 - acc: 0.9802 - val_loss: 3.5789 - val_acc: 0.6450\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9793\n",
      "Epoch 00035: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1431 - acc: 0.9794 - val_loss: 3.7676 - val_acc: 0.6278\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9840\n",
      "Epoch 00036: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1115 - acc: 0.9840 - val_loss: 4.3205 - val_acc: 0.5984\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9841\n",
      "Epoch 00037: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1102 - acc: 0.9841 - val_loss: 3.5808 - val_acc: 0.6508\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9846\n",
      "Epoch 00038: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1105 - acc: 0.9845 - val_loss: 4.3363 - val_acc: 0.5970\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9835\n",
      "Epoch 00039: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1169 - acc: 0.9835 - val_loss: 3.7417 - val_acc: 0.6450\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9860\n",
      "Epoch 00040: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0973 - acc: 0.9860 - val_loss: 3.5854 - val_acc: 0.6576\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9809\n",
      "Epoch 00041: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1347 - acc: 0.9809 - val_loss: 3.8093 - val_acc: 0.6392\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9870\n",
      "Epoch 00042: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0922 - acc: 0.9870 - val_loss: 4.2440 - val_acc: 0.6068\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9876\n",
      "Epoch 00043: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0861 - acc: 0.9876 - val_loss: 4.0541 - val_acc: 0.6327\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9872\n",
      "Epoch 00044: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0921 - acc: 0.9871 - val_loss: 3.8857 - val_acc: 0.6292\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9807\n",
      "Epoch 00045: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1376 - acc: 0.9807 - val_loss: 3.7576 - val_acc: 0.6511\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9873\n",
      "Epoch 00046: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0958 - acc: 0.9872 - val_loss: 4.0493 - val_acc: 0.6345\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9787\n",
      "Epoch 00047: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1495 - acc: 0.9787 - val_loss: 4.0112 - val_acc: 0.6375\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9867\n",
      "Epoch 00048: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0986 - acc: 0.9866 - val_loss: 3.7995 - val_acc: 0.6373\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9890\n",
      "Epoch 00049: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0865 - acc: 0.9889 - val_loss: 3.7354 - val_acc: 0.6541\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9840\n",
      "Epoch 00050: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1122 - acc: 0.9840 - val_loss: 3.8451 - val_acc: 0.6427\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9899\n",
      "Epoch 00051: val_loss did not improve from 2.18174\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0792 - acc: 0.9899 - val_loss: 3.7946 - val_acc: 0.6515\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 638us/sample - loss: 2.3505 - acc: 0.4444\n",
      "Loss: 2.350521321311547 Accuracy: 0.44444445\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8009 - acc: 0.5028\n",
      "Epoch 00001: val_loss improved from inf to 1.52066, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_5_conv_checkpoint/001-1.5207.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.8008 - acc: 0.5028 - val_loss: 1.5207 - val_acc: 0.5351\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.7227\n",
      "Epoch 00002: val_loss improved from 1.52066 to 1.28679, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_5_conv_checkpoint/002-1.2868.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9718 - acc: 0.7227 - val_loss: 1.2868 - val_acc: 0.6462\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.8238\n",
      "Epoch 00003: val_loss did not improve from 1.28679\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5930 - acc: 0.8237 - val_loss: 1.4412 - val_acc: 0.6343\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3938 - acc: 0.8838\n",
      "Epoch 00004: val_loss did not improve from 1.28679\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3938 - acc: 0.8838 - val_loss: 1.9058 - val_acc: 0.5632\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9306\n",
      "Epoch 00005: val_loss improved from 1.28679 to 1.25015, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_5_conv_checkpoint/005-1.2501.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2608 - acc: 0.9305 - val_loss: 1.2501 - val_acc: 0.6916\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9403\n",
      "Epoch 00006: val_loss improved from 1.25015 to 1.13776, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_5_conv_checkpoint/006-1.1378.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2261 - acc: 0.9403 - val_loss: 1.1378 - val_acc: 0.7093\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9672\n",
      "Epoch 00007: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1430 - acc: 0.9672 - val_loss: 1.4472 - val_acc: 0.6522\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9692\n",
      "Epoch 00008: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1300 - acc: 0.9691 - val_loss: 1.2969 - val_acc: 0.6990\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9753\n",
      "Epoch 00009: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1096 - acc: 0.9753 - val_loss: 1.2887 - val_acc: 0.7167\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9730\n",
      "Epoch 00010: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1103 - acc: 0.9730 - val_loss: 1.3705 - val_acc: 0.7049\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9805\n",
      "Epoch 00011: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0896 - acc: 0.9804 - val_loss: 1.5321 - val_acc: 0.6844\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9787\n",
      "Epoch 00012: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0908 - acc: 0.9786 - val_loss: 1.5045 - val_acc: 0.6855\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9748\n",
      "Epoch 00013: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0991 - acc: 0.9747 - val_loss: 1.4302 - val_acc: 0.7107\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9786\n",
      "Epoch 00014: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0876 - acc: 0.9786 - val_loss: 1.6596 - val_acc: 0.6865\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9797\n",
      "Epoch 00015: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0829 - acc: 0.9796 - val_loss: 1.4601 - val_acc: 0.7165\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9780\n",
      "Epoch 00016: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0870 - acc: 0.9780 - val_loss: 1.4567 - val_acc: 0.7261\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9840\n",
      "Epoch 00017: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0687 - acc: 0.9840 - val_loss: 1.4580 - val_acc: 0.7167\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9882\n",
      "Epoch 00018: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0554 - acc: 0.9881 - val_loss: 1.4655 - val_acc: 0.7181\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9889\n",
      "Epoch 00019: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0520 - acc: 0.9889 - val_loss: 1.4989 - val_acc: 0.7247\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9815\n",
      "Epoch 00020: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0752 - acc: 0.9815 - val_loss: 1.6048 - val_acc: 0.7140\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9870\n",
      "Epoch 00021: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0594 - acc: 0.9870 - val_loss: 1.4410 - val_acc: 0.7317\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9891\n",
      "Epoch 00022: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0505 - acc: 0.9891 - val_loss: 1.6092 - val_acc: 0.7133\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9877\n",
      "Epoch 00023: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0531 - acc: 0.9876 - val_loss: 1.9646 - val_acc: 0.6618\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9815\n",
      "Epoch 00024: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0751 - acc: 0.9815 - val_loss: 1.5470 - val_acc: 0.7221\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9883\n",
      "Epoch 00025: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0519 - acc: 0.9883 - val_loss: 1.6677 - val_acc: 0.7137\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9919\n",
      "Epoch 00026: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0409 - acc: 0.9919 - val_loss: 1.6376 - val_acc: 0.7307\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9902\n",
      "Epoch 00027: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0438 - acc: 0.9902 - val_loss: 1.7935 - val_acc: 0.7165\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9900\n",
      "Epoch 00028: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0453 - acc: 0.9899 - val_loss: 1.9242 - val_acc: 0.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9870\n",
      "Epoch 00029: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0541 - acc: 0.9870 - val_loss: 1.6841 - val_acc: 0.7209\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9917\n",
      "Epoch 00030: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0421 - acc: 0.9916 - val_loss: 1.7651 - val_acc: 0.7205\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9820\n",
      "Epoch 00031: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0736 - acc: 0.9820 - val_loss: 1.8222 - val_acc: 0.7214\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9930\n",
      "Epoch 00032: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0361 - acc: 0.9930 - val_loss: 1.8113 - val_acc: 0.7270\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0583 - acc: 0.9860 - val_loss: 1.6163 - val_acc: 0.7396\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9879\n",
      "Epoch 00034: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0517 - acc: 0.9878 - val_loss: 1.8480 - val_acc: 0.7165\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9904\n",
      "Epoch 00035: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0467 - acc: 0.9903 - val_loss: 1.6482 - val_acc: 0.7382\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9931\n",
      "Epoch 00036: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0328 - acc: 0.9931 - val_loss: 1.7790 - val_acc: 0.7284\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9892\n",
      "Epoch 00037: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0519 - acc: 0.9892 - val_loss: 1.6929 - val_acc: 0.7354\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9933\n",
      "Epoch 00038: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0331 - acc: 0.9932 - val_loss: 1.7390 - val_acc: 0.7326\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9855\n",
      "Epoch 00039: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0640 - acc: 0.9855 - val_loss: 1.7223 - val_acc: 0.7391\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9923\n",
      "Epoch 00040: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0360 - acc: 0.9923 - val_loss: 1.9071 - val_acc: 0.7165\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9935\n",
      "Epoch 00041: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0320 - acc: 0.9934 - val_loss: 1.7577 - val_acc: 0.7352\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9892\n",
      "Epoch 00042: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0472 - acc: 0.9891 - val_loss: 1.8208 - val_acc: 0.7335\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9910\n",
      "Epoch 00043: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0384 - acc: 0.9909 - val_loss: 1.9055 - val_acc: 0.7209\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9868\n",
      "Epoch 00044: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0568 - acc: 0.9868 - val_loss: 1.8114 - val_acc: 0.7382\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9950\n",
      "Epoch 00045: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0287 - acc: 0.9950 - val_loss: 1.7915 - val_acc: 0.7382\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9923\n",
      "Epoch 00046: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0358 - acc: 0.9922 - val_loss: 1.7982 - val_acc: 0.7307\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9916\n",
      "Epoch 00047: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0404 - acc: 0.9916 - val_loss: 1.8346 - val_acc: 0.7307\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9911\n",
      "Epoch 00048: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0423 - acc: 0.9910 - val_loss: 1.7730 - val_acc: 0.7405\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9896\n",
      "Epoch 00049: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0480 - acc: 0.9895 - val_loss: 1.8247 - val_acc: 0.7452\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9930\n",
      "Epoch 00050: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0328 - acc: 0.9930 - val_loss: 2.0683 - val_acc: 0.7095\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9923\n",
      "Epoch 00051: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0351 - acc: 0.9922 - val_loss: 1.9179 - val_acc: 0.7300\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9901\n",
      "Epoch 00052: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0447 - acc: 0.9901 - val_loss: 1.7795 - val_acc: 0.7482\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9928\n",
      "Epoch 00053: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0339 - acc: 0.9928 - val_loss: 1.9672 - val_acc: 0.7352\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9920\n",
      "Epoch 00054: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0362 - acc: 0.9920 - val_loss: 1.9886 - val_acc: 0.7298\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9938\n",
      "Epoch 00055: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0332 - acc: 0.9938 - val_loss: 2.0734 - val_acc: 0.7321\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9905\n",
      "Epoch 00056: val_loss did not improve from 1.13776\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0417 - acc: 0.9904 - val_loss: 1.9147 - val_acc: 0.7338\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4lMX2x7+TzaaHZBMSCEkgdEIKoRpEmgUpAgIiiAiCgnoVVBRF9Co/vZZ7bYji5aKCgEgRUKqgIlWKCRB6J0ASSnqvmz2/P85udpPsbnaT3TTm8zzzvLvvO+/MvFvmzJwzc44gIkgkEolEUhUOdd0AiUQikTQMpMCQSCQSiUVIgSGRSCQSi5ACQyKRSCQWIQWGRCKRSCxCCgyJRCKRWIQUGBKJRCKxCCkwJBKJRGIRUmBIJBKJxCIc67oBtqRp06YUEhJS182QSCSSBsORI0dSicjPkryNSmCEhIQgNja2rpshkUgkDQYhxDVL80qVlEQikUgsQgoMiUQikViEFBgSiUQisYhGZcMwRklJCRITE1FYWFjXTWmQuLi4ICgoCEqlsq6bIpFI6phGLzASExPh6emJkJAQCCHqujkNCiJCWloaEhMT0bp167pujkQiqWMavUqqsLAQvr6+UlhUAyEEfH195exMIpEAuAMEBgApLGqA/OwkEomOO0JgSCQSSYOntBRYvBhIT6+zJkiBYWcyMzPx9ddfV+veoUOHIjMz0+L88+bNwyeffFKtuiQSSTUYPx547rnaqevTT4FnngE+/7x26jOCFBh2xpzAUKvVZu/dtm0bvL297dEsiURSUwoLgZ9/Br77DkhNtW9dZ84A//wnv/7hB4DIvvWZQAoMOzNnzhxcvnwZUVFRmD17Nnbv3o2+fftixIgR6Ny5MwDg4YcfRvfu3REWFobFixeX3RsSEoLU1FRcvXoVoaGhmDZtGsLCwjBo0CAUFBSYrTcuLg7R0dGIjIzEqFGjkJGRAQBYsGABOnfujMjISIwfPx4AsGfPHkRFRSEqKgpdu3ZFTk6OnT4NiaQRERsLFBcDJSXAjz/arx61Gpg8GWjSBPjkE+DqVeCvv+xXnxka/bJaQy5efAm5uXE2LdPDIwrt2883ef2jjz7CqVOnEBfH9e7evRtHjx7FqVOnypaqLlmyBD4+PigoKEDPnj0xZswY+Pr6Vmj7RaxatQrffPMNHn30Uaxfvx4TJ040We+kSZPw5Zdfon///nj77bfxf//3f5g/fz4++ugjxMfHw9nZuUzd9cknn2DhwoXo06cPcnNz4eLiUtOPRSJp/Ozfz8f27YGlS4GZM+1Tz0cfsXBauxYYMgR4+22eZdxzj33qM4PdZhhCiGAhxC4hxBkhxGkhxItG8gghxAIhxCUhxAkhRDeDa5OFEBe1abK92lkX9OrVq9y+hgULFqBLly6Ijo5GQkICLl68WOme1q1bIyoqCgDQvXt3XL161WT5WVlZyMzMRP/+/QEAkydPxt69ewEAkZGRePzxx/HDDz/A0ZHHC3369MGsWbOwYMECZGZmlp2XSCRm2L8f6NQJePFFIC4OOHbM9nUcPw68+y4wbhwwdizg4QGMGsXCo6jI9vVVgT17BjWAV4joqBDCE8ARIcTvRHTGIM8QAO216S4A/wVwlxDCB8A7AHoAIO29m4gooyYNMjcTqE3c3d3LXu/evRt//PEHDh48CDc3NwwYMMDovgdnZ+ey1wqFokqVlCm2bt2KvXv3YvPmzXj//fdx8uRJzJkzB8OGDcO2bdvQp08f7NixA506dapW+RLJHYFGw2qhRx4BHnsMmDWLZxldu9qujuJiVkX5+AALF+rPT5wIrFwJbNvGwqMWsdsMg4huEtFR7escAGcBBFbINhLAcmIOAfAWQgQAeBDA70SUrhUSvwMYbK+22hNPT0+zNoGsrCyoVCq4ubnh3LlzOHToUI3r9PLygkqlwr59+wAAK1asQP/+/aHRaJCQkICBAwfi3//+N7KyspCbm4vLly8jIiICr7/+Onr27Ilz587VuA0SSaPmzBkgM5PVQj4+wMMPcyduy1H/v/7FM4z//Q8wVFHffz/QrBmrpWqZWjF6CyFCAHQFcLjCpUAACQbvE7XnTJ03VvZ0IUSsECI2JSXFVk22Gb6+vujTpw/Cw8Mxe/bsStcHDx4MtVqN0NBQzJkzB9HR0Tapd9myZZg9ezYiIyMRFxeHt99+G6WlpZg4cSIiIiLQtWtXzJw5E97e3pg/fz7Cw8MRGRkJpVKJIUOG2KQNEkmjRTsYQ9++fJw6lfdHbN5sm/L/+gv44APgiSeAkSPLX3N05FnNli1ARo2ULtZDRHZNADwAHAEw2si1LQDuMXi/E6yGehXAWwbn/wng1arq6t69O1XkzJkzlc5JrEN+hhJJBSZMIAoIINJo+L1aTRQURDRkSPXLzMoi+vZbonvuIQKIAgOJ0tON542N5Tz/+1/169MCIJYs7M/tOsMQQigBrAewkog2GMmSBCDY4H2Q9pyp8xKJRFL37N/P6iid6xyFgu0NO3YASVZ2VXv38kyieXPg6ad5T8dHHwFHjgAqlfF7unVjg3stq6XsuUpKAPgOwFki+sxEtk0AJmlXS0UDyCKimwB2ABgkhFAJIVQABmnPSSTVIyEBaNsWOHu2rlsiaehcv86p4rLWJ59kY/iKFZaXFRMD9O/P6qUnnwQOHWL7yOuvs53CFEKwkNm3j/dl1BL2nGH0AfAEgHuFEHHaNFQI8awQ4lltnm0ArgC4BOAbAP8AACJKB/AegBhteld7TiKpHn/9BVy5ol87L5FUF92muYoCo107tmksWWL5Tuz169kmcfky8PXXwF136WctVTFhAh9XrrQsvw2w27JaItoPwOyTa/Vnz5u4tgTAEjs0TXInoptZXL5ct+2QNHz27wc8PYHIyMrXpk4FpkwBDhwA+vSpuqxNm4ABA3illbWEhLCA+uEHYO5cywVNDZCuQSR3BrUtMIjYu6ik8bFvH9C7N88MKvLII4C7O+/JqIqLF/l3OWJE9dvyxBPAuXPA0aPVL8MKpMCQ3Bmc0e4XvXSpdup7911WUWg0tVOfpHbIyABOnTLtlsPDA3j0UWDNGiAvz3xZmzbxcfjw6rfnkUcAJyfr7CY1QAoMW0AE5OfbrDgPDw+rzkuqQK0GLlzg15cv29/TZ24uu6C+ehU4fdq+ddV3iICNG4FqeiYAwDuer1+3XZtqwsGD/Ezm/Dg99RT/BlatMl/Wpk2s1goJqX57VCrgoYe4riq8X9sCKTBsQUYGj2DrwLeLxAKuXGGPolFRQE6O/V1RL1sGZGXxa90GrzuVQ4d4F/Snn1bvfiJg9GigY8famx2aY/9+VkX16mU6z913AxERbMQ2NThJS+OyaqKO0vHGG+wt18H+3bkUGLZA5/upuLjSpTlz5mChgR8YXZCj3Nxc3HfffejWrRsiIiKwceNGi6sjIsyePRvh4eGIiIjAmjVrAAA3b95Ev379EBUVhfDwcOzbtw+lpaV48skny/J+XofBV+oMnf1CN/W3px1DowG++II7lMBAXmN/J/PLL3xcurR66rmlS4GtW3kw9sILdRYHooz9+3kPhIE/uEoIwUGVjh0D/v7beJ5t2/jzsIXA6NEDuO++WhEYd5Zb0pdeYq+StiQqissFjE4Jx40bh5deegnPP8+LwdauXYsdO3bAxcUFP//8M5o0aYLU1FRER0djxIgRFsXQ3rBhA+Li4nD8+HGkpqaiZ8+e6NevH3788Uc8+OCDePPNN1FaWor8/HzExcUhKSkJp06dAgCrIvg1GnT2i2HDgPfeY4FhIxcslfj1VzZmrlrFqpi9e7mTu1Njo//yC+v1r1zhz2LAAMvvvX6d/1v9+7N7jFmzgA0bgDFj7NZcsxQVsQB44YWq806cCLz2mn6pbEU2bQICAoDu3W3fTjsiZxi2QKeKKimpdKlr165ITk7GjRs3cPz4cahUKgQHB4OIMHfuXERGRuL+++9HUlISbt++bVF1+/fvx2OPPQaFQoFmzZqhf//+iImJQc+ePbF06VLMmzcPJ0+ehKenJ9q0aYMrV65gxowZ2L59O5o0aWLLJ28YnD0LtGgBdOnCHbc9Zxjz5/PMYswYXvJ44wYQH2+/+uoz586x7eiddwAvL96fYClEbAsg4lnGjBn8/b30EtsH6oIjR/i/bkkcCk9PYNIkNn6npZW/VlQEbN/OM95amBXYkjtrhjHfTu7NT57kowmj09ixY7Fu3TrcunUL48aNAwCsXLkSKSkpOHLkCJRKJUJCQoy6NbeGfv36Ye/evdi6dSuefPJJzJo1C5MmTcLx48exY8cOLFq0CGvXrsUSa/64jYGzZ4HOnQEXF+7M7SUwTp0C/vgD+PBDQKnUO6bbtw9o08Y+ddZndGrWceP4M1+2DPjySxYeVbFoEX+WixYButgx//0v2wfefRf4z3/s125T6OxRluyvAFgt9fXXLPBefVV/fvduFnq2UEfVMg1LvNVHiPS2CyMzDIDVUqtXr8a6deswduxYAOzW3N/fH0qlErt27cK1a9csrrJv375Ys2YNSktLkZKSgr1796JXr164du0amjVrhmnTpuHpp5/G0aNHkZqaCo1GgzFjxuBf//oXjtbSeu16AxELjNBQft+2rf0ExhdfAK6uwLRp/D4sjFex1JXhOyEBePZZm67gs4pffmGVS3Awb2grKOARd1VcuQLMng0MGgRMn64/37s3zzo+/5yFc22zfz8b3/38LMsfHg7068eCztB+s2kT4OYG3HuvfdppR6TAqCklJXpDnAmBERYWhpycHAQGBiIgIAAA8PjjjyM2NhYRERFYvny5VQGLRo0ahcjISHTp0gX33nsv/vOf/6B58+bYvXs3unTpgq5du2LNmjV48cUXkZSUhAEDBiAqKgoTJ07Ehx9+WONHblAkJPB6eHsLjJQUXgs/aZI+doGDA49G60pgrFjBsRR+/bX26751Czh8WO+au0cP7kC/+878fRoN75RWKIBvv61s+/noI45t/fzzNTeA377NsyBLytEFTLI2LOo//sEC8Lff+D0RC4xBg3hw0dCw1K1tQ0h14t48J4coJobT2bP2rauOaNDuzbdvZzfQu3bx+/ff5/e5ubat5733uNzTp8uf//e/+fytW9aXefs2UUKC3oW2tTzwANf91FPVu78mLF7MdZ84oT/3+ed87uRJ0/fp8ixdajrPN99wnuXLTecxh0ZD9N13RCoVlzNnTtX3nDjBeb//3rq6ioqImjUjGj6c3x89yuUsWWJ9u+0E6ot78zsCncHb1bVWNs5IrES3pLZzZz62bcvHK1dsV0dxMYfQfPBBfT06dHYMa50e/vEH0KoVq3P8/XlEOmcOq3Qs2cRWUsL+jACeYdT2ctRffmHbQ3i4/tzjj7Ntx5TbjIMHeU/BQw+xq3BTTJ3Kq9xefdX6AEIXL7Iq6KmnuG1PPMGzFnM2kcREDljk5GS9GsnJiV2Wb9nCGzk3beJZ00MPWVdOfcFSydIQUnVmGBqNhtTqAiotLTabzyQ3bvDsIj6eRw+NkAY9w5g+ncjHRz9Kj4nhEd6GDbarY8UKLvPXXytfKyoicnUlevFFy8v7/XciFxeiyEiiBQuIpk4l6tqVSKnkepRKoqQk82UcPMh5hw7lY1ycdc9UE3JyiJydiV56qfK1MWOI/Pz4czHk9Gke8bdrxzOrqjh2jMjBgcjLi2jKFKLffiMqKTGdv7iYZ5fOznzP4sVEpaUc+Gj8eP6MFi+ufN+ZM0TBwUSenkR//ll1u4xx7Rq39Y03iLp1I+rTp3rl2AlYMcOo807elqm6AiM7O5YKChLM5jNJfDz/eHWCo7S0euXUYxq0wOjbt/wfND2df/Yff2yb8jUa7gQ6dTL93Q8YwHks4bff9MIiJaX8taIiom3buP3ffGO+nI8+0gsKgOjDDy2r3xasW8d17t5d+drWrZUF9vXrHK2ueXOiK1csr2ffPqJJk7gzB4j8/YleeIFo/XqiRYuI5s4lmjiRqF8/LhsgeuQR/q8aUlRENHgwkRBEa9fqzx84wIONZs1qPhgcOZIFFcBqynqEFBgGWNLZ5eTEUUFBfJX5jHL+PI9CkpNZYFQcOTUCGrTA8PUlevrp8udUKqJnn7VN+bpQmQsXms7zz3/yCDMry3xZ5oSFDo2GqGVL7oDMMXgwUWgov46KYsFZFdW1lVRk4kTuaI2N+EtKiFq0IHroIX6flsbtbNKk+rOg/HwWEo88wp8fK+CIFAqiVq045OmECUSbNpkuIy+PBxZKJdGOHUSbN/PMsF07osuXq9cuQ3bs0Lerntk664XAAMeySAZwysT12QDitOkUgFIAPtprVwGc1F6z+GGqKzByc09Sfv4lSz7bypw8SXTpElFGBgsMWxtT6wENVmAkJ/NP/LPPyp/v2ZMNwrZg5kwiJyfTsZeJWBAAbIA3l6cqYaHjueeI3N2JCguNXy8pIfLw4HxErApRKPg3aoqDB7kjNzYrsIbiYhbIkyaZzvPGGyxAL10i6t2b1UQ1rVdHVhbR4cO8WECttu7ejAyiLl1YUCgURD16WKYes4TSUhY+7dvbTjDbCGv6WHsavb8HMNjURSL6mIiiiCgKwBsA9lD5qHoDtdd72LGNAAAhHEFUDYM1ERu9nZz0vvGl4bv+oDN465bU6rDV0tqSEnb6NmKE6djLAO8fUChML6/du5fL6NAB2LkTaNrUfL3DhvFS4T17jF8/dow3hvXvz++HDuXYHH/8YbrMjz7iXelTplTtltsc+/ezIfrhh03nmTKFl6lGR7Nzwh9/1Le1pjRpwn68goL4M7cGb2+Oyd26NS9g+PNPXnBgCxwcgM2bgZ9/btBuYuwmMIhoLwBLw6o+BqAKX8D2pJoCQ61moeHszKs/gEp7MTIzM/H1119Xq1VDhw69M30/2QpzAuPaNZP7Zixm+3b2fDtpkvl8Hh5A167GBUZ6OofabNnSMmEBAAMH8q71rVuNX9cJEl0nHB3NneG2bcbzX77Mq3eGDOGVPG+8UXUbTPHLL9y2QYNM52nfnlePpabyprbRo6tfn61p1ow3BW7dyu49bEmnTryZswFT58tqhRBu4JnIeoPTBOA3IcQRIcR043fasg0KEFUjOppuSa2ZGYY5gaGuYjaybds2eHt7W98uCXPmDO+oDQ4uf75tWx5x1zTGwvLl3MEPNjmR1tO3L29kM3SBTwQ88wyQnMzOCi0RFoB+l/DWrcaXy+7Zw7OV5s35vaMj8MADLOCM5f/yS87z7bfss+nLL03PXnSkpFSOKEja2Bf332/emyvAG/g2buTnr2804BmAvalzgQFgOIC/Kqij7iGibgCGAHheCNHP1M1CiOlCiFghRGxKSkq1GlBtlZTOJYizM09/HRwqjVrnzJmDy5cvIyoqCrNnz8bu3bvRt29fjBgxAp21a/YffvhhdO/eHWFhYVi8eHHZvSEhIUhNTcXVq1cRGhqKadOmISwsDIMGDUKBkYA0mzdvxl133YWuXbvi/vvvL3NmmJubiylTpiAiIgKRkZFYv55l8/bt29GtWzd06dIF9913n/XPX9/RuQSp6OBNtxejJmqpjAwelU+YoJ9dmqNvXxYWsbH6c8uWAevWsQfdbt2sq3/YMG6/LjCUjtJS415hhw4Fbt4Ejh8vfz47m50CjhvHDho/+IA/n6lTjaum1GrglVdYVdOsGXtlXbWKZ0onTvDMzZw6Skf79g3Sl9Idj6XGjuokACEwYfQ2yPMzgAlmrs8D8Kol9VVl9H7xRaL+/Sunvn0L6Z57sql/f43R6yZT70J6cfwtvXHt+PFKywLj4+MpLCys7P2uXbvIzc2NrhjkS0tLIyKi/Px8CgsLo9TUVCIiatWqFaWkpFB8fDwpFAo6duwYERGNHTuWVqxYUelZ09PTSaM1qH3zzTc0a9YsIiJ67bXX6EWDfQDp6emUnJxMQUFBZe3QtcEYDdboHRTEK3YqkpjIRuivv65+2YsWcRmxsZbl1xngdctbL11iw3T//tYbZ4mIrl7l8j75pPz5I0f4/MqV5c/fvMnn33+//HndzmrD59izh8/NnFk+b2oq0f3387UpU4ieeIKoaVN+7+BAFBjIS1Ors6tdUmegnhi9q0QI4QWgP4CNBufchRCeutcABoFXUdmzHdpXVu6G1RCPXnXGNaXSIr14r1690FrngRPAggUL0KVLF0RHRyMhIQEXL16sdE/r1q0RFRUFAOjevTuuXr1aKU9iYiIefPBBRERE4OOPP8ZpbXjQP/74oyweBwCoVCocOnQI/fr1K2uHj4+PxY/dIMjO5h26Fe0XAMchcHGp2Qxj+XLe1W3pzMDPj3XY+/bxKP2JJ/h3s3y59cZZgHeBh4dXtmNUtF/oaN6c7SiGfqVKS4EFC9g/kmFchn79WDW1YIE+ANSJE0DPnvx+yRJOy5ezz6iDB4E33+Q6Jk7kmYekUWI39+ZCiFUABgBoKoRIBPAOACUAENEibbZRAH4jIsO5bzMAP2s7cUcAPxLRdlu0yZR385KSHBQWxsPNLQwKhRUOwS4maAWE1nulo6NFAsPdQL+7e/du/PHHHzh48CDc3NwwYMAAo27OnZ2dy14rFAqjKqkZM2Zg1qxZGDFiBHbv3o158+ZZ/iyNjXPn+GhMYDg4sLvx6gqMS5fY7cZHH1mn7+7bF1i7lt1zHzzIqpyWLavXBoDVUp9+yuFgdS7D9+xhlVJgYOX8Q4dymzMyeFXX5s0cq+Pjjyvn/fBDFkZTpwJvv82uur29WWAYBgRSKNioHh3NzyVp1NhzldRjRBRAREoiCiKi74hokYGwABF9T0TjK9x3hYi6aFMYEb1vrzbqEMJRW7eVhm/dklodRmYYnp6eyMnJMVlEVlYWVCoV3NzccO7cORw6dMi6NlQoK1DbUSxbtqzs/AMPPFAuTGxGRgaio6Oxd+9exGuD+6SnW7qgrYFQ0YdURdq2rX6M6BUrWFA8/rh19/Xty537e+/xSHz8+KrvMcewYTxb+f13fq/RmI9qN2QIzyp0+efP55mKzqOsIe7ubJi+fJn9OkVFcQAhY9HjJHcM9cHoXefoBYYVhm/SxsEwGPlDqdQvtdXi6+uLPn36IDw8HLNnz65UzODBg6FWqxEaGoo5c+YgugahQ+fNm4exY8eie/fuaGqw4uatt95CRkYGwsPD0aVLF+zatQt+fn5YvHgxRo8ejS5dupQFdmo0nD3L34fOwF2Rtm3ZAaGxVUPm0GhYFXPffbzW3xp0jghbtQK++sq6e43RuzfPFHRqqZMnefZgak/DXXfxLOHXX3mvxp49rHpyNKFoGDCAnfLNns17EnSrriR3LpYaOxpCqu5Ob7W6gLKzY6i4OLXKvGUUF/PObkMD361bfM6cE7QGSIM0eo8YQdS5s+nrX37JxtqKfoV0nDxJVFBQ+fzevVRt19oaDRu9tYsXbML48exDqbSU6IsvuG3XrpnOP24c+1V64gneLW5u97fkjgANxehdX6jWDMNwSa0O3UitphvC6hv5+bwksyFx5oxx+4UOc0tr9+0DIiJ4L8N335XfW7N8OatrRo2yvk1CsIty7eIFmzBsGO/jiI3lGUNIiHm7yJAhbKj+4QfecS33+UisQAoM8MY9oJoCo6INA2hc7kGKi3mT1nvv1XVLLKewkNVNpuwXgHmB8cEHvKopIIBjGYSHAz/9xIJz7VpgzBjevV0fGDyYBdGWLebtF4b5AVbFzZxp9+ZJGhdSYEC3rNbK3d6Gu7x1mHAP0qDRGezrIghPdbl4kW0N5mYYISG8WqqiwDh6lHdEv/wy+zn6+WdeCfToozzjyM6u2hVIbdK0Ka9QWrSIXW1U5ZOpWTMWKmPH8uY5icQKpMDQYvVu7+Ji7kgMDYaN0QGhTmBcvQqcP1+nTbEYUz6kDHFyYtVNRYGhixn9j3/wyP3hh3kPwvLlPCDo0KHqUXxtM2wYzwIBy5z4/f47O/yTSKxECgwtVvuTqrikFmicNoycHP1zmnJeV984e5Y7+44dzeer6LX2/Hl21fH88/p9DQAPDJ54gmcux49Xb6OdPdGF+wwO5plTVTg6ml4ZJZGYQQoMLdWaYVQUGELwH7GxzDCKi1kwuruzPcBwl3Btk5HBLrst4cwZdlHtWsUmzIoC4z//4UUML71kPL+jI+8Qr29ERrJ6acgQ6ThPYlekwNBi1QzD2B4MHRa6BzGHR30xqOrUUS4uvEt4717LO21bUlTEbiks2SinVgO7dwM9LAij0rYt6/2zs4GEBFY7Pf207WIg1BZCADExwBdf1HVLJI0cKTC08NJaC2cGpaWcKs4wgMY1w8jNZfWLUsmj1+Ji3sBV2yxaxDOBrVv1unpT7NnDy0zHjq26XMOVUp9+yq9ffbVmba0rvLzq5+xH0qiQAkOLTiVFlqwEMrYHQ0eFGcacOXPKueWYN28ePvnkE+Tm5uK+++5Dt27dEBERgY0bN1YuqwKm3KAbc1NuyqW5VeTk8PJRIdhBnYdH7dsxdK40OnViIf3TT+bzr1nDKrShQ6suWycwDh0CFi/mGUyrVjVvs0TSSLmjLF8vbX8JcbfijF4jKoZGUwSFwgNAFXpgtRooKACOuyEqsDvmDzbwaljBAeG4cePw0ksvlXmLXbt2LXbs2AEXFxf8/PPPaNKkCVJTUxEdHY0RI0YYeM6tzJIlS+Dj44OCggL07NkTY8aMgUajwbRp07B37160bt26zCfUe++9By8vL5w8eRIA+4+yiuJi3s/QtCnbD5ycODCObnltbenKP/4YSEvj0JmTJ7PDvn/8w3jekhJg/XqOs+DmVnXZOoHxzjv8rK+/brt2SySNEDnDKMMKF+caDR8rBuYBeIah0ZTl6dq1K5KTk3Hjxg0cP34cKpUKwcHBICLMnTsXkZGRuP/++5GUlFQW8MgUxtygm3JTbsyluVXo7BeGYSqHDuUodWfOWFdWdbl5E/jsM+Cxx9j99oQJHDPaVKS8nTs5kI+lfrE8PXmDXkoK79w2twxXIpHcWTOMcjOBCpSUZKCw8DLc3DpDoahidJqQwJ1M166VR9qGS2u1KquxY8di3bp1uHXrVpmTv5UrVyIfIPYjAAAgAElEQVQlJQVHjhyBUqlESEiIUbfmOix1g24zdPYLw5H6kCF8/PXX2olNPG8ez+b+9S9+P348x11YvRp47bXK+desYV2+JSFTdbRty99lTeJYSyR3CHKGocUqf1K6PRjG1DJG3IOMGzcOq1evxrp16zBWa4zNysqCv78/lEoldu3ahWvXrpmt0pQbdFNuystcmms0gFptvUrK0H6hIyiIfSzVhh3j3Dn24/Tssxy7AuDjXXcZ33RWVMS7sh9+2LhtyRSPPspxpS1ZVSWR3OHYTWAIIZYIIZKFEEaj5QkhBgghsoQQcdr0tsG1wUKI80KIS0KIOfZqYxmlpRCl3DFaJDBMLakFjG7eCwsLQ05ODgIDAxEQEAAAePzxxxEbG4uIiAgsX74cnTp1MlulKTfoptyUl7k0Dw1Fl86dsWvtWr07E0uer7CwvDpKx5AhrBbKzrasrOry5ps8u/nnP8ufnzCBN89VVIvt2MEGcmvdtL/8Mq/CkkgkVWOpW1trE4B+ALrBRExvcDS+LUbOKwBcBtAGgBOA4wA6W1Jntdybl5YSHTlCmoRrlJ0dQ0VFyVX7Az52jGMqG6OwkF2cp6RUXY69Uas5xvOJExyzOSaGY47n55u/LzWV8+bmElGFz3DXLnahvWGD/dp94ADX8e67la/dvMnxo996q/z5CROIfHzY7bxEIrEY1Af35kS0F0B1wrj1AnCJOPJeMYDVAIyEBLMRDg68IziXo8RWOcMoLWV1kxUzjDojM5NVUiEhrEpq1oxXPJ0+zXsPdMuDK2LMfqGjTx+eedhr1zcRr1Zq1oxH/xVp3hy4915eLaVbAl1QAGzaBIwerVcJSiQSm1PXNozeQojjQohfhRA6K2oggASDPInac/bD0xMiLx/QiKp3extza26IQsFCqD5s3ktP53Z6ePAxOJgFR0AAq28uX9av+DLEmP1Ch1IJPPAA2zFs7b02M5Ndbu/bxwZvUzveH3uM2x4Tw++3bWMh19iiBkok9Yy6FBhHAbQioi4AvgTwS3UKEUJMF0LECiFiU0zsAqaqOjYPD4AIjoWKqmcYxtyaV8QG7kFqTEkJ2xl8fMp3/EolEBjIvpby8oDExPL3VbBfGP3shg4FkpKAU0bNU9ZTWsob59q3BxYuBJ57DnjqKdP5R4/mz3/VKn6/Zg2786hvXmQlkkZGnQkMIsomolzt620AlEKIpgCSAAQbZA3SnjNVzmIi6kFEPfz8/Cpdd3FxQVpamnmhoR3JKgqAKt2DmNvlraM+uAfJyOAZgHZfRiVUKlb7JCfzxjgdBvsviAhpaWlwqehyQrds1RarpfbvZz9RzzzDu7mPHAG+/tq8asnbm4XW6tU8U9qyBXjkEemBVSKxM3X2DxNCNAdwm4hICNELLLzSAGQCaC+EaA0WFOMBTKhuPUFBQUhMTISp2UcZ2dnQZJdAnZUJJyczs4OMDO5UL10ynSc5mQVGqRXu0m3NrVusbjK3XJeInyU2ltVUSiULj7w8vk8IuLi4ICgoqPx9gYFAly7AL7/waN/Li5MpIUrE0eri4zkSni6dOcOb7YKCeLYwbpzlO8gfe4zrf/VVtmFIdZREYn8stY5bmwCsAnATQAnYDvEUgGcBPKu9/gKA0+BVUIcA3G1w71AAF8Crpd60tE5jq6Qs5qWXqNTZgWL2h5vP98gjRB07ms8zbRpRs2bVb0tNiY/nVUYffFB13qQkbmvHjkTZ2Xx86KGq73vzTa7DMDk7E/n7c3k+PkQeHkROTkRCVM7r4UEUGUn09ttlq7GsIi+PywCIWrTg1W4SicRqYMUqKbvNMIjosSqufwXgKxPXtgGoXS93/fvDYf58uJxMBvqYyXftWtUO6vz9efewRmPcfYgxZs5ku8N//2txk02i0+0/ZvYrYFq0YNXOffdxrOrz54Fp06q+b+5coHdvnnFlZrJqSJeI2MagS0olr7hq3Zo337VpA/j61swflZsbb9L74Qf2TGvp5yyRSKqNVPrquOceAID7ETM7ovPyWI0yZYr5svz9WVikpbGvIkv46ScWMu+8w0tHqwsRsHIlL3+1JPoawMbiDz4A5szRv68KNzcODVqXTJ3Kwq4+xdiWSBoxclimo2lTFHdsBq+4Emg0JvYnbNjAQuPRR82X1awZH5OTLav79m22OZSW8oi5Jpw4wfssLAk2ZMhrr/GIvVkzICqqZm2oLQYO5BlOt2513RKJ5I5ACgwDiqM7wusUoC4w0dEvW8bqFO1sxCS6iG2WCozjx/no6QksWVKz/Q0//sirhSwJIGSIEBzP+syZ+hez2hz1JTqhRHIHIAWGAeq7o6AoBEpj/6p8MSGBo81NmlS17l0nMKpwV16GTmC8+SZw9izw99+WN9oQjYbtFw8+yHEsrEWhML0MVyKR3PFIgWEA9e3NL/burXxxxQoe+VuiL7dWJRUXx7uwn3uO3ZQsWWLZfRXZt48Fm7XqKIlEIrEAKTAMUAS2RX4woNgfU/4CEauj+vXjlT5V4ePDq3asUUl16QI0acIb0Fav5n0L1rJyJYcnHTHC+nslEomkCqTAMECp9EFmJOB46HT5TXeHDwMXLli+GsfBgVdHWaKSKizk2A9duvD7qVPZpceGDdY1vrCQbRAPP8xCQyKRSGyMFBgGODr6ILML4JCdz6uNdCxbxqoiawzJOrcbVXFaK5x0K5N0s5ilS61r/Dvv8Iqhp5+27j6JRCKxECkwDHB09EKWdqBfZscoLGQV0ahRrDKyFH9/ywSGzuCtm2E4OABPPskG9qtXLatrzx7g44+B6dOlAz6JRGI3pMAwQAgHlLZQoSSoCXfCALB5M+9knjzZusL8/S1TSR0/ziqktm315yZP5pVY339f9f1ZWawqa9sW+PRT69ookUgkViAFRgUcHX2Q16MpzzA0GmD5cna2d9991hVkqUoqLg6IjCzv2qJVK67v+++Nx6swZMYMdjX+ww9yT4JEIrErUmBUQKlUIaebG7v12L2bI8tNnGj9ZjZ/f94VnpdnOg+RfoVURaZOZb9Vu3aZvv+nn3i571tvAXfdZV37JBKJxEqkwKiAo6MPMiO1wmHGDDZIW6uOAizb7X39OquUjAmMhx9ml+GmjN9JSRxDolcv3vAnkUgkdkYKjAoolT7Ib5bHMRrOnOHgPqGh1hdkyea9uDg+GvPd5OoKTJgArF8P/Pwzr6YqLORrGg07QCwqYlWUjGMtkUhqASkwKuDoqEKJOoOXtwLV94RqiXuQ48fZuB0RYfz6s8+yKmz0aCA8nD3EtmrFs4rffwc++4zDmkokEkktYDf35kKIJQAeApBMROFGrj8O4HUAAkAOgOeI6Lj22lXtuVIAaiLqYa92VsTR0QdqdQZo9CiIP/+0LKaEMSxRSR0/zh2+qY12kZGserpwAbh4kdOlS3x8+mleRiuRSCS1hD3jYXwPDpC03MT1eAD9iShDCDEEwGIAhpbbgUSUasf2GUWp9AGgQenIB+A45mb1C7JEYMTFAd27my/Hy4vVYj17Vr8tEolEYgPsppIior0A0s1cP0BEumhFhwAEmcpbmzg6qgAAJSUmm24Zrq7srtyUSio7m+NaGzN4SyQSST2kvtgwngLwq8F7AvCbEOKIEKJW9S48wwDUajOR9yzF359XQhnj5Ek+NpRgRRKJ5I6nzgWGEGIgWGC8bnD6HiLqBmAIgOeFEP3M3D9dCBErhIhNSUmpcXscHVlg1HiGAQBDhwJbtvBMoiK6FVJyhiGRSBoIdSowhBCRAL4FMJKI0nTniShJe0wG8DOAXqbKIKLFRNSDiHr4WRo/2ww6lZRabQOBMWcOR797773K144fZzfogYE1r0cikUhqgToTGEKIlgA2AHiCiC4YnHcXQnjqXgMYBOBUbbXLpiqpFi04KNLy5byyyZDjx1kdVVX0PolEIqkn2E1gCCFWATgIoKMQIlEI8ZQQ4lkhxLPaLG8D8AXwtRAiTggRqz3fDMB+IcRxAH8D2EpE2+3VzorYzOit4/XXAWdn4N139edKS9mGIdVREomkAWG3ZbVEZHYDAxE9DaBS8AYiugKgznpShcIVDg6utlFJAbzj+4UXgE8+AebO5V3jFy8CBQVSYEgkkgZFnRu96yOOjiqUlNhAJaXjtdd4c97//R+/18XAkCukJBJJA0IKDCMolT62m2EAQNOmwMyZwJo1rIqKi2P/T9XxUSWRSCR1hBQYRnB09LGdDUPHK69wxL5583iGERoKODnZtg6JRCKxI1JgGMHRUWWbVVKG+PgAL78MbNjAwZmkOkoikTQwpMAwgs1VUjpeegnw9uagStLgLZFIGhhSYBjBLiopgIXFK6/w665dbV++RCKR2BF7eqttsCiVKmg0+dBoiuDg4GzbwmfPZpfm/fvbtlyJRCKxMxbNMIQQLwohmgjmOyHEUSHEIHs3rq7Q+5OysR0D4E1848YBDnJyJ5FIGhaW9lpTiSgb7KZDBeAJAB/ZrVV1jN49iB3UUhKJRNJAsVRg6BweDQWwgohOG5xrdNjcPYhEIpE0AiwVGEeEEL+BBcYOrXNAjf2aVbfoVFI2X1orkUgkDRhLjd5PAYgCcIWI8oUQPgCm2K9ZdYtUSUkkEkllLJ1h9AZwnogyhRATAbwFIMt+zapbpEpKIpFIKmOpwPgvgHwhRBcArwC4DGC53VpVxzg6egEQUiUlkUgkBlgqMNRERABGAviKiBYC8LRfs+oWIRy0HmvlDEMikUh0WCowcoQQb4CX024VQjgAUFZ1kxBiiRAiWQhhNGKedl/HAiHEJSHECSFEN4Nrk4UQF7VpsoXttBnsT0oKDIlEItFhqcAYB6AIvB/jFoAgAB9bcN/3AAabuT4EQHttmg5WfUFrVH8HwF3geN7vCCFUFrbVJrA/KamSkkgkEh0WCQytkFgJwEsI8RCAQiKq0oZBRHsBmBumjwSwnJhDALyFEAEAHgTwOxGlE1EGgN9hXvDYHLv5k5JIJJIGikXLaoUQj4JnFLvBG/a+FELMJqJ1Naw/EECCwftE7TlT52sNpVKFwsLLtVmlRFIlREBxMXuYsSUlJRw12MOjaq81ajWQmwt4eQHCwu27paVch2FSKACVio+2oKQEiI8HkpM5wKWHhz65u9veGw8RUFjI9To68nPokqWfCxFQVMRtUyj4aHhvaSlfLyzk76e4GHBz48/excW2z2MJlu7DeBNATyJKBgAhhB+APwDUVGDUGCHEdLA6Cy1btrRZuTzDkCqpmqJWA7dvAzdvAmlpQHq6/piezn+C0lLOV1rKycEBCA4GWrfWp5Yt+fzt28CtW1zerVvcOeTlcTn5+fojEcencnbWJxcXoHlzIDBQn1q04D9oUhJw/Tpw7RofExK4fZmZQEYGHzMz+c/r4sJJV6azs76TMExKJXdWnp765OHB7U1JAVJT9ceSEqBjRyA8HAgL4xQayp/VsWOcjh7lY3Iyh1cJCQFatdInV1d+/sJCfSoo4PoqptxcTjk5fCwu5u9LCO6MVCp9EkL/faWnA9nZnNfVFWjTBmjbVn90cdF/hrrPMzGRn88YQrAT56ZNAV9ffi5dR1xYqO8sifiaj48+n7c3fxYXL3K6do1/P6ao2KkbJsMO28GBvzvd70b3O1Io+Nmzsvi3kJWl/9wqolDw5+Pqyh28mxu/BvSfvS5pNJXvVSj4mU19bgC3y8uLU8uWwM6dpvPaCksFhoNOWGhJg21coycBCDZ4H6Q9lwRgQIXzu40VQESLASwGgB49epAN2gRAb8Mg0oBt/HULEXceSUn8Bywo0P8QdcnZma/Hx3O6coWPGRn6Ts7VVf/ay0v/J1Sp+OjkxH+G9HS+T5d0Iym1Wj9C1GjKd8a6P1dmJrcjKYk7+Ip/CB2entxuhaL8n1mt5mdUq/V5daMuMvINOzjo/5C6o4MDdzbFxfpjfr7xP7iDQ+U2Nm3KSdeZtW/Pr52d9Z2YYdJouG2GqaQEuHGDO2Vdx5yXx21s2hTw8+Njx47chrNngW++4XZWxNGRhcjQoSxAb97kDvLcOWDHDuP36L4Xd3d98vDg5wgKKj8C9/Tk/NnZ5b/3jAx+loAArl/3e3F35+/3yhXg8mXgjz/0bXBwYEHcqhXQuzfX5e7OnbBhUqv5N22Ybt3i34CzM9/j68vPQMS/yevXOcJxWhrX5+nJ303PnsCECUC7dtzWgoLyAjEnh78P3aDEMGk0+qPudUlJ+d+PLvn5cR1eXvw5ennxsxiWp1ZzKijQD2B0Sfe7N/zc3dz4+QwHTmo1/+YN/68uLvz/ys/XCyxdsvWM0xSWCoztQogdAFZp348DsM0G9W8C8IIQYjXYwJ1FRDe1dX1gYOgeBOANG9RnMeweRAO1OrNs57e9KCwEVqzgP4JuhKj7seXlcaeTlMQ/WEvRjdLbtOE/umHnlp3NP7qsLP0o31QZulGmqyt3WoZ/eIWC783MLD8a9Pbm0XtEhH4kHxDAnaNuhKhScRmmKC0tL/zi4/kPFBDAs4Tmzfm1vz//WSxRARBxB6gTZjrhW1rKnVvLlnwMDtaPBm2NRmNeNaLRsCA4dYoFiI8Ph04JDzfdKegGEyUl+kGBk1PtOkQm4s6+qIi/b3Pfra0oLuZ6LFX/SGqOIGNDNmMZhRgDoI/27T4i+tmCe1aBZwpNAdwGr3xSAgARLRJCCABfgQ3a+QCmEFGs9t6pAOZqi3qfiJZWVV+PHj0oNjbWouepipSUDTh9egy6d4+Fp2d3m5RZkaws4L//BebP55G4SsWjKt1UVjc6bN6cR2mBgfqju3v50UteHnfWAQEsJIKDLf/TFhbqR5NFRXoh4ekpvbBLJI0dIcQRIuphSV6LAygR0XoA661pCBE9VsV1AvC8iWtLACyxpj5b4ubWEQCQn3/e5gLj5k3giy9YWGRnA4MHA3PmAP361c1oycWFBU1AQO3XLZFIGg5mBYYQIgeAsSmIAPf3TezSqnqAq2s7AAL5+eerXYahekGXTp/mpNEAjz4KvPaajNYqkUgaBmYFBhE1WvcfVeHg4AwXlxAUFFyo1v0XLwKjR7OQ0NGyJeuihw4FnnqKV5VIJBJJQ0HG9DaDm1vHas0wfvuNo7AqFMBXXwHdugGdO/OKColEImmoSIFhBlfXjsjM3AcigrDAuEDEBuxXX+WVSRs38hJIiUQiaQzINTBmcHPrAI0mD8XFN6rMW1gITJkCzJoFjBwJHDgghYVEImlcSIFhBsOVUuZITgYGDACWLQPmzQPWreNNORKJRNKYkCopM7i66gWGSnWv0Tzp6cADD7CRe906YMyY2myhRCKR1B5SYJjB2bkFHBzcTK6U0u2hOHcO2LKFBYdEIpE0VqTAMIMQDnBz62BUJZWfDzz0EDuE27BBCguJRNL4kTaMKnB1rby0tqgIGDUK2L8f+OEHYMSIOmqcRCKR1CJSYFSBm1sHFBZehUbDnv9KSniPxW+/Ad9+C4wfX8cNlEgkklpCCowq4JVSGhQUcDCl6dN5f8WCBcDUqXXbNolEIqlNpMCoAsOVUn/+CXz/PTB3LjBjRt22SyKRSGobKTCqwM2tAwAgN/ciXn6Z4yX885913CiJRCKpA+QqqSpwdGwCJ6fm+OEHP5w4AaxZUzexdCUSiaSukTMMC9BoovDFFyPQpw8wdmxdt0YikUjqBrsKDCHEYCHEeSHEJSHEHCPXPxdCxGnTBSFEpsG1UoNrm+zZzqpYsWIm0tJ88fnnMhykRCK5c7GbSkoIoQCwEMADABIBxAghNhHRGV0eInrZIP8MAIahhAqIKMpe7bOUq1eBZcsewAMPrEBU1DAA9o3vLZFIJPUVe84wegG4RERXiKgYwGoAI83kfwzAKju2p1rMmQM4OAhMm/ZGjaLvSSSShkuhuhA5RTnVvr+ktAR/xv+Jzec3IyUvxYYtq13safQOBJBg8D4RwF3GMgohWgFoDeBPg9MuQohYAGoAHxHRLybunQ5gOgC0bNnSBs3Wc+AAG7nfeCMTfn5JKCi4AC+v3jatQyKpCRkFGfg76W/E3IhBK69WeKTzI3BVuprMX1Jagh2XdyC3OBfh/uHo4NsBTgono3k1pMGt3FsoVBfCw8kDnk6ecHF0KYsNo9aocTPnJhKyE3A96zoSshKgclXh0bBH0cTZfPTm4tJi3Mq9BSeFE5wVznB2dIazwhkKB4VFz3079zZ+v/I7MgszK11zUjghxDsErb1bo5V3K5PPZ0hKXgpOJZ8qS4k5iUjJS0FqfipS8lOQW5wLB+GAmb1m4t2B78LTuepgpPkl+dhxaQd+PvcztlzYgozCjLJrHXw74O7gu9EnuA+ig6IR4h0CD6fqu7jWkAYOwv4maUFkLGS3DQoW4hEAg4noae37JwDcRUQvGMn7OoAgIpphcC6QiJKEEG3AguQ+Irpsrs4ePXpQbGysTdqv0QC9ewOJicC5cyU4etQNwcGz0abNBzYpvzGSXZSN86nn4eXihTaqNnB0sH48UqQuQlZRFvzc/EwGrdKQBocTD2Pj+Y2IuRGDe4LvwZjOYxDhH2FRoCtrKSktwV8Jf2HLhS1IzU/FPS3vwYCQAWiramuX+ipCREgvSMfN3Ju4mXMTVzKu4HDSYRxMPIhzqefK5VW5qDCpyyQ80/0ZhPqFlp0/efsklsYtxQ8nfkBKvn6E6+jgiI6+HRHuH442qjZIzkvG1cyruJZ1DdezrqO4tLhc+QqhgIeTB5wdnZGanwoNaSq1103phnFh4zCt2zREB0WXfUYFJQX47fJvWHd2HTaf34ysoqxK9yodlAj1C0XvoN6IDopGdFA0Ovh2gIDAyeST2Hx+MzZf2Iy/k/4Goeq+S0AgqEkQWqtaw9OpciefW5yLMylnyn0mKhcVQrxD4OfuBz83bXL3Q3xGPL479h0CmwTiyyFf4uFOD1cqr6CkAFsubMGqU6uw/dJ2FKgLoHJRYXjH4RjVaRR8XH1wMOEgDiQewF/X/0JaQVrZve5KdzT3aF6W3JRuyC/JR15JHvJL8stSoboQhepCFKmLUFRahEJ1IZq6NcXNV25W+XkY/YyEOEJEPSzKa0eB0RvAPCJ6UPv+DQAgog+N5D0G4HkiOmCirO8BbCGidebqtKXA2LCBXZV//z0weTJw+HAnuLuHITx8vU3Kry4a0iD2RiwyCzPRPaA7fN1866Qdqfmp2Hx+M04ln8KZ1DM4nXwaCdn6CaWTwgkdfDugs19nhDYNRWvv1iilUhSXFpelInURkvOSkZCdUDZKTc5LBgD4uPogqnkUoppFIap5FLo074KErARsPL8Rmy9sxq3cW3B0cESnpp1wOvk0CIR2Pu0wutNojA4djZ6BPasccRER3tj5BrZf2o52Pu3QwbcD2vu0RwffDmjh2aJMSGy/tB1ZRVlwUjjBy9mrrHNp4dkC/Vv1x4CQARjeYTgCPANM1qUhDbZd3IYlx5YgrSANDsIBDsIBCqEoa6daoy6XSjQlSMtPw83cm5U67qZuTblDDeROtWdgTxy9eRT/O/I/rD+zHiWaEvRt2ReD2g7CL+d+wZGbR6B0UGJ4x+GYEjUFwU2CcTrldNmI+nTKaVzNvAp/d3+EeIeglVcrTt6t4K50R25xLnKKc/hYlIMCdQH83f3R0qslgpsEI9grGMFNgnEh7QIWH1mMVadWIa8kD2F+YXgs/DGcTD6JLRe2IK8kDz6uPhjZcSTuDr4bao0aRWru9IpKi5BbnIvjt4/jcOLhMoGiclHB3ckdidmJAICeLXpieIfheKjDQwj2Cq70WeeX5ONq5lXEZ8QjPlObMuKRX5JfKa+zozNCm4Yi3D8cYX5hCPcPR3OP5iYHAocTD2P6luk4cfsERnQcgS+HfIlAz0DsubYHK0+sxLqz65BdlI0AjwCMDh2NUZ1GoV+rflAqlEZ/fxfTLyImKQZJOUm4lXsLt/Nu41buLdzKvYX8kny4K93hpnSDuxMf3ZRucHF0gbPCuezo7OgMbxdvvNbnNZO/P3PUF4HhCOACgPsAJAGIATCBiE5XyNcJwHYArUnbGCGECkA+ERUJIZoCOAhgpKHB3Bi2FBiTJwPbtgG3bwMODsDJkyNRWHgFPXuerHaZqfmp2HphK47dOoa5fefC393fovsK1YX4M/5PbDzHneXNXP1Ioo2qDXq26ImeLXqiV2AvRAdFG/1xVkVCVgIWxizE1cyreDLqSQxqO8hoh5tdlI1PD3yKzw59htziXLg4uqBT004I8wsrEw7ZRdk4m3oWZ1LO4GzqWVxOv2xyNOjp5FnW2QQ3CUZLr5Zo4twEZ1LOIO52HE7cPoFCdWG5/EPaD8HIjiMxtP1QeLt443bubWw8vxEbzm7AzvidUGvUiA6Kxqbxm+Dn7me0XiLCK7+9gs8PfY7eQb2RVpCGKxlXoNaoy+Vr5t4Mw9oPw/COw3F/m/vhrnTH+bTz2H11N/Zc24PdV3fjVu4tCAgMbD0QE8InYHToaKhcVQCAzMJMLD22FAtjFuJyxmUEeASgg28HaEhTLhEISgclHB0c4ejgCKWCX6tcVAjwCECAZ0DZMbhJMEK8Q0x2asl5yfg+7nssPrIYlzMuI6p5FKZETcGEiAlo6tbU5G/AlmqNnKIcrD61Gt8c/QYxN2Lg7+6PUZ1GYUzoGAwIGVDlb1RDGpxLPYdDiYdwKPEQMgoz8GDbBzGs/TCzgrk2KCktwReHv8A7u9+BgIC3izeScpLg6eSJMZ3HYGLERAwIGWCxeq2uqRcCQ9uQoQDmA1AAWEJE7wsh3gUQS0SbtHnmAXAhojkG990N4H8ANGDD/Hwi+q6q+mwlMIiAli1ZJbV2LZ+7fHk2EhO/RL9+eeAFYJZxOf0yNp7fiI3nN2L/9f1lU/gwvzD8OflPs0IjISsBs3+fja0XtyK3OBceTh4Y3G4wRnYciRaeLRB7IxYxN2IQkxSDa1nXAAB+bn4YHz4eEyMnomeLnmuoTHoAABzhSURBVFWqTA4nHsb8w/Px0+mfQCCoXFRIK0hDO592eL7n85gSNQVeLl4oKCnAwpiF+HD/h0gvSMcjnR/B3HvmIrJZZJV/jIKSAtzIuQGlQgknhVOZ3lr33hxqjRoX0y4i7lYcVK4qDAwZCGdHZ5P5MwoysOb0Gry842UENwnGjok70FpVOVbuO7vewbt738XMXjMxf/B8CCGg1qhxNfMqLqZdxPWs6+gW0A3dW3Q324kSEc6knMHa02vx46kfcSn9EpwUThjSbgiauTfDypMrkVeShz7BfTCj1wyMDh1dLYFeHTSkwe3c23Xewd7MuQl/d/8G04FayrXMa3hj5xvIL8nHhIgJGN5huFn7UX2l3giM2sZWAuPCBaBjR2DRIuCZZ/jcjRvf4sKFabjrritwdbUsWPfjGx7Hjyd/BABE+EdgZMeRGNlpJLIKszB81XC0UbUxKTT+uv4XRq8djYKSAkyImICRHUfi3tb3muwsk/OSsf/6fqw+tRqbzm9CUWkR2vm0w8SIiRjYeiA0pEFxaTFKSkvKVB3fHfsOBxMPoolzEzzd9WnMuGsGWni2wPoz6/FVzFc4kHAA7kp3jA4djZ3xO3Ej5wYebPsg3r/3fXRv0b16H24t8df1vzB81XA4Ozrj18d/RVRz/QrtTw58gtm/z8aUqCn4dsS3NhtVExGO3DyCH0/+iNWnViO9IB2PRTyGGb1moFtAN5vUIZHYGmsEBoio0aTu3buTLfj6ayKA6MIF/bmMjL20axcoLW27RWXEJsUS5oGmb5pOV9KvVLq+88pOcv2XK4UtDKPbubfLXfv2yLekfFdJ7Ra0ozPJZ6xuf2ZBJn139Du6d9m9JOYJwjwYTW2/aEsLDi2g7MJso+UcuXGEpvwyhVz+5UJ3f3c37Y7fbXVb6pLTyacp+LNg8vzAk3Ze2UlERItiFhHmgR796VFSl6rtVre6VE0FJQV2K18isRVgjY9FfaycYRhh7Fjg8GHg2jX9zu7i4mQcONAM7dp9gaCgmVWWMXHDRGw8vxGJLyfCy8XLaJ5d8bsw7MdhaK1qjT8n/QlfN1/M2jELX/79JQa1HYTVY1aX6cKrS2J2Is6mnIVSoYTSgVVASoUSzgpndPDtYJGagIhqZTWQPUjMTsSQlUNwIe0CpnWbhq9jvsbQ9kOxYdwGi5ZbSiSNHWtmGNL5YAU0GmDXLmD48PJuQJRKPygUXhZt3kvMTsSa02vwQs8XTAoLABjYeiC2Pb4Nw34chnuX34sAjwDsjN+JWdGz8O8H/l2tZakVCWoShKAmQTUqo6EKC4Cff++TezFy9UgsjFmIgSED8dPYn6SwkEiqgRQYFTh+HEhLA6L6JeHDfcsR5h+Gni16IsAzAG5uHVFQcKHKMr76+ytoSIOZd1U9ExkQMgBbJ2zFsB+H4VL6JSwduRRPRj1pgyeR6FC5qvDbE7/hp9M/YVToqAZpmJRI6gNSYFTgT+1e852Os7D5z7Vl54OaBKGTp0A71/N4s3WiyVF7bnEu/nfkfxgTOsbo6hxjDAgZgL+f/hsAEOYfVrMHkBjFxdEFT3R5oq6bIZE0aKTAqMDOnUBI9HFsvrIWs++ejZEdR/LS1RsxOHB1O/5IysJvS/shZnosfFwrOyL8Pu57ZBZmYlbvWVbVKwWFRCKp70ijtwElJYBKBfjNHIlMr72IfzEe3i7eZdeTk3/C2oOPYtYJRwwIYfuDoZ2hVFOKDl91QDP3ZjjwlNFN6xKJRFKvsMboLQMoGfD330Ce99+46rwJr/Z+tZywAAA3t44I9wI+6jsVv1/5HXN3zi13fdP5TbiSccXq2YVEIpE0BKTAMGDnTgAD/wlfl6ZGDdauru0AAKNDgvCPHv/Axwc+LtuYBwCfHfoMrb1bY1SnUbXVZIlEIqk1pA3DgA1H9gLdfsMbfT8x6r5YoXCDi0sb5OYexeeD1+Bk8kk8tekpdGraCWqNGvuv78f8B+c3OhcIEolEAkiBUUZeHuFE07fgrgnAcz2fM5lPpXoAyck/wlEA6x5dh+6Lu2PUmlEI8wtDE+cmmNp1ai22WiKRSGoPqZLSsmDrH6CW+zC5zZtwU7qZzOfrOwSlpTnIyvoL/u7++GXcL0jOS8avl37F9G7TLQqsIpFIJA0RKTDAri++PP0WkBWM9x5+2mxeb+/7IIQS6em/AgC6t+iOpSOXIrRpqEUb9SQSiaShIgUGgC0XtuCmw99of+Nt+HiZdp0NAI6OHvD27o+0tG1l58aHj8eZ588YDeYikUgkjYU7XmBoSIO5f/wTSG+L8aGTLbrHx2cI8vNPo7Dwup1bJ5FIJPUHuwoMIcRgIcR5IcQlIcQcI9efFEKkCCHitOlpg2uThRAXtcmynrwa5BbnwlsdCuz6Pzxwr2WBbXx8hgJAmVpKIpFI7gTstkpKcFi6hQAeAJAIIEYIsYkqh1ldQ0QvVLjXB8A7AHoAIABHtPdm2LqdTZybIOrSKhy9DNx1l2X3uLl1hItLCNLStqFFi2ds3SSJRCKpl9hzhtELwCUiukJExQBWAxhp4b0PAvidiNK1QuJ3AIPt1E7s3An06wc4WejxWggBH5+hyMjYCY2myF7NkkgkknqFPQVGIIAEg/eJ2nMVGSOEOCGEWCeE0FmNLb0XQojpQohYIURsSkqK1Y0sLAS8vYFBg6y7z8dnCDSaPGRm7rO6TolEImmI1LXRezOAECKKBM8illlbABEtJqIeRNTDz8/P6ga4uAAHDgAvv2zdfSrVQAjhLO0YEonkjsGeAiMJgOE60yDtuTKIKI2IdDqdbwF0t/TeukahcIe39wCkp2+rOrNEIpE0AuwpMGIAtBdCtBZCOAEYD2CTYQYhRIDB2xEAzmpf7wAwSAihEkKoAAzSnqtX+PoOQX7+ORQUxNd1UyQSicTu2E1gEJEawAvgjv4sgLVEdFoI8a4QYoQ220whxGkhxHEAMwE8qb03HcB7YKETA+Bd7bl6hVxeK5FI7iRkAKUacuhQO7i5dUJk5JZarVcikUhsgQygVIv4+g5FZuafKC0trOumSCQSiV2RAqOG+PgMhUZTgKysPXXdFIlEIrErUmDUEG/v/nBwcEFamrRjSCSSxo0UGDVEoXCFt/e9cnmtRCJp9EiBYQN8fYeioOAi8vJO13VTJBKJxG5IgWED/PzGQQhnJCUtrOumSCQSid2QAsMGODk1RbNmj+PWrWUoKbG5Q12JRCKpF0iBYSMCA2dAo8nHrVtL6ropEolEYhekwLARnp5R8PLqh6Skr0BUWtfNkUgkEpsjBYYNCQqaicLCq0hLk7u+JRJJ40MKDBvi6zsSzs4tkZj4RV03RSKRSGyOFBg2xOH/27v3KDnqKoHj39vVPd3Tj5n0PMn7BSThERIToxBXkNfGNYKurDxdjwePehZXPa67Cz53WVl1XRdfrILCioAKsoCsu+eAvEXFECA8zAOSkJDMTGZ6MtOZ6emeftXdP7oydEJCOpOZ9PTkfs6pU12/qq65v5mavlW/qv79fH6mT7+KZPJRUqkXqx2OMcaMKUsYY2zq1I/i89XT0fG9aodijDFjyhLGGAsEmmhvv4Lu7tvJ53dXOxxjjBkzljDGQekR2wxdXTdXOxRjjBkz45owRGSViGwSkc0icvUB1n9WRNaLyAsi8rCIzC5bVxSRdd50//7vncii0VOZMuVddHR8H9ctVDscY4wZE+OWMETEAW4A3g2cBFwqIiftt9lzwHJVXQzcDfxb2bqMqi7xpguoMdOnf4psdge9vfdVOxRjjBkT43mFsQLYrKpbVTUH/AK4sHwDVX1UVdPe4lPAjHGM56hqaXkvodB8Nm36CDt2fAvXzVc7JGOMOSLjmTCmAzvKlnd6ZQdzJVA+qERIRNaKyFMi8r7xCHA8iTicdtpDNDaeyZYtn+OZZ95CMvnbaodljDGjNiFueovIFcBy4JtlxbO9cWYvA74tIvMP8t6PeYllbSKROArRVq6+fg6nnvo/nHLKfRQKg6xb9042bPgwuVx3tUMzxpjDNp4JowOYWbY8wyvbh4icC3wBuEBVs3vLVbXDm28FHgOWHuiHqOpNqrpcVZe3traOXfRjRERoabmQFSvWM2vW5+np+Tlr1iwklXqp2qEZY8xhGc+E8TRwgojMFZE64BJgn6edRGQpcCOlZNFTVh4XkaD3ugVYCawfx1jHneOEmTfvOpYvfwGROjZsuALXzVU7LGOMqdi4JQxVLQCfBB4ANgB3qeqfRORaEdn71NM3gSjwy/0en10ErBWR54FHga+rak0njL0ikYUsWPAjhoaeZ9u2f652OMYYUzFR1WrHMGaWL1+ua9eurXYYFdm48Up27foJS5c+SWPj6dUOxxhzjBKRZ7z7xYc0IW56H4uOP/56QqFZbNz41xSLQ9UOxxhjDskSRpX4/Q0sXPgTMpktbNny99UOxxhjDskSRhVNmXImM2Z8ls7OH9DX90C1wzHGmDdlCaPK5s79KuHwyWzc+BHy+b5qh2OMMQdlCaPKHCfEokW3kc8nePbZM3jllc/Q03MXw8M7qx2aMcbsw1/tAAzEYktZtOh2Ojp+QFfXTXR0lIZ4DQZn0NCwktbWD9DcvBrHqa9ypMaYY5kljAmire1i2touxnXzpFLPMzDwBwYGfk8y+RiJxJ04TgOtrR+gvf1ypkw5i1JnwMYYc/RYwphgfL4ADQ3LaWhYDvwtqkWSycfo7r6DROJudu36L+rqptHYeAaqLqoFVItAERCmTDmb9vbLCAanVbkmxpjJxr64V0OKxQy7d/+a7u47yGRe9q4yHET8iDi4bpqhoZcAIR4/h/b2D9HS8n78/tgB96fqks12kMm8TDr9CpnMK7huhmh0CbHYMiKRU/H56o5qHY0xR9fhfHHPEsYkk06/THf37XR3387w8Kv4fPXEYisAF9U8rpv35sNks6/husMj7/X5QojUUSwOACASIBJZTCy2jFBoNoFAK4FAK3V1pbnjRMjn+ykU+sjn+ygU+igU+gmHFxGPn4PPF6zSb8EYUylLGAZVZWDg9+zadRtDQy/h8wUQeX3y+eoIBmdSX38C4fCJ1NefQDA4HRCGh7cyOPjMyJRKPUuh0H9YP99xYjQ3r6al5S9palqF3x89orqkUs+h6hKLvQWRsXu4L53ehOsOE4ksRkTGbL9jIZdL4Pc32lWeGVeWMMyYKxYz5PO95PMJ8vkEuVwC1x3C728iEGgamTtOAwMDT9Hbew+9vfeRz/fi84WIx8+lsfFMGhtXEostq+hDsFAYpLv7Drq6biSVWgeA399MU9N5xOPnE4+fRyh0+IM0FgqD9PTcya5dtzAw8AcAotGlTJv2CdraLjui5DYWcrletm//Fzo7f0B9/XwWLLilJvoby+f7cN1hu39WYyxhmAnBdQsMDPyOROIe+vr+j0xmM1Bq+orF3kpj4zuorz8ev78Rx2nE7y9NhcIeurpupqfnDorFFJHIaUyb9gn8/gb6+h6kv/9BcrkuAMLhhUQiiwmHFxEOL/SmE3GcMKpFisUhisUUxeIQ2ewOurtvp6fnLlx3iHB4EVOnXonPV09n540MDb2A48Rob7+cqVM/TjR62phddagq2exOMpnNBIPTqK8/4Q1XSsViho6O77J9+9coFgdpb7+cZPIJstnXmDHj08yd+1UcJ/KG/Q4MPEV3908pFlMEg7MIhWYRDM70Xs/EcRpGXQ/XLQDg8x38+ZhCYZAdO77Fjh3/jutmaGu7lNmzv0AksmhUP/NQSlec6+jre4BY7C3E4+dNuKvDWmIJw0xIuVw3e/b8bmRKpZ6h1Av+G/l89bS1XcK0aR8nFluxzweCqjI09BL9/Q+STD7G0NB6hodfBfYey4LPF9zn/sxejhOlre0SjjvuShoa3jay39IH7x/p7PwhicSduO4wfn8zsdhSotHXp1BoJsViikJhkGJx75TCdYdx3SyqOVw3i+tmKRT6SKc3kU5v9Jq+0mVxNNLQ8FZisRU0NKygUEjy6qtfIpvdQXPzaubN+zqRyMkUCoNs3XoNnZ03EArNY8GCHxGPn00ul6C7+za6un5MOr0Bny9CINBCNruT0hNzrxMJUlfXRiDQRl1dO3V1bYTDC2lsfCex2HJ8vsA+27tujv7+h0kkfklv772oKi0tF9DaehHx+Pk4Tmhku66uH7Ft27Xk8z20tl5EMDiLzs4f4roZWlsvYvbsLxKNLq7g2EiQTD7Cnj1P4vdPIRxeSH39AsLhBfj9MVRdBgaeIpG4h97ee7y/d0ks9jbmzPkyTU3vHlXiUHW9v18G1x0euZrO5TrJ5brIZjvJ5TopFlM0NJxOPH4ukcgpFTeNqhbJZF4lnV4PKNHoEoLBWWOW5Fw3Ry7XTSg089AbH4AlDFMTSv+YPRQKeygUkt58D6A0N68mEIgfxr6GyWRe8T6cN1AspnCcqDdFcJwofv8UGhvPPGSTUz7fRyJxN4ODTzM4+BxDQy+iOprBroRQaA7h8IKRD8D6+vlkszsZHFzDwMAahoZeGEma0egy5s//JvH4u96wp2TyCTZtupJMZjMNDSsZHFyDap6GhrczdepHaW39oPfBWiSb7SKb3UE2+xrZ7E5yuW5yuR7y+dI8l9tFLtcJgM8XpqHhdKZMeSfh8EL6+h6gt/deCoV+HKeBlpYLEXHo7f2VVxalufm9xGIr6Oj4PsPDW2hsPJP5879BQ8PbgFKT2s6d19PR8T2KxcGR7cubLv3+JgqF3fT3P0R//0MjTY4+X8RL9K8nvbq6aYBLLrcLkQDx+Hm0tpbuje3e/b9s3/6vZLPbiUaXMWfOl2lufi+qRXK5DoaHt41MuVy316xaPvVRNtDnQTjU1R2Hz1c3kqgCgTbi8XOIx88lFJpDsTiE66YpFtO4bppCIcnQ0AbS6fWk0xvecPLi98eJRpcQjS4lEjkFYJ/3F4tpQHGcBvz+GI4Tw3EacJx6hodf855sfJlM5mUymVepqzuOM84YXe8QEyZhiMgq4DuAA/xYVb++3/og8FNgGbAbuFhVt3nrrgGupHTkfEpVD9k7nyUMMx5cN086vYFU6jmy2a6yf+DS5PfHvCfMgvh8pUmkzit/8yfFisUMqdQ6isUU8fg5b3rWWiym2bbtK/T2/orm5tVMnXolkcjJo6pTLtfDnj2/JZl8gmTycYaGXqD0ARWjpeVCWls/SFPT+SPxu26OZPJREom7SSTupVDYTSRyKvPmfYOmplUHPFvO5/vp6PguHR3/ST7f84b1ACJ1NDauJB4/l3j8XGKxZd4Z+ZaRK7N0eiOqOZqbV9Pc/B78/sZ99uG6ebq7b2P79usYHt5KINBCoZDc7+pVCASaCQRa9pn8/iYcJ4zPF8Lnq/emEIFAM3V1UwkGpxEItIx8UXZ4eCfJ5MMjiS6X23XQ33EwOJNI5GTC4ZOJRE4mEjkJVZdUat3INDT0wgGvhH2+EKp60GTm84W9h1VO9OYLaG+/fFRXLRMiYUjpN/wycB6wk9KQrZeWj5wnIn8DLFbVT4jIJcD7VfViETkJ+DmwApgGPAScqKVvqB2UJQxjRief7yed3kQ0umSkyelgXLdAJvMy4fCCinsccN0chUL/yOPX+XwfPl+IxsaVOE54LKqA6xbo6fkZ/f2PEAxOJxSaSyg0x5tmjvlj3qpKOr2efH43Pl8YxwnjOBHvdbSirnxct0A2ux0R/8g+Sicfjrc+P9L0WSgM4LppgsEZ1NVNG7MmrYmSME4H/klV/9xbvgZAVb9Wts0D3jZ/EBE/sAtoBa4u37Z8uzf7mZYwjDHm8EyUEfemAzvKlnd6ZQfcxhsDfA/QXOF7jTHGHEU13725iHxMRNaKyNpEIlHtcIwxZtIaz4TRAZQ/5zXDKzvgNl6TVCOlm9+VvBcAVb1JVZer6vLW1tYxCt0YY8z+xjNhPA2cICJzRaQOuAS4f79t7gc+7L2+CHhESzdV7gcuEZGgiMwFTgDWjGOsxhhjDmHcujdX1YKIfBJ4gNJjtbeo6p9E5FpgrareD9wM3CYim4E+SkkFb7u7gPVAAbjqUE9IGWOMGV/2xT1jjDmGTZSnpIwxxkwiljCMMcZUZFI1SYlIAtg+yre3AL1jGM5EYnWrXZO5fla3iWG2qlb0iOmkShhHQkTWVtqOV2usbrVrMtfP6lZ7rEnKGGNMRSxhGGOMqYgljNfdVO0AxpHVrXZN5vpZ3WqM3cMwxhhTEbvCMMYYU5FjPmGIyCoR2SQim0Xk6mrHc6RE5BYR6RGRl8rKmkTkNyLyijevfOzTCUREZorIoyKyXkT+JCKf9sprvn4iEhKRNSLyvFe3f/bK54rIH73j806vX7aaJCKOiDwnIr/2lidT3baJyIsisk5E1nplNX9c7u+YThjeqIA3AO8GTgIu9Ub7q2U/AVbtV3Y18LCqngA87C3XogLwd6p6EvB24Crv7zUZ6pcFzlbV04AlwCoReTvwDeB6VT0e6Kc0bHGt+jSwoWx5MtUN4F2quqTscdrJcFzu45hOGJSGgN2sqltVNQf8AriwyjEdEVV9glJHjuUuBG71Xt8KvO+oBjVGVLVLVZ/1Xg9S+vCZziSon5akvMWANylwNnC3V16TdQMQkRnAe4Afe8vCJKnbm6j543J/x3rCOFZG9mtX1S7v9S6gvZrBjAURmQMsBf7IJKmf12SzDugBfgNsAZLeaJRQ28fnt4F/AFxvuZnJUzcoJfcHReQZEfmYVzYpjsty49a9uZmYVFVFpKYfjRORKPDfwGdUdaB0slpSy/XzuvBfIiJTgHuBhVUOaUyIyGqgR1WfEZGzqh3POHmHqnaISBvwGxHZWL6ylo/Lcsf6FUbFI/vVuG4RmQrgzXuqHM+oiUiAUrK4Q1Xv8YonTf0AVDUJPAqcDkzxRqOE2j0+VwIXiMg2Ss2+ZwPfYXLUDQBV7fDmPZSS/Qom2XEJljAqGRVwMigf2fDDwK+qGMuoee3eNwMbVPU/ylbVfP1EpNW7skBE6oHzKN2jeZTSaJRQo3VT1WtUdYaqzqH0P/aIql7OJKgbgIhERCS29zVwPvASk+C43N8x/8U9EfkLSu2re0cFvK7KIR0REfk5cBal3jK7ga8A9wF3AbMo9eb7QVXd/8b4hCci7wB+C7zI623hn6d0H6Om6yciiyndGHUoncjdparXisg8SmflTcBzwBWqmq1epEfGa5L6nKqunix18+pxr7foB36mqteJSDM1flzu75hPGMYYYypzrDdJGWOMqZAlDGOMMRWxhGGMMaYiljCMMcZUxBKGMcaYiljCMGYCEJGz9vbiasxEZQnDGGNMRSxhGHMYROQKb9yKdSJyo9dhYEpErvfGsXhYRFq9bZeIyFMi8oKI3Lt3PAQROV5EHvLGvnhWROZ7u4+KyN0islFE7pDyTrKMmQAsYRhTIRFZBFwMrFTVJUARuByIAGtV9WTgcUrfrgf4KfCPqrqY0rfT95bfAdzgjX1xBrC3R9OlwGcojc0yj1IfTMZMGNZbrTGVOwdYBjztnfzXU+pQzgXu9La5HbhHRBqBKar6uFd+K/BLr8+h6ap6L4CqDgN4+1ujqju95XXAHODJ8a+WMZWxhGFM5QS4VVWv2adQ5Ev7bTfa/nbK+1EqYv+fZoKxJiljKvcwcJE35sHeMZtnU/o/2tvr6mXAk6q6B+gXkT/zyj8EPO6NFLhTRN7n7SMoIuGjWgtjRsnOYIypkKquF5EvUhpZzQfkgauAIWCFt66H0n0OKHVp/UMvIWwFPuKVfwi4UUSu9fbxV0exGsaMmvVWa8wREpGUqkarHYcx482apIwxxlTErjCMMcZUxK4wjDHGVMQShjHGmIpYwjDGGFMRSxjGGGMqYgnDGGNMRSxhGGOMqcj/A9SY8ULRP4kTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 684us/sample - loss: 1.2836 - acc: 0.6746\n",
      "Loss: 1.2835506092350801 Accuracy: 0.6745587\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5931 - acc: 0.5233\n",
      "Epoch 00001: val_loss improved from inf to 1.39215, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/001-1.3922.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 1.5931 - acc: 0.5233 - val_loss: 1.3922 - val_acc: 0.5628\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7249\n",
      "Epoch 00002: val_loss improved from 1.39215 to 1.00847, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/002-1.0085.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9361 - acc: 0.7248 - val_loss: 1.0085 - val_acc: 0.7114\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6622 - acc: 0.8088\n",
      "Epoch 00003: val_loss improved from 1.00847 to 0.76943, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/003-0.7694.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6623 - acc: 0.8087 - val_loss: 0.7694 - val_acc: 0.7796\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8557\n",
      "Epoch 00004: val_loss improved from 0.76943 to 0.76060, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/004-0.7606.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5012 - acc: 0.8558 - val_loss: 0.7606 - val_acc: 0.7932\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8906\n",
      "Epoch 00005: val_loss improved from 0.76060 to 0.62232, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/005-0.6223.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3802 - acc: 0.8906 - val_loss: 0.6223 - val_acc: 0.8192\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9161\n",
      "Epoch 00006: val_loss did not improve from 0.62232\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2988 - acc: 0.9161 - val_loss: 0.6298 - val_acc: 0.8337\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9339\n",
      "Epoch 00007: val_loss improved from 0.62232 to 0.60083, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/007-0.6008.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2382 - acc: 0.9339 - val_loss: 0.6008 - val_acc: 0.8428\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9475\n",
      "Epoch 00008: val_loss did not improve from 0.60083\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1923 - acc: 0.9475 - val_loss: 0.6986 - val_acc: 0.8111\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9573\n",
      "Epoch 00009: val_loss did not improve from 0.60083\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1615 - acc: 0.9572 - val_loss: 0.6268 - val_acc: 0.8407\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9633\n",
      "Epoch 00010: val_loss improved from 0.60083 to 0.54645, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/010-0.5465.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1372 - acc: 0.9633 - val_loss: 0.5465 - val_acc: 0.8535\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9725\n",
      "Epoch 00011: val_loss improved from 0.54645 to 0.54156, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/011-0.5416.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1144 - acc: 0.9724 - val_loss: 0.5416 - val_acc: 0.8581\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9752\n",
      "Epoch 00012: val_loss improved from 0.54156 to 0.50037, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_6_conv_checkpoint/012-0.5004.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1023 - acc: 0.9751 - val_loss: 0.5004 - val_acc: 0.8656\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9792\n",
      "Epoch 00013: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0848 - acc: 0.9792 - val_loss: 0.5857 - val_acc: 0.8584\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9818\n",
      "Epoch 00014: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0804 - acc: 0.9818 - val_loss: 0.5410 - val_acc: 0.8698\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9822\n",
      "Epoch 00015: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0756 - acc: 0.9821 - val_loss: 0.5692 - val_acc: 0.8584\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9829\n",
      "Epoch 00016: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0750 - acc: 0.9828 - val_loss: 0.5058 - val_acc: 0.8696\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9849\n",
      "Epoch 00017: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0665 - acc: 0.9849 - val_loss: 0.5530 - val_acc: 0.8633\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9910\n",
      "Epoch 00018: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0460 - acc: 0.9910 - val_loss: 0.5279 - val_acc: 0.8698\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9891\n",
      "Epoch 00019: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0516 - acc: 0.9891 - val_loss: 0.5751 - val_acc: 0.8619\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9832\n",
      "Epoch 00020: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0658 - acc: 0.9832 - val_loss: 0.6757 - val_acc: 0.8456\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9894\n",
      "Epoch 00021: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0475 - acc: 0.9893 - val_loss: 0.5894 - val_acc: 0.8598\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9894\n",
      "Epoch 00022: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0469 - acc: 0.9893 - val_loss: 0.5751 - val_acc: 0.8670\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9828\n",
      "Epoch 00023: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0698 - acc: 0.9827 - val_loss: 0.5932 - val_acc: 0.8602\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9929\n",
      "Epoch 00024: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0373 - acc: 0.9929 - val_loss: 0.5511 - val_acc: 0.8763\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9917\n",
      "Epoch 00025: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0389 - acc: 0.9917 - val_loss: 0.5026 - val_acc: 0.8826\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9962\n",
      "Epoch 00026: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0253 - acc: 0.9962 - val_loss: 0.6381 - val_acc: 0.8595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9896\n",
      "Epoch 00027: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0438 - acc: 0.9896 - val_loss: 0.5512 - val_acc: 0.8763\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9957\n",
      "Epoch 00028: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0261 - acc: 0.9956 - val_loss: 0.6078 - val_acc: 0.8642\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9858\n",
      "Epoch 00029: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0579 - acc: 0.9857 - val_loss: 0.5695 - val_acc: 0.8770\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9940\n",
      "Epoch 00030: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0309 - acc: 0.9940 - val_loss: 0.5727 - val_acc: 0.8765\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9903\n",
      "Epoch 00031: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0443 - acc: 0.9902 - val_loss: 0.5874 - val_acc: 0.8765\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9915\n",
      "Epoch 00032: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0372 - acc: 0.9915 - val_loss: 0.5735 - val_acc: 0.8772\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9944\n",
      "Epoch 00033: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0290 - acc: 0.9943 - val_loss: 0.6297 - val_acc: 0.8670\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0367 - acc: 0.9917 - val_loss: 0.5414 - val_acc: 0.8821\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9973\n",
      "Epoch 00035: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0184 - acc: 0.9972 - val_loss: 0.6225 - val_acc: 0.8724\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9898\n",
      "Epoch 00036: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0423 - acc: 0.9898 - val_loss: 0.5606 - val_acc: 0.8810\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9975\n",
      "Epoch 00037: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0170 - acc: 0.9975 - val_loss: 0.5976 - val_acc: 0.8744\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9888\n",
      "Epoch 00038: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0451 - acc: 0.9888 - val_loss: 0.6085 - val_acc: 0.8761\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9964\n",
      "Epoch 00039: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0214 - acc: 0.9964 - val_loss: 0.6053 - val_acc: 0.8772\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9921\n",
      "Epoch 00040: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0333 - acc: 0.9921 - val_loss: 0.6095 - val_acc: 0.8728\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9930\n",
      "Epoch 00041: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0291 - acc: 0.9929 - val_loss: 0.5828 - val_acc: 0.8819\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9906\n",
      "Epoch 00042: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0393 - acc: 0.9906 - val_loss: 0.6066 - val_acc: 0.8751\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9965\n",
      "Epoch 00043: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0201 - acc: 0.9965 - val_loss: 0.5843 - val_acc: 0.8791\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9939\n",
      "Epoch 00044: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0276 - acc: 0.9939 - val_loss: 0.6075 - val_acc: 0.8807\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9963\n",
      "Epoch 00045: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0203 - acc: 0.9963 - val_loss: 0.5668 - val_acc: 0.8835\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9967\n",
      "Epoch 00046: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0181 - acc: 0.9967 - val_loss: 0.6790 - val_acc: 0.8707\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9958\n",
      "Epoch 00047: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0209 - acc: 0.9957 - val_loss: 0.7009 - val_acc: 0.8651\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9904\n",
      "Epoch 00048: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0368 - acc: 0.9904 - val_loss: 0.5967 - val_acc: 0.8828\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9946\n",
      "Epoch 00049: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0229 - acc: 0.9946 - val_loss: 0.7173 - val_acc: 0.8595\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9944\n",
      "Epoch 00050: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0239 - acc: 0.9944 - val_loss: 0.6176 - val_acc: 0.8796\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9974\n",
      "Epoch 00051: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0154 - acc: 0.9974 - val_loss: 0.6219 - val_acc: 0.8875\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9920\n",
      "Epoch 00052: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0323 - acc: 0.9920 - val_loss: 0.7193 - val_acc: 0.8637\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9970\n",
      "Epoch 00053: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0160 - acc: 0.9970 - val_loss: 0.6145 - val_acc: 0.8777\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9915\n",
      "Epoch 00054: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0339 - acc: 0.9915 - val_loss: 0.5949 - val_acc: 0.8854\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9950\n",
      "Epoch 00055: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0228 - acc: 0.9950 - val_loss: 0.5901 - val_acc: 0.8842\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9976\n",
      "Epoch 00056: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0146 - acc: 0.9976 - val_loss: 0.6625 - val_acc: 0.8749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9930\n",
      "Epoch 00057: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0308 - acc: 0.9930 - val_loss: 0.7020 - val_acc: 0.8744\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9962\n",
      "Epoch 00058: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0175 - acc: 0.9962 - val_loss: 0.6392 - val_acc: 0.8779\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9949\n",
      "Epoch 00059: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0213 - acc: 0.9949 - val_loss: 0.6907 - val_acc: 0.8763\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9966\n",
      "Epoch 00060: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0167 - acc: 0.9966 - val_loss: 0.6592 - val_acc: 0.8740\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9965\n",
      "Epoch 00061: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0179 - acc: 0.9965 - val_loss: 0.8761 - val_acc: 0.8523\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9957\n",
      "Epoch 00062: val_loss did not improve from 0.50037\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0209 - acc: 0.9957 - val_loss: 0.7080 - val_acc: 0.8754\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.5836 - acc: 0.8459\n",
      "Loss: 0.5835922305457688 Accuracy: 0.8458982\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4875 - acc: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 1.24950, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/001-1.2495.hdf5\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 1.4875 - acc: 0.5495 - val_loss: 1.2495 - val_acc: 0.5993\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7549 - acc: 0.7784\n",
      "Epoch 00002: val_loss improved from 1.24950 to 0.67897, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/002-0.6790.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.7552 - acc: 0.7784 - val_loss: 0.6790 - val_acc: 0.7973\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8524\n",
      "Epoch 00003: val_loss improved from 0.67897 to 0.50318, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/003-0.5032.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.5064 - acc: 0.8524 - val_loss: 0.5032 - val_acc: 0.8570\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8922\n",
      "Epoch 00004: val_loss improved from 0.50318 to 0.45223, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/004-0.4522.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3764 - acc: 0.8922 - val_loss: 0.4522 - val_acc: 0.8616\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9134\n",
      "Epoch 00005: val_loss improved from 0.45223 to 0.41779, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/005-0.4178.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2971 - acc: 0.9134 - val_loss: 0.4178 - val_acc: 0.8782\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9297\n",
      "Epoch 00006: val_loss improved from 0.41779 to 0.36076, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/006-0.3608.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2449 - acc: 0.9297 - val_loss: 0.3608 - val_acc: 0.9059\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9408\n",
      "Epoch 00007: val_loss improved from 0.36076 to 0.35850, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/007-0.3585.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2018 - acc: 0.9408 - val_loss: 0.3585 - val_acc: 0.8945\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9515\n",
      "Epoch 00008: val_loss improved from 0.35850 to 0.33111, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/008-0.3311.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1680 - acc: 0.9515 - val_loss: 0.3311 - val_acc: 0.9059\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9610\n",
      "Epoch 00009: val_loss improved from 0.33111 to 0.31475, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/009-0.3147.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1370 - acc: 0.9610 - val_loss: 0.3147 - val_acc: 0.9154\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9688\n",
      "Epoch 00010: val_loss did not improve from 0.31475\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1136 - acc: 0.9687 - val_loss: 0.3179 - val_acc: 0.9099\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9675\n",
      "Epoch 00011: val_loss improved from 0.31475 to 0.30534, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/011-0.3053.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1134 - acc: 0.9675 - val_loss: 0.3053 - val_acc: 0.9173\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9803\n",
      "Epoch 00012: val_loss did not improve from 0.30534\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0767 - acc: 0.9802 - val_loss: 0.3466 - val_acc: 0.9089\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9788\n",
      "Epoch 00013: val_loss did not improve from 0.30534\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0769 - acc: 0.9788 - val_loss: 0.4748 - val_acc: 0.8740\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9849\n",
      "Epoch 00014: val_loss improved from 0.30534 to 0.30338, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/014-0.3034.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0608 - acc: 0.9849 - val_loss: 0.3034 - val_acc: 0.9192\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9876\n",
      "Epoch 00015: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0517 - acc: 0.9875 - val_loss: 0.3304 - val_acc: 0.9175\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9864\n",
      "Epoch 00016: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0523 - acc: 0.9864 - val_loss: 0.3268 - val_acc: 0.9147\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9865\n",
      "Epoch 00017: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0514 - acc: 0.9865 - val_loss: 0.3145 - val_acc: 0.9236\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9887\n",
      "Epoch 00018: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0444 - acc: 0.9886 - val_loss: 0.4597 - val_acc: 0.8989\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9914\n",
      "Epoch 00019: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0376 - acc: 0.9914 - val_loss: 0.3317 - val_acc: 0.9140\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9919\n",
      "Epoch 00020: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0356 - acc: 0.9919 - val_loss: 0.3403 - val_acc: 0.9224\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9936\n",
      "Epoch 00021: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0286 - acc: 0.9936 - val_loss: 0.3509 - val_acc: 0.9143\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9945\n",
      "Epoch 00022: val_loss did not improve from 0.30338\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0248 - acc: 0.9945 - val_loss: 0.3243 - val_acc: 0.9206\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9893\n",
      "Epoch 00023: val_loss improved from 0.30338 to 0.27381, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_7_conv_checkpoint/023-0.2738.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0397 - acc: 0.9893 - val_loss: 0.2738 - val_acc: 0.9364\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9948\n",
      "Epoch 00024: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0240 - acc: 0.9947 - val_loss: 0.3243 - val_acc: 0.9222\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9919\n",
      "Epoch 00025: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0304 - acc: 0.9919 - val_loss: 0.3225 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9926\n",
      "Epoch 00026: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0299 - acc: 0.9926 - val_loss: 0.2903 - val_acc: 0.9350\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9962\n",
      "Epoch 00027: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0181 - acc: 0.9963 - val_loss: 0.3275 - val_acc: 0.9266\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9974\n",
      "Epoch 00028: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0135 - acc: 0.9974 - val_loss: 0.3389 - val_acc: 0.9222\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9953\n",
      "Epoch 00029: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0206 - acc: 0.9953 - val_loss: 0.3228 - val_acc: 0.9245\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9941\n",
      "Epoch 00030: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0236 - acc: 0.9941 - val_loss: 0.2790 - val_acc: 0.9357\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9985\n",
      "Epoch 00031: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0097 - acc: 0.9985 - val_loss: 0.3285 - val_acc: 0.9250\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9921\n",
      "Epoch 00032: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0285 - acc: 0.9921 - val_loss: 0.2755 - val_acc: 0.9352\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9968\n",
      "Epoch 00033: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0142 - acc: 0.9967 - val_loss: 0.4348 - val_acc: 0.9066\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9948\n",
      "Epoch 00034: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0210 - acc: 0.9948 - val_loss: 0.2800 - val_acc: 0.9343\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9984\n",
      "Epoch 00035: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0092 - acc: 0.9984 - val_loss: 0.2994 - val_acc: 0.9359\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9965\n",
      "Epoch 00036: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0149 - acc: 0.9965 - val_loss: 0.3130 - val_acc: 0.9317\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 00037: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0168 - acc: 0.9956 - val_loss: 0.4031 - val_acc: 0.9196\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9973\n",
      "Epoch 00038: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.4116 - val_acc: 0.9117\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9900\n",
      "Epoch 00039: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0340 - acc: 0.9900 - val_loss: 0.3609 - val_acc: 0.9241\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9947\n",
      "Epoch 00040: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0199 - acc: 0.9947 - val_loss: 0.2998 - val_acc: 0.9334\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9990\n",
      "Epoch 00041: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0068 - acc: 0.9990 - val_loss: 0.3079 - val_acc: 0.9369\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9925\n",
      "Epoch 00042: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0236 - acc: 0.9925 - val_loss: 0.3090 - val_acc: 0.9299\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9990\n",
      "Epoch 00043: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.2948 - val_acc: 0.9359\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9971\n",
      "Epoch 00044: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0113 - acc: 0.9971 - val_loss: 0.3173 - val_acc: 0.9341\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9981\n",
      "Epoch 00045: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0082 - acc: 0.9980 - val_loss: 0.4602 - val_acc: 0.9047\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9941\n",
      "Epoch 00046: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0216 - acc: 0.9941 - val_loss: 0.3101 - val_acc: 0.9345\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 00047: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0055 - acc: 0.9990 - val_loss: 0.3336 - val_acc: 0.9311\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9946\n",
      "Epoch 00048: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0205 - acc: 0.9946 - val_loss: 0.3125 - val_acc: 0.9355\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9975\n",
      "Epoch 00049: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0103 - acc: 0.9975 - val_loss: 0.3510 - val_acc: 0.9290\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00050: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0175 - acc: 0.9948 - val_loss: 0.2917 - val_acc: 0.9345\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0191 - acc: 0.9946 - val_loss: 0.3037 - val_acc: 0.9357\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9959\n",
      "Epoch 00052: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0162 - acc: 0.9959 - val_loss: 0.3187 - val_acc: 0.9385\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 00053: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2992 - val_acc: 0.9378\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9994\n",
      "Epoch 00054: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2946 - val_acc: 0.9385\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9962\n",
      "Epoch 00055: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0138 - acc: 0.9962 - val_loss: 0.3199 - val_acc: 0.9304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9976\n",
      "Epoch 00056: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0099 - acc: 0.9976 - val_loss: 0.3854 - val_acc: 0.9248\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9983\n",
      "Epoch 00057: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0072 - acc: 0.9983 - val_loss: 0.2872 - val_acc: 0.9404\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9963\n",
      "Epoch 00058: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.3168 - val_acc: 0.9378\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00059: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.3109 - val_acc: 0.9357\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 00060: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0096 - acc: 0.9976 - val_loss: 0.3710 - val_acc: 0.9243\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9950\n",
      "Epoch 00061: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0177 - acc: 0.9950 - val_loss: 0.3036 - val_acc: 0.9383\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9958\n",
      "Epoch 00062: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0140 - acc: 0.9958 - val_loss: 0.3114 - val_acc: 0.9364\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 00063: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0055 - acc: 0.9988 - val_loss: 0.2882 - val_acc: 0.9408\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0036 - acc: 0.9993 - val_loss: 0.2924 - val_acc: 0.9401\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9974\n",
      "Epoch 00065: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0101 - acc: 0.9974 - val_loss: 0.3361 - val_acc: 0.9317\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00066: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0088 - acc: 0.9975 - val_loss: 0.3514 - val_acc: 0.9269\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9945\n",
      "Epoch 00067: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0192 - acc: 0.9945 - val_loss: 0.3192 - val_acc: 0.9369\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n",
      "Epoch 00068: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0050 - acc: 0.9988 - val_loss: 0.3147 - val_acc: 0.9373\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00069: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0058 - acc: 0.9985 - val_loss: 0.3540 - val_acc: 0.9290\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 00070: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0070 - acc: 0.9980 - val_loss: 0.3376 - val_acc: 0.9329\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00071: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.3717 - val_acc: 0.9299\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9961\n",
      "Epoch 00072: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0117 - acc: 0.9961 - val_loss: 0.3255 - val_acc: 0.9383\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 00073: val_loss did not improve from 0.27381\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0046 - acc: 0.9988 - val_loss: 0.4008 - val_acc: 0.9213\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 758us/sample - loss: 0.3127 - acc: 0.9161\n",
      "Loss: 0.31269655174741123 Accuracy: 0.91609555\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3525 - acc: 0.5899\n",
      "Epoch 00001: val_loss improved from inf to 0.99449, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/001-0.9945.hdf5\n",
      "36805/36805 [==============================] - 77s 2ms/sample - loss: 1.3524 - acc: 0.5899 - val_loss: 0.9945 - val_acc: 0.6986\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5757 - acc: 0.8304\n",
      "Epoch 00002: val_loss improved from 0.99449 to 0.45314, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/002-0.4531.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5758 - acc: 0.8304 - val_loss: 0.4531 - val_acc: 0.8684\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8870\n",
      "Epoch 00003: val_loss improved from 0.45314 to 0.35295, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/003-0.3529.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3849 - acc: 0.8870 - val_loss: 0.3529 - val_acc: 0.8945\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9150\n",
      "Epoch 00004: val_loss improved from 0.35295 to 0.30907, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/004-0.3091.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2914 - acc: 0.9150 - val_loss: 0.3091 - val_acc: 0.9071\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9306\n",
      "Epoch 00005: val_loss improved from 0.30907 to 0.25923, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/005-0.2592.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2367 - acc: 0.9306 - val_loss: 0.2592 - val_acc: 0.9248\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9418\n",
      "Epoch 00006: val_loss improved from 0.25923 to 0.24428, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/006-0.2443.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1981 - acc: 0.9418 - val_loss: 0.2443 - val_acc: 0.9264\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9521\n",
      "Epoch 00007: val_loss improved from 0.24428 to 0.22535, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/007-0.2254.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1652 - acc: 0.9520 - val_loss: 0.2254 - val_acc: 0.9336\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9579\n",
      "Epoch 00008: val_loss improved from 0.22535 to 0.21046, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/008-0.2105.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1451 - acc: 0.9579 - val_loss: 0.2105 - val_acc: 0.9376\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9651\n",
      "Epoch 00009: val_loss improved from 0.21046 to 0.20275, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/009-0.2027.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1194 - acc: 0.9651 - val_loss: 0.2027 - val_acc: 0.9411\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9694\n",
      "Epoch 00010: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1048 - acc: 0.9694 - val_loss: 0.2220 - val_acc: 0.9343\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9767\n",
      "Epoch 00011: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0845 - acc: 0.9767 - val_loss: 0.2536 - val_acc: 0.9248\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9801\n",
      "Epoch 00012: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0743 - acc: 0.9801 - val_loss: 0.2093 - val_acc: 0.9385\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9800\n",
      "Epoch 00013: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0716 - acc: 0.9800 - val_loss: 0.2661 - val_acc: 0.9241\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9864\n",
      "Epoch 00014: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0543 - acc: 0.9864 - val_loss: 0.2631 - val_acc: 0.9269\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9861\n",
      "Epoch 00015: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0502 - acc: 0.9861 - val_loss: 0.2667 - val_acc: 0.9255\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9895\n",
      "Epoch 00016: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0423 - acc: 0.9895 - val_loss: 0.2176 - val_acc: 0.9411\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9915\n",
      "Epoch 00017: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0356 - acc: 0.9915 - val_loss: 0.2344 - val_acc: 0.9350\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9903\n",
      "Epoch 00018: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0368 - acc: 0.9903 - val_loss: 0.2512 - val_acc: 0.9311\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9935\n",
      "Epoch 00019: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0281 - acc: 0.9935 - val_loss: 0.2365 - val_acc: 0.9341\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9934\n",
      "Epoch 00020: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0267 - acc: 0.9934 - val_loss: 0.2329 - val_acc: 0.9364\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9951\n",
      "Epoch 00021: val_loss did not improve from 0.20275\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0227 - acc: 0.9950 - val_loss: 0.2826 - val_acc: 0.9299\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9874\n",
      "Epoch 00022: val_loss improved from 0.20275 to 0.20006, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/022-0.2001.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0451 - acc: 0.9874 - val_loss: 0.2001 - val_acc: 0.9506\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9965\n",
      "Epoch 00023: val_loss improved from 0.20006 to 0.19672, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/023-0.1967.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0171 - acc: 0.9965 - val_loss: 0.1967 - val_acc: 0.9502\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9933\n",
      "Epoch 00024: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0273 - acc: 0.9932 - val_loss: 0.2070 - val_acc: 0.9471\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9921\n",
      "Epoch 00025: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0292 - acc: 0.9921 - val_loss: 0.2212 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9982\n",
      "Epoch 00026: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0110 - acc: 0.9982 - val_loss: 0.2133 - val_acc: 0.9471\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9975\n",
      "Epoch 00027: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0134 - acc: 0.9975 - val_loss: 0.2398 - val_acc: 0.9453\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9969\n",
      "Epoch 00028: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0144 - acc: 0.9969 - val_loss: 0.2104 - val_acc: 0.9478\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9953\n",
      "Epoch 00029: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0191 - acc: 0.9953 - val_loss: 0.2410 - val_acc: 0.9394\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9959\n",
      "Epoch 00030: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0166 - acc: 0.9959 - val_loss: 0.2427 - val_acc: 0.9399\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9964\n",
      "Epoch 00031: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 0.2614 - val_acc: 0.9392\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9966\n",
      "Epoch 00032: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0139 - acc: 0.9966 - val_loss: 0.2576 - val_acc: 0.9404\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9927\n",
      "Epoch 00033: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0255 - acc: 0.9927 - val_loss: 0.2045 - val_acc: 0.9502\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9977\n",
      "Epoch 00034: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0101 - acc: 0.9977 - val_loss: 0.2297 - val_acc: 0.9485\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9927\n",
      "Epoch 00035: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0250 - acc: 0.9927 - val_loss: 0.2292 - val_acc: 0.9485\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9945\n",
      "Epoch 00036: val_loss did not improve from 0.19672\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0192 - acc: 0.9945 - val_loss: 0.1983 - val_acc: 0.9515\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 00037: val_loss improved from 0.19672 to 0.19004, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/037-0.1900.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0051 - acc: 0.9993 - val_loss: 0.1900 - val_acc: 0.9557\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9995\n",
      "Epoch 00038: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0044 - acc: 0.9994 - val_loss: 0.2110 - val_acc: 0.9525\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9931\n",
      "Epoch 00039: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0251 - acc: 0.9931 - val_loss: 0.2388 - val_acc: 0.9439\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9958\n",
      "Epoch 00040: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0156 - acc: 0.9958 - val_loss: 0.2131 - val_acc: 0.9520\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9948\n",
      "Epoch 00041: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0177 - acc: 0.9948 - val_loss: 0.2005 - val_acc: 0.9511\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9990\n",
      "Epoch 00042: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0057 - acc: 0.9990 - val_loss: 0.2128 - val_acc: 0.9527\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9990\n",
      "Epoch 00043: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0058 - acc: 0.9990 - val_loss: 0.2284 - val_acc: 0.9495\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9987\n",
      "Epoch 00044: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.2422 - val_acc: 0.9425\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00045: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0119 - acc: 0.9967 - val_loss: 0.3559 - val_acc: 0.9213\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9902\n",
      "Epoch 00046: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0301 - acc: 0.9901 - val_loss: 0.2139 - val_acc: 0.9515\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9954\n",
      "Epoch 00047: val_loss did not improve from 0.19004\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0163 - acc: 0.9954 - val_loss: 0.1939 - val_acc: 0.9539\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00048: val_loss improved from 0.19004 to 0.18570, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_BN_8_conv_checkpoint/048-0.1857.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0043 - acc: 0.9992 - val_loss: 0.1857 - val_acc: 0.9560\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9995\n",
      "Epoch 00049: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0036 - acc: 0.9995 - val_loss: 0.2172 - val_acc: 0.9546\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9981\n",
      "Epoch 00050: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0080 - acc: 0.9981 - val_loss: 0.2256 - val_acc: 0.9534\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9991\n",
      "Epoch 00051: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 0.2261 - val_acc: 0.9485\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9939\n",
      "Epoch 00052: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0203 - acc: 0.9939 - val_loss: 0.2297 - val_acc: 0.9509\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 00053: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0066 - acc: 0.9984 - val_loss: 0.1977 - val_acc: 0.9543\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9951\n",
      "Epoch 00054: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0159 - acc: 0.9951 - val_loss: 0.2170 - val_acc: 0.9529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9995\n",
      "Epoch 00055: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0034 - acc: 0.9995 - val_loss: 0.1929 - val_acc: 0.9606\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9986\n",
      "Epoch 00056: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.2234 - val_acc: 0.9581\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0092 - acc: 0.9974 - val_loss: 0.2135 - val_acc: 0.9541\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9979\n",
      "Epoch 00058: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0074 - acc: 0.9979 - val_loss: 0.2164 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00059: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.2088 - val_acc: 0.9564\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 00060: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.2797 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9947\n",
      "Epoch 00061: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0177 - acc: 0.9947 - val_loss: 0.2248 - val_acc: 0.9502\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9946\n",
      "Epoch 00062: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.1930 - val_acc: 0.9581\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9964\n",
      "Epoch 00063: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0125 - acc: 0.9964 - val_loss: 0.1897 - val_acc: 0.9543\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00064: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2021 - val_acc: 0.9590\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9975\n",
      "Epoch 00065: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0092 - acc: 0.9974 - val_loss: 0.2151 - val_acc: 0.9534\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9980\n",
      "Epoch 00066: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0081 - acc: 0.9980 - val_loss: 0.1884 - val_acc: 0.9576\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00067: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2070 - val_acc: 0.9553\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9950\n",
      "Epoch 00068: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.2160 - val_acc: 0.9515\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9971\n",
      "Epoch 00069: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0107 - acc: 0.9971 - val_loss: 0.2149 - val_acc: 0.9525\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00070: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.2360 - val_acc: 0.9525\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9946\n",
      "Epoch 00071: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0185 - acc: 0.9946 - val_loss: 0.2234 - val_acc: 0.9522\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00072: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.2174 - val_acc: 0.9562\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00073: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0034 - acc: 0.9994 - val_loss: 0.2142 - val_acc: 0.9513\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9959\n",
      "Epoch 00074: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0152 - acc: 0.9958 - val_loss: 0.2049 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9963\n",
      "Epoch 00075: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0117 - acc: 0.9963 - val_loss: 0.1994 - val_acc: 0.9550\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 00076: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1908 - val_acc: 0.9588\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00077: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.2070 - val_acc: 0.9553\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00078: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.2545 - val_acc: 0.9462\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9991\n",
      "Epoch 00079: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.2063 - val_acc: 0.9546\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
      "Epoch 00080: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.2384 - val_acc: 0.9511\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 00081: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0093 - acc: 0.9971 - val_loss: 0.2105 - val_acc: 0.9583\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 00082: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.2429 - val_acc: 0.9485\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
      "Epoch 00083: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9985 - val_loss: 0.2800 - val_acc: 0.9455\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9948\n",
      "Epoch 00084: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0177 - acc: 0.9948 - val_loss: 0.2064 - val_acc: 0.9539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 00085: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2069 - val_acc: 0.9562\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9989\n",
      "Epoch 00086: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.2256 - val_acc: 0.9550\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 00087: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 0.2189 - val_acc: 0.9550\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00088: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2625 - val_acc: 0.9469\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971\n",
      "Epoch 00089: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0102 - acc: 0.9971 - val_loss: 0.2152 - val_acc: 0.9546\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00090: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1892 - val_acc: 0.9595\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00091: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.2107 - val_acc: 0.9578\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980\n",
      "Epoch 00092: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0072 - acc: 0.9980 - val_loss: 0.2541 - val_acc: 0.9499\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9947\n",
      "Epoch 00093: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0164 - acc: 0.9947 - val_loss: 0.2382 - val_acc: 0.9525\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9977\n",
      "Epoch 00094: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0077 - acc: 0.9977 - val_loss: 0.2414 - val_acc: 0.9539\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993\n",
      "Epoch 00095: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0026 - acc: 0.9993 - val_loss: 0.2120 - val_acc: 0.9574\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994\n",
      "Epoch 00096: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0022 - acc: 0.9994 - val_loss: 0.2014 - val_acc: 0.9602\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9981\n",
      "Epoch 00097: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.2720 - val_acc: 0.9439\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976\n",
      "Epoch 00098: val_loss did not improve from 0.18570\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0091 - acc: 0.9976 - val_loss: 0.2231 - val_acc: 0.9518\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmckkk31fIAQTkC0sCRAWRdxQ61JR64K74t5WW79Wf8Xdam3V2tZStYhWrVXcqyKitCqIC6js+04g+0YSss5MZs7vj5PJRhImkCGBPO/XayAzc+fcc+/ce56z3DlXaa0RQgghACw9nQEhhBC9hwQFIYQQTSQoCCGEaCJBQQghRBMJCkIIIZpIUBBCCNFEgoIQQogmEhSEEEI0kaAghBCiSYC/ElZKvQz8FCjWWo/qZLkJwDLgcq31ewdLNy4uTqempnZbPoUQoi9YuXJlqdY6/mDL+S0oAK8CzwKvdbSAUsoKPAn819dEU1NTWbFixWFnTggh+hKl1B5flvNb95HWeimw7yCL3QG8DxT7Kx9CCCF812NjCkqpZOAi4B8+LHuLUmqFUmpFSUmJ/zMnhBB9VE8OND8D/FZr7TnYglrruVrrLK11Vnz8QbvEhBBCHCJ/jikcTBbwllIKIA44VynVoLX+sKsJuVwucnNzqa+v7+489hl2u50BAwZgs9l6OitCiB7UY0FBa53m/Vsp9Sqw4FACAkBubi7h4eGkpqbSGGREF2itKSsrIzc3l7S0tIN/QAhxzPLnJalvAqcCcUqpXOBhwAagtZ7Tneuqr6+XgHAYlFLExsYi4zVCCL8FBa31FV1Y9vrDXZ8EhMMj+08IAX3oF81udx0ORx4ej6unsyKEEL1WnwkKHk8dTmcBWnd/UKioqOD5558/pM+ee+65VFRU+Lz8I488wtNPP31I6xJCiIPpM0GheVN1t6fcWVBoaGjo9LMLFy4kKiqq2/MkhBCHos8EBW+fudbdHxRmzZrFzp07yczM5J577mHJkiVMnTqV6dOnk56eDsCFF17I+PHjGTlyJHPnzm36bGpqKqWlpWRnZzNixAhuvvlmRo4cyVlnnUVdXV2n612zZg2TJ09mzJgxXHTRRZSXlwMwe/Zs0tPTGTNmDJdffjkAX331FZmZmWRmZjJ27Fiqqqq6fT8IIY5+Pfk7Bb/Yvv1OqqvXHPC61m48nloslhDMlEu+CwvLZMiQZzp8/4knnmDDhg2sWWPWu2TJElatWsWGDRuaLvF8+eWXiYmJoa6ujgkTJnDxxRcTGxvbJu/befPNN3nxxRe57LLLeP/997n66qs7XO+1117L3//+d0455RQeeughfve73/HMM8/wxBNPsHv3boKCgpq6pp5++mmee+45pkyZQnV1NXa7vUv7QAjRN/SZlkKz7m8ptGfixImtrvmfPXs2GRkZTJ48mZycHLZv337AZ9LS0sjMzARg/PjxZGdnd5h+ZWUlFRUVnHLKKQBcd911LF26FIAxY8Zw1VVX8frrrxMQYOL+lClTuOuuu5g9ezYVFRVNrwshREvHXMnQUY3e7a6ltnYTdvtgbLZov+cjNDS06e8lS5bw+eefs2zZMkJCQjj11FPb/fV1UFBQ099Wq/Wg3Ucd+eSTT1i6dCkff/wxjz/+OOvXr2fWrFmcd955LFy4kClTprBo0SKGDx9+SOkLIY5dfail4L0Ov/tbCuHh4Z320VdWVhIdHU1ISAhbtmxh+fLlh73OyMhIoqOj+frrrwH497//zSmnnILH4yEnJ4fTTjuNJ598ksrKSqqrq9m5cyejR4/mt7/9LRMmTGDLli2HnQchxLHnmGspdKT5x1kHnX+vy2JjY5kyZQqjRo3inHPO4bzzzmv1/tlnn82cOXMYMWIEw4YNY/Lkyd2y3n/961/cdttt1NbWMmjQIF555RXcbjdXX301lZWVaK351a9+RVRUFA8++CCLFy/GYrEwcuRIzjnnnG7JgxDi2KL8cTWOP2VlZem2N9nZvHkzI0aM6PRzHo+Tmpp1BAUdR2CgzLTaHl/2oxDi6KSUWqm1zjrYcn2w+6j7WwpCCHGs6DNBQSmzqUdby0gIIY6kPhMU/DnQLIQQx4o+GBSk+0gIITrSZ4KCufpISfeREEJ0os8EBcOCtBSEEKJjfSoomNZC72gphIWFdel1IYQ4EvpUUACLdB8JIUQn+lhQUPij+2jWrFk899xzTc+9N8Kprq5m2rRpjBs3jtGjR/PRRx/5nKbWmnvuuYdRo0YxevRo3n77bQAKCgo4+eSTyczMZNSoUXz99de43W6uv/76pmX/+te/dvs2CiH6hmNvmos774Q1B06dDRDsrgFlAUtw19LMzIRnOp46e8aMGdx555388pe/BOCdd95h0aJF2O12PvjgAyIiIigtLWXy5MlMnz7dp/sh/+c//2HNmjWsXbuW0tJSJkyYwMknn8y8efP4yU9+wv3334/b7aa2tpY1a9aQl5fHhg0bALp0JzchhGjJby0FpdTLSqlipdSGDt6/Sim1Tim1Xin1nVIqw195abFWv6Q6duxYiouLyc/PZ+3atURHR5OSkoLWmvvuu48xY8ZwxhlnkJeXR1FRkU9pfvPNN1xxxRVYrVYSExM55ZRT+PHHH5kwYQKvvPIKjzzyCOvXryc8PJxBgwaxa9cu7rjjDj777DMiIiL8sp1CiGOfP1sKrwLPAq918P5u4BStdblS6hxgLjDpsNfaSY2+vmYzSlkJCRl62Ktp69JLL+W9996jsLCQGTNmAPDGG29QUlLCypUrsdlspKamtjtldlecfPLJLF26lE8++YTrr7+eu+66i2uvvZa1a9eyaNEi5syZwzvvvMPLL7/cHZslhOhj/NZS0FovBfZ18v53WuvyxqfLgQH+youXmerCP5ekzpgxg7feeov33nuPSy+9FDBTZickJGCz2Vi8eDF79uzxOb2pU6fy9ttv43a7KSkpYenSpUycOJE9e/aQmJjIzTffzE033cSqVasoLS3F4/Fw8cUX8/vf/55Vq1b5ZRuFEMe+3jKmcCPwqf9Xo9DaP0Fh5MiRVFVVkZycTL9+/QC46qqrOP/88xk9ejRZWVlduqnNRRddxLJly8jIyEApxVNPPUVSUhL/+te/+NOf/oTNZiMsLIzXXnuNvLw8Zs6cicdjtu2Pf/yjX7ZRCHHs8+vU2UqpVGCB1npUJ8ucBjwPnKS1LutgmVuAWwAGDhw4vm2N29cpn2trd6C1g9DQkb5uQp8iU2cLcew6KqbOVkqNAV4CLugoIABoredqrbO01lnx8Yd+L4Te9OM1IYTojXosKCilBgL/Aa7RWm87QmuVH68JIUQn/DamoJR6EzgViFNK5QIPAzYArfUc4CEgFni+8br9Bl+aNodH5j4SQojO+C0oaK2vOMj7NwE3+Wv97VFKWgpCCNGZPjbNhbQUhBCiM30qKMhAsxBCdK5PBQWzubrbu5AqKip4/vnnD+mz5557rsxVJIToNfpYUPDPfZo7CwoNDQ2dfnbhwoVERUV1a36EEOJQ9amg0Dw7afeOK8yaNYudO3eSmZnJPffcw5IlS5g6dSrTp08nPT0dgAsvvJDx48czcuRI5s6d2/TZ1NRUSktLyc7OZsSIEdx8882MHDmSs846i7q6ugPW9fHHHzNp0iTGjh3LGWec0TTBXnV1NTNnzmT06NGMGTOG999/H4DPPvuMcePGkZGRwbRp07p1u4UQx57eMs1Ft+lk5my0jsHjCcVq7VosPMjM2TzxxBNs2LCBNY0rXrJkCatWrWLDhg2kpaUB8PLLLxMTE0NdXR0TJkzg4osvJjY2tlU627dv58033+TFF1/ksssu4/333+fqq69utcxJJ53E8uXLUUrx0ksv8dRTT/HnP/+Zxx57jMjISNavXw9AeXk5JSUl3HzzzSxdupS0tDT27etwKiohhACOwaDQW0ycOLEpIADMnj2bDz74AICcnBy2b99+QFBIS0sjMzMTgPHjx5OdnX1Aurm5ucyYMYOCggKcTmfTOj7//HPeeuutpuWio6P5+OOPOfnkk5uWiYmJ6dZtFEIce465oNBZjd7lqqK+fjchIaOwWu1+zUdoaGjT30uWLOHzzz9n2bJlhISEcOqpp7Y7hXZQUFDT31artd3uozvuuIO77rqL6dOns2TJEh555BG/5F8I0Tf1qTEFfw00h4eHU1VV1eH7lZWVREdHExISwpYtW1i+fPkhr6uyspLk5GQA/vWvfzW9fuaZZ7a6JWh5eTmTJ09m6dKl7N69G0C6j4QQB9XHgoJ3c7t3oDk2NpYpU6YwatQo7rnnngPeP/vss2loaGDEiBHMmjWLyZMnH/K6HnnkES699FLGjx9PXFxc0+sPPPAA5eXljBo1ioyMDBYvXkx8fDxz587lZz/7GRkZGU03/xFCiI74depsf8jKytIrVqxo9ZqvUz43NFRSV7ed4ODhBASE+SuLRy2ZOluIY9dRMXX2keefloIQQhwr+lhQ8M+YghBCHCv6VFDw/njtaOsyE0KII6VPBQXpPhJCiM71saAg3UdCCNGZPhUUlDKbq7W0FIQQoj19Kij0ppZCWJhcEiuE6H36VFDw1yypQghxrOhTQcG7ud199dGsWbNaTTHxyCOP8PTTT1NdXc20adMYN24co0eP5qOPPjpoWh1Nsd3eFNgdTZcthBCHym8T4imlXgZ+ChRrrUe1874C/gacC9QC12utVx3ueu/87E7WFHYwdzbgdlehVBAWS6DPaWYmZfLM2R3PtDdjxgzuvPNOfvnLXwLwzjvvsGjRIux2Ox988AERERGUlpYyefJkpk+f3qLFcqD2ptj2eDztToHd3nTZQghxOPw5S+qrwLPAax28fw4wpPExCfhH4/9HQPe2FMaOHUtxcTH5+fmUlJQQHR1NSkoKLpeL++67j6VLl2KxWMjLy6OoqIikpKQO02pviu2SkpJ2p8Bub7psIYQ4HH4LClrrpUqp1E4WuQB4TZu+nOVKqSilVD+tdcHhrLezGj1AVdUqbLZ47PaUw1nNAS699FLee+89CgsLmyaee+ONNygpKWHlypXYbDZSU1PbnTLby9cptoUQwl968n4KyUBOi+e5ja8dVlDokNsNDQ0oLPhjoHnGjBncfPPNlJaW8tVXXwFmmuuEhARsNhuLFy9mz549nabR0RTbkydP5he/+AW7d+9u6j6KiYlpmi77mcabSJSXl7dqLWgNDQ3mYbVCYJseM6cT9u83rwcFmeVb2r8fvvgC/vtfqKiA4GDzsNvN8kFBEBYGMTEQGwvh4eBwQH29+d/pBJfLPNxu89Aa0tIgIwNSUqCqCpYvh+++g4AAmDIFJk6E0FDweKC8HHbuhGXLzGPjRrM9Hg/YbDB1Kvz0p3DaaSbP2dmwZ495PyzMpJOfD6tWmUdhoVlOKfOwWMy+CQqCxERISjL5mjYNRo0yy4DZhlWrYPVqWL/ePPbta96uwECIijKPwECoqTEPqxUmTzb5HD0aNm+GH34w25GUBMOGweDBsHcvrFgBK1dCbS2EhJi8h4VBZKR52O1mvzoczd+p1WryWF1tvi+HA4YMgbFjzT6OjGze1spKKCmB4mKzj7Zvh23bzPqSkqBfP5N/l8t8d7W1zcvv32/yf+KJ5vtxuaCgwOzb3FzIyTGP6GizzJQp5lj54Qfz2LWr9b4aOhRGjjR5Vcqsr64Oduww+2jrVrMOm80sHxsLxx0HAweabc7NNY/KyubjNzjY5D862vydn2+2Mz/f7LuICPMYNMhsS3q62b5ly8wxWFrafFyHhJh9Hx5u0q+tNd+ny2XSj401aRUWmmMuJ8e85xUeDnFxEB9v8jx8uHnY7WYbt28333lZmTmOKiubzw8waXuPp+Bgk4fAQDjrLLjgAp+KpEPm11lSG1sKCzoYU1gAPKG1/qbx+RfAb7XWK9pZ9hbgFoCBAweOb1u4+jS75759sGsXtWkBqJAogoNTD2WTOjV69Gji4uJYvHgxAKWlpZx//vlUV1eTlZXF8uXL+fTTT0lNTSUsLIzq6upWn3c4HFx44YVkZ2czbNgwKioquP/+R5g06VQWLvyU3//+PtxuD7GxCbz22v+or6/m4Yd/ybp1K1HKyk03Pcxpp/2sKb22X21oqCnA7XZzAlRUtF6mrGwzM2eOIDzcLLNpkyl8wsNNgVlfb05cb6Hf0HB4+ysiwhRmHo8pnLU2j4AAc0KVlrZeR0qKKezs9uaC8KuvzP8BAZ3nx2IxJ+XAgc2veTzm4XabbSoqMgWd995GaWlw5pmmQPvuO1MwePM9erTZJ96C2ek0J3Z5udk33gK9ttYU9C0LDKUgNdUUtjU1za9HRcH48abQ8RZC1dUm3cpKk0dvoRUQ0DrQhoebh81mCtX9+w++/wcONIVyeLgp3AoLzTHhLYDsdlOoJSSYQnL1atiy5cB04uLMdzNggElj9erW38Xxx5t9b7OZfVVXZ9LZtevAY9RuN4Fy+HBTGDqdZn+WlJhCNDfXfGf9+pn1RUebfetwmH1WUWG+g9pa6N/fbGP//mYZ7/ezfbv5u+U6s7JMet5KTV2d2fdVVSYPISHmERBg1lFWZvZxYmJzsLI33rdLa/Neaan5jrOzW3/P3uOxf3+z77wBJiDAvO7xmPWWl5t1eStYTifccQc89NDBv9v2+DpLak8GhReAJVrrNxufbwVOPVj30SFPnV1RATt2UJtqQ4WGExw8qCub0q3a7vL6epO9ysrmAkmp5kKrpYAA89C6uVAICDAHpN1uTjovpZqXd7nMgexN32ptPiAbGsyBt337Zl55ZQRVVeaEGD0azj7b1PxstgO3w+Mxy5WVmUdVVXM+goJMwWKzmYf3gNfanJRr15radnw8nHSSqU273abW9s03poBOSDAnXUoKTJoEjfcWasXhgKVL4csvTeGWmmoeVmtzoRoXZ2rNLW6G1+l3U1AACxbA/PmweLEp1E45xTwmTDD56eRagQPU1cH335vCesQIGDfOFAJam1rsjh1m2wYP7lq6HfF4YPdus39ra5uDbUREcyHfv78pdLuqrMwU+iEhJo2kpObC0Ku21rR66utNYdvRXWBra01N3mIxx0hQkEmv5THcljcIBhxGH4fWkJdnWmsxMebYaNuK7k7e9W3ZYo7X4483x2iLGy0eEUdDUDgPuB1z9dEkYLbWeuLB0jzkoLB/P2zbRt1xgRAWSnDwYF8347A5HM21l/p682hb2ENzkxXMgaRUc7PYbm8uWA+Ht6YfEXHgySf3UxDi2OVrUPDnJalvAqcCcUqpXOBhwAagtZ4DLMQEhB2YS1Jn+isvQFNpqjwKj5+nuXC7m5v/5eXNTUdvkzwurnVNx2Yzfb/+rK14eccFhBCiPf68+uiKg7yvgV924/o6vf6/qYqtm/7pNlqbGrh3wMjbRQOm9p+cbJqpR7q52BUynXjvUFJTgsvjIiksCYvqY78tFXi0h1pXLWGBPTcNTk9efdRt7HY7ZWVlxMbGdhwYWrQUuiMouN3NV3x4B4PA9Gv37988MHUkav+HS2tNWVkZ9radw72E1pr8qnxWFaxiT+UeUqNSGRY7jLToNAIsvh/C3sDXaeWhA1WOKnL355JflU9BdQGF1YVorYkOjibaHk2UPYqIoAgigiJICksi0h7ZYR5KakvYuW8n9gA7UfYo7AF2Fu1cxOvrXufL3V+i0QRaA0mJSGFI7BAyEjPISMxgaOzQpvVpNBuLN7K+eD2ltaVcM+Ya0qLTDlhfjbOGD7Z8wFsb3sJqsTIqfhSjE0dT46zh+7zvWZ67HKfbyamppzItbRqDogexvng9qwpWsbtiN4mhiSSHJ5MSmUJ6fDoj40d2uG3t7bP1xetZW7iWKmcVk5InMSF5AiG2EPbV7WNl/ko2lWzCarFiD7BjVVZ27NvB+uL1bC7dTExwDCPiRpAen86Zg85kbL+xrdIvqCpgV/kuhsUNIy4kDq013+d9zzsb32FF/gouG3kZMzNnEhrY/mBSQVUBm0s3k7s/l9z9ubjcLiKCIggPCic4ILjpOIm2RzNt0DQCrc0ns6PBwRe7v+Cbvd/wbc63rMhfQYAlgNjgWOJC4jgx5USuy7iOzKTMVseb2+Om0lFJeV05+VX5bC7dzKaSTWwt28qu8l1kV2TjdDu5+4S7+cO0P2CzNg/maa1xa3eXjvlDcUzco9nlcpGbm9v5Nf1uN+Tm0hBpwxNiITCw4x+QdcZ7FYO3S8jb7+8NAp0NkvUG9Q31VDmqzBMFCkWAJQC3xU1gdCBj+o/BHtAcHLTW7Kncg8vtIjEskfDAcDzaQ2F1IXsq9xBiCyEjMaPLBW12RTYvrHiBivoKYkPMiRRiC2l6v7yunB37drCjfAebSjZRXFN8QBr2ADsXj7iYW8ffykkDTyK/Kp/X1r7GvA3zqKyvJDQwlFBbKE63k9LaUsrqyrBZbAyNHcqwuGFkJmZy4fALGRY3rN08FlQVcOm7l7K+eD37HT5cztNCenw6U1KmMDxuOMU1xeTuz2VP5R42l2ymrK6s3c8Mjh7MVaOvIiksib2Ve83ypZvZWLwRl8fV7me8rMrKNRnX8JsTfkOVo4pVBatYlruMj7Z+RLWzmtSoVEJsIWwt3YpbuwFT2E0aMAmrsrJ0z1KqnFVN6YXaQhkcM5iSmhITAFtUpJLDk7EoCzWuGmpdtbg97qb3lFJYlAWrslLjanPJDRBgCaBfWD9y9ucc8J73/WGxwxgRP4LyunI2lWyioNpce3JiyoncPuF2wgLDeGn1S3yy7ZOmbYkPicdmtZFflU+gNZDB0YObAsvPs37O1IFTGRI7hH5h/fh0x6e8tOolPtvxWavt6kxcSBzXjLmGU1NPZcG2Bby76V0q6isIsAQwNmksk5InYVEWyurKKKgu4Ju93+B0OxmdMJohsUPM91mxh5LakgPSDrWFMixuGIOiBzEoahCFNYW8tvY1Thp4Em9d/BZKKd5Y9wavrXuNGzJv4P9O+D+f8txWrxho9of2goJPKiogOpr8e9LJvyyIrKyuzaixahU89hh8+KG5imXmTJg+3Vw5ExwMDZ4Gdu7bSc7+HLL6ZxFlj+p6HjvQ4Gngx7wfGR43nOjgQ//V8mc7PuOity8iMiiShNAEnG4n1c5q8qvym04Om8XG+P7jmdh/IrlVuXy791uKaoqa0rAH2HF73K0KqTGJY7hp7E1cNOIirMqKy+OiwdNAkDWIYFswgdZAapw17HfsJ78qnzkr5/DuxndRShFtj6asrqzdcZ6E0ASOjzmeobFDGZc0jnH9xpEWnUZ2RTbbyrbxQ94PzFs/j0pHJQMjB5K7PxeP9jB14FQGxwym2llNjbOGQGsgcSFxxIXEUd9Qz9ayrWwt3cruit0AjEoYxVWjr+LuE+9uqoV5tIdz3ziXpXuWctO4m0iJSCE5Ipn+4f3pF9aPpLAklFJU1FdQXldOpaOS/Y797HfsZ1f5Lr7L+Y5lucuoqK8g0BrYVNseHjuc9Ph0hsQOwel2UlFfwX7HfiYmT2RS8qR2g6vT7WRzyWayK7Ipry+nvK4cj/aQHp/O6MTRKBR/+u5PvLDyBeobmitG8SHxTB82nWszruWkgSdhURYcDQ62lm0lyBrE0NihTetr8DSwIn8FOZU5jEkcw5DYIU3dVy63i9z9uWws2ciG4g1sLt2MQhFqCyU0MLRVzdWjPXi0B7fHTZQ9isykTDKSMgi1hbI8dznf5XzH7ordjEkcw4T+ExiTOAalFPUN9TjdTpLDkwkKaN3PWlpbyuvrXue5H59jx74dACSGJjIzcyZTBk5he9l2NpVsotJRyflDz2f6sOlE2iP5du+3/Om7PzF/6/wDCv/+4f25IfMGTks7rem7DbIGUe2sZr9jP3UNzX3A28u28+raV/loy0e4PC5CbaFcNOIirhx1JaekntKqMuO1r24fb294m9fXv86+un0cF3kcx0UeR7/wfkTbo4kOjiYhNIERcSNIiUw5oKtw3vp53PLxLU3B16M9TB4wmbtPuJuL0y8+YH2+kKDQltMJQUEU/Woke67STJy40aePbdoEDz4I//mPuY78V7+CG26rZkvNt6wpXMPaorWsK1rHtrJtTQWlzWLjtLTTuGj4RcwYOaNVQb6uaB23L7yd2JBYnjv3OfqH9+90/Xsr93L1f67m671fY1EWJg+YzE8G/4TJAyaTkZhBYlgiWmsKqwvZsW8Heyv3NjWHI+2RnD/0fCYkT2DBtgVc+u6lpMen879r/kdcSFzTOuob6tldvpstpVvMiZv7HT/m/UhyRDInppzICQNOICwwjOKaYoqqiwiwBHBclDnI91Tu4Z+r/8mKfN+/k/DAcG4dfyu/nvxrBkQMwKM9VNRXtCrQwgLDiAiKOGhata5a3t7wNu9vfp+xSWO5PvN6Bsf4dmVZ7v5c/rP5P7y36T2+3vs1M0bO4PWfvU6AJYDZ38/m15/9mufPfZ6fT/i5z9vWkkd7KK8rJyY45pC6rLqqqLqI9za9R0pkCuP6jSM5PPmIrPdI8WgPX+z6AqfbyVmDz2rVtdKZ4ppitpZuZce+HWRXZDMxeSI/Of4nXe6GKa0tZVXBKqakTOmwS6o7bSndwoOLH2RY7DCuGXNNhy1aX0lQaI/NRskNw9g5s57Jk3d0umhNDdx1F8x9URMaVcvNvyoj47zlLNj9Dp9s/6SpAEuJSGFM4hhGJYwiPT6dpLAkPt/1OR9s+YAd+3YQagvlhrE3cFvWbcxbP48nv32SKHsUNc4aggKC+Ps5f+eq0Vfh8rjYU7GH8vpyouxRRNuj+Xrv19w0/yZcHhePn/44pbWlfLrj01YFcHxIPLWu2gOa6pFBkVQ7q3FrN4mhiZTVlTE2aSyLrl7kU2vjoAP3bawpXMM3e78hwBJAoDUQq7LicDuoc9XhcDsItYUSERRBpD2SU447xed+6SPl6e+e5p7/3cMl6Zdw30n3ccI/T+DMwWcy//L5x1TBKvouCQrtiYxk34UD2Hrbfk44of0+TYA1a+DSa8vYMep6rEM/x62aa7BJYUlcMuISLhh+AeP6jSMmuP1f5mitWV24mr99/zfeXP9mUyviuozr+PNZf6asrowSIos3AAAgAElEQVSZH83ku5zvSAxNpLimuN3+zfH9xvPWJW9xfMzxTa/tq9vH2sK1rC1ay/qi9YQHhTMkZgiDYwaTFpVGckQyYYFhlNeV8+mOT/lo60cEWgN57tznfKp991V/XfZX7vrvXQRaA4myR7H+5+tJCE3o6WwJ0S0kKLSnXz8qTolm4x2lTJly4MAlwAsvwB2PbsIzYzqWqBx+PuE2+of3JzYklqGxQ5mSMgWrpWujyflV+cxbP49x/cZxetrpTa+7PW6e//F5VhSsIDUylUHRg4gJjmm6OsEeYOe6zOtaXfUg/Gv297OZ9fks3rvsPc4dcm5PZ0eIbiNBoT2DB7N/VCBr785n6tTKA96ePx8uuOdjrJddRUx4CPOv/JDJAyYfZo7F0cbldvncXy3E0cLXoNC3fh0TEoLF4cHjcRzw1tfr9nDJO5fAldMZnXw8K2/9UQJCHyUBQfRlfS8o1LvR2tH0QyatNb/78nFOeW84rtRP+U3m71l203ekRHbv/RaEEOJo0PeCQp35sYvWTgD++M0feeTrB9BbzuefY7fw9AX3t/rxlhBC9CXHxDQXPgsJwVJmJnr3eBx8vO0z7v/yflh3Jb8Z9Do3XCyXHgoh+ra+FRRCQ1H1JiisL1rD1R9cTaI7i9KFL/HbvRIQhBCiz3UfqToXdW742bvXEGYLw/3Gh5x/djDx8T2dOSGE6Hl9MCg42bIfsiv3ckPiPyjdlcxM/97JQQghjhp9MigUNl6RuuKTMSQkwDnn9Gy2hBCit+iTQaGgDizKwhcfpHDNNe3ff1gIIfqivhcUtKaoFiJ0Em6nTbqOhBCihT4XFAAK68BZksaECTByZA/nSQghepG+FRRCzRzoubUB1OYdz3XX9XB+hBCil/FrUFBKna2U2qqU2qGUmtXO+wOVUouVUquVUuuUUv6dljIkBKcVyj0NUJFKZqZf1yaEEEcdvwUFpZQVeA44B0gHrlBKpbdZ7AHgHa31WOBy4Hl/5QeAkBByIkAroCKVxES/rk0IIY46/mwpTAR2aK13aTPR0FvABW2W0YD3ri+RQL4f8wMhIez23nSsPI0EuX+KEEK04s9pLpKBlrc3ywUmtVnmEeC/Sqk7gFDgDD/mB0JCyI4yfwbVDSA83K9rE0KIo05PDzRfAbyqtR4AnAv8Wyl1QJ6UUrcopVYopVaUlJQc+toag4LyWIgLikZuvSuEEK35MyjkAS1vSjCg8bWWbgTeAdBaLwPsQFzbhLTWc7XWWVrrrPjDmaQoJITdURBYk0B8rPPQ0xFCiGOUP4PCj8AQpVSaUioQM5A8v80ye4FpAEqpEZigcBhNgYNobClYK1OIi6vy22qEEOJo5begoLVuAG4HFgGbMVcZbVRKPaqUmt642G+Am5VSa4E3geu1P28aHRpKdhR49g0iNlaCghBCtOXX+ylorRcCC9u89lCLvzcBU/yZh5YcgVbyI0CVjSAmvfJIrVYIIY4aPT3QfETtqS8EQJcPIja2oodzI4QQvU+fCgrZlXvMHxWpxMaW92xmhBCiF+pbQaEi2/xRkUZMTFmP5kUIIXqjPhUUdpfvxuq2QFU/CQpCCNGOPhUUsiuzia6OAm0lJqa4p7MjhBC9Tt8KChXZhFUlEaBchIbKmIIQQrTlU1BQSv1aKRWhjH8qpVYppc7yd+a62+7y3QTuH0B8QAlK1fd0doQQotfxtaVwg9Z6P3AWEA1cAzzht1z5QZ2rjqKaItifRoK1GI/H0dNZEkKIXsfXoOCdOu5c4N9a640tXjsq7Gm8HNVVPpQEiwQFIYRoj69BYaVS6r+YoLBIKRUOePyXre63u3w3ADVlw0mkSIKCEEK0w9egcCMwC5igta4FbMBMv+XKD8ICw/jpkJ9SUTyGBE8xWktQEEKItnyd++gEYI3WukYpdTUwDvib/7LV/aYeN5UxUVOJ2g9JgYXSUhBCiHb42lL4B1CrlMrAzGy6E3jNb7nyk6Ii83+SqxCPW64+EkKItnwNCg2NU1pfADyrtX4OOOpuZtkUFHQh2ilBQQgh2vK1+6hKKXUv5lLUqY23zLT5L1v+4Q0KiRRRXSvdR0II0ZavLYUZgAPze4VCzK01/+S3XPlJy6BgqZegIIQQbfkUFBoDwRtApFLqp0C91vqoG1MoLgalNLGUoWod+PMmb0IIcTTydZqLy4AfgEuBy4DvlVKX+DNj/lBUBHHhDgJwY6kHc8dQIYQQXr6OKdyP+Y1CMYBSKh74HHjPXxnzh6IiSIxxwX6wOsDjcWCxHHVDI0II4Te+jilYvAGhUVkXPttrFBVBYqxpHZiWgowrCCFES762FD5TSi0C3mx8PgNY6J8s+U9REUweYmbn8LYUhBBCNPN1oPkeYC4wpvExV2v924N9Til1tlJqq1Jqh1JqVgfLXKaU2qSU2qiUmteVzHdVcTEkJpq/LfUSFIQQoi1fWwpord8H3vd1eaWUFXgOOBPIBX5USs3XWm9qscwQ4F5gita6XCmV4HPOu6i2FqqrISHJxEGrBAUhhDhAp0FBKVUFtHfdpgK01jqik49PBHZorXc1pvUW5hfRm1osczPwnNa6HJOg3+6R2fQbhWQrYLqPZExBCCFa6zQoaK0PZyqLZCCnxfNcYFKbZYYCKKW+BazAI1rrz9ompJS6BbgFYODAgYeUmaagMCAQkO4jIYRoT09fQRQADAFOBa4AXlRKRbVdSGs9V2udpbXOio+PP6QVNQWFFBMUZKBZCCEO5M+gkAektHg+oPG1lnKB+Vprl9Z6N7ANEyS63YABcOutMDDVgrYHSktBCCHa4c+g8CMwRCmVppQKBC4H5rdZ5kNMKwGlVBymO2mXPzIzfjzMmQPx8aCD7Y0tBZkpVQghWvJbUNBmDonbgUXAZuAdrfVGpdSjSqnpjYstAsqUUpuAxcA9Wusyf+WpSUgIlnpwuUr9viohhDia+HxJ6qHQWi+kzY/ctNYPtfhbA3c1Po4YFRqOtb6QWkfOwRcWQog+pKcHmnuECg0jwBmIQ4KCEEK00ieDAiEhBDiDJCgIIUQbfTgoBOBw5PZ0ToQQolfps0HB6rBQXy8tBSGEaKnPBgWLA9zuShoaqno6N0II0Wv03aBQb6bQlnEFIYRo1neDQq0LQMYVhBCihb4ZFEJDoc5McSEtBSGEaNY3g0JICMrlQjUgg81CCNFCnw0KAHadKC0FIYRooW8HBU8/GVMQQogW+nhQkJaCEEK01MeDQhwORw5mXj4hhBB9Oyi4Y3C7q2loqOzhDAkhRO/Qp4NCYIO586eMKwghhNE3g0JoKACBzjBAfqsghBBefTMoDB4MQODOCkCCghBCePXNoBAbC4MHE7ByC2CRoCCEEI36ZlAAmDgR9cMPBAX1lzEFIYRo1HeDwqRJkJdHWGWCTHUhhBCN/BoUlFJnK6W2KqV2KKVmdbLcxUoprZTK8md+Wpk0CYDIrXbpPhJCiEZ+CwpKKSvwHHAOkA5coZRKb2e5cODXwPf+yku7MjPBZiN8k0t+wCaEEI382VKYCOzQWu/SWjuBt4AL2lnuMeBJoN6PeTmQ3Q4ZGQSvL8fjqaOhofyIrl4IIXojfwaFZKBlv0xu42tNlFLjgBSt9Sd+zEfHJk0icF0uuOWyVCGEgB4caFZKWYC/AL/xYdlblFIrlFIrSkpKui8TkyZhqakndK/cV0EIIcC/QSEPSGnxfEDja17hwChgiVIqG5gMzG9vsFlrPVdrnaW1zoqPj+++HDYONodvAodjb/elK4QQRyl/BoUfgSFKqTSlVCBwOTDf+6bWulJrHae1TtVapwLLgela6xV+zFNrxx+PjooiclsQVVUrj9hqhRCit/JbUNBaNwC3A4uAzcA7WuuNSqlHlVLT/bXeLrFYUBMnErU1iP37v+vp3AghRI8L8GfiWuuFwMI2rz3UwbKn+jMvHZo0Cfvn/6O+bAsuVxk2W2yPZEMIIXqDvvuLZq9Jk1AeTfg2qKyU1kKfV1kJd98NNTU9nRMheoQEhYkT0UoR+4OFyspvezo3oqfNnw9//jN8+WVP50SIHiFBIT4edfHFJH+kqMlb2tO5ET1tzRrz/9atPZsPIXqIBAWABx7AWuMm4tUf8XicPZ0b0ZNWrzb/b9vWs/kQoodIUADIyMBxdhbJ7zZQlfd1T+dG9BStpaUg+jwJCo3Ug49hqwb97F96Oiuip+TkQHk5BAZKUBB9lgSFRoEnnk35CcGEvfi5XHnSV3m7js49F4qKzJVIQvQxEhRaqLzjZALKneg5c3o6K6InrFkDSsEll5jnMq4g+iAJCi3YTr6QigzQs/8KDQ09nR1xpK1ZA0OHwtix5rl0IYk+SIJCC5GRU8i9GCx78+Djj3s6O+JIW7PG3Hxp8GCwWKSlIPokCQothIamUzE1Emf/UJg9u6ezI46k8nLIzjZBISgI0tKkpSD6JAkKLShlJb7fDHKnO2DJEli3rqez1D2eew7OP7+nc9G7rV1r/vd2HQ0devQFhYYG+Mc/5EKJjmgNc+bAnj09nZNeTYJCG/363UT+OQ147Db4+997OjuHr74efvc7WLDA1IRF+7y/T8jMNP8PGwbbt4PH03N56qr58+EXvzCBQRzo/ffh5z+Hp57q6Zz47ssvzTl8BElQaCM8PIugfhmUnRMJr78OZWX+XeH338MTT8C775qCaft2+Ogj+MMf4OGHwe0+vPTffhu8d6uT+Xw6tmYNJCVBYqJ5PmwY1NZCXl7nn+tNvONgL71kasWiWV0d3HOP+fuTT46O/bN+PUybBo89dkRXK0GhDaUU/frdRPb5pSZC+7O1oDXMnAn33guXXWa6LoYOhQsvhPvvh0cfNQHicNL/298gPd0Udl980X1599WKFXDaaXDxxabF8tFHvbP2vXp1c9cRmO8Bjp7BZrfbtAZjYky317cyuWMrf/mLaSlfeaXpPtq8uetp/Pe/MGDAkWtxe8/9OXNMBeVI0VofVY/x48drf3M69+mvvrLrqmmpWoPWN96o9f79h56gx6P1736n9XfftX79++9N+n/7m9arV2v97rtav/KK1suXa11ernVqqtZTp3ae9rJlWpeUtP/e11+b9F94QesrrtA6Kcnk5UiZP1/rkBCt+/XTeuhQrZUy+Xn00SOXB1/U12sdEKD1vfc2v5aba/L63HM9l6+u+PZbk9+XXtI6PFzr667r6Rz5x1NPmXOpK3JzzXF4ySVa791r9tNTT3UtDY9H63HjzGd//WvfP/fEE1o/9JDWTmfX1qe11hMmaB0X13wOHyZghfahjO3xQr6rjyMRFLTWetOmq/XXX4Rr92/v1tpi0XrQoAMLdV89/7zZ1SNGaO12N79+221aBwdrXVHR/uf+/GfzuRUrDnzP6TQHJ2gdH6/1hx8euMwll2gdHa11TY0pLEDrjRsPbRu6wuHQ+tlnzX6bMEHrwkLzek2N1meeqXX//lq7XP7Ph69WrTL75u23m1/zeLQODdX6V7/quXx1xaxZJrCVl2t9yy2mEOzouDpaFRZqHRSktdWq9Z49vn/u6qvN53bvNs/HjNH61FO7tu4FC8wxMmCAOS727Tv4Z7ZsMecAaD1pUvP6fZGXZz73+9+bYDRixGFX6CQoHKby8q/04sXogoJXTY07LU1rm03rN97oWkI7dpiDKDnZ7O733jOv19ZqHRmp9VVXdfzZigqtw8K0vuaa1q/n52t90kkmvVtu0Toz0/w9c6Y5mDwec9JYrVr/v/9nPrNrl1nm73/vWv7beuUVrV999cDXP/xQ6yFDtI6IMOsBrc8/X+vq6tbLffSRee+DDw4vHy01NGj9z3+a/dJVZWVaX3yxydO2ba3fGztW67PP7p48+lt6utann27+/uEHsz1z5hzZPNTV+TfYP/SQaW1arVrffXfr90pKtP7Pfw4sOJcsMfvigQeaX/MGUF+Dpsej9cSJpuW+YoVJ749/PPjnrrnGVPrmzDHnRVSU1vfco/Xll5tj66STOu6BmDvXrGf9eq1fe838vWiRb/ntgASFw+TxePTy5UP1jz9mao/HbWpgp55qdtmTT/oWtRsaTPdPZKTW2dmm0MzMNJ99802T1uefd57GHXeYYFRQYJ5/843pjgkJ0XrePPOaw6H1/fc310piYswBbLGY9XqlpWl94YWHtkO0NicdmBNz8eLm17OzzUE/YoSpWT/6qNb//rfZ/rZcLhMg2xa2VVVar12r9ebNWu/cabp0fPXQQyZfGRkmHV94PKZlkJBgCoj2uiQuv9zss+6wf78JzP6wc6fZ/r/+1Tz3eExtOCvLP+vrKA/HHWf214cfdn83ZU2N1rGxWl9wgdYzZpjjzVugut1an3aaPqC7r7ratPAHD25dOfF2q77zjm/r/uwz3aoL56yzzDnY2TG6bZs5/+66yzzfudMEFqXMPjrzTPP+1Ve3//mf/tQs5/GY9SQlHXYFRYJCNygsfL2xtfCaeaG+3hQUoPXPf37wgsvb/fOvf5nnr7xini9YYA6sgQNbdye1Z9s2cyA9+KAZewgI0Pr447Vet+7AZdetM8vcequphcya1fr9G280tZX2CuuDWb/etFomTjTBLTlZ69JSk9Ypp5j3du70La1HHjHb5C0kCwpMgeJtYYAZg6isPHhan3xilj/5ZHOSXXjhwffpvn2maw1MwblmTfvLeWumdXW+bVdVlWmx3HKL1tdfr/WVV2p97rmtt629br6OFBZqnZNz8OWeecakvWNH82uzZ5vXVq/2fX2Havt2060SE2NaLKD1T36i9cqV3RccvF2wX3/deiyu5XupqaYC9f335nVv9+qSJa3TcrlMt+r117e/ri+/1Prxx02rwOPR+sQTtU5JaT7f//tfk+4rr3Sc3+uv19pub67MaW3Scjian//ud63LB6+aGvPZll2Xjz5qlt20qeN1HkSvCArA2cBWYAcwq5337wI2AeuAL4DjDpbmkQwKHo9br1iRpb/7boBuaKgxL7rdpksGtB492tRu2/Pxx6Yfc/r05hPD6TQFRHp6c0Hvi/PPN01mb5dMefmhbdC8eSaNH3/sfLkFC7QeNcqc2PPmmYG6QYNMbSUvz5zsNpuptT35pEnz5Zd9z0dOjinA773XnAATJ5qWz8svmxbUM8+Y92+4ofN0du82J3dGhknHWzi2DYYtffONCcYBAWYQsLPujjfeMOlt2NB5PrZtM4EgLEw3tdQGDjTBOzPTDPI//ripvScmmmDa0o8/HtjCqagwx4rVaioiK1ea1x0Os74NG5qPq9NPN8dUS2VlJj/TpnWtYD5YQNXaBP+tW0133bp1ZowoLs4EV6fTfA/ebsTkZFMZef11s52HMs7R0GAqIhMnNm/LlCmmJr19u+mePesss18HDjSPjz4y59jtt7ef5uWXm1Zi2+31joV5g3hSkj6gBeJtiY0c2f6+3bnTfG933nnw7Tr5ZJP/rVubX/d2sbbsRSgqMuXJL37ReZqd6PGgAFiBncAgIBBYC6S3WeY0IKTx758Dbx8s3SMZFLTWurx8iV68GJ2d/XjrNxYsMCd4YKDWf/hD64P9pZfMQZGVdeCVQd5aTduaXWe++cYUmo895ttJ25GCAt3U/dWeffvMVSug9fDh5uTy5jUwsPVA+1/+0vzeRRd1vUZ4wQXmpPzZz8zJ27YGfd99utOxh9parcePN11z27eb1zweUziDqYW1bMlVV5suNqvVBDhvbbIz3v5jb7dMe/btMwVfcLCpHX7zTcf7Ys0aE4yuvLI5v97a4pQpZpu8rrrK5PXmm83VRN4CynsFF5iC8oEHTJrtBcI5c8xyzz9/8G3dvFnrM84wBXzLLkcvh8MEyRNOaF6/9xEfb1qSLZWUmPPgkkvMd9Ry+ZQUs91FRQfPl9bmGGjb3fP+++a1/v3N/vEOPP/wgzlWwQSNjroT//3v1hUkt1vr3/ymueKVnW1aAhdeaLqN27YWvf38//zngWnfeKMpwPPyDr5tOTmmEpGR0TymdeONZp+1vWJp4UIT7A9RbwgKJwCLWjy/F7i3k+XHAt8eLN0jHRS01nrdugv00qVh2uEobP1GcbEpEME09y67zDT5wPT/tXdA1tWZA7mrVz8cSpdPe0aONLWqlmpqTFM8KckURA8+aApUt9s0pW+5xVwu25LHY1pBKSkdXxLbmYULmwuJP//5wPcdDjMYFxfXfPVSy/yefnr7wcTpNN+Dt9BcsMAUAN6B/muv9a1bSmvTijj1VLOejgZtr7nG7LODtb68vEHg3XdNNx8094dfdJH5nr0tOu84R0WF1n/6kwnYDz9suhteeMF8zhsk2rsyzuMxfdehoR137VVXm4Bis5muxYgIs99rapqX+eKL5hrzkCFaP/20CRD/+If57g7WbehymZbNBx+YS0HPPru5onHttWZfdHQM5eWZFnlaWutWXUODeQ3MoGxLc+aYIP3FFx3nqaTE7LuBA02r2FsBuv123841l8scg20rS889Z9I5WCuhpQULTH4tFnM8JSSYlkw36w1B4RLgpRbPrwGe7WT5Z4EHDpZuTwSFmpotevFiq9669bYD3/R4zO8Kbr+9+Zria6/t/LrkPXtMQOkJd9xhDuSf/cx03zz4oKnpefvlvd0UvnC7W9duu8LbdP7NbzquWW/caGpcp5/e3IVTVWXGMCwWU1vryGefmXEJb+DJyjLX8ndVba0ZFwDT3dSSd+D94Yd9T8/pNIWutzC/916z/X/9q3l+9dWmljh5sm9X8uTkmD7ujuzdawr6k09u3cr0eEzwGTDArPe660zNfcECk7errjLLvPqqaYmkp5t9ejgt1ZY2bzZdId5WkPdCgSeeaG5B/O9/5tgMDTX5auu//zUtyvaOH1/GgR5+WOvzzjPBeMYMrV98sWst3tJSM4idmGi+B28vwPnntx478EVhoTkXgoNNGl29ytEHR1VQAK4GlgNBHbx/C7ACWDFw4MBu31m+2LbtDr14sdIVFcs6XsjpNE3AI/kDsa7atMk0iYcNMye7t1Xz9dc9nbP2zZ3bnM9Jk8zvHqzW5iuvOuNwmBr1vHmHV5g5HKbQANPKeucdUwjEx5sCvqs/TFq3zowXzJ7d+vW77zbrCAvzvWvRFy+/bNI96SRTeXnmGTN4CuYa+Lbf/eOPm/emTTP/n3GG/37z4HSamvbjjzdfZm2zmXUrZVq2mzf7Z93dYeNGE9hSUkzef/rTrl0511ZhoWnZ+uHS3t4QFHzqPgLOADYDCb6k2xMtBa21drkq9Xffpejvvx+h3e7D+NJ7E6ez51osXVFUZLop0tNNgeHrpYTdqaHBFFzek99iMS2utn3ph8PtNleZLFzYfWlqbSopDz5oxmC8g+EJCaY/vL2uEo+n+bcbN9xwaL/GPVSbN2v9f/9n9vONNx74O5feyNu6Ou+8wwsIfuZrUFBm2e6nlAoAtgHTgDzgR+BKrfXGFsuMBd4DztZab/cl3aysLL1ixQo/5Pjgyso+Y/36czjuuAdJS3u0R/LQp2kN1dUQHt5zeXC7zRxSb7wBp58O113Xc3k5FFpDYSFERUFwcMfL1dfDDz/A1KnmFqWic3v3Qv/+EBDQ0znpkFJqpdY666DL+SsoNGbiXOAZzJVIL2utH1dKPYqJWPOVUp8Do4GCxo/s1VpP7yzNngwKAJs3X0dx8TzGj19BWFhGj+VDCCG6olcEBX/o6aDgcpXxww/pBAX1JzPzKwICInosL0II4Stfg4JMnd1FNlssw4e/TE3NBtasOR2ns7SnsySEEN1GgsIhiI09j5EjP6C2diNr1pyMw3EU3YhFCCE6IUHhEMXF/ZQxYz7D4chl9eqpOBwFB/+QEEL0chIUDkNU1ClkZHyB01nMhg0X4HbX9XSWhBDisEhQOEwRERNIT59HVdUKtmyZydE2cC+EEC1JUOgGcXHTGTToCUpK3mbPniN7k20hhOhOvfeXFkeZlJR7qKnZRHb2w7jdVaSmPobVau/pbAkhRJdIUOgmSimGDXsBi8VOTs7TlJUtZMSI1wgPH9/TWRNCCJ9J91E3sliCGDZsDqNHf0pDQwUrV04iP39uT2dLCCF8JkHBD2Jjz2bChA3ExPyEbdtuZe/ep3s6S0II4RMJCn5is0UzatQHxMdfxq5d97B790NyZZIQoteToOBHFksg6enzSEq6kT17HmP9+vOprFze09kSQogOyUCznyllZdiwFwkJGcrevU+yevUJREWdRkLClQQHp2G3pxIUdBwWi3wVQoieJ7OkHkENDdUUFMwlJ+dpnM7maTGCg4cwatQHhIaO7MHcCSGOZTJ1di/m8TTgcORQX59NXd02du9+GI+nhuHD/018/IU9nT0hxDFIps7uxSyWAIKD04iOPo3+/W8lK2slISHpbNx4Edu3/5qysoUywZ4QokdIR3YvEBSUTGbmV2zffjt5ebPJy5sNQGBgMgkJl5OUdB1hYaO7ZV1aaxoaKrHZorolPSHEsUW6j3qZhoZKqqvXUl29hvLyL9i3byFaNxAWlkl8/CXExl5AaOhI1CHcN1drN5s3X0dJyXtkZi4hMnKyH7ZACNEbyZjCMcLpLKG4+E2KiuZRVfU9AEFBx2G1htDQsB+Pp4aIiBPo3/9WYmLO6/AqJq09bN16C4WF/8RqjSQgIILx41cRGBh3JDdHCNFDJCgcgxyOfMrKPqa8/HMArNZwlLJRVrYApzOfwMBkoqJOJiAgmoCAKIKCBhAWlkFo6Gh2776fvLy/c9xxDxIXdwGrVp1IVNSpjBmzEKWs1NbuoLDwn9TXZ+N0FtHQUE6/fjeTnPwLv21PdfVagoIGYLPFtnq9oaEal6sEuz3V5xZRbe3WxmDZfZMQlpV9xrZtNxMffylpaY9htYZ2W9pCHGkSFPoQj6eBsrIFFBS8RF3dVhoaKnC5ygF3q+UGDLiLwYOfRilFfv5ctm27lf79f4HLVUpJyXsoZSUoaCCBgRoGhL0AAA/iSURBVIl4PA6qq1eSnHw7gwf/tdPfUZhl11Bfv4egoOTGNPp1+Jmamk3s2nUvZWXzsdkSGT78VWJjzwZMQbx160yczkKCggYSHX0mcXEXEht7XocBIjd3Njt2/BqbLZ7+/X9BcvLPCQxMxO2ux+UqxmZL6HKwyM19lh07fk1QUH8cjlzs9lSGDn2BmJizupSOV319DsXFb5KUdD2BgQmHlEZXORwFNDRUoJS18bsdgMUSdETW7XQW4fHUY7cfd0TWdyhcrgqs1hAslsCezsoR0SuCglLqbOBvgBV4SWv9RJv3g4DXgPFAGTBDa53dWZoSFHyjtcbhyKW6eg3V1Wuw2eLo3/+2poJVa82WLddTVPQaVmsEycm/YMCAOwkMTGx8383Onf+P3Ny/EBNzDikpv8HhyMXhyMXlKsPtrsbtrqG+fjdVVavQ2tEmBxZsthhstnhstlis1gis1lA8HidlZR9jtYYxYMCvKC39kJqaDQwYcCcej5P8/OcJCRlJv343UVm5lPLyL3G7K4mJOY+hQ+dgtw9otZa9e59k165ZxMSci1IWysoWoFQgVmsoDQ3lAAQGJpGa+hj9+s1EKWun+83pLGbPnsfIy3uW2NjzGTFiHtXVq9m69Sbq6rYRHz+DQYOeIDg41efvoajodbZvvwO3u5KAgBiOP/4vJCZei9u9n4KCVygufpPIyCkcd9xD3XIBgNNZwu7dD1BQ8CLQfH4rFURExAQiI08iNvaCLo8pmW15jV277iMoqD/R0WcRE3MWEREnYrHYmpYrKXmfrVtvwu2uY/DgP5GcfPsBAV1rN0VFr1NQ8ApBQf0JDR1JSMhIYmLOwmoN6TAP9fU5aO0iOHhQl/LekttdS07O0+zd+wR2eyojRrxOePi4Lqfj8TT49KNTj8dJScm75OU9R0NDBUlJMxsrB/FdXqfW+pDGE6EXBAVlzr5twJlALvAjcIXWelOLZX4BjNFa36aUuhy4SGs9o7N0JSh0H7e7jrKy+URH/6TDwig//wW2bfslLVsdVmsYVms4VmsogYFJRERMJiJiMsHBx+NwFOBw7G0MHiW4XKW4XKWNQaQaj6eeuLgLGTjwfgID43C769i16/+Rl/csoBgw4P9IS3u8qWbv8TSQl/csu3ffh1I2UlN/R0jIMCwWO+Xl/2Pv3j+SkHAFw4f/C4vFRm3tNvLz5+Lx1BMYmITNFkdR0Wvs37+M0NDR9Ot3M0oFABqtPYAbrd04nUWUl/+P6urVgLdV9VRTEHG769m79wlycp5Caw8DBtxJQsLlWK3BWCx2HI4CqqpWUFW1ApermKCgFOz246iqWklp6X+IiJhCauqD/7+9ew+Oq7oPOP797VMrafVYSzLWC8vINnFJcGpKME4TCqQ1aVoyCSQ05DFMKG0GGuiUgZA+Q4bJdKYJSds0BPIiCaVJCAQngTxwgEAmgB1MXGyiRH4iW5L1XK1W2tU+fv3jHm0lWbKFyVrK7u8z47HuvWeuztFvd3/3nLv3HA4e/DhjYz+junoTk5Nd5HLjVFWdQzK5h2BwBR0dd1BX92ZSqcOk04eZnOxmYqKLiYku962xBkKhRvz+KLncBPl8EtUsFRWriUTWAj56ej5NPp+kufl6ams3o5pDNUMy+SLx+NMkEjtRzVJTs5m2tpupqnodw8PfZ3BwG8nkHvz+agKBKMFgI/X1lxCLvZVQqImurr9iaOhhotE3IBJgbOwZIEcw2EhT03toarqKvr4v09t7N9HoHxAMNjA8/Cix2GWsW3cXgUAdqnkSiR3s338L4+MvEImsJ59PkU4fAiAQiNHc/CFaWm4gHD6j8LcfGvouvb33uKFTJRo9n5Urr6au7iJyuXGy2VHS6aPuImgX6XQP9fUX09h4JfX1l6KaZXKym0RiJwcPfox0+jANDW9nbGwHmUw/q1ffTnv7Lahm3cXOIRKJHSQSO0injxCJnEUksp5gMMbY2M8ZHX2CZHIPdXVvZtWqv6Sh4R2zeqNTUwPE4z8jHv8p/f3/TSbTTySyjlCoiXj8aURCrFjxZ9TX/xG1tW+kquqcWRcs6XQfyeRuxsd3Mzn5GyYnu5mc7Ka5+TrOPPPvT+n9vhySwmbgX1T1T9z2bQCq+okZZX7oyvxcvHdqH9CoJ6iUJYXTL5ncy9RUP+FwG+FwC35/5Lf+O0ZGHsfnC1Nbe+G8xycn99HVdS2jo0/M2n/GGdewfv09J+wBqCoDAw+wf/+tpFIH5i0jEqCmZgux2B8Ti21d8MoxlerhwIGP0t//tXmPB4Mr3ZDTy2Qyg4gE6ej4OG1tNyPiRzXP0aN309PzSWpqLqS19cNEo5tIJHbR3X0j8fhTx9UrEumksvJsAoGYS7LHyGYT+P2V7j6Hj1TqAKnUISBPLLaVs866k6qqs+etYzY7Tl/fV+jp+dSsv0dl5WuoqdlMPp8il0uQSh0imdztjvoRCbBmzR20tt6EiJ9sNs7IyGP099/P0NB3UZ0CoK3tVjo6bkckyNGj/8W+fTeTz6dm1aGiYjUdHZ+gqeldiPjIZsdJJJ7jyJH/YHDwYUQC+P015HLjhV5oONzOqlUfxO+vor//64yPv3Bc2/z+aqqrNxIMrmRk5DFyuTg+X8Ws319dvZHOzs9QV/cmMplhfv3rDzEw8E1AmNmzAggE6gmH20ml9pHLjQPg81W5D/INDA4+TCq1H7+/llCokXw+Qz6fIpPpd/ELU19/Ka2tf0N9/VsQ8ZFM7uXo0bsYGHiQqakjhXI+XwU+XxDVXKGX672mGohEOolEOmloeOcpP+C6HJLCFcBWVb3Wbb8PeIOq3jCjzIuuTI/b3ufKDM4513XAdQDt7e2bDh06VJQ6m+VNNc/ExEuFHodIkJqaCxBZ3DOY+XyWTGYAENcF9yESQMTv3pCLH1tOJvcwMdFFPp8in08TCNQTjZ5HONxS6N7nct5VfCBQu8j2KcPDj5LJDFNR0U443E443DJraObE7ZsikxkkFFq1qCEG1RyDg98hne4lFttKZWXncWXS6SMMDT1KMvlLmpv/esGpWDKZEYaGthGJdFJbu2XWsWTyVwwPP+K2vGHFpqZ3L3h/Y2Kim97ee8jlEq5HGiUaPY9Y7C2zkv/4+ItMTOxx36arIxRqpKKio/B6yOfTjIxsZ2TkxwQCK6isXEskspbq6nNnnUdVGRx8iETiefz+Kvz+SkKhM4hGz6OiYg0igqoyNdVHJnOMysoNhZio5hkdfYJjx+4nl0siEsLnCxKJrKO2dgvR6KYF26mqpFKHiMefIpncTT4/hWoWgMrKdVRVnUt19WuP+yLGqSqppDCT9RSMMeaVWw7TXBwB2mZst7p985Zxw0e1eDecjTHGLIFiJoUdwFoR6RCREHAVsG1OmW3AB9zPVwA/OdH9BGOMMcVVtLmPVDUrIjcAP8T7SuqXVHWPiNwO7FTVbcAXga+JSDcwjJc4jDHGLJGiToinqo8Aj8zZ908zfk4BVxazDsYYYxbPps42xhhTYEnBGGNMgSUFY4wxBZYUjDHGFPzOzZIqIgPAqT7S3AAs+GBcibO2l6dybXu5thsWbvuZqnrSWfh+55LCqyEiOxfzRF8psrZb28tJubYbXn3bbfjIGGNMgSUFY4wxBeWWFO5e6gosIWt7eSrXtpdru+FVtr2s7ikYY4w5sXLrKRhjjDmBskkKIrJVRLpEpFtEPrLU9SkmEWkTkcdFZK+I7BGRG93+mIj8WER+4/6vX+q6FoOI+EVkl4h8z213iMizLvbfcLP2lhwRqRORB0TkVyLykohsLqOY/617rb8oIveLSEWpxl1EviQix9x6NNP75o2zeP7d/Q12i8hJF6Mui6Tg1ov+LHAZsAH4CxHZsLS1Kqos8HequgG4ALjetfcjwHZVXQtsd9ul6EbgpRnb/wrcqaqdwAjwwSWpVfF9BviBqp4NnIv3Nyj5mItIC/Bh4DxVPQdvVuarKN24fwXYOmffQnG+DFjr/l0HfO5kJy+LpACcD3Sr6n71FpL9H+DyJa5T0ahqr6o+735O4H04tOC1+V5X7F7g1BZ7XcZEpBX4U+ALbluAi4EHXJFSbXct8Ca86ehR1SlVHaUMYu4EgIhbrKsS6KVE466qP8VbamCmheJ8OfBV9TwD1InIqhOdv1ySQgvw8oztHrev5InIauD1wLPASlXtdYf6gJVLVK1i+jRwC5B32yuAUZ1e/LZ0Y98BDABfdkNnXxCRKsog5qp6BPg34DBeMogDv6A84j5toTi/4s++ckkKZUlEqoFvAzep6tjMY26Fu5L66pmIvA04pqq/WOq6LIEA8PvA51T19UCSOUNFpRhzADd+fjleYmwGqjh+eKVsvNo4l0tSWMx60SVFRIJ4CeE+VX3Q7e6f7jq6/48tVf2KZAvw5yJyEG+I8GK8cfY6N6wApRv7HqBHVZ912w/gJYlSjznApcABVR1Q1QzwIN5roRziPm2hOL/iz75ySQqLWS+6ZLhx9C8CL6nqp2Ycmrkm9geAh0933YpJVW9T1VZVXY0X45+o6tXA43hrgEMJthtAVfuAl0Vkvdt1CbCXEo+5cxi4QEQq3Wt/uu0lH/cZForzNuD97ltIFwDxGcNM8yqbh9dE5K14483T60XfscRVKhoReSPwFPC//P/Y+kfx7it8E2jHm2n2Xao694ZVSRCRi4CbVfVtIrIGr+cQA3YB71XV9FLWrxhEZCPeDfYQsB+4Bu/Cr+RjLiIfA96N9827XcC1eGPnJRd3EbkfuAhvNtR+4J+B7zBPnF2S/E+84bQJ4BpV3XnC85dLUjDGGHNy5TJ8ZIwxZhEsKRhjjCmwpGCMMabAkoIxxpgCSwrGGGMKLCkYcxqJyEXTs7casxxZUjDGGFNgScGYeYjIe0XkORF5QUQ+79ZoGBeRO928/dtFpNGV3Sgiz7j56h+aMZd9p4g8JiK/FJHnReQsd/rqGese3OceMDJmWbCkYMwcIvIavKdjt6jqRiAHXI030dpOVf094Em8J0kBvgrcqqqvw3uKfHr/fcBnVfVc4EK8GTzBm7X2Jry1PdbgzdNjzLIQOHkRY8rOJcAmYIe7iI/gTTCWB77hynwdeNCtY1Cnqk+6/fcC3xKRKNCiqg8BqGoKwJ3vOVXtcdsvAKuBp4vfLGNOzpKCMccT4F5VvW3WTpF/nFPuVOeImTn/Tg57H5plxIaPjDneduAKEWmCwvq3Z+K9X6Zn3XwP8LSqxoEREflDt/99wJNuxbseEXm7O0dYRCpPayuMOQV2hWLMHKq6V0T+AfiRiPiADHA93sI157tjx/DuO4A3VfFd7kN/enZS8BLE50XkdneOK09jM4w5JTZLqjGLJCLjqlq91PUwpphs+MgYY0yB9RSMMcYUWE/BGGNMgSUFY4wxBZYUjDHGFFhSMMYYU2BJwRhjTIElBWOMMQX/B/JJ1qNYJFJrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 776us/sample - loss: 0.2438 - acc: 0.9425\n",
      "Loss: 0.2438399968372884 Accuracy: 0.94247144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 492928)       1971712     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 9,900,816\n",
      "Trainable params: 8,914,576\n",
      "Non-trainable params: 986,240\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 844us/sample - loss: 5.5172 - acc: 0.6268\n",
      "Loss: 5.517237427078675 Accuracy: 0.6267913\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 164224)       656896      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,347,536\n",
      "Trainable params: 3,018,576\n",
      "Non-trainable params: 328,960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 801us/sample - loss: 2.3505 - acc: 0.4444\n",
      "Loss: 2.350521321311547 Accuracy: 0.44444445\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 58816)        235264      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,280,976\n",
      "Trainable params: 1,162,576\n",
      "Non-trainable params: 118,400\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 817us/sample - loss: 1.2836 - acc: 0.6746\n",
      "Loss: 1.2835506092350801 Accuracy: 0.6745587\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 23616)        94464       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 659,536\n",
      "Trainable params: 611,280\n",
      "Non-trainable params: 48,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 843us/sample - loss: 0.5836 - acc: 0.8459\n",
      "Loss: 0.5835922305457688 Accuracy: 0.8458982\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 11904)        47616       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 507,856\n",
      "Trainable params: 482,768\n",
      "Non-trainable params: 25,088\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 862us/sample - loss: 0.3127 - acc: 0.9161\n",
      "Loss: 0.31269655174741123 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 3840)         15360       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 429,136\n",
      "Trainable params: 419,920\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 928us/sample - loss: 0.2438 - acc: 0.9425\n",
      "Loss: 0.2438399968372884 Accuracy: 0.94247144\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 341312)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 492928)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 492928)       1971712     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           7886864     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 9,900,816\n",
      "Trainable params: 8,914,576\n",
      "Non-trainable params: 986,240\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 949us/sample - loss: 5.8865 - acc: 0.6100\n",
      "Loss: 5.886476277364253 Accuracy: 0.60996884\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 113728)       0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 164224)       0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 164224)       656896      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2627600     batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,347,536\n",
      "Trainable params: 3,018,576\n",
      "Non-trainable params: 328,960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 911us/sample - loss: 4.2760 - acc: 0.6066\n",
      "Loss: 4.276038798513442 Accuracy: 0.6066459\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 37888)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 58816)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 58816)        235264      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           941072      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,280,976\n",
      "Trainable params: 1,162,576\n",
      "Non-trainable params: 118,400\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 924us/sample - loss: 2.1342 - acc: 0.7061\n",
      "Loss: 2.1342451857629223 Accuracy: 0.7061267\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 12608)        0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 23616)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 23616)        94464       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           377872      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 659,536\n",
      "Trainable params: 611,280\n",
      "Non-trainable params: 48,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 935us/sample - loss: 0.8073 - acc: 0.8511\n",
      "Loss: 0.8073117482699338 Accuracy: 0.8510904\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 8320)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 11904)        0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 11904)        47616       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           190480      batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 507,856\n",
      "Trainable params: 482,768\n",
      "Non-trainable params: 25,088\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 980us/sample - loss: 0.4684 - acc: 0.9047\n",
      "Loss: 0.46841598327657513 Accuracy: 0.9046729\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 2688)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3840)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 3840)         15360       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           61456       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 429,136\n",
      "Trainable params: 419,920\n",
      "Non-trainable params: 9,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2766 - acc: 0.9439\n",
      "Loss: 0.27661314223955125 Accuracy: 0.94392526\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
