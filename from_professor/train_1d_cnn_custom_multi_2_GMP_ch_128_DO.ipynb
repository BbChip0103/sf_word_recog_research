{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4778 - acc: 0.1876\n",
      "Epoch 00001: val_loss improved from inf to 2.10802, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/001-2.1080.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 2.4778 - acc: 0.1876 - val_loss: 2.1080 - val_acc: 0.3452\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1103 - acc: 0.2949\n",
      "Epoch 00002: val_loss improved from 2.10802 to 1.82852, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/002-1.8285.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.1103 - acc: 0.2949 - val_loss: 1.8285 - val_acc: 0.4263\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9269 - acc: 0.3581\n",
      "Epoch 00003: val_loss improved from 1.82852 to 1.64222, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/003-1.6422.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.9270 - acc: 0.3581 - val_loss: 1.6422 - val_acc: 0.4913\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7808 - acc: 0.4038\n",
      "Epoch 00004: val_loss improved from 1.64222 to 1.49773, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/004-1.4977.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.7808 - acc: 0.4037 - val_loss: 1.4977 - val_acc: 0.5369\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6718 - acc: 0.4405\n",
      "Epoch 00005: val_loss improved from 1.49773 to 1.39027, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/005-1.3903.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6718 - acc: 0.4405 - val_loss: 1.3903 - val_acc: 0.5723\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5855 - acc: 0.4677\n",
      "Epoch 00006: val_loss improved from 1.39027 to 1.29427, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/006-1.2943.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5856 - acc: 0.4677 - val_loss: 1.2943 - val_acc: 0.5870\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5129 - acc: 0.4948\n",
      "Epoch 00007: val_loss improved from 1.29427 to 1.22904, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/007-1.2290.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5129 - acc: 0.4948 - val_loss: 1.2290 - val_acc: 0.6124\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4529 - acc: 0.5146\n",
      "Epoch 00008: val_loss improved from 1.22904 to 1.17411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/008-1.1741.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4530 - acc: 0.5145 - val_loss: 1.1741 - val_acc: 0.6243\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3957 - acc: 0.5338\n",
      "Epoch 00009: val_loss improved from 1.17411 to 1.11391, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/009-1.1139.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3959 - acc: 0.5338 - val_loss: 1.1139 - val_acc: 0.6413\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3525 - acc: 0.5521\n",
      "Epoch 00010: val_loss improved from 1.11391 to 1.06443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/010-1.0644.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3525 - acc: 0.5521 - val_loss: 1.0644 - val_acc: 0.6580\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3115 - acc: 0.5639\n",
      "Epoch 00011: val_loss improved from 1.06443 to 1.03054, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/011-1.0305.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3114 - acc: 0.5639 - val_loss: 1.0305 - val_acc: 0.6713\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2717 - acc: 0.5792\n",
      "Epoch 00012: val_loss improved from 1.03054 to 1.00178, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/012-1.0018.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2716 - acc: 0.5792 - val_loss: 1.0018 - val_acc: 0.6783\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2382 - acc: 0.5901\n",
      "Epoch 00013: val_loss improved from 1.00178 to 0.96304, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/013-0.9630.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2382 - acc: 0.5901 - val_loss: 0.9630 - val_acc: 0.6944\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2096 - acc: 0.6014\n",
      "Epoch 00014: val_loss improved from 0.96304 to 0.95050, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/014-0.9505.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2095 - acc: 0.6014 - val_loss: 0.9505 - val_acc: 0.6974\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1740 - acc: 0.6127\n",
      "Epoch 00015: val_loss improved from 0.95050 to 0.90683, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/015-0.9068.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1739 - acc: 0.6128 - val_loss: 0.9068 - val_acc: 0.7147\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1581 - acc: 0.6195\n",
      "Epoch 00016: val_loss improved from 0.90683 to 0.88923, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/016-0.8892.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1580 - acc: 0.6196 - val_loss: 0.8892 - val_acc: 0.7202\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1290 - acc: 0.6282\n",
      "Epoch 00017: val_loss improved from 0.88923 to 0.86806, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/017-0.8681.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1289 - acc: 0.6282 - val_loss: 0.8681 - val_acc: 0.7251\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1032 - acc: 0.6397\n",
      "Epoch 00018: val_loss improved from 0.86806 to 0.84793, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/018-0.8479.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1032 - acc: 0.6397 - val_loss: 0.8479 - val_acc: 0.7305\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0918 - acc: 0.6424\n",
      "Epoch 00019: val_loss improved from 0.84793 to 0.82636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/019-0.8264.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0919 - acc: 0.6424 - val_loss: 0.8264 - val_acc: 0.7438\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0691 - acc: 0.6506\n",
      "Epoch 00020: val_loss did not improve from 0.82636\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0691 - acc: 0.6506 - val_loss: 0.8286 - val_acc: 0.7414\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0533 - acc: 0.6561\n",
      "Epoch 00021: val_loss improved from 0.82636 to 0.79841, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/021-0.7984.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0533 - acc: 0.6560 - val_loss: 0.7984 - val_acc: 0.7508\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0389 - acc: 0.6632\n",
      "Epoch 00022: val_loss improved from 0.79841 to 0.79246, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/022-0.7925.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0389 - acc: 0.6632 - val_loss: 0.7925 - val_acc: 0.7496\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0193 - acc: 0.6693\n",
      "Epoch 00023: val_loss improved from 0.79246 to 0.77464, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/023-0.7746.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0192 - acc: 0.6694 - val_loss: 0.7746 - val_acc: 0.7610\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0099 - acc: 0.6732\n",
      "Epoch 00024: val_loss improved from 0.77464 to 0.76907, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/024-0.7691.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0099 - acc: 0.6732 - val_loss: 0.7691 - val_acc: 0.7664\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0052 - acc: 0.6727\n",
      "Epoch 00025: val_loss improved from 0.76907 to 0.75930, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/025-0.7593.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0052 - acc: 0.6727 - val_loss: 0.7593 - val_acc: 0.7608\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9820 - acc: 0.6832\n",
      "Epoch 00026: val_loss improved from 0.75930 to 0.74032, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/026-0.7403.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9820 - acc: 0.6832 - val_loss: 0.7403 - val_acc: 0.7629\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9744 - acc: 0.6868\n",
      "Epoch 00027: val_loss improved from 0.74032 to 0.73414, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/027-0.7341.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9744 - acc: 0.6869 - val_loss: 0.7341 - val_acc: 0.7743\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9701 - acc: 0.6890\n",
      "Epoch 00028: val_loss improved from 0.73414 to 0.72813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/028-0.7281.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9700 - acc: 0.6890 - val_loss: 0.7281 - val_acc: 0.7752\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.6946\n",
      "Epoch 00029: val_loss improved from 0.72813 to 0.71730, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/029-0.7173.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9558 - acc: 0.6947 - val_loss: 0.7173 - val_acc: 0.7785\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9481 - acc: 0.6962\n",
      "Epoch 00030: val_loss improved from 0.71730 to 0.71182, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/030-0.7118.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9481 - acc: 0.6962 - val_loss: 0.7118 - val_acc: 0.7785\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9404 - acc: 0.6973\n",
      "Epoch 00031: val_loss improved from 0.71182 to 0.70652, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/031-0.7065.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9404 - acc: 0.6973 - val_loss: 0.7065 - val_acc: 0.7850\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.6973\n",
      "Epoch 00032: val_loss improved from 0.70652 to 0.69974, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/032-0.6997.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9320 - acc: 0.6974 - val_loss: 0.6997 - val_acc: 0.7838\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9297 - acc: 0.7004\n",
      "Epoch 00033: val_loss improved from 0.69974 to 0.68950, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/033-0.6895.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9298 - acc: 0.7004 - val_loss: 0.6895 - val_acc: 0.7859\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9092 - acc: 0.7067\n",
      "Epoch 00034: val_loss did not improve from 0.68950\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9091 - acc: 0.7067 - val_loss: 0.6910 - val_acc: 0.7864\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9119 - acc: 0.7092\n",
      "Epoch 00035: val_loss improved from 0.68950 to 0.68581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/035-0.6858.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9118 - acc: 0.7092 - val_loss: 0.6858 - val_acc: 0.7864\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8966 - acc: 0.7088\n",
      "Epoch 00036: val_loss improved from 0.68581 to 0.66688, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/036-0.6669.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8966 - acc: 0.7087 - val_loss: 0.6669 - val_acc: 0.8008\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8965 - acc: 0.7140\n",
      "Epoch 00037: val_loss improved from 0.66688 to 0.66333, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/037-0.6633.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8965 - acc: 0.7140 - val_loss: 0.6633 - val_acc: 0.8008\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8921 - acc: 0.7140\n",
      "Epoch 00038: val_loss improved from 0.66333 to 0.65511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/038-0.6551.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8920 - acc: 0.7140 - val_loss: 0.6551 - val_acc: 0.8041\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7195\n",
      "Epoch 00039: val_loss did not improve from 0.65511\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8820 - acc: 0.7195 - val_loss: 0.6653 - val_acc: 0.7959\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8763 - acc: 0.7205\n",
      "Epoch 00040: val_loss improved from 0.65511 to 0.64757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/040-0.6476.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8764 - acc: 0.7205 - val_loss: 0.6476 - val_acc: 0.8055\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.7232\n",
      "Epoch 00041: val_loss improved from 0.64757 to 0.64238, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/041-0.6424.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8687 - acc: 0.7232 - val_loss: 0.6424 - val_acc: 0.8062\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8643 - acc: 0.7237\n",
      "Epoch 00042: val_loss improved from 0.64238 to 0.63853, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/042-0.6385.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8644 - acc: 0.7237 - val_loss: 0.6385 - val_acc: 0.8011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8673 - acc: 0.7232\n",
      "Epoch 00043: val_loss improved from 0.63853 to 0.63245, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/043-0.6325.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8673 - acc: 0.7231 - val_loss: 0.6325 - val_acc: 0.8074\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8548 - acc: 0.7252\n",
      "Epoch 00044: val_loss improved from 0.63245 to 0.63114, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/044-0.6311.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8548 - acc: 0.7252 - val_loss: 0.6311 - val_acc: 0.8083\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8492 - acc: 0.7283\n",
      "Epoch 00045: val_loss did not improve from 0.63114\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8492 - acc: 0.7283 - val_loss: 0.6337 - val_acc: 0.8050\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8461 - acc: 0.7321\n",
      "Epoch 00046: val_loss improved from 0.63114 to 0.61804, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/046-0.6180.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8460 - acc: 0.7321 - val_loss: 0.6180 - val_acc: 0.8153\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7336\n",
      "Epoch 00047: val_loss improved from 0.61804 to 0.61697, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/047-0.6170.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8384 - acc: 0.7336 - val_loss: 0.6170 - val_acc: 0.8150\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8404 - acc: 0.7302\n",
      "Epoch 00048: val_loss did not improve from 0.61697\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8405 - acc: 0.7302 - val_loss: 0.6186 - val_acc: 0.8104\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7339\n",
      "Epoch 00049: val_loss improved from 0.61697 to 0.61065, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/049-0.6106.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8327 - acc: 0.7339 - val_loss: 0.6106 - val_acc: 0.8157\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8308 - acc: 0.7354\n",
      "Epoch 00050: val_loss did not improve from 0.61065\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8309 - acc: 0.7353 - val_loss: 0.6167 - val_acc: 0.8132\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8303 - acc: 0.7365\n",
      "Epoch 00051: val_loss did not improve from 0.61065\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8302 - acc: 0.7365 - val_loss: 0.6182 - val_acc: 0.8097\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8209 - acc: 0.7382\n",
      "Epoch 00052: val_loss improved from 0.61065 to 0.60222, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/052-0.6022.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8208 - acc: 0.7382 - val_loss: 0.6022 - val_acc: 0.8197\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7400\n",
      "Epoch 00053: val_loss improved from 0.60222 to 0.58976, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/053-0.5898.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8175 - acc: 0.7400 - val_loss: 0.5898 - val_acc: 0.8248\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7423- ETA: 2s - los\n",
      "Epoch 00054: val_loss did not improve from 0.58976\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8136 - acc: 0.7422 - val_loss: 0.5927 - val_acc: 0.8227\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.7426\n",
      "Epoch 00055: val_loss improved from 0.58976 to 0.58538, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/055-0.5854.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8096 - acc: 0.7426 - val_loss: 0.5854 - val_acc: 0.8218\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.7413\n",
      "Epoch 00056: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8123 - acc: 0.7412 - val_loss: 0.5986 - val_acc: 0.8211\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8051 - acc: 0.7437\n",
      "Epoch 00057: val_loss did not improve from 0.58538\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8051 - acc: 0.7436 - val_loss: 0.5878 - val_acc: 0.8216\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.7449\n",
      "Epoch 00058: val_loss improved from 0.58538 to 0.58140, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/058-0.5814.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8025 - acc: 0.7449 - val_loss: 0.5814 - val_acc: 0.8276\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7466\n",
      "Epoch 00059: val_loss did not improve from 0.58140\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7966 - acc: 0.7466 - val_loss: 0.5841 - val_acc: 0.8171\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7914 - acc: 0.7478\n",
      "Epoch 00060: val_loss did not improve from 0.58140\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7914 - acc: 0.7478 - val_loss: 0.5910 - val_acc: 0.8181\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7940 - acc: 0.7479\n",
      "Epoch 00061: val_loss did not improve from 0.58140\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7940 - acc: 0.7479 - val_loss: 0.5859 - val_acc: 0.8160\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7856 - acc: 0.7507\n",
      "Epoch 00062: val_loss improved from 0.58140 to 0.57437, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/062-0.5744.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7856 - acc: 0.7507 - val_loss: 0.5744 - val_acc: 0.8239\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7807 - acc: 0.7526\n",
      "Epoch 00063: val_loss did not improve from 0.57437\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7807 - acc: 0.7526 - val_loss: 0.5766 - val_acc: 0.8251\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7808 - acc: 0.7519\n",
      "Epoch 00064: val_loss did not improve from 0.57437\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7809 - acc: 0.7519 - val_loss: 0.5774 - val_acc: 0.8244\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7516\n",
      "Epoch 00065: val_loss improved from 0.57437 to 0.56222, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/065-0.5622.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7792 - acc: 0.7516 - val_loss: 0.5622 - val_acc: 0.8307\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7529\n",
      "Epoch 00066: val_loss did not improve from 0.56222\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7779 - acc: 0.7529 - val_loss: 0.5740 - val_acc: 0.8318\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7741 - acc: 0.7546\n",
      "Epoch 00067: val_loss did not improve from 0.56222\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7741 - acc: 0.7547 - val_loss: 0.5660 - val_acc: 0.8269\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7545\n",
      "Epoch 00068: val_loss improved from 0.56222 to 0.55685, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/068-0.5569.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7708 - acc: 0.7545 - val_loss: 0.5569 - val_acc: 0.8334\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7580\n",
      "Epoch 00069: val_loss did not improve from 0.55685\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7658 - acc: 0.7580 - val_loss: 0.5607 - val_acc: 0.8318\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7571\n",
      "Epoch 00070: val_loss improved from 0.55685 to 0.55571, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/070-0.5557.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7662 - acc: 0.7571 - val_loss: 0.5557 - val_acc: 0.8341\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7641 - acc: 0.7552\n",
      "Epoch 00071: val_loss did not improve from 0.55571\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7642 - acc: 0.7552 - val_loss: 0.5733 - val_acc: 0.8281\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7626 - acc: 0.7586\n",
      "Epoch 00072: val_loss improved from 0.55571 to 0.55292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/072-0.5529.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7626 - acc: 0.7586 - val_loss: 0.5529 - val_acc: 0.8316\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7573 - acc: 0.7574\n",
      "Epoch 00073: val_loss did not improve from 0.55292\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7572 - acc: 0.7575 - val_loss: 0.5619 - val_acc: 0.8300\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7525 - acc: 0.7604\n",
      "Epoch 00074: val_loss did not improve from 0.55292\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7526 - acc: 0.7604 - val_loss: 0.5626 - val_acc: 0.8309\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7524 - acc: 0.7610\n",
      "Epoch 00075: val_loss improved from 0.55292 to 0.53361, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/075-0.5336.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7524 - acc: 0.7610 - val_loss: 0.5336 - val_acc: 0.8383\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7474 - acc: 0.7635\n",
      "Epoch 00076: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7475 - acc: 0.7634 - val_loss: 0.5494 - val_acc: 0.8365\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7464 - acc: 0.7621\n",
      "Epoch 00077: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7464 - acc: 0.7621 - val_loss: 0.5408 - val_acc: 0.8388\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7618\n",
      "Epoch 00078: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7468 - acc: 0.7618 - val_loss: 0.5433 - val_acc: 0.8372\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7436 - acc: 0.7641\n",
      "Epoch 00079: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7436 - acc: 0.7641 - val_loss: 0.5366 - val_acc: 0.8458\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7435 - acc: 0.7631\n",
      "Epoch 00080: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7435 - acc: 0.7631 - val_loss: 0.5388 - val_acc: 0.8397\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7383 - acc: 0.7654\n",
      "Epoch 00081: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7385 - acc: 0.7653 - val_loss: 0.5465 - val_acc: 0.8330\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.7668\n",
      "Epoch 00082: val_loss did not improve from 0.53361\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7323 - acc: 0.7667 - val_loss: 0.5366 - val_acc: 0.8372\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7350 - acc: 0.7658\n",
      "Epoch 00083: val_loss improved from 0.53361 to 0.53128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/083-0.5313.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7349 - acc: 0.7658 - val_loss: 0.5313 - val_acc: 0.8444\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7311 - acc: 0.7677\n",
      "Epoch 00084: val_loss did not improve from 0.53128\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7311 - acc: 0.7677 - val_loss: 0.5411 - val_acc: 0.8334\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.7697\n",
      "Epoch 00085: val_loss did not improve from 0.53128\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7246 - acc: 0.7696 - val_loss: 0.5384 - val_acc: 0.8453\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7298 - acc: 0.7678\n",
      "Epoch 00086: val_loss improved from 0.53128 to 0.52814, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/086-0.5281.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7300 - acc: 0.7677 - val_loss: 0.5281 - val_acc: 0.8395\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7242 - acc: 0.7715- ETA: 2s \n",
      "Epoch 00087: val_loss did not improve from 0.52814\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7241 - acc: 0.7716 - val_loss: 0.5329 - val_acc: 0.8383\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7226 - acc: 0.7709\n",
      "Epoch 00088: val_loss did not improve from 0.52814\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7227 - acc: 0.7709 - val_loss: 0.5296 - val_acc: 0.8409\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7691\n",
      "Epoch 00089: val_loss improved from 0.52814 to 0.52217, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/089-0.5222.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7230 - acc: 0.7691 - val_loss: 0.5222 - val_acc: 0.8444\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7712\n",
      "Epoch 00090: val_loss improved from 0.52217 to 0.52050, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/090-0.5205.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7165 - acc: 0.7712 - val_loss: 0.5205 - val_acc: 0.8411\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7733\n",
      "Epoch 00091: val_loss improved from 0.52050 to 0.51790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/091-0.5179.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7096 - acc: 0.7733 - val_loss: 0.5179 - val_acc: 0.8428\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7735\n",
      "Epoch 00092: val_loss did not improve from 0.51790\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7142 - acc: 0.7735 - val_loss: 0.5191 - val_acc: 0.8423\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7057 - acc: 0.7756\n",
      "Epoch 00093: val_loss did not improve from 0.51790\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7057 - acc: 0.7756 - val_loss: 0.5224 - val_acc: 0.8414\n",
      "Epoch 94/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7037 - acc: 0.7772\n",
      "Epoch 00094: val_loss did not improve from 0.51790\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7037 - acc: 0.7772 - val_loss: 0.5213 - val_acc: 0.8451\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7769\n",
      "Epoch 00095: val_loss did not improve from 0.51790\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7058 - acc: 0.7768 - val_loss: 0.5197 - val_acc: 0.8479\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.7765- ETA: 2s - loss:\n",
      "Epoch 00096: val_loss improved from 0.51790 to 0.51581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/096-0.5158.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6952 - acc: 0.7766 - val_loss: 0.5158 - val_acc: 0.8488\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7085 - acc: 0.7741\n",
      "Epoch 00097: val_loss did not improve from 0.51581\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7085 - acc: 0.7741 - val_loss: 0.5165 - val_acc: 0.8463\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6998 - acc: 0.7790\n",
      "Epoch 00098: val_loss improved from 0.51581 to 0.51196, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/098-0.5120.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6999 - acc: 0.7790 - val_loss: 0.5120 - val_acc: 0.8432\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7027 - acc: 0.7774\n",
      "Epoch 00099: val_loss improved from 0.51196 to 0.50631, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/099-0.5063.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7027 - acc: 0.7774 - val_loss: 0.5063 - val_acc: 0.8484\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.7788- ETA: 0s - loss: 0.6935 - acc: 0.7\n",
      "Epoch 00100: val_loss did not improve from 0.50631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6934 - acc: 0.7788 - val_loss: 0.5178 - val_acc: 0.8477\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6959 - acc: 0.7783\n",
      "Epoch 00101: val_loss did not improve from 0.50631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6959 - acc: 0.7783 - val_loss: 0.5233 - val_acc: 0.8456\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.7758\n",
      "Epoch 00102: val_loss did not improve from 0.50631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6992 - acc: 0.7758 - val_loss: 0.5119 - val_acc: 0.8505\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6894 - acc: 0.7813\n",
      "Epoch 00103: val_loss improved from 0.50631 to 0.50355, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/103-0.5035.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6894 - acc: 0.7813 - val_loss: 0.5035 - val_acc: 0.8488\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6874 - acc: 0.7811\n",
      "Epoch 00104: val_loss did not improve from 0.50355\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6874 - acc: 0.7811 - val_loss: 0.5080 - val_acc: 0.8460\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6911 - acc: 0.7798\n",
      "Epoch 00105: val_loss improved from 0.50355 to 0.50347, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/105-0.5035.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6911 - acc: 0.7799 - val_loss: 0.5035 - val_acc: 0.8514\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.7814\n",
      "Epoch 00106: val_loss did not improve from 0.50347\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6861 - acc: 0.7814 - val_loss: 0.5046 - val_acc: 0.8516\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.7801\n",
      "Epoch 00107: val_loss did not improve from 0.50347\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6820 - acc: 0.7801 - val_loss: 0.5077 - val_acc: 0.8505\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6841 - acc: 0.7813\n",
      "Epoch 00108: val_loss improved from 0.50347 to 0.49813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/108-0.4981.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6841 - acc: 0.7813 - val_loss: 0.4981 - val_acc: 0.8535\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6816 - acc: 0.7804\n",
      "Epoch 00109: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6815 - acc: 0.7804 - val_loss: 0.5101 - val_acc: 0.8521\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6734 - acc: 0.7861\n",
      "Epoch 00110: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6733 - acc: 0.7861 - val_loss: 0.5032 - val_acc: 0.8523\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.7859\n",
      "Epoch 00111: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6785 - acc: 0.7859 - val_loss: 0.5015 - val_acc: 0.8467\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7862\n",
      "Epoch 00112: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6725 - acc: 0.7862 - val_loss: 0.5022 - val_acc: 0.8521\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6757 - acc: 0.7848\n",
      "Epoch 00113: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6757 - acc: 0.7848 - val_loss: 0.5023 - val_acc: 0.8519\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.7860\n",
      "Epoch 00114: val_loss did not improve from 0.49813\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6702 - acc: 0.7860 - val_loss: 0.4985 - val_acc: 0.8439\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.7867\n",
      "Epoch 00115: val_loss improved from 0.49813 to 0.49340, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/115-0.4934.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6755 - acc: 0.7867 - val_loss: 0.4934 - val_acc: 0.8546\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.7892\n",
      "Epoch 00116: val_loss did not improve from 0.49340\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6666 - acc: 0.7892 - val_loss: 0.5024 - val_acc: 0.8456\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.7883\n",
      "Epoch 00117: val_loss did not improve from 0.49340\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6618 - acc: 0.7883 - val_loss: 0.5171 - val_acc: 0.8442\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7890\n",
      "Epoch 00118: val_loss did not improve from 0.49340\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6611 - acc: 0.7890 - val_loss: 0.5009 - val_acc: 0.8535\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.7881\n",
      "Epoch 00119: val_loss did not improve from 0.49340\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6608 - acc: 0.7880 - val_loss: 0.4968 - val_acc: 0.8551\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6621 - acc: 0.7907\n",
      "Epoch 00120: val_loss improved from 0.49340 to 0.49318, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/120-0.4932.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6621 - acc: 0.7907 - val_loss: 0.4932 - val_acc: 0.8567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6588 - acc: 0.7917\n",
      "Epoch 00121: val_loss improved from 0.49318 to 0.49057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/121-0.4906.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6588 - acc: 0.7917 - val_loss: 0.4906 - val_acc: 0.8521\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.7892\n",
      "Epoch 00122: val_loss did not improve from 0.49057\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6597 - acc: 0.7891 - val_loss: 0.4934 - val_acc: 0.8542\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6587 - acc: 0.7902\n",
      "Epoch 00123: val_loss did not improve from 0.49057\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6588 - acc: 0.7902 - val_loss: 0.5025 - val_acc: 0.8484\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.7946\n",
      "Epoch 00124: val_loss improved from 0.49057 to 0.48777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/124-0.4878.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6485 - acc: 0.7946 - val_loss: 0.4878 - val_acc: 0.8521\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.7919\n",
      "Epoch 00125: val_loss did not improve from 0.48777\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6581 - acc: 0.7919 - val_loss: 0.4922 - val_acc: 0.8556\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6553 - acc: 0.7901\n",
      "Epoch 00126: val_loss did not improve from 0.48777\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6552 - acc: 0.7901 - val_loss: 0.5084 - val_acc: 0.8453\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6472 - acc: 0.7940\n",
      "Epoch 00127: val_loss improved from 0.48777 to 0.48511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/127-0.4851.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6472 - acc: 0.7940 - val_loss: 0.4851 - val_acc: 0.8563\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.7920\n",
      "Epoch 00128: val_loss improved from 0.48511 to 0.47800, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/128-0.4780.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6527 - acc: 0.7920 - val_loss: 0.4780 - val_acc: 0.8591\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.7913\n",
      "Epoch 00129: val_loss did not improve from 0.47800\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6494 - acc: 0.7913 - val_loss: 0.4930 - val_acc: 0.8537\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.7906\n",
      "Epoch 00130: val_loss did not improve from 0.47800\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6513 - acc: 0.7906 - val_loss: 0.4783 - val_acc: 0.8577\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6465 - acc: 0.7926- \n",
      "Epoch 00131: val_loss did not improve from 0.47800\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6468 - acc: 0.7926 - val_loss: 0.4823 - val_acc: 0.8614\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6417 - acc: 0.7954\n",
      "Epoch 00132: val_loss improved from 0.47800 to 0.47582, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/132-0.4758.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6417 - acc: 0.7954 - val_loss: 0.4758 - val_acc: 0.8635\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.7929- ET\n",
      "Epoch 00133: val_loss did not improve from 0.47582\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6446 - acc: 0.7929 - val_loss: 0.4858 - val_acc: 0.8546\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6381 - acc: 0.7979\n",
      "Epoch 00134: val_loss did not improve from 0.47582\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6381 - acc: 0.7979 - val_loss: 0.4760 - val_acc: 0.8605\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6357 - acc: 0.7989\n",
      "Epoch 00135: val_loss did not improve from 0.47582\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6357 - acc: 0.7989 - val_loss: 0.4781 - val_acc: 0.8577\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.7999\n",
      "Epoch 00136: val_loss did not improve from 0.47582\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6346 - acc: 0.7998 - val_loss: 0.4760 - val_acc: 0.8591\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6375 - acc: 0.7990\n",
      "Epoch 00137: val_loss did not improve from 0.47582\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6375 - acc: 0.7991 - val_loss: 0.4938 - val_acc: 0.8537\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6341 - acc: 0.7972\n",
      "Epoch 00138: val_loss improved from 0.47582 to 0.47081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/138-0.4708.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6341 - acc: 0.7972 - val_loss: 0.4708 - val_acc: 0.8623\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6269 - acc: 0.7994\n",
      "Epoch 00139: val_loss did not improve from 0.47081\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6269 - acc: 0.7994 - val_loss: 0.4804 - val_acc: 0.8602\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6295 - acc: 0.8004\n",
      "Epoch 00140: val_loss improved from 0.47081 to 0.46998, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/140-0.4700.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6294 - acc: 0.8004 - val_loss: 0.4700 - val_acc: 0.8591\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6263 - acc: 0.8021\n",
      "Epoch 00141: val_loss did not improve from 0.46998\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6263 - acc: 0.8021 - val_loss: 0.4724 - val_acc: 0.8574\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6279 - acc: 0.8021\n",
      "Epoch 00142: val_loss did not improve from 0.46998\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6279 - acc: 0.8021 - val_loss: 0.4774 - val_acc: 0.8602\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.7986\n",
      "Epoch 00143: val_loss improved from 0.46998 to 0.46648, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/143-0.4665.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6285 - acc: 0.7985 - val_loss: 0.4665 - val_acc: 0.8633\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8014\n",
      "Epoch 00144: val_loss did not improve from 0.46648\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6194 - acc: 0.8014 - val_loss: 0.4701 - val_acc: 0.8598\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.7993\n",
      "Epoch 00145: val_loss improved from 0.46648 to 0.45888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/145-0.4589.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6263 - acc: 0.7993 - val_loss: 0.4589 - val_acc: 0.8658\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.8040\n",
      "Epoch 00146: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6206 - acc: 0.8040 - val_loss: 0.4634 - val_acc: 0.8602\n",
      "Epoch 147/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.8048\n",
      "Epoch 00147: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6207 - acc: 0.8048 - val_loss: 0.4695 - val_acc: 0.8614\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6176 - acc: 0.8046\n",
      "Epoch 00148: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6178 - acc: 0.8046 - val_loss: 0.4624 - val_acc: 0.8635\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6173 - acc: 0.8043\n",
      "Epoch 00149: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6173 - acc: 0.8044 - val_loss: 0.4698 - val_acc: 0.8640\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6150 - acc: 0.8057\n",
      "Epoch 00150: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6149 - acc: 0.8057 - val_loss: 0.4621 - val_acc: 0.8633\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6182 - acc: 0.8033\n",
      "Epoch 00151: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6181 - acc: 0.8033 - val_loss: 0.4667 - val_acc: 0.8623\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6145 - acc: 0.8048\n",
      "Epoch 00152: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6145 - acc: 0.8048 - val_loss: 0.4744 - val_acc: 0.8574\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.8032- ETA: 0s - loss: 0.6112 - acc:\n",
      "Epoch 00153: val_loss did not improve from 0.45888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6112 - acc: 0.8032 - val_loss: 0.4710 - val_acc: 0.8633\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6061 - acc: 0.8082\n",
      "Epoch 00154: val_loss improved from 0.45888 to 0.45788, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/154-0.4579.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6060 - acc: 0.8082 - val_loss: 0.4579 - val_acc: 0.8644\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.8092\n",
      "Epoch 00155: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6033 - acc: 0.8092 - val_loss: 0.4657 - val_acc: 0.8586\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6049 - acc: 0.8078\n",
      "Epoch 00156: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6049 - acc: 0.8078 - val_loss: 0.4637 - val_acc: 0.8614\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6044 - acc: 0.8058\n",
      "Epoch 00157: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6044 - acc: 0.8058 - val_loss: 0.4687 - val_acc: 0.8593\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.8087\n",
      "Epoch 00158: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5980 - acc: 0.8087 - val_loss: 0.4720 - val_acc: 0.8649\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6044 - acc: 0.8078\n",
      "Epoch 00159: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6044 - acc: 0.8078 - val_loss: 0.4590 - val_acc: 0.8630\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.8073\n",
      "Epoch 00160: val_loss did not improve from 0.45788\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6048 - acc: 0.8073 - val_loss: 0.4586 - val_acc: 0.8647\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.8078\n",
      "Epoch 00161: val_loss improved from 0.45788 to 0.45721, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/161-0.4572.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6050 - acc: 0.8078 - val_loss: 0.4572 - val_acc: 0.8668\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.8111\n",
      "Epoch 00162: val_loss improved from 0.45721 to 0.45359, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/162-0.4536.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5970 - acc: 0.8111 - val_loss: 0.4536 - val_acc: 0.8677\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5990 - acc: 0.8081\n",
      "Epoch 00163: val_loss did not improve from 0.45359\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5990 - acc: 0.8082 - val_loss: 0.4622 - val_acc: 0.8628\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5952 - acc: 0.8097\n",
      "Epoch 00164: val_loss improved from 0.45359 to 0.45214, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/164-0.4521.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5952 - acc: 0.8097 - val_loss: 0.4521 - val_acc: 0.8668\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5911 - acc: 0.8122\n",
      "Epoch 00165: val_loss did not improve from 0.45214\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5912 - acc: 0.8122 - val_loss: 0.4532 - val_acc: 0.8656\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5894 - acc: 0.8143\n",
      "Epoch 00166: val_loss improved from 0.45214 to 0.45048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/166-0.4505.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5894 - acc: 0.8143 - val_loss: 0.4505 - val_acc: 0.8661\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5934 - acc: 0.8077\n",
      "Epoch 00167: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5934 - acc: 0.8077 - val_loss: 0.4603 - val_acc: 0.8644\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8101\n",
      "Epoch 00168: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5911 - acc: 0.8102 - val_loss: 0.4560 - val_acc: 0.8623\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5938 - acc: 0.8090\n",
      "Epoch 00169: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5937 - acc: 0.8090 - val_loss: 0.4598 - val_acc: 0.8612\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8090\n",
      "Epoch 00170: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5921 - acc: 0.8090 - val_loss: 0.4619 - val_acc: 0.8605\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.8127\n",
      "Epoch 00171: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5860 - acc: 0.8127 - val_loss: 0.4539 - val_acc: 0.8677\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5902 - acc: 0.8118\n",
      "Epoch 00172: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5903 - acc: 0.8117 - val_loss: 0.4527 - val_acc: 0.8684\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8136\n",
      "Epoch 00173: val_loss did not improve from 0.45048\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5883 - acc: 0.8136 - val_loss: 0.4549 - val_acc: 0.8644\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5832 - acc: 0.8133\n",
      "Epoch 00174: val_loss improved from 0.45048 to 0.44272, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/174-0.4427.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5832 - acc: 0.8133 - val_loss: 0.4427 - val_acc: 0.8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8151- ETA: 2s - loss:\n",
      "Epoch 00175: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5776 - acc: 0.8151 - val_loss: 0.4671 - val_acc: 0.8558\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.8130\n",
      "Epoch 00176: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5834 - acc: 0.8130 - val_loss: 0.4578 - val_acc: 0.8640\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5864 - acc: 0.8133\n",
      "Epoch 00177: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5863 - acc: 0.8134 - val_loss: 0.4482 - val_acc: 0.8682\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8096\n",
      "Epoch 00178: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5884 - acc: 0.8096 - val_loss: 0.4536 - val_acc: 0.8649\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5814 - acc: 0.8135\n",
      "Epoch 00179: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5815 - acc: 0.8135 - val_loss: 0.4494 - val_acc: 0.8649\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8151\n",
      "Epoch 00180: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5814 - acc: 0.8151 - val_loss: 0.4444 - val_acc: 0.8691\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.8168\n",
      "Epoch 00181: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5798 - acc: 0.8168 - val_loss: 0.4456 - val_acc: 0.8712\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.8163\n",
      "Epoch 00182: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5771 - acc: 0.8163 - val_loss: 0.4543 - val_acc: 0.8651\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8170\n",
      "Epoch 00183: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5741 - acc: 0.8170 - val_loss: 0.4459 - val_acc: 0.8677\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5753 - acc: 0.8154\n",
      "Epoch 00184: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5753 - acc: 0.8154 - val_loss: 0.4495 - val_acc: 0.8686\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.8150\n",
      "Epoch 00185: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5770 - acc: 0.8150 - val_loss: 0.4439 - val_acc: 0.8714\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.8171\n",
      "Epoch 00186: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5710 - acc: 0.8171 - val_loss: 0.4437 - val_acc: 0.8682\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5690 - acc: 0.8157- ETA:\n",
      "Epoch 00187: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5690 - acc: 0.8158 - val_loss: 0.4432 - val_acc: 0.8686\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.8162\n",
      "Epoch 00188: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5722 - acc: 0.8162 - val_loss: 0.4432 - val_acc: 0.8700\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8183\n",
      "Epoch 00189: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5707 - acc: 0.8183 - val_loss: 0.4464 - val_acc: 0.8658\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5675 - acc: 0.8186\n",
      "Epoch 00190: val_loss did not improve from 0.44272\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5674 - acc: 0.8186 - val_loss: 0.4492 - val_acc: 0.8651\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5692 - acc: 0.8178\n",
      "Epoch 00191: val_loss improved from 0.44272 to 0.44127, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/191-0.4413.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5692 - acc: 0.8178 - val_loss: 0.4413 - val_acc: 0.8682\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.8189\n",
      "Epoch 00192: val_loss did not improve from 0.44127\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5668 - acc: 0.8189 - val_loss: 0.4414 - val_acc: 0.8714\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.8181\n",
      "Epoch 00193: val_loss did not improve from 0.44127\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5695 - acc: 0.8181 - val_loss: 0.4550 - val_acc: 0.8628\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8172\n",
      "Epoch 00194: val_loss improved from 0.44127 to 0.43994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/194-0.4399.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5700 - acc: 0.8172 - val_loss: 0.4399 - val_acc: 0.8707\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8206\n",
      "Epoch 00195: val_loss did not improve from 0.43994\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5626 - acc: 0.8206 - val_loss: 0.4535 - val_acc: 0.8679\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8215\n",
      "Epoch 00196: val_loss did not improve from 0.43994\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5620 - acc: 0.8215 - val_loss: 0.4477 - val_acc: 0.8658\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.8220\n",
      "Epoch 00197: val_loss improved from 0.43994 to 0.43875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/197-0.4387.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5589 - acc: 0.8220 - val_loss: 0.4387 - val_acc: 0.8700\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8187\n",
      "Epoch 00198: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5643 - acc: 0.8187 - val_loss: 0.4509 - val_acc: 0.8693\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.8186\n",
      "Epoch 00199: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5659 - acc: 0.8186 - val_loss: 0.4490 - val_acc: 0.8668\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8206\n",
      "Epoch 00200: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5537 - acc: 0.8206 - val_loss: 0.4544 - val_acc: 0.8721\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8212\n",
      "Epoch 00201: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5582 - acc: 0.8212 - val_loss: 0.4482 - val_acc: 0.8668\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8190\n",
      "Epoch 00202: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5622 - acc: 0.8190 - val_loss: 0.4483 - val_acc: 0.8677\n",
      "Epoch 203/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8214\n",
      "Epoch 00203: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5650 - acc: 0.8214 - val_loss: 0.4494 - val_acc: 0.8682\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8215\n",
      "Epoch 00204: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5577 - acc: 0.8215 - val_loss: 0.4447 - val_acc: 0.8700\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8222\n",
      "Epoch 00205: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5558 - acc: 0.8222 - val_loss: 0.4421 - val_acc: 0.8700\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.8228\n",
      "Epoch 00206: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5591 - acc: 0.8229 - val_loss: 0.4498 - val_acc: 0.8693\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.8211\n",
      "Epoch 00207: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5565 - acc: 0.8211 - val_loss: 0.4423 - val_acc: 0.8693\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8243\n",
      "Epoch 00208: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5526 - acc: 0.8243 - val_loss: 0.4518 - val_acc: 0.8654\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.8230\n",
      "Epoch 00209: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5491 - acc: 0.8230 - val_loss: 0.4553 - val_acc: 0.8661\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5580 - acc: 0.8201\n",
      "Epoch 00210: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5582 - acc: 0.8201 - val_loss: 0.4442 - val_acc: 0.8677\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8208\n",
      "Epoch 00211: val_loss did not improve from 0.43875\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5566 - acc: 0.8208 - val_loss: 0.4464 - val_acc: 0.8670\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.8239\n",
      "Epoch 00212: val_loss improved from 0.43875 to 0.43480, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/212-0.4348.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5515 - acc: 0.8239 - val_loss: 0.4348 - val_acc: 0.8710\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8253\n",
      "Epoch 00213: val_loss did not improve from 0.43480\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5481 - acc: 0.8253 - val_loss: 0.4401 - val_acc: 0.8724\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.8258\n",
      "Epoch 00214: val_loss did not improve from 0.43480\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5478 - acc: 0.8258 - val_loss: 0.4351 - val_acc: 0.8698\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8238\n",
      "Epoch 00215: val_loss did not improve from 0.43480\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5476 - acc: 0.8238 - val_loss: 0.4363 - val_acc: 0.8717\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.8212\n",
      "Epoch 00216: val_loss did not improve from 0.43480\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5520 - acc: 0.8212 - val_loss: 0.4465 - val_acc: 0.8658\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.8224\n",
      "Epoch 00217: val_loss did not improve from 0.43480\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5498 - acc: 0.8224 - val_loss: 0.4441 - val_acc: 0.8679\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5446 - acc: 0.8238\n",
      "Epoch 00218: val_loss improved from 0.43480 to 0.43280, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/218-0.4328.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5445 - acc: 0.8238 - val_loss: 0.4328 - val_acc: 0.8712\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8236\n",
      "Epoch 00219: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5435 - acc: 0.8236 - val_loss: 0.4435 - val_acc: 0.8684\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8294- ETA: -\n",
      "Epoch 00220: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5408 - acc: 0.8294 - val_loss: 0.4426 - val_acc: 0.8710\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5434 - acc: 0.8273\n",
      "Epoch 00221: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5433 - acc: 0.8273 - val_loss: 0.4382 - val_acc: 0.8675\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.8252\n",
      "Epoch 00222: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5395 - acc: 0.8252 - val_loss: 0.4404 - val_acc: 0.8707\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5406 - acc: 0.8243\n",
      "Epoch 00223: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5406 - acc: 0.8243 - val_loss: 0.4390 - val_acc: 0.8693\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.8283\n",
      "Epoch 00224: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5350 - acc: 0.8283 - val_loss: 0.4449 - val_acc: 0.8693\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8260\n",
      "Epoch 00225: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5391 - acc: 0.8260 - val_loss: 0.4422 - val_acc: 0.8707\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8290\n",
      "Epoch 00226: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5370 - acc: 0.8290 - val_loss: 0.4427 - val_acc: 0.8661\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5409 - acc: 0.8259\n",
      "Epoch 00227: val_loss did not improve from 0.43280\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5409 - acc: 0.8259 - val_loss: 0.4366 - val_acc: 0.8700\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8264\n",
      "Epoch 00228: val_loss improved from 0.43280 to 0.43143, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv_checkpoint/228-0.4314.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5387 - acc: 0.8263 - val_loss: 0.4314 - val_acc: 0.8735\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.8259\n",
      "Epoch 00229: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5376 - acc: 0.8259 - val_loss: 0.4403 - val_acc: 0.8710\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.8289\n",
      "Epoch 00230: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5342 - acc: 0.8289 - val_loss: 0.4401 - val_acc: 0.8712\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5271 - acc: 0.8302\n",
      "Epoch 00231: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5271 - acc: 0.8301 - val_loss: 0.4461 - val_acc: 0.8707\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5345 - acc: 0.8284\n",
      "Epoch 00232: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5345 - acc: 0.8284 - val_loss: 0.4490 - val_acc: 0.8679\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8281\n",
      "Epoch 00233: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5348 - acc: 0.8281 - val_loss: 0.4611 - val_acc: 0.8626\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5287 - acc: 0.8309\n",
      "Epoch 00234: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5287 - acc: 0.8309 - val_loss: 0.4455 - val_acc: 0.8689\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8316\n",
      "Epoch 00235: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5289 - acc: 0.8316 - val_loss: 0.4365 - val_acc: 0.8693\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5272 - acc: 0.8337\n",
      "Epoch 00236: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5273 - acc: 0.8337 - val_loss: 0.4321 - val_acc: 0.8742\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.8299\n",
      "Epoch 00237: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5269 - acc: 0.8299 - val_loss: 0.4437 - val_acc: 0.8682\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.8324\n",
      "Epoch 00238: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5282 - acc: 0.8324 - val_loss: 0.4391 - val_acc: 0.8714\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8301\n",
      "Epoch 00239: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5310 - acc: 0.8302 - val_loss: 0.4341 - val_acc: 0.8705\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5241 - acc: 0.8313\n",
      "Epoch 00240: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5240 - acc: 0.8313 - val_loss: 0.4349 - val_acc: 0.8689\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.8299\n",
      "Epoch 00241: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5258 - acc: 0.8299 - val_loss: 0.4421 - val_acc: 0.8679\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.8308\n",
      "Epoch 00242: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5225 - acc: 0.8308 - val_loss: 0.4332 - val_acc: 0.8730\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5204 - acc: 0.8318\n",
      "Epoch 00243: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5205 - acc: 0.8318 - val_loss: 0.4393 - val_acc: 0.8689\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.8308\n",
      "Epoch 00244: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5252 - acc: 0.8308 - val_loss: 0.4359 - val_acc: 0.8719\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8309\n",
      "Epoch 00245: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5274 - acc: 0.8309 - val_loss: 0.4417 - val_acc: 0.8696\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.8323\n",
      "Epoch 00246: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5230 - acc: 0.8323 - val_loss: 0.4458 - val_acc: 0.8700\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.8311\n",
      "Epoch 00247: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5225 - acc: 0.8311 - val_loss: 0.4357 - val_acc: 0.8724\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8331\n",
      "Epoch 00248: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5167 - acc: 0.8331 - val_loss: 0.4512 - val_acc: 0.8661\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8323\n",
      "Epoch 00249: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5180 - acc: 0.8323 - val_loss: 0.4426 - val_acc: 0.8682\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8344\n",
      "Epoch 00250: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5182 - acc: 0.8344 - val_loss: 0.4359 - val_acc: 0.8700\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8328\n",
      "Epoch 00251: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5203 - acc: 0.8328 - val_loss: 0.4557 - val_acc: 0.8621\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8302- ETA: 2s - \n",
      "Epoch 00252: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5202 - acc: 0.8302 - val_loss: 0.4344 - val_acc: 0.8717\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8368\n",
      "Epoch 00253: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5089 - acc: 0.8368 - val_loss: 0.4351 - val_acc: 0.8698\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8336\n",
      "Epoch 00254: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5174 - acc: 0.8336 - val_loss: 0.4398 - val_acc: 0.8737\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8335\n",
      "Epoch 00255: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5177 - acc: 0.8335 - val_loss: 0.4506 - val_acc: 0.8679\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8375\n",
      "Epoch 00256: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5110 - acc: 0.8375 - val_loss: 0.4374 - val_acc: 0.8742\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8350\n",
      "Epoch 00257: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5086 - acc: 0.8350 - val_loss: 0.4381 - val_acc: 0.8696\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8332\n",
      "Epoch 00258: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5154 - acc: 0.8332 - val_loss: 0.4527 - val_acc: 0.8698\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5088 - acc: 0.8347\n",
      "Epoch 00259: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5088 - acc: 0.8347 - val_loss: 0.4436 - val_acc: 0.8721\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8361\n",
      "Epoch 00260: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5111 - acc: 0.8361 - val_loss: 0.4369 - val_acc: 0.8721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.8363\n",
      "Epoch 00261: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5162 - acc: 0.8363 - val_loss: 0.4334 - val_acc: 0.8730\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8359\n",
      "Epoch 00262: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5074 - acc: 0.8359 - val_loss: 0.4394 - val_acc: 0.8724\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8365\n",
      "Epoch 00263: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5105 - acc: 0.8365 - val_loss: 0.4468 - val_acc: 0.8668\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8381\n",
      "Epoch 00264: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5018 - acc: 0.8381 - val_loss: 0.4409 - val_acc: 0.8686\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8357\n",
      "Epoch 00265: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5143 - acc: 0.8356 - val_loss: 0.4362 - val_acc: 0.8693\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.8357\n",
      "Epoch 00266: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5080 - acc: 0.8357 - val_loss: 0.4363 - val_acc: 0.8712\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5086 - acc: 0.8376\n",
      "Epoch 00267: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5086 - acc: 0.8376 - val_loss: 0.4425 - val_acc: 0.8703\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8366\n",
      "Epoch 00268: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5114 - acc: 0.8366 - val_loss: 0.4391 - val_acc: 0.8721\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8366\n",
      "Epoch 00269: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5062 - acc: 0.8366 - val_loss: 0.4380 - val_acc: 0.8714\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8375\n",
      "Epoch 00270: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5028 - acc: 0.8375 - val_loss: 0.4497 - val_acc: 0.8679\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5097 - acc: 0.8350\n",
      "Epoch 00271: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5098 - acc: 0.8350 - val_loss: 0.4385 - val_acc: 0.8721\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.8360\n",
      "Epoch 00272: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5093 - acc: 0.8360 - val_loss: 0.4489 - val_acc: 0.8693\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5025 - acc: 0.8351\n",
      "Epoch 00273: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5025 - acc: 0.8351 - val_loss: 0.4437 - val_acc: 0.8737\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8376\n",
      "Epoch 00274: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5069 - acc: 0.8376 - val_loss: 0.4466 - val_acc: 0.8705\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.8385\n",
      "Epoch 00275: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5023 - acc: 0.8385 - val_loss: 0.4371 - val_acc: 0.8707\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5053 - acc: 0.8368- ETA: 0s - loss: 0.5060 - acc: \n",
      "Epoch 00276: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.5052 - acc: 0.8368 - val_loss: 0.4382 - val_acc: 0.8737\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.8395\n",
      "Epoch 00277: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4964 - acc: 0.8395 - val_loss: 0.4467 - val_acc: 0.8696\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4991 - acc: 0.8400- ETA: 1s - loss: 0.4983\n",
      "Epoch 00278: val_loss did not improve from 0.43143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.4991 - acc: 0.8400 - val_loss: 0.4413 - val_acc: 0.8730\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 661us/sample - loss: 0.5234 - acc: 0.8390\n",
      "Loss: 0.5234299747248181 Accuracy: 0.83904463\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3679 - acc: 0.2254\n",
      "Epoch 00001: val_loss improved from inf to 1.89667, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/001-1.8967.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.3680 - acc: 0.2254 - val_loss: 1.8967 - val_acc: 0.4149\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8928 - acc: 0.3671\n",
      "Epoch 00002: val_loss improved from 1.89667 to 1.54686, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/002-1.5469.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.8928 - acc: 0.3671 - val_loss: 1.5469 - val_acc: 0.5136\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6713 - acc: 0.4405\n",
      "Epoch 00003: val_loss improved from 1.54686 to 1.34456, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/003-1.3446.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.6714 - acc: 0.4405 - val_loss: 1.3446 - val_acc: 0.5910\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5056 - acc: 0.4964\n",
      "Epoch 00004: val_loss improved from 1.34456 to 1.18370, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/004-1.1837.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.5055 - acc: 0.4964 - val_loss: 1.1837 - val_acc: 0.6406\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3705 - acc: 0.5443\n",
      "Epoch 00005: val_loss improved from 1.18370 to 1.05681, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/005-1.0568.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.3707 - acc: 0.5442 - val_loss: 1.0568 - val_acc: 0.6783\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2654 - acc: 0.5821\n",
      "Epoch 00006: val_loss improved from 1.05681 to 0.95610, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/006-0.9561.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.2654 - acc: 0.5821 - val_loss: 0.9561 - val_acc: 0.7079\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1849 - acc: 0.6148\n",
      "Epoch 00007: val_loss improved from 0.95610 to 0.87248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/007-0.8725.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1848 - acc: 0.6149 - val_loss: 0.8725 - val_acc: 0.7342\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1113 - acc: 0.6390\n",
      "Epoch 00008: val_loss improved from 0.87248 to 0.83519, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/008-0.8352.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1114 - acc: 0.6390 - val_loss: 0.8352 - val_acc: 0.7419\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0585 - acc: 0.6547\n",
      "Epoch 00009: val_loss improved from 0.83519 to 0.78507, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/009-0.7851.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0584 - acc: 0.6547 - val_loss: 0.7851 - val_acc: 0.7703\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0079 - acc: 0.6733\n",
      "Epoch 00010: val_loss improved from 0.78507 to 0.73767, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/010-0.7377.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0079 - acc: 0.6733 - val_loss: 0.7377 - val_acc: 0.7757\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9563 - acc: 0.6910\n",
      "Epoch 00011: val_loss improved from 0.73767 to 0.68837, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/011-0.6884.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9563 - acc: 0.6910 - val_loss: 0.6884 - val_acc: 0.7899\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9222 - acc: 0.7028\n",
      "Epoch 00012: val_loss improved from 0.68837 to 0.64797, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/012-0.6480.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9223 - acc: 0.7028 - val_loss: 0.6480 - val_acc: 0.8050\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8911 - acc: 0.7142\n",
      "Epoch 00013: val_loss improved from 0.64797 to 0.63048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/013-0.6305.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8910 - acc: 0.7142 - val_loss: 0.6305 - val_acc: 0.8134\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.7304\n",
      "Epoch 00014: val_loss improved from 0.63048 to 0.61865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/014-0.6187.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8522 - acc: 0.7304 - val_loss: 0.6187 - val_acc: 0.8134\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8263 - acc: 0.7375\n",
      "Epoch 00015: val_loss improved from 0.61865 to 0.58705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/015-0.5871.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8263 - acc: 0.7375 - val_loss: 0.5871 - val_acc: 0.8218\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7919 - acc: 0.7486\n",
      "Epoch 00016: val_loss improved from 0.58705 to 0.56697, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/016-0.5670.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7918 - acc: 0.7486 - val_loss: 0.5670 - val_acc: 0.8283\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7803 - acc: 0.7531\n",
      "Epoch 00017: val_loss improved from 0.56697 to 0.53641, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/017-0.5364.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7803 - acc: 0.7530 - val_loss: 0.5364 - val_acc: 0.8437\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7569 - acc: 0.7607\n",
      "Epoch 00018: val_loss did not improve from 0.53641\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7568 - acc: 0.7607 - val_loss: 0.5411 - val_acc: 0.8388\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7709\n",
      "Epoch 00019: val_loss improved from 0.53641 to 0.49671, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/019-0.4967.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7274 - acc: 0.7709 - val_loss: 0.4967 - val_acc: 0.8551\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7150 - acc: 0.7729\n",
      "Epoch 00020: val_loss did not improve from 0.49671\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7151 - acc: 0.7729 - val_loss: 0.5061 - val_acc: 0.8539\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6955 - acc: 0.7815\n",
      "Epoch 00021: val_loss improved from 0.49671 to 0.49308, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/021-0.4931.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6956 - acc: 0.7816 - val_loss: 0.4931 - val_acc: 0.8523\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.7846\n",
      "Epoch 00022: val_loss improved from 0.49308 to 0.48139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/022-0.4814.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6831 - acc: 0.7846 - val_loss: 0.4814 - val_acc: 0.8577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6687 - acc: 0.7888\n",
      "Epoch 00023: val_loss improved from 0.48139 to 0.44714, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/023-0.4471.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6687 - acc: 0.7888 - val_loss: 0.4471 - val_acc: 0.8714\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6546 - acc: 0.7935\n",
      "Epoch 00024: val_loss improved from 0.44714 to 0.43578, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/024-0.4358.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6547 - acc: 0.7935 - val_loss: 0.4358 - val_acc: 0.8735\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.7979\n",
      "Epoch 00025: val_loss improved from 0.43578 to 0.42492, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/025-0.4249.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6441 - acc: 0.7979 - val_loss: 0.4249 - val_acc: 0.8805\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6268 - acc: 0.8030\n",
      "Epoch 00026: val_loss did not improve from 0.42492\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6268 - acc: 0.8030 - val_loss: 0.4552 - val_acc: 0.8637\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.8043\n",
      "Epoch 00027: val_loss improved from 0.42492 to 0.41424, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/027-0.4142.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6217 - acc: 0.8044 - val_loss: 0.4142 - val_acc: 0.8779\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6119 - acc: 0.8086\n",
      "Epoch 00028: val_loss improved from 0.41424 to 0.41132, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/028-0.4113.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6118 - acc: 0.8087 - val_loss: 0.4113 - val_acc: 0.8754\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5971 - acc: 0.8114\n",
      "Epoch 00029: val_loss improved from 0.41132 to 0.40371, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/029-0.4037.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5970 - acc: 0.8115 - val_loss: 0.4037 - val_acc: 0.8789\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.8196\n",
      "Epoch 00030: val_loss improved from 0.40371 to 0.39959, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/030-0.3996.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5825 - acc: 0.8196 - val_loss: 0.3996 - val_acc: 0.8810\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.8174\n",
      "Epoch 00031: val_loss improved from 0.39959 to 0.38541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/031-0.3854.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5832 - acc: 0.8175 - val_loss: 0.3854 - val_acc: 0.8845\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.8218\n",
      "Epoch 00032: val_loss improved from 0.38541 to 0.38031, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/032-0.3803.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5677 - acc: 0.8218 - val_loss: 0.3803 - val_acc: 0.8870\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5574 - acc: 0.8259\n",
      "Epoch 00033: val_loss did not improve from 0.38031\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5574 - acc: 0.8259 - val_loss: 0.3959 - val_acc: 0.8751\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.8271\n",
      "Epoch 00034: val_loss improved from 0.38031 to 0.36718, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/034-0.3672.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5507 - acc: 0.8270 - val_loss: 0.3672 - val_acc: 0.8910\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.8287\n",
      "Epoch 00035: val_loss improved from 0.36718 to 0.36099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/035-0.3610.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5502 - acc: 0.8286 - val_loss: 0.3610 - val_acc: 0.8917\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8286\n",
      "Epoch 00036: val_loss did not improve from 0.36099\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5444 - acc: 0.8286 - val_loss: 0.3657 - val_acc: 0.8924\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.8323\n",
      "Epoch 00037: val_loss did not improve from 0.36099\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5321 - acc: 0.8323 - val_loss: 0.3635 - val_acc: 0.8905\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.8360\n",
      "Epoch 00038: val_loss did not improve from 0.36099\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5263 - acc: 0.8360 - val_loss: 0.3649 - val_acc: 0.8901\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5175 - acc: 0.8371\n",
      "Epoch 00039: val_loss did not improve from 0.36099\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5175 - acc: 0.8371 - val_loss: 0.3789 - val_acc: 0.8828\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8373\n",
      "Epoch 00040: val_loss improved from 0.36099 to 0.34489, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/040-0.3449.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5168 - acc: 0.8373 - val_loss: 0.3449 - val_acc: 0.8968\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5082 - acc: 0.8401\n",
      "Epoch 00041: val_loss improved from 0.34489 to 0.34350, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/041-0.3435.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5082 - acc: 0.8400 - val_loss: 0.3435 - val_acc: 0.8973\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8426\n",
      "Epoch 00042: val_loss did not improve from 0.34350\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5071 - acc: 0.8426 - val_loss: 0.3525 - val_acc: 0.8891\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4978 - acc: 0.8417\n",
      "Epoch 00043: val_loss improved from 0.34350 to 0.33343, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/043-0.3334.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4977 - acc: 0.8417 - val_loss: 0.3334 - val_acc: 0.8996\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4930 - acc: 0.8443\n",
      "Epoch 00044: val_loss did not improve from 0.33343\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4930 - acc: 0.8442 - val_loss: 0.3678 - val_acc: 0.8861\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8464\n",
      "Epoch 00045: val_loss improved from 0.33343 to 0.32610, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/045-0.3261.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4954 - acc: 0.8464 - val_loss: 0.3261 - val_acc: 0.9031\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.8499\n",
      "Epoch 00046: val_loss improved from 0.32610 to 0.32173, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/046-0.3217.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4806 - acc: 0.8499 - val_loss: 0.3217 - val_acc: 0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8508\n",
      "Epoch 00047: val_loss did not improve from 0.32173\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4783 - acc: 0.8508 - val_loss: 0.3239 - val_acc: 0.9001\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.8474\n",
      "Epoch 00048: val_loss did not improve from 0.32173\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4791 - acc: 0.8474 - val_loss: 0.3351 - val_acc: 0.8970\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8512\n",
      "Epoch 00049: val_loss did not improve from 0.32173\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4713 - acc: 0.8512 - val_loss: 0.3267 - val_acc: 0.9003\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4685 - acc: 0.8534\n",
      "Epoch 00050: val_loss did not improve from 0.32173\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4685 - acc: 0.8534 - val_loss: 0.3238 - val_acc: 0.9003\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8540\n",
      "Epoch 00051: val_loss improved from 0.32173 to 0.31980, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/051-0.3198.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4632 - acc: 0.8540 - val_loss: 0.3198 - val_acc: 0.8996\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8586\n",
      "Epoch 00052: val_loss improved from 0.31980 to 0.30965, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/052-0.3096.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4536 - acc: 0.8586 - val_loss: 0.3096 - val_acc: 0.9029\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8576\n",
      "Epoch 00053: val_loss improved from 0.30965 to 0.30866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/053-0.3087.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4499 - acc: 0.8577 - val_loss: 0.3087 - val_acc: 0.9078\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8605\n",
      "Epoch 00054: val_loss did not improve from 0.30866\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4491 - acc: 0.8605 - val_loss: 0.3112 - val_acc: 0.9064\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8610\n",
      "Epoch 00055: val_loss did not improve from 0.30866\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4430 - acc: 0.8610 - val_loss: 0.3267 - val_acc: 0.9001\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8590\n",
      "Epoch 00056: val_loss did not improve from 0.30866\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4443 - acc: 0.8590 - val_loss: 0.3123 - val_acc: 0.9066\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8623\n",
      "Epoch 00057: val_loss did not improve from 0.30866\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4375 - acc: 0.8623 - val_loss: 0.3108 - val_acc: 0.9052\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4353 - acc: 0.8635\n",
      "Epoch 00058: val_loss improved from 0.30866 to 0.30496, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/058-0.3050.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4353 - acc: 0.8635 - val_loss: 0.3050 - val_acc: 0.9071\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8617\n",
      "Epoch 00059: val_loss improved from 0.30496 to 0.30302, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/059-0.3030.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4313 - acc: 0.8617 - val_loss: 0.3030 - val_acc: 0.9071\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8660\n",
      "Epoch 00060: val_loss did not improve from 0.30302\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4258 - acc: 0.8660 - val_loss: 0.3056 - val_acc: 0.9033\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8684\n",
      "Epoch 00061: val_loss did not improve from 0.30302\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4233 - acc: 0.8684 - val_loss: 0.3047 - val_acc: 0.9068\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8669\n",
      "Epoch 00062: val_loss did not improve from 0.30302\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4213 - acc: 0.8669 - val_loss: 0.3082 - val_acc: 0.9033\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8683\n",
      "Epoch 00063: val_loss improved from 0.30302 to 0.29947, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/063-0.2995.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4186 - acc: 0.8683 - val_loss: 0.2995 - val_acc: 0.9110\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8678\n",
      "Epoch 00064: val_loss improved from 0.29947 to 0.29669, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/064-0.2967.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4177 - acc: 0.8678 - val_loss: 0.2967 - val_acc: 0.9110\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8706\n",
      "Epoch 00065: val_loss did not improve from 0.29669\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4135 - acc: 0.8706 - val_loss: 0.2997 - val_acc: 0.9082\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8729\n",
      "Epoch 00066: val_loss did not improve from 0.29669\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4082 - acc: 0.8729 - val_loss: 0.3037 - val_acc: 0.9066\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8716\n",
      "Epoch 00067: val_loss did not improve from 0.29669\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4114 - acc: 0.8716 - val_loss: 0.3046 - val_acc: 0.9038\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8745\n",
      "Epoch 00068: val_loss did not improve from 0.29669\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3943 - acc: 0.8745 - val_loss: 0.2995 - val_acc: 0.9073\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8733\n",
      "Epoch 00069: val_loss improved from 0.29669 to 0.29389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/069-0.2939.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3992 - acc: 0.8733 - val_loss: 0.2939 - val_acc: 0.9108\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8749\n",
      "Epoch 00070: val_loss improved from 0.29389 to 0.28936, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/070-0.2894.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3982 - acc: 0.8749 - val_loss: 0.2894 - val_acc: 0.9126\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8753\n",
      "Epoch 00071: val_loss did not improve from 0.28936\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3939 - acc: 0.8753 - val_loss: 0.3002 - val_acc: 0.9096\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8748\n",
      "Epoch 00072: val_loss did not improve from 0.28936\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3931 - acc: 0.8748 - val_loss: 0.2996 - val_acc: 0.9075\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8776\n",
      "Epoch 00073: val_loss improved from 0.28936 to 0.28761, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/073-0.2876.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3884 - acc: 0.8776 - val_loss: 0.2876 - val_acc: 0.9115\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8781\n",
      "Epoch 00074: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3814 - acc: 0.8781 - val_loss: 0.2877 - val_acc: 0.9119\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.8786\n",
      "Epoch 00075: val_loss did not improve from 0.28761\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3834 - acc: 0.8786 - val_loss: 0.2968 - val_acc: 0.9094\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8810\n",
      "Epoch 00076: val_loss improved from 0.28761 to 0.28124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/076-0.2812.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3737 - acc: 0.8810 - val_loss: 0.2812 - val_acc: 0.9161\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.8829\n",
      "Epoch 00077: val_loss did not improve from 0.28124\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3679 - acc: 0.8829 - val_loss: 0.2834 - val_acc: 0.9131\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8810\n",
      "Epoch 00078: val_loss did not improve from 0.28124\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3739 - acc: 0.8810 - val_loss: 0.2895 - val_acc: 0.9145\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8821\n",
      "Epoch 00079: val_loss did not improve from 0.28124\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3710 - acc: 0.8822 - val_loss: 0.2844 - val_acc: 0.9126\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8835\n",
      "Epoch 00080: val_loss did not improve from 0.28124\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3645 - acc: 0.8835 - val_loss: 0.2870 - val_acc: 0.9096\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3658 - acc: 0.8842\n",
      "Epoch 00081: val_loss improved from 0.28124 to 0.27994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/081-0.2799.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3658 - acc: 0.8842 - val_loss: 0.2799 - val_acc: 0.9145\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8838\n",
      "Epoch 00082: val_loss did not improve from 0.27994\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3575 - acc: 0.8837 - val_loss: 0.2906 - val_acc: 0.9115\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8863\n",
      "Epoch 00083: val_loss did not improve from 0.27994\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3593 - acc: 0.8863 - val_loss: 0.2824 - val_acc: 0.9129\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.8858\n",
      "Epoch 00084: val_loss improved from 0.27994 to 0.27670, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/084-0.2767.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3605 - acc: 0.8858 - val_loss: 0.2767 - val_acc: 0.9180\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8876\n",
      "Epoch 00085: val_loss did not improve from 0.27670\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3566 - acc: 0.8876 - val_loss: 0.2789 - val_acc: 0.9152\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8864\n",
      "Epoch 00086: val_loss improved from 0.27670 to 0.27407, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/086-0.2741.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3566 - acc: 0.8863 - val_loss: 0.2741 - val_acc: 0.9161\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8894\n",
      "Epoch 00087: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3448 - acc: 0.8894 - val_loss: 0.2816 - val_acc: 0.9140\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8887\n",
      "Epoch 00088: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3499 - acc: 0.8887 - val_loss: 0.2773 - val_acc: 0.9150\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8907\n",
      "Epoch 00089: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3435 - acc: 0.8907 - val_loss: 0.2776 - val_acc: 0.9119\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3413 - acc: 0.8921\n",
      "Epoch 00090: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3414 - acc: 0.8921 - val_loss: 0.2819 - val_acc: 0.9136\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8898\n",
      "Epoch 00091: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3451 - acc: 0.8899 - val_loss: 0.2917 - val_acc: 0.9066\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8923\n",
      "Epoch 00092: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3388 - acc: 0.8923 - val_loss: 0.2775 - val_acc: 0.9133\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8928\n",
      "Epoch 00093: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3392 - acc: 0.8928 - val_loss: 0.2922 - val_acc: 0.9108\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8940\n",
      "Epoch 00094: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3322 - acc: 0.8940 - val_loss: 0.2783 - val_acc: 0.9122\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8932\n",
      "Epoch 00095: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3314 - acc: 0.8932 - val_loss: 0.2892 - val_acc: 0.9096\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8939\n",
      "Epoch 00096: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3314 - acc: 0.8939 - val_loss: 0.2884 - val_acc: 0.9115\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8945\n",
      "Epoch 00097: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3338 - acc: 0.8946 - val_loss: 0.2948 - val_acc: 0.9106\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8977\n",
      "Epoch 00098: val_loss did not improve from 0.27407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3213 - acc: 0.8978 - val_loss: 0.2926 - val_acc: 0.9099\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8955\n",
      "Epoch 00099: val_loss improved from 0.27407 to 0.26995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/099-0.2700.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3227 - acc: 0.8955 - val_loss: 0.2700 - val_acc: 0.9154\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8949\n",
      "Epoch 00100: val_loss did not improve from 0.26995\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3258 - acc: 0.8949 - val_loss: 0.2721 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8973\n",
      "Epoch 00101: val_loss did not improve from 0.26995\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3158 - acc: 0.8973 - val_loss: 0.2770 - val_acc: 0.9140\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9001\n",
      "Epoch 00102: val_loss improved from 0.26995 to 0.26829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/102-0.2683.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3107 - acc: 0.9000 - val_loss: 0.2683 - val_acc: 0.9194\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.8964\n",
      "Epoch 00103: val_loss improved from 0.26829 to 0.26663, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/103-0.2666.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3204 - acc: 0.8965 - val_loss: 0.2666 - val_acc: 0.9196\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8985\n",
      "Epoch 00104: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3172 - acc: 0.8985 - val_loss: 0.2834 - val_acc: 0.9124\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.8980\n",
      "Epoch 00105: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3155 - acc: 0.8980 - val_loss: 0.2701 - val_acc: 0.9173\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8981\n",
      "Epoch 00106: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3119 - acc: 0.8981 - val_loss: 0.2781 - val_acc: 0.9140\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9002\n",
      "Epoch 00107: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3128 - acc: 0.9003 - val_loss: 0.2693 - val_acc: 0.9192\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9014\n",
      "Epoch 00108: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3058 - acc: 0.9013 - val_loss: 0.2687 - val_acc: 0.9201\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9027\n",
      "Epoch 00109: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3055 - acc: 0.9027 - val_loss: 0.2715 - val_acc: 0.9196\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9034\n",
      "Epoch 00110: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3000 - acc: 0.9034 - val_loss: 0.2845 - val_acc: 0.9124\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9049\n",
      "Epoch 00111: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2967 - acc: 0.9048 - val_loss: 0.2763 - val_acc: 0.9140\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3006 - acc: 0.9017\n",
      "Epoch 00112: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3006 - acc: 0.9018 - val_loss: 0.2768 - val_acc: 0.9175\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9047\n",
      "Epoch 00113: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2958 - acc: 0.9047 - val_loss: 0.2741 - val_acc: 0.9182\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9067\n",
      "Epoch 00114: val_loss did not improve from 0.26663\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2902 - acc: 0.9067 - val_loss: 0.2765 - val_acc: 0.9154\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9051\n",
      "Epoch 00115: val_loss improved from 0.26663 to 0.26548, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv_checkpoint/115-0.2655.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2978 - acc: 0.9051 - val_loss: 0.2655 - val_acc: 0.9189\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9051\n",
      "Epoch 00116: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2943 - acc: 0.9050 - val_loss: 0.2733 - val_acc: 0.9171\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9051\n",
      "Epoch 00117: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2914 - acc: 0.9051 - val_loss: 0.2825 - val_acc: 0.9145\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9065\n",
      "Epoch 00118: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2870 - acc: 0.9065 - val_loss: 0.2710 - val_acc: 0.9180\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9061\n",
      "Epoch 00119: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2905 - acc: 0.9061 - val_loss: 0.2750 - val_acc: 0.9152\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9078\n",
      "Epoch 00120: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2847 - acc: 0.9078 - val_loss: 0.2679 - val_acc: 0.9196\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9085\n",
      "Epoch 00121: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2845 - acc: 0.9085 - val_loss: 0.2788 - val_acc: 0.9138\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9082\n",
      "Epoch 00122: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2842 - acc: 0.9082 - val_loss: 0.2772 - val_acc: 0.9147\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9100\n",
      "Epoch 00123: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2786 - acc: 0.9100 - val_loss: 0.2761 - val_acc: 0.9157\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9076\n",
      "Epoch 00124: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2816 - acc: 0.9076 - val_loss: 0.2779 - val_acc: 0.9159\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9097\n",
      "Epoch 00125: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2776 - acc: 0.9097 - val_loss: 0.2721 - val_acc: 0.9173\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.9111\n",
      "Epoch 00126: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2730 - acc: 0.9111 - val_loss: 0.2695 - val_acc: 0.9201\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9108\n",
      "Epoch 00127: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2724 - acc: 0.9108 - val_loss: 0.2801 - val_acc: 0.9178\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9111\n",
      "Epoch 00128: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2742 - acc: 0.9111 - val_loss: 0.2830 - val_acc: 0.9173\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9117\n",
      "Epoch 00129: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2701 - acc: 0.9117 - val_loss: 0.2661 - val_acc: 0.9217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9134\n",
      "Epoch 00130: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2644 - acc: 0.9134 - val_loss: 0.2776 - val_acc: 0.9161\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9130\n",
      "Epoch 00131: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2679 - acc: 0.9130 - val_loss: 0.2698 - val_acc: 0.9217\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9125\n",
      "Epoch 00132: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2647 - acc: 0.9125 - val_loss: 0.2803 - val_acc: 0.9145\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9132\n",
      "Epoch 00133: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2655 - acc: 0.9132 - val_loss: 0.2752 - val_acc: 0.9194\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9158\n",
      "Epoch 00134: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2633 - acc: 0.9158 - val_loss: 0.2741 - val_acc: 0.9194\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9144\n",
      "Epoch 00135: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2654 - acc: 0.9144 - val_loss: 0.2706 - val_acc: 0.9189\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9136\n",
      "Epoch 00136: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2655 - acc: 0.9136 - val_loss: 0.2832 - val_acc: 0.9129\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9168\n",
      "Epoch 00137: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2614 - acc: 0.9168 - val_loss: 0.2764 - val_acc: 0.9168\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9151\n",
      "Epoch 00138: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2587 - acc: 0.9151 - val_loss: 0.2811 - val_acc: 0.9187\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9162\n",
      "Epoch 00139: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2533 - acc: 0.9162 - val_loss: 0.2720 - val_acc: 0.9189\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9157\n",
      "Epoch 00140: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2566 - acc: 0.9157 - val_loss: 0.2776 - val_acc: 0.9171\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9153\n",
      "Epoch 00141: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2536 - acc: 0.9153 - val_loss: 0.2823 - val_acc: 0.9173\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9187\n",
      "Epoch 00142: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2529 - acc: 0.9188 - val_loss: 0.2857 - val_acc: 0.9131\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9158\n",
      "Epoch 00143: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2521 - acc: 0.9158 - val_loss: 0.2797 - val_acc: 0.9180\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9193\n",
      "Epoch 00144: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2466 - acc: 0.9194 - val_loss: 0.2805 - val_acc: 0.9189\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9167\n",
      "Epoch 00145: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2507 - acc: 0.9167 - val_loss: 0.2848 - val_acc: 0.9154\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9184\n",
      "Epoch 00146: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2478 - acc: 0.9184 - val_loss: 0.2787 - val_acc: 0.9182\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9210\n",
      "Epoch 00147: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2407 - acc: 0.9210 - val_loss: 0.2843 - val_acc: 0.9157\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9205\n",
      "Epoch 00148: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2448 - acc: 0.9205 - val_loss: 0.2809 - val_acc: 0.9152\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9208\n",
      "Epoch 00149: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2441 - acc: 0.9208 - val_loss: 0.2821 - val_acc: 0.9187\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9198\n",
      "Epoch 00150: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2458 - acc: 0.9198 - val_loss: 0.2745 - val_acc: 0.9178\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9214\n",
      "Epoch 00151: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2431 - acc: 0.9214 - val_loss: 0.2806 - val_acc: 0.9159\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9233\n",
      "Epoch 00152: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2375 - acc: 0.9233 - val_loss: 0.2869 - val_acc: 0.9192\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9222\n",
      "Epoch 00153: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2396 - acc: 0.9222 - val_loss: 0.2851 - val_acc: 0.9171\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9239\n",
      "Epoch 00154: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2333 - acc: 0.9239 - val_loss: 0.2980 - val_acc: 0.9136\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9214\n",
      "Epoch 00155: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2392 - acc: 0.9214 - val_loss: 0.2813 - val_acc: 0.9157\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9227\n",
      "Epoch 00156: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2344 - acc: 0.9227 - val_loss: 0.2820 - val_acc: 0.9192\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9243\n",
      "Epoch 00157: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2309 - acc: 0.9243 - val_loss: 0.2926 - val_acc: 0.9159\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9233\n",
      "Epoch 00158: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2338 - acc: 0.9233 - val_loss: 0.2808 - val_acc: 0.9157\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9239\n",
      "Epoch 00159: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2344 - acc: 0.9239 - val_loss: 0.2843 - val_acc: 0.9166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9241\n",
      "Epoch 00160: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2316 - acc: 0.9241 - val_loss: 0.2759 - val_acc: 0.9175\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9240\n",
      "Epoch 00161: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2320 - acc: 0.9240 - val_loss: 0.2837 - val_acc: 0.9164\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9260\n",
      "Epoch 00162: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2266 - acc: 0.9260 - val_loss: 0.2883 - val_acc: 0.9180\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9261\n",
      "Epoch 00163: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2244 - acc: 0.9261 - val_loss: 0.2764 - val_acc: 0.9210\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9267\n",
      "Epoch 00164: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2243 - acc: 0.9266 - val_loss: 0.2917 - val_acc: 0.9175\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9263\n",
      "Epoch 00165: val_loss did not improve from 0.26548\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2277 - acc: 0.9263 - val_loss: 0.2885 - val_acc: 0.9217\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 677us/sample - loss: 0.3335 - acc: 0.8989\n",
      "Loss: 0.3334528922725194 Accuracy: 0.8988577\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1821 - acc: 0.2801\n",
      "Epoch 00001: val_loss improved from inf to 1.52750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/001-1.5275.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 2.1820 - acc: 0.2801 - val_loss: 1.5275 - val_acc: 0.5430\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5209 - acc: 0.5050\n",
      "Epoch 00002: val_loss improved from 1.52750 to 1.03462, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/002-1.0346.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.5209 - acc: 0.5050 - val_loss: 1.0346 - val_acc: 0.6979\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6238\n",
      "Epoch 00003: val_loss improved from 1.03462 to 0.76647, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/003-0.7665.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.1687 - acc: 0.6238 - val_loss: 0.7665 - val_acc: 0.7768\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.7007\n",
      "Epoch 00004: val_loss improved from 0.76647 to 0.64916, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/004-0.6492.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9408 - acc: 0.7007 - val_loss: 0.6492 - val_acc: 0.8053\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.7448\n",
      "Epoch 00005: val_loss improved from 0.64916 to 0.51233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/005-0.5123.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8062 - acc: 0.7448 - val_loss: 0.5123 - val_acc: 0.8512\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.7817\n",
      "Epoch 00006: val_loss improved from 0.51233 to 0.44147, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/006-0.4415.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7003 - acc: 0.7818 - val_loss: 0.4415 - val_acc: 0.8707\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6365 - acc: 0.8010\n",
      "Epoch 00007: val_loss improved from 0.44147 to 0.42247, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/007-0.4225.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6365 - acc: 0.8010 - val_loss: 0.4225 - val_acc: 0.8758\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8145\n",
      "Epoch 00008: val_loss improved from 0.42247 to 0.40159, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/008-0.4016.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5855 - acc: 0.8145 - val_loss: 0.4016 - val_acc: 0.8772\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8314\n",
      "Epoch 00009: val_loss improved from 0.40159 to 0.35026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/009-0.3503.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5392 - acc: 0.8314 - val_loss: 0.3503 - val_acc: 0.8970\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8431\n",
      "Epoch 00010: val_loss improved from 0.35026 to 0.32345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/010-0.3234.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5014 - acc: 0.8431 - val_loss: 0.3234 - val_acc: 0.9064\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4739 - acc: 0.8532\n",
      "Epoch 00011: val_loss improved from 0.32345 to 0.29361, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/011-0.2936.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4739 - acc: 0.8532 - val_loss: 0.2936 - val_acc: 0.9124\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8616\n",
      "Epoch 00012: val_loss improved from 0.29361 to 0.27570, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/012-0.2757.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4489 - acc: 0.8616 - val_loss: 0.2757 - val_acc: 0.9180\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8677\n",
      "Epoch 00013: val_loss did not improve from 0.27570\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4265 - acc: 0.8677 - val_loss: 0.2940 - val_acc: 0.9106\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4126 - acc: 0.8720\n",
      "Epoch 00014: val_loss improved from 0.27570 to 0.26317, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/014-0.2632.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4127 - acc: 0.8720 - val_loss: 0.2632 - val_acc: 0.9257\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8754\n",
      "Epoch 00015: val_loss did not improve from 0.26317\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3963 - acc: 0.8754 - val_loss: 0.2875 - val_acc: 0.9117\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8796\n",
      "Epoch 00016: val_loss did not improve from 0.26317\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3838 - acc: 0.8796 - val_loss: 0.2765 - val_acc: 0.9194\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8848\n",
      "Epoch 00017: val_loss improved from 0.26317 to 0.24437, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/017-0.2444.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3664 - acc: 0.8848 - val_loss: 0.2444 - val_acc: 0.9245\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8902\n",
      "Epoch 00018: val_loss improved from 0.24437 to 0.23671, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/018-0.2367.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3479 - acc: 0.8902 - val_loss: 0.2367 - val_acc: 0.9280\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8929\n",
      "Epoch 00019: val_loss improved from 0.23671 to 0.23003, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/019-0.2300.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3453 - acc: 0.8929 - val_loss: 0.2300 - val_acc: 0.9334\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8986\n",
      "Epoch 00020: val_loss did not improve from 0.23003\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3289 - acc: 0.8986 - val_loss: 0.2371 - val_acc: 0.9262\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9001\n",
      "Epoch 00021: val_loss improved from 0.23003 to 0.21995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/021-0.2200.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3221 - acc: 0.9001 - val_loss: 0.2200 - val_acc: 0.9352\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9034\n",
      "Epoch 00022: val_loss improved from 0.21995 to 0.21212, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/022-0.2121.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3121 - acc: 0.9034 - val_loss: 0.2121 - val_acc: 0.9385\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9055\n",
      "Epoch 00023: val_loss improved from 0.21212 to 0.21057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/023-0.2106.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2987 - acc: 0.9055 - val_loss: 0.2106 - val_acc: 0.9373\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.9074\n",
      "Epoch 00024: val_loss did not improve from 0.21057\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2945 - acc: 0.9074 - val_loss: 0.2110 - val_acc: 0.9387\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9088\n",
      "Epoch 00025: val_loss did not improve from 0.21057\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2931 - acc: 0.9088 - val_loss: 0.2126 - val_acc: 0.9327\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9117\n",
      "Epoch 00026: val_loss improved from 0.21057 to 0.19812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/026-0.1981.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2807 - acc: 0.9116 - val_loss: 0.1981 - val_acc: 0.9387\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9146\n",
      "Epoch 00027: val_loss improved from 0.19812 to 0.19799, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/027-0.1980.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2748 - acc: 0.9146 - val_loss: 0.1980 - val_acc: 0.9376\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9166\n",
      "Epoch 00028: val_loss improved from 0.19799 to 0.19285, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/028-0.1928.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2654 - acc: 0.9166 - val_loss: 0.1928 - val_acc: 0.9399\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9186\n",
      "Epoch 00029: val_loss improved from 0.19285 to 0.19134, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/029-0.1913.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2569 - acc: 0.9186 - val_loss: 0.1913 - val_acc: 0.9408\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9228\n",
      "Epoch 00030: val_loss improved from 0.19134 to 0.19071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/030-0.1907.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2489 - acc: 0.9228 - val_loss: 0.1907 - val_acc: 0.9399\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9199\n",
      "Epoch 00031: val_loss improved from 0.19071 to 0.18235, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/031-0.1823.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2502 - acc: 0.9199 - val_loss: 0.1823 - val_acc: 0.9432\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9224\n",
      "Epoch 00032: val_loss improved from 0.18235 to 0.18145, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/032-0.1815.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2422 - acc: 0.9224 - val_loss: 0.1815 - val_acc: 0.9455\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.9253\n",
      "Epoch 00033: val_loss improved from 0.18145 to 0.17961, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/033-0.1796.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2351 - acc: 0.9253 - val_loss: 0.1796 - val_acc: 0.9450\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9265\n",
      "Epoch 00034: val_loss did not improve from 0.17961\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2276 - acc: 0.9266 - val_loss: 0.1857 - val_acc: 0.9422\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9292\n",
      "Epoch 00035: val_loss improved from 0.17961 to 0.17668, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/035-0.1767.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2227 - acc: 0.9292 - val_loss: 0.1767 - val_acc: 0.9455\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9291\n",
      "Epoch 00036: val_loss improved from 0.17668 to 0.17658, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/036-0.1766.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2243 - acc: 0.9291 - val_loss: 0.1766 - val_acc: 0.9453\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9310\n",
      "Epoch 00037: val_loss did not improve from 0.17658\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2189 - acc: 0.9310 - val_loss: 0.1776 - val_acc: 0.9476\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9337\n",
      "Epoch 00038: val_loss improved from 0.17658 to 0.17464, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/038-0.1746.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2113 - acc: 0.9337 - val_loss: 0.1746 - val_acc: 0.9464\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9341\n",
      "Epoch 00039: val_loss did not improve from 0.17464\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2070 - acc: 0.9341 - val_loss: 0.1767 - val_acc: 0.9443\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9350\n",
      "Epoch 00040: val_loss improved from 0.17464 to 0.17300, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/040-0.1730.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2038 - acc: 0.9350 - val_loss: 0.1730 - val_acc: 0.9478\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9377\n",
      "Epoch 00041: val_loss improved from 0.17300 to 0.16888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/041-0.1689.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1958 - acc: 0.9377 - val_loss: 0.1689 - val_acc: 0.9490\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9393\n",
      "Epoch 00042: val_loss improved from 0.16888 to 0.16474, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/042-0.1647.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1901 - acc: 0.9393 - val_loss: 0.1647 - val_acc: 0.9513\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9386\n",
      "Epoch 00043: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1944 - acc: 0.9386 - val_loss: 0.1781 - val_acc: 0.9455\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9408\n",
      "Epoch 00044: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1866 - acc: 0.9408 - val_loss: 0.1872 - val_acc: 0.9460\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9409\n",
      "Epoch 00045: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1827 - acc: 0.9409 - val_loss: 0.1769 - val_acc: 0.9457\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9422\n",
      "Epoch 00046: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1776 - acc: 0.9422 - val_loss: 0.1784 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9427\n",
      "Epoch 00047: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1759 - acc: 0.9427 - val_loss: 0.1713 - val_acc: 0.9481\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9441\n",
      "Epoch 00048: val_loss did not improve from 0.16474\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1723 - acc: 0.9441 - val_loss: 0.1690 - val_acc: 0.9495\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9461\n",
      "Epoch 00049: val_loss improved from 0.16474 to 0.16358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/049-0.1636.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1684 - acc: 0.9461 - val_loss: 0.1636 - val_acc: 0.9520\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9449\n",
      "Epoch 00050: val_loss improved from 0.16358 to 0.16351, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv_checkpoint/050-0.1635.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1705 - acc: 0.9449 - val_loss: 0.1635 - val_acc: 0.9518\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9468\n",
      "Epoch 00051: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1638 - acc: 0.9468 - val_loss: 0.1682 - val_acc: 0.9504\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9494\n",
      "Epoch 00052: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1589 - acc: 0.9494 - val_loss: 0.1674 - val_acc: 0.9481\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9495\n",
      "Epoch 00053: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1554 - acc: 0.9495 - val_loss: 0.1648 - val_acc: 0.9492\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9492\n",
      "Epoch 00054: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1551 - acc: 0.9492 - val_loss: 0.1711 - val_acc: 0.9495\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9500\n",
      "Epoch 00055: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1524 - acc: 0.9500 - val_loss: 0.1647 - val_acc: 0.9515\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9533\n",
      "Epoch 00056: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1482 - acc: 0.9533 - val_loss: 0.1691 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9532\n",
      "Epoch 00057: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1449 - acc: 0.9532 - val_loss: 0.1735 - val_acc: 0.9495\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9538\n",
      "Epoch 00058: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1402 - acc: 0.9538 - val_loss: 0.1723 - val_acc: 0.9513\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9540\n",
      "Epoch 00059: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1415 - acc: 0.9541 - val_loss: 0.1781 - val_acc: 0.9504\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9562\n",
      "Epoch 00060: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1366 - acc: 0.9562 - val_loss: 0.1822 - val_acc: 0.9450\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9558\n",
      "Epoch 00061: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1349 - acc: 0.9558 - val_loss: 0.1680 - val_acc: 0.9492\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9571\n",
      "Epoch 00062: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1309 - acc: 0.9571 - val_loss: 0.1814 - val_acc: 0.9464\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9562\n",
      "Epoch 00063: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1319 - acc: 0.9562 - val_loss: 0.1731 - val_acc: 0.9495\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9574\n",
      "Epoch 00064: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1281 - acc: 0.9575 - val_loss: 0.1833 - val_acc: 0.9448\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9608\n",
      "Epoch 00065: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1211 - acc: 0.9608 - val_loss: 0.1778 - val_acc: 0.9490\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9582\n",
      "Epoch 00066: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1264 - acc: 0.9581 - val_loss: 0.1698 - val_acc: 0.9515\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9609\n",
      "Epoch 00067: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1196 - acc: 0.9609 - val_loss: 0.1777 - val_acc: 0.9497\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9614\n",
      "Epoch 00068: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1169 - acc: 0.9614 - val_loss: 0.1779 - val_acc: 0.9492\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9615\n",
      "Epoch 00069: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1184 - acc: 0.9615 - val_loss: 0.1738 - val_acc: 0.9506\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9643\n",
      "Epoch 00070: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1129 - acc: 0.9643 - val_loss: 0.1775 - val_acc: 0.9483\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9623\n",
      "Epoch 00071: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1134 - acc: 0.9623 - val_loss: 0.1789 - val_acc: 0.9469\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9628\n",
      "Epoch 00072: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1120 - acc: 0.9628 - val_loss: 0.1715 - val_acc: 0.9536\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9655\n",
      "Epoch 00073: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1054 - acc: 0.9655 - val_loss: 0.1725 - val_acc: 0.9520\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9651\n",
      "Epoch 00074: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1054 - acc: 0.9651 - val_loss: 0.1817 - val_acc: 0.9481\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9651\n",
      "Epoch 00075: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1037 - acc: 0.9650 - val_loss: 0.1934 - val_acc: 0.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9648\n",
      "Epoch 00076: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1087 - acc: 0.9648 - val_loss: 0.1845 - val_acc: 0.9509\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9682\n",
      "Epoch 00077: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0962 - acc: 0.9682 - val_loss: 0.1980 - val_acc: 0.9450\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9670\n",
      "Epoch 00078: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1015 - acc: 0.9670 - val_loss: 0.1740 - val_acc: 0.9504\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9691\n",
      "Epoch 00079: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0940 - acc: 0.9691 - val_loss: 0.1921 - val_acc: 0.9497\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9688\n",
      "Epoch 00080: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0911 - acc: 0.9688 - val_loss: 0.1868 - val_acc: 0.9497\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9692\n",
      "Epoch 00081: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0932 - acc: 0.9692 - val_loss: 0.1873 - val_acc: 0.9509\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9704\n",
      "Epoch 00082: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0898 - acc: 0.9704 - val_loss: 0.1854 - val_acc: 0.9509\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9697\n",
      "Epoch 00083: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0896 - acc: 0.9697 - val_loss: 0.1784 - val_acc: 0.9525\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9701\n",
      "Epoch 00084: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0883 - acc: 0.9701 - val_loss: 0.1804 - val_acc: 0.9497\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9709\n",
      "Epoch 00085: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0855 - acc: 0.9709 - val_loss: 0.1889 - val_acc: 0.9485\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9710\n",
      "Epoch 00086: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0849 - acc: 0.9710 - val_loss: 0.1888 - val_acc: 0.9509\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9717\n",
      "Epoch 00087: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0836 - acc: 0.9717 - val_loss: 0.1781 - val_acc: 0.9532\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9723\n",
      "Epoch 00088: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0841 - acc: 0.9723 - val_loss: 0.2025 - val_acc: 0.9446\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9726\n",
      "Epoch 00089: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0834 - acc: 0.9726 - val_loss: 0.1976 - val_acc: 0.9495\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9739\n",
      "Epoch 00090: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0805 - acc: 0.9739 - val_loss: 0.1938 - val_acc: 0.9485\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9721\n",
      "Epoch 00091: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0829 - acc: 0.9721 - val_loss: 0.2014 - val_acc: 0.9497\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9742\n",
      "Epoch 00092: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0774 - acc: 0.9742 - val_loss: 0.1868 - val_acc: 0.9509\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9745\n",
      "Epoch 00093: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0783 - acc: 0.9745 - val_loss: 0.2037 - val_acc: 0.9457\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9751\n",
      "Epoch 00094: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0763 - acc: 0.9751 - val_loss: 0.2003 - val_acc: 0.9481\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9737\n",
      "Epoch 00095: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0766 - acc: 0.9737 - val_loss: 0.1880 - val_acc: 0.9497\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9748\n",
      "Epoch 00096: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0738 - acc: 0.9748 - val_loss: 0.1998 - val_acc: 0.9490\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9770\n",
      "Epoch 00097: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0702 - acc: 0.9770 - val_loss: 0.2044 - val_acc: 0.9462\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9752\n",
      "Epoch 00098: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0736 - acc: 0.9752 - val_loss: 0.2070 - val_acc: 0.9490\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9766\n",
      "Epoch 00099: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0698 - acc: 0.9766 - val_loss: 0.1978 - val_acc: 0.9492\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9765\n",
      "Epoch 00100: val_loss did not improve from 0.16351\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0723 - acc: 0.9765 - val_loss: 0.2043 - val_acc: 0.9488\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 710us/sample - loss: 0.2227 - acc: 0.9298\n",
      "Loss: 0.22265147672635371 Accuracy: 0.9298027\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9215 - acc: 0.3730\n",
      "Epoch 00001: val_loss improved from inf to 1.02986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/001-1.0299.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 1.9214 - acc: 0.3731 - val_loss: 1.0299 - val_acc: 0.7102\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0451 - acc: 0.6673\n",
      "Epoch 00002: val_loss improved from 1.02986 to 0.66868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/002-0.6687.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0450 - acc: 0.6674 - val_loss: 0.6687 - val_acc: 0.8123\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7565 - acc: 0.7610\n",
      "Epoch 00003: val_loss improved from 0.66868 to 0.47660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/003-0.4766.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7567 - acc: 0.7610 - val_loss: 0.4766 - val_acc: 0.8558\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.8119\n",
      "Epoch 00004: val_loss improved from 0.47660 to 0.39899, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/004-0.3990.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6007 - acc: 0.8118 - val_loss: 0.3990 - val_acc: 0.8882\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8389\n",
      "Epoch 00005: val_loss improved from 0.39899 to 0.33272, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/005-0.3327.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5131 - acc: 0.8388 - val_loss: 0.3327 - val_acc: 0.8982\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.8564\n",
      "Epoch 00006: val_loss improved from 0.33272 to 0.27182, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/006-0.2718.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4531 - acc: 0.8564 - val_loss: 0.2718 - val_acc: 0.9257\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8720\n",
      "Epoch 00007: val_loss improved from 0.27182 to 0.25361, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/007-0.2536.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4097 - acc: 0.8720 - val_loss: 0.2536 - val_acc: 0.9243\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8821\n",
      "Epoch 00008: val_loss improved from 0.25361 to 0.21651, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/008-0.2165.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3752 - acc: 0.8821 - val_loss: 0.2165 - val_acc: 0.9345\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8908\n",
      "Epoch 00009: val_loss improved from 0.21651 to 0.20099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/009-0.2010.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3460 - acc: 0.8907 - val_loss: 0.2010 - val_acc: 0.9392\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.8970\n",
      "Epoch 00010: val_loss improved from 0.20099 to 0.19141, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/010-0.1914.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3283 - acc: 0.8970 - val_loss: 0.1914 - val_acc: 0.9427\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9039\n",
      "Epoch 00011: val_loss improved from 0.19141 to 0.18184, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/011-0.1818.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3087 - acc: 0.9040 - val_loss: 0.1818 - val_acc: 0.9453\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9087\n",
      "Epoch 00012: val_loss did not improve from 0.18184\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2898 - acc: 0.9087 - val_loss: 0.1944 - val_acc: 0.9415\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9151\n",
      "Epoch 00013: val_loss improved from 0.18184 to 0.16946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/013-0.1695.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2698 - acc: 0.9150 - val_loss: 0.1695 - val_acc: 0.9504\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9182\n",
      "Epoch 00014: val_loss improved from 0.16946 to 0.15691, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/014-0.1569.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2590 - acc: 0.9182 - val_loss: 0.1569 - val_acc: 0.9520\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9212\n",
      "Epoch 00015: val_loss improved from 0.15691 to 0.15113, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/015-0.1511.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2468 - acc: 0.9212 - val_loss: 0.1511 - val_acc: 0.9550\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9268\n",
      "Epoch 00016: val_loss improved from 0.15113 to 0.15079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/016-0.1508.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2310 - acc: 0.9268 - val_loss: 0.1508 - val_acc: 0.9525\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9298\n",
      "Epoch 00017: val_loss did not improve from 0.15079\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2226 - acc: 0.9298 - val_loss: 0.1601 - val_acc: 0.9474\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9296\n",
      "Epoch 00018: val_loss improved from 0.15079 to 0.14436, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/018-0.1444.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2194 - acc: 0.9296 - val_loss: 0.1444 - val_acc: 0.9550\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9348\n",
      "Epoch 00019: val_loss improved from 0.14436 to 0.12883, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/019-0.1288.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2073 - acc: 0.9348 - val_loss: 0.1288 - val_acc: 0.9616\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9362\n",
      "Epoch 00020: val_loss did not improve from 0.12883\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1986 - acc: 0.9362 - val_loss: 0.1594 - val_acc: 0.9518\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9393\n",
      "Epoch 00021: val_loss did not improve from 0.12883\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1920 - acc: 0.9393 - val_loss: 0.1346 - val_acc: 0.9599\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9423\n",
      "Epoch 00022: val_loss did not improve from 0.12883\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1838 - acc: 0.9423 - val_loss: 0.1433 - val_acc: 0.9562\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9436\n",
      "Epoch 00023: val_loss did not improve from 0.12883\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1747 - acc: 0.9436 - val_loss: 0.1342 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9470\n",
      "Epoch 00024: val_loss improved from 0.12883 to 0.12161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/024-0.1216.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1689 - acc: 0.9469 - val_loss: 0.1216 - val_acc: 0.9641\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9470\n",
      "Epoch 00025: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1649 - acc: 0.9470 - val_loss: 0.1304 - val_acc: 0.9634\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9515\n",
      "Epoch 00026: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1573 - acc: 0.9516 - val_loss: 0.1398 - val_acc: 0.9574\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9498\n",
      "Epoch 00027: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1526 - acc: 0.9498 - val_loss: 0.1243 - val_acc: 0.9634\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9530\n",
      "Epoch 00028: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1472 - acc: 0.9530 - val_loss: 0.1243 - val_acc: 0.9595\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9550\n",
      "Epoch 00029: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1434 - acc: 0.9550 - val_loss: 0.1256 - val_acc: 0.9602\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9578\n",
      "Epoch 00030: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1355 - acc: 0.9578 - val_loss: 0.1265 - val_acc: 0.9578\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9553\n",
      "Epoch 00031: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1363 - acc: 0.9553 - val_loss: 0.1334 - val_acc: 0.9576\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9599\n",
      "Epoch 00032: val_loss did not improve from 0.12161\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1256 - acc: 0.9599 - val_loss: 0.1218 - val_acc: 0.9613\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9588\n",
      "Epoch 00033: val_loss improved from 0.12161 to 0.12106, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/033-0.1211.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1253 - acc: 0.9588 - val_loss: 0.1211 - val_acc: 0.9613\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9607\n",
      "Epoch 00034: val_loss improved from 0.12106 to 0.11749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/034-0.1175.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1205 - acc: 0.9607 - val_loss: 0.1175 - val_acc: 0.9639\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9612\n",
      "Epoch 00035: val_loss did not improve from 0.11749\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1188 - acc: 0.9612 - val_loss: 0.1261 - val_acc: 0.9620\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9614\n",
      "Epoch 00036: val_loss improved from 0.11749 to 0.11506, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/036-0.1151.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1171 - acc: 0.9613 - val_loss: 0.1151 - val_acc: 0.9646\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9633\n",
      "Epoch 00037: val_loss improved from 0.11506 to 0.11035, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/037-0.1103.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1123 - acc: 0.9633 - val_loss: 0.1103 - val_acc: 0.9660\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9644\n",
      "Epoch 00038: val_loss did not improve from 0.11035\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1073 - acc: 0.9644 - val_loss: 0.1262 - val_acc: 0.9592\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9658\n",
      "Epoch 00039: val_loss did not improve from 0.11035\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1033 - acc: 0.9658 - val_loss: 0.1218 - val_acc: 0.9623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9660\n",
      "Epoch 00040: val_loss did not improve from 0.11035\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1038 - acc: 0.9660 - val_loss: 0.1185 - val_acc: 0.9630\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9678\n",
      "Epoch 00041: val_loss did not improve from 0.11035\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0983 - acc: 0.9678 - val_loss: 0.1140 - val_acc: 0.9632\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9675\n",
      "Epoch 00042: val_loss improved from 0.11035 to 0.10854, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/042-0.1085.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0959 - acc: 0.9675 - val_loss: 0.1085 - val_acc: 0.9660\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9709\n",
      "Epoch 00043: val_loss did not improve from 0.10854\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0913 - acc: 0.9709 - val_loss: 0.1117 - val_acc: 0.9637\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9712\n",
      "Epoch 00044: val_loss improved from 0.10854 to 0.10847, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/044-0.1085.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0882 - acc: 0.9712 - val_loss: 0.1085 - val_acc: 0.9660\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9713\n",
      "Epoch 00045: val_loss improved from 0.10847 to 0.10677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/045-0.1068.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0867 - acc: 0.9713 - val_loss: 0.1068 - val_acc: 0.9674\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9717\n",
      "Epoch 00046: val_loss did not improve from 0.10677\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0850 - acc: 0.9717 - val_loss: 0.1221 - val_acc: 0.9641\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9732\n",
      "Epoch 00047: val_loss improved from 0.10677 to 0.10579, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv_checkpoint/047-0.1058.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0802 - acc: 0.9732 - val_loss: 0.1058 - val_acc: 0.9676\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9726\n",
      "Epoch 00048: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0815 - acc: 0.9726 - val_loss: 0.1276 - val_acc: 0.9613\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9743\n",
      "Epoch 00049: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0747 - acc: 0.9743 - val_loss: 0.1208 - val_acc: 0.9627\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9745\n",
      "Epoch 00050: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0781 - acc: 0.9745 - val_loss: 0.1248 - val_acc: 0.9618\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9762\n",
      "Epoch 00051: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0694 - acc: 0.9763 - val_loss: 0.1173 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9762\n",
      "Epoch 00052: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0711 - acc: 0.9762 - val_loss: 0.1185 - val_acc: 0.9658\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9773\n",
      "Epoch 00053: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0679 - acc: 0.9773 - val_loss: 0.1149 - val_acc: 0.9658\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9785\n",
      "Epoch 00054: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0652 - acc: 0.9785 - val_loss: 0.1195 - val_acc: 0.9648\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9784\n",
      "Epoch 00055: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0645 - acc: 0.9784 - val_loss: 0.1185 - val_acc: 0.9655\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9795\n",
      "Epoch 00056: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0610 - acc: 0.9795 - val_loss: 0.1301 - val_acc: 0.9651\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9789\n",
      "Epoch 00057: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0597 - acc: 0.9789 - val_loss: 0.1149 - val_acc: 0.9658\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9782\n",
      "Epoch 00058: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0622 - acc: 0.9782 - val_loss: 0.1302 - val_acc: 0.9627\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9799\n",
      "Epoch 00059: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0582 - acc: 0.9799 - val_loss: 0.1197 - val_acc: 0.9655\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9800\n",
      "Epoch 00060: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0593 - acc: 0.9800 - val_loss: 0.1129 - val_acc: 0.9669\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9819\n",
      "Epoch 00061: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0537 - acc: 0.9819 - val_loss: 0.1219 - val_acc: 0.9634\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9824\n",
      "Epoch 00062: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0524 - acc: 0.9824 - val_loss: 0.1109 - val_acc: 0.9662\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9814\n",
      "Epoch 00063: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0545 - acc: 0.9814 - val_loss: 0.1302 - val_acc: 0.9646\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9827\n",
      "Epoch 00064: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0512 - acc: 0.9827 - val_loss: 0.1225 - val_acc: 0.9676\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9831\n",
      "Epoch 00065: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0498 - acc: 0.9831 - val_loss: 0.1229 - val_acc: 0.9655\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9826\n",
      "Epoch 00066: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0515 - acc: 0.9826 - val_loss: 0.1322 - val_acc: 0.9620\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9829\n",
      "Epoch 00067: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0505 - acc: 0.9829 - val_loss: 0.1241 - val_acc: 0.9669\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9854\n",
      "Epoch 00068: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0438 - acc: 0.9853 - val_loss: 0.1317 - val_acc: 0.9620\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9818\n",
      "Epoch 00069: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0534 - acc: 0.9818 - val_loss: 0.1150 - val_acc: 0.9669\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9857\n",
      "Epoch 00070: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0404 - acc: 0.9857 - val_loss: 0.1292 - val_acc: 0.9644\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9857\n",
      "Epoch 00071: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0420 - acc: 0.9857 - val_loss: 0.1227 - val_acc: 0.9667\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9857\n",
      "Epoch 00072: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0417 - acc: 0.9857 - val_loss: 0.1193 - val_acc: 0.9674\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9852\n",
      "Epoch 00073: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0436 - acc: 0.9852 - val_loss: 0.1246 - val_acc: 0.9651\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9863\n",
      "Epoch 00074: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0389 - acc: 0.9863 - val_loss: 0.1403 - val_acc: 0.9630\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9878\n",
      "Epoch 00075: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0378 - acc: 0.9878 - val_loss: 0.1344 - val_acc: 0.9658\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9864\n",
      "Epoch 00076: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0382 - acc: 0.9864 - val_loss: 0.1286 - val_acc: 0.9674\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9863\n",
      "Epoch 00077: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0413 - acc: 0.9863 - val_loss: 0.1332 - val_acc: 0.9655\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9890\n",
      "Epoch 00078: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0344 - acc: 0.9891 - val_loss: 0.1274 - val_acc: 0.9669\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9882\n",
      "Epoch 00079: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0361 - acc: 0.9882 - val_loss: 0.1447 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9876\n",
      "Epoch 00080: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0360 - acc: 0.9876 - val_loss: 0.1477 - val_acc: 0.9639\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9885\n",
      "Epoch 00081: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0354 - acc: 0.9885 - val_loss: 0.1599 - val_acc: 0.9620\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9887\n",
      "Epoch 00082: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0329 - acc: 0.9888 - val_loss: 0.1391 - val_acc: 0.9658\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00083: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.1454 - val_acc: 0.9653\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9897\n",
      "Epoch 00084: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0320 - acc: 0.9897 - val_loss: 0.1486 - val_acc: 0.9639\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9885\n",
      "Epoch 00085: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0347 - acc: 0.9885 - val_loss: 0.1454 - val_acc: 0.9665\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9898\n",
      "Epoch 00086: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0309 - acc: 0.9898 - val_loss: 0.1365 - val_acc: 0.9669\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9885\n",
      "Epoch 00087: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0334 - acc: 0.9885 - val_loss: 0.1366 - val_acc: 0.9660\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9900\n",
      "Epoch 00088: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0295 - acc: 0.9900 - val_loss: 0.1342 - val_acc: 0.9658\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9887\n",
      "Epoch 00089: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0323 - acc: 0.9888 - val_loss: 0.1413 - val_acc: 0.9695\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9910\n",
      "Epoch 00090: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0270 - acc: 0.9910 - val_loss: 0.1453 - val_acc: 0.9658\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9900\n",
      "Epoch 00091: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.1506 - val_acc: 0.9651\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9902\n",
      "Epoch 00092: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0295 - acc: 0.9902 - val_loss: 0.1349 - val_acc: 0.9672\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9904\n",
      "Epoch 00093: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0270 - acc: 0.9904 - val_loss: 0.1314 - val_acc: 0.9690\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9906\n",
      "Epoch 00094: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0287 - acc: 0.9906 - val_loss: 0.1475 - val_acc: 0.9660\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9901\n",
      "Epoch 00095: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0296 - acc: 0.9901 - val_loss: 0.1408 - val_acc: 0.9655\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9903\n",
      "Epoch 00096: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0294 - acc: 0.9903 - val_loss: 0.1564 - val_acc: 0.9634\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9914\n",
      "Epoch 00097: val_loss did not improve from 0.10579\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0256 - acc: 0.9914 - val_loss: 0.1565 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 737us/sample - loss: 0.1419 - acc: 0.9545\n",
      "Loss: 0.1418726274969979 Accuracy: 0.9545171\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6322 - acc: 0.4745\n",
      "Epoch 00001: val_loss improved from inf to 0.69649, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/001-0.6965.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.6321 - acc: 0.4746 - val_loss: 0.6965 - val_acc: 0.8048\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7278 - acc: 0.7728\n",
      "Epoch 00002: val_loss improved from 0.69649 to 0.37831, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/002-0.3783.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7278 - acc: 0.7728 - val_loss: 0.3783 - val_acc: 0.8966\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8364\n",
      "Epoch 00003: val_loss improved from 0.37831 to 0.30563, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/003-0.3056.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5216 - acc: 0.8364 - val_loss: 0.3056 - val_acc: 0.9140\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8643\n",
      "Epoch 00004: val_loss improved from 0.30563 to 0.23928, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/004-0.2393.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4302 - acc: 0.8643 - val_loss: 0.2393 - val_acc: 0.9248\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3609 - acc: 0.8867\n",
      "Epoch 00005: val_loss did not improve from 0.23928\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3609 - acc: 0.8867 - val_loss: 0.2431 - val_acc: 0.9245\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8985\n",
      "Epoch 00006: val_loss improved from 0.23928 to 0.19298, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/006-0.1930.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3234 - acc: 0.8985 - val_loss: 0.1930 - val_acc: 0.9436\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9086\n",
      "Epoch 00007: val_loss improved from 0.19298 to 0.16188, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/007-0.1619.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2923 - acc: 0.9085 - val_loss: 0.1619 - val_acc: 0.9518\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9158\n",
      "Epoch 00008: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2674 - acc: 0.9157 - val_loss: 0.1726 - val_acc: 0.9506\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9204\n",
      "Epoch 00009: val_loss did not improve from 0.16188\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2518 - acc: 0.9204 - val_loss: 0.1654 - val_acc: 0.9502\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9281\n",
      "Epoch 00010: val_loss improved from 0.16188 to 0.13571, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/010-0.1357.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2242 - acc: 0.9281 - val_loss: 0.1357 - val_acc: 0.9606\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9334\n",
      "Epoch 00011: val_loss improved from 0.13571 to 0.12705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/011-0.1271.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2071 - acc: 0.9334 - val_loss: 0.1271 - val_acc: 0.9620\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9369\n",
      "Epoch 00012: val_loss improved from 0.12705 to 0.12511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/012-0.1251.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1972 - acc: 0.9369 - val_loss: 0.1251 - val_acc: 0.9611\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9390\n",
      "Epoch 00013: val_loss did not improve from 0.12511\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1893 - acc: 0.9390 - val_loss: 0.1286 - val_acc: 0.9585\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9437\n",
      "Epoch 00014: val_loss improved from 0.12511 to 0.12170, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/014-0.1217.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1757 - acc: 0.9437 - val_loss: 0.1217 - val_acc: 0.9648\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9457\n",
      "Epoch 00015: val_loss improved from 0.12170 to 0.11945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/015-0.1195.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1628 - acc: 0.9457 - val_loss: 0.1195 - val_acc: 0.9618\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9512\n",
      "Epoch 00016: val_loss improved from 0.11945 to 0.10926, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/016-0.1093.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1504 - acc: 0.9512 - val_loss: 0.1093 - val_acc: 0.9653\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9516\n",
      "Epoch 00017: val_loss improved from 0.10926 to 0.10905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/017-0.1090.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1475 - acc: 0.9516 - val_loss: 0.1090 - val_acc: 0.9653\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9556\n",
      "Epoch 00018: val_loss improved from 0.10905 to 0.10822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/018-0.1082.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1388 - acc: 0.9556 - val_loss: 0.1082 - val_acc: 0.9662\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9579\n",
      "Epoch 00019: val_loss did not improve from 0.10822\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1277 - acc: 0.9579 - val_loss: 0.1137 - val_acc: 0.9644\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9590\n",
      "Epoch 00020: val_loss did not improve from 0.10822\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1237 - acc: 0.9590 - val_loss: 0.1142 - val_acc: 0.9655\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9615\n",
      "Epoch 00021: val_loss improved from 0.10822 to 0.10763, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/021-0.1076.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1186 - acc: 0.9615 - val_loss: 0.1076 - val_acc: 0.9665\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9629\n",
      "Epoch 00022: val_loss did not improve from 0.10763\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1131 - acc: 0.9629 - val_loss: 0.1088 - val_acc: 0.9672\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9665\n",
      "Epoch 00023: val_loss improved from 0.10763 to 0.10375, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/023-0.1038.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1045 - acc: 0.9665 - val_loss: 0.1038 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9666\n",
      "Epoch 00024: val_loss did not improve from 0.10375\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1022 - acc: 0.9666 - val_loss: 0.1117 - val_acc: 0.9655\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9680\n",
      "Epoch 00025: val_loss improved from 0.10375 to 0.10051, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/025-0.1005.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0966 - acc: 0.9680 - val_loss: 0.1005 - val_acc: 0.9713\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9699\n",
      "Epoch 00026: val_loss did not improve from 0.10051\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0906 - acc: 0.9699 - val_loss: 0.1013 - val_acc: 0.9686\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9696\n",
      "Epoch 00027: val_loss improved from 0.10051 to 0.09839, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/027-0.0984.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0887 - acc: 0.9697 - val_loss: 0.0984 - val_acc: 0.9690\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9733\n",
      "Epoch 00028: val_loss improved from 0.09839 to 0.09636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/028-0.0964.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0804 - acc: 0.9733 - val_loss: 0.0964 - val_acc: 0.9695\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9730\n",
      "Epoch 00029: val_loss improved from 0.09636 to 0.08741, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv_checkpoint/029-0.0874.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0798 - acc: 0.9730 - val_loss: 0.0874 - val_acc: 0.9723\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9750\n",
      "Epoch 00030: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0753 - acc: 0.9750 - val_loss: 0.1172 - val_acc: 0.9630\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9761\n",
      "Epoch 00031: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0731 - acc: 0.9761 - val_loss: 0.1121 - val_acc: 0.9706\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9756\n",
      "Epoch 00032: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0727 - acc: 0.9756 - val_loss: 0.0948 - val_acc: 0.9711\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9773\n",
      "Epoch 00033: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0666 - acc: 0.9773 - val_loss: 0.0987 - val_acc: 0.9709\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9780\n",
      "Epoch 00034: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0638 - acc: 0.9780 - val_loss: 0.1002 - val_acc: 0.9702\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9793\n",
      "Epoch 00035: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0605 - acc: 0.9794 - val_loss: 0.0919 - val_acc: 0.9718\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9794\n",
      "Epoch 00036: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0603 - acc: 0.9794 - val_loss: 0.0995 - val_acc: 0.9711\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9809\n",
      "Epoch 00037: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0584 - acc: 0.9809 - val_loss: 0.1102 - val_acc: 0.9702\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9820\n",
      "Epoch 00038: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0553 - acc: 0.9820 - val_loss: 0.1020 - val_acc: 0.9737\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9825\n",
      "Epoch 00039: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0525 - acc: 0.9825 - val_loss: 0.1080 - val_acc: 0.9693\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9820\n",
      "Epoch 00040: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0543 - acc: 0.9820 - val_loss: 0.1170 - val_acc: 0.9688\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9840\n",
      "Epoch 00041: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0468 - acc: 0.9840 - val_loss: 0.1022 - val_acc: 0.9676\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9847\n",
      "Epoch 00042: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0471 - acc: 0.9847 - val_loss: 0.1121 - val_acc: 0.9679\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9849\n",
      "Epoch 00043: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0445 - acc: 0.9849 - val_loss: 0.1086 - val_acc: 0.9704\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9860\n",
      "Epoch 00044: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0424 - acc: 0.9860 - val_loss: 0.1116 - val_acc: 0.9693\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9874\n",
      "Epoch 00045: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0393 - acc: 0.9874 - val_loss: 0.0939 - val_acc: 0.9734\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9861\n",
      "Epoch 00046: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0424 - acc: 0.9861 - val_loss: 0.1180 - val_acc: 0.9695\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9876\n",
      "Epoch 00047: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0382 - acc: 0.9876 - val_loss: 0.1182 - val_acc: 0.9672\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9876\n",
      "Epoch 00048: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0375 - acc: 0.9875 - val_loss: 0.1224 - val_acc: 0.9704\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9859\n",
      "Epoch 00049: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0408 - acc: 0.9859 - val_loss: 0.1107 - val_acc: 0.9716\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0348 - acc: 0.9887 - val_loss: 0.1322 - val_acc: 0.9679\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9870\n",
      "Epoch 00051: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0383 - acc: 0.9870 - val_loss: 0.1121 - val_acc: 0.9709\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9881\n",
      "Epoch 00052: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0342 - acc: 0.9881 - val_loss: 0.1038 - val_acc: 0.9741\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9896\n",
      "Epoch 00053: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0320 - acc: 0.9896 - val_loss: 0.1309 - val_acc: 0.9697\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9892\n",
      "Epoch 00054: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0327 - acc: 0.9892 - val_loss: 0.1094 - val_acc: 0.9720\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9905\n",
      "Epoch 00055: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0281 - acc: 0.9905 - val_loss: 0.1144 - val_acc: 0.9706\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9889\n",
      "Epoch 00056: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0331 - acc: 0.9889 - val_loss: 0.1027 - val_acc: 0.9723\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9902\n",
      "Epoch 00057: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0288 - acc: 0.9902 - val_loss: 0.1201 - val_acc: 0.9720\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9903\n",
      "Epoch 00058: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0300 - acc: 0.9903 - val_loss: 0.1344 - val_acc: 0.9665\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9908\n",
      "Epoch 00059: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0280 - acc: 0.9908 - val_loss: 0.1127 - val_acc: 0.9727\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9924\n",
      "Epoch 00060: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0235 - acc: 0.9924 - val_loss: 0.1498 - val_acc: 0.9690\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9908\n",
      "Epoch 00061: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0269 - acc: 0.9908 - val_loss: 0.1244 - val_acc: 0.9711\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9908\n",
      "Epoch 00062: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0272 - acc: 0.9908 - val_loss: 0.1141 - val_acc: 0.9732\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9911\n",
      "Epoch 00063: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0279 - acc: 0.9911 - val_loss: 0.1247 - val_acc: 0.9718\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9919\n",
      "Epoch 00064: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0249 - acc: 0.9919 - val_loss: 0.1324 - val_acc: 0.9727\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9921\n",
      "Epoch 00065: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0261 - acc: 0.9921 - val_loss: 0.1378 - val_acc: 0.9718\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9930\n",
      "Epoch 00066: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0212 - acc: 0.9930 - val_loss: 0.1322 - val_acc: 0.9713\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9933\n",
      "Epoch 00067: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0211 - acc: 0.9933 - val_loss: 0.1231 - val_acc: 0.9711\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9932\n",
      "Epoch 00068: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0212 - acc: 0.9932 - val_loss: 0.1336 - val_acc: 0.9683\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9932\n",
      "Epoch 00069: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0201 - acc: 0.9932 - val_loss: 0.1514 - val_acc: 0.9704\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 00070: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0190 - acc: 0.9940 - val_loss: 0.1364 - val_acc: 0.9716\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9931\n",
      "Epoch 00071: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0210 - acc: 0.9931 - val_loss: 0.1408 - val_acc: 0.9711\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0220 - acc: 0.9929 - val_loss: 0.1407 - val_acc: 0.9690\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9936\n",
      "Epoch 00073: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0195 - acc: 0.9936 - val_loss: 0.1537 - val_acc: 0.9697\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9931\n",
      "Epoch 00074: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0213 - acc: 0.9931 - val_loss: 0.1412 - val_acc: 0.9725\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9948\n",
      "Epoch 00075: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0155 - acc: 0.9948 - val_loss: 0.1221 - val_acc: 0.9734\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9943\n",
      "Epoch 00076: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0187 - acc: 0.9943 - val_loss: 0.1223 - val_acc: 0.9741\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9937\n",
      "Epoch 00077: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0191 - acc: 0.9937 - val_loss: 0.1282 - val_acc: 0.9723\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9940\n",
      "Epoch 00078: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0179 - acc: 0.9940 - val_loss: 0.1353 - val_acc: 0.9697\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9936\n",
      "Epoch 00079: val_loss did not improve from 0.08741\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0187 - acc: 0.9936 - val_loss: 0.1413 - val_acc: 0.9723\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 724us/sample - loss: 0.1401 - acc: 0.9556\n",
      "Loss: 0.14011576245089186 Accuracy: 0.95555556\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4874 - acc: 0.5238\n",
      "Epoch 00001: val_loss improved from inf to 0.63468, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/001-0.6347.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.4873 - acc: 0.5238 - val_loss: 0.6347 - val_acc: 0.8134\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6311 - acc: 0.8005\n",
      "Epoch 00002: val_loss improved from 0.63468 to 0.38305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/002-0.3830.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.6311 - acc: 0.8005 - val_loss: 0.3830 - val_acc: 0.8810\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4495 - acc: 0.8579\n",
      "Epoch 00003: val_loss improved from 0.38305 to 0.27817, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/003-0.2782.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.4494 - acc: 0.8579 - val_loss: 0.2782 - val_acc: 0.9129\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8863\n",
      "Epoch 00004: val_loss improved from 0.27817 to 0.25255, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/004-0.2526.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3632 - acc: 0.8862 - val_loss: 0.2526 - val_acc: 0.9227\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8994\n",
      "Epoch 00005: val_loss improved from 0.25255 to 0.19356, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/005-0.1936.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3158 - acc: 0.8994 - val_loss: 0.1936 - val_acc: 0.9378\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9127\n",
      "Epoch 00006: val_loss improved from 0.19356 to 0.18160, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/006-0.1816.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2722 - acc: 0.9127 - val_loss: 0.1816 - val_acc: 0.9394\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9244\n",
      "Epoch 00007: val_loss improved from 0.18160 to 0.14413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/007-0.1441.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2398 - acc: 0.9244 - val_loss: 0.1441 - val_acc: 0.9562\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9307\n",
      "Epoch 00008: val_loss did not improve from 0.14413\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2178 - acc: 0.9306 - val_loss: 0.1486 - val_acc: 0.9513\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9366\n",
      "Epoch 00009: val_loss did not improve from 0.14413\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2010 - acc: 0.9366 - val_loss: 0.1491 - val_acc: 0.9555\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9416\n",
      "Epoch 00010: val_loss improved from 0.14413 to 0.12563, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/010-0.1256.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1825 - acc: 0.9416 - val_loss: 0.1256 - val_acc: 0.9620\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9465\n",
      "Epoch 00011: val_loss did not improve from 0.12563\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1640 - acc: 0.9465 - val_loss: 0.1356 - val_acc: 0.9569\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9503\n",
      "Epoch 00012: val_loss did not improve from 0.12563\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1523 - acc: 0.9502 - val_loss: 0.2325 - val_acc: 0.9294\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9519\n",
      "Epoch 00013: val_loss did not improve from 0.12563\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1503 - acc: 0.9519 - val_loss: 0.1297 - val_acc: 0.9623\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9585\n",
      "Epoch 00014: val_loss improved from 0.12563 to 0.11360, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/014-0.1136.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1280 - acc: 0.9585 - val_loss: 0.1136 - val_acc: 0.9651\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9611\n",
      "Epoch 00015: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1182 - acc: 0.9611 - val_loss: 0.1414 - val_acc: 0.9550\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9633\n",
      "Epoch 00016: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1134 - acc: 0.9633 - val_loss: 0.1191 - val_acc: 0.9634\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9646\n",
      "Epoch 00017: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1060 - acc: 0.9646 - val_loss: 0.1602 - val_acc: 0.9529\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9668\n",
      "Epoch 00018: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1019 - acc: 0.9668 - val_loss: 0.1187 - val_acc: 0.9616\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9703\n",
      "Epoch 00019: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0896 - acc: 0.9703 - val_loss: 0.1294 - val_acc: 0.9595\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9712\n",
      "Epoch 00020: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0866 - acc: 0.9712 - val_loss: 0.1190 - val_acc: 0.9637\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9733\n",
      "Epoch 00021: val_loss did not improve from 0.11360\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0797 - acc: 0.9733 - val_loss: 0.1175 - val_acc: 0.9627\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9745\n",
      "Epoch 00022: val_loss improved from 0.11360 to 0.10318, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv_checkpoint/022-0.1032.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0783 - acc: 0.9745 - val_loss: 0.1032 - val_acc: 0.9697\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9755\n",
      "Epoch 00023: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0705 - acc: 0.9755 - val_loss: 0.1293 - val_acc: 0.9634\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9770\n",
      "Epoch 00024: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0687 - acc: 0.9770 - val_loss: 0.1102 - val_acc: 0.9690\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9802\n",
      "Epoch 00025: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0583 - acc: 0.9802 - val_loss: 0.1179 - val_acc: 0.9653\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9812\n",
      "Epoch 00026: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0580 - acc: 0.9812 - val_loss: 0.1133 - val_acc: 0.9693\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9811\n",
      "Epoch 00027: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0575 - acc: 0.9811 - val_loss: 0.1129 - val_acc: 0.9686\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9822\n",
      "Epoch 00028: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0541 - acc: 0.9822 - val_loss: 0.1125 - val_acc: 0.9669\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9830\n",
      "Epoch 00029: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0508 - acc: 0.9830 - val_loss: 0.1109 - val_acc: 0.9683\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9846\n",
      "Epoch 00030: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0453 - acc: 0.9846 - val_loss: 0.1189 - val_acc: 0.9662\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9841\n",
      "Epoch 00031: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0464 - acc: 0.9841 - val_loss: 0.1428 - val_acc: 0.9606\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9856\n",
      "Epoch 00032: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0418 - acc: 0.9856 - val_loss: 0.1278 - val_acc: 0.9667\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0416 - acc: 0.9861 - val_loss: 0.1149 - val_acc: 0.9700\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9868\n",
      "Epoch 00034: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0392 - acc: 0.9868 - val_loss: 0.1413 - val_acc: 0.9648\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9877\n",
      "Epoch 00035: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0358 - acc: 0.9877 - val_loss: 0.1291 - val_acc: 0.9665\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9879\n",
      "Epoch 00036: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0360 - acc: 0.9879 - val_loss: 0.1256 - val_acc: 0.9697\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9881\n",
      "Epoch 00037: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0351 - acc: 0.9881 - val_loss: 0.1270 - val_acc: 0.9674\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9902\n",
      "Epoch 00038: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0294 - acc: 0.9902 - val_loss: 0.1810 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9884\n",
      "Epoch 00039: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0353 - acc: 0.9884 - val_loss: 0.1325 - val_acc: 0.9704\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9903\n",
      "Epoch 00040: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0290 - acc: 0.9903 - val_loss: 0.1509 - val_acc: 0.9648\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9902\n",
      "Epoch 00041: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0298 - acc: 0.9902 - val_loss: 0.1699 - val_acc: 0.9606\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 00042: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0283 - acc: 0.9913 - val_loss: 0.1369 - val_acc: 0.9676\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9917\n",
      "Epoch 00043: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0254 - acc: 0.9917 - val_loss: 0.1400 - val_acc: 0.9681\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9908\n",
      "Epoch 00044: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0278 - acc: 0.9908 - val_loss: 0.1454 - val_acc: 0.9695\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9925\n",
      "Epoch 00045: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0227 - acc: 0.9925 - val_loss: 0.1443 - val_acc: 0.9695\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9912\n",
      "Epoch 00046: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0261 - acc: 0.9912 - val_loss: 0.1460 - val_acc: 0.9660\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9931\n",
      "Epoch 00047: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0228 - acc: 0.9931 - val_loss: 0.1491 - val_acc: 0.9672\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9923\n",
      "Epoch 00048: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0244 - acc: 0.9922 - val_loss: 0.1630 - val_acc: 0.9653\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9903\n",
      "Epoch 00049: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0355 - acc: 0.9903 - val_loss: 0.1459 - val_acc: 0.9679\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00050: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0188 - acc: 0.9940 - val_loss: 0.1505 - val_acc: 0.9662\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9930\n",
      "Epoch 00051: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0206 - acc: 0.9930 - val_loss: 0.1641 - val_acc: 0.9669\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9935\n",
      "Epoch 00052: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.1749 - val_acc: 0.9667\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9940\n",
      "Epoch 00053: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0186 - acc: 0.9940 - val_loss: 0.1485 - val_acc: 0.9711\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9936\n",
      "Epoch 00054: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0199 - acc: 0.9936 - val_loss: 0.1423 - val_acc: 0.9690\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9937\n",
      "Epoch 00055: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0200 - acc: 0.9937 - val_loss: 0.1896 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9942\n",
      "Epoch 00056: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0188 - acc: 0.9942 - val_loss: 0.1488 - val_acc: 0.9679\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9955\n",
      "Epoch 00057: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0167 - acc: 0.9955 - val_loss: 0.1638 - val_acc: 0.9653\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.1884 - val_acc: 0.9620\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9933\n",
      "Epoch 00059: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0217 - acc: 0.9933 - val_loss: 0.1656 - val_acc: 0.9653\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9954\n",
      "Epoch 00060: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0148 - acc: 0.9954 - val_loss: 0.1611 - val_acc: 0.9681\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9939\n",
      "Epoch 00061: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0182 - acc: 0.9939 - val_loss: 0.1667 - val_acc: 0.9662\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9948\n",
      "Epoch 00062: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0166 - acc: 0.9948 - val_loss: 0.1805 - val_acc: 0.9681\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0116 - acc: 0.9963 - val_loss: 0.1757 - val_acc: 0.9700\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9946\n",
      "Epoch 00064: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0171 - acc: 0.9946 - val_loss: 0.1669 - val_acc: 0.9674\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9941\n",
      "Epoch 00065: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0179 - acc: 0.9941 - val_loss: 0.1639 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9956\n",
      "Epoch 00066: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0142 - acc: 0.9956 - val_loss: 0.1466 - val_acc: 0.9706\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9958\n",
      "Epoch 00067: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0124 - acc: 0.9958 - val_loss: 0.1543 - val_acc: 0.9711\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9949\n",
      "Epoch 00068: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0155 - acc: 0.9949 - val_loss: 0.1484 - val_acc: 0.9688\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9968\n",
      "Epoch 00069: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0112 - acc: 0.9968 - val_loss: 0.1803 - val_acc: 0.9683\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9943\n",
      "Epoch 00070: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0190 - acc: 0.9943 - val_loss: 0.1685 - val_acc: 0.9648\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 00071: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0092 - acc: 0.9974 - val_loss: 0.1551 - val_acc: 0.9730\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9960\n",
      "Epoch 00072: val_loss did not improve from 0.10318\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0127 - acc: 0.9960 - val_loss: 0.1515 - val_acc: 0.9697\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 788us/sample - loss: 0.1606 - acc: 0.9522\n",
      "Loss: 0.1606425730590849 Accuracy: 0.9522326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 689us/sample - loss: 0.5234 - acc: 0.8390\n",
      "Loss: 0.5234299747248181 Accuracy: 0.83904463\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 734us/sample - loss: 0.3335 - acc: 0.8989\n",
      "Loss: 0.3334528922725194 Accuracy: 0.8988577\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 779us/sample - loss: 0.2227 - acc: 0.9298\n",
      "Loss: 0.22265147672635371 Accuracy: 0.9298027\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 754us/sample - loss: 0.1419 - acc: 0.9545\n",
      "Loss: 0.1418726274969979 Accuracy: 0.9545171\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 803us/sample - loss: 0.1401 - acc: 0.9556\n",
      "Loss: 0.14011576245089186 Accuracy: 0.95555556\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 828us/sample - loss: 0.1606 - acc: 0.9522\n",
      "Loss: 0.1606425730590849 Accuracy: 0.9522326\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_128_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 719us/sample - loss: 0.5250 - acc: 0.8390\n",
      "Loss: 0.5249722844096112 Accuracy: 0.83904463\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 736us/sample - loss: 0.3829 - acc: 0.8962\n",
      "Loss: 0.3828686581779492 Accuracy: 0.89615786\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 815us/sample - loss: 0.2569 - acc: 0.9346\n",
      "Loss: 0.2568769861233197 Accuracy: 0.93457943\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 859us/sample - loss: 0.1969 - acc: 0.9549\n",
      "Loss: 0.1969316920889822 Accuracy: 0.9549325\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 871us/sample - loss: 0.2082 - acc: 0.9595\n",
      "Loss: 0.20822969780146142 Accuracy: 0.95950156\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 895us/sample - loss: 0.2392 - acc: 0.9556\n",
      "Loss: 0.23915178022101882 Accuracy: 0.95555556\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
