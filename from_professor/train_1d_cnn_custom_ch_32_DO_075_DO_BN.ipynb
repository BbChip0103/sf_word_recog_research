{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 319,280\n",
      "Trainable params: 319,024\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 228,464\n",
      "Trainable params: 228,080\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 114,096\n",
      "Trainable params: 113,584\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 89,840\n",
      "Trainable params: 89,200\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 96,304\n",
      "Trainable params: 95,536\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 134,832\n",
      "Trainable params: 133,808\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9764 - acc: 0.2288\n",
      "Epoch 00001: val_loss improved from inf to 1.93603, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/001-1.9360.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.9766 - acc: 0.2288 - val_loss: 1.9360 - val_acc: 0.3841\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0970 - acc: 0.3770\n",
      "Epoch 00002: val_loss improved from 1.93603 to 1.61369, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/002-1.6137.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 2.0971 - acc: 0.3770 - val_loss: 1.6137 - val_acc: 0.5017\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8014 - acc: 0.4538\n",
      "Epoch 00003: val_loss improved from 1.61369 to 1.52055, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/003-1.5205.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.8014 - acc: 0.4538 - val_loss: 1.5205 - val_acc: 0.5255\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6161 - acc: 0.4999\n",
      "Epoch 00004: val_loss improved from 1.52055 to 1.44355, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/004-1.4436.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.6161 - acc: 0.5000 - val_loss: 1.4436 - val_acc: 0.5563\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4635 - acc: 0.5459\n",
      "Epoch 00005: val_loss improved from 1.44355 to 1.40508, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/005-1.4051.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.4634 - acc: 0.5459 - val_loss: 1.4051 - val_acc: 0.5556\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3636 - acc: 0.5756\n",
      "Epoch 00006: val_loss improved from 1.40508 to 1.38639, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/006-1.3864.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.3636 - acc: 0.5756 - val_loss: 1.3864 - val_acc: 0.5663\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2826 - acc: 0.5958\n",
      "Epoch 00007: val_loss improved from 1.38639 to 1.28846, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/007-1.2885.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.2826 - acc: 0.5958 - val_loss: 1.2885 - val_acc: 0.6024\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1975 - acc: 0.6240\n",
      "Epoch 00008: val_loss did not improve from 1.28846\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.1975 - acc: 0.6240 - val_loss: 1.3234 - val_acc: 0.5968\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6400\n",
      "Epoch 00009: val_loss improved from 1.28846 to 1.27310, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/009-1.2731.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.1318 - acc: 0.6399 - val_loss: 1.2731 - val_acc: 0.6117\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0788 - acc: 0.6584\n",
      "Epoch 00010: val_loss improved from 1.27310 to 1.26510, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/010-1.2651.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0788 - acc: 0.6584 - val_loss: 1.2651 - val_acc: 0.6166\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0170 - acc: 0.6741\n",
      "Epoch 00011: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0169 - acc: 0.6741 - val_loss: 1.3848 - val_acc: 0.5861\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9811 - acc: 0.6881\n",
      "Epoch 00012: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9815 - acc: 0.6881 - val_loss: 1.2758 - val_acc: 0.6182\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9388 - acc: 0.7001\n",
      "Epoch 00013: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9388 - acc: 0.7001 - val_loss: 1.3021 - val_acc: 0.6080\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9044 - acc: 0.7113\n",
      "Epoch 00014: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9045 - acc: 0.7112 - val_loss: 1.2926 - val_acc: 0.6094\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8592 - acc: 0.7219\n",
      "Epoch 00015: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8592 - acc: 0.7219 - val_loss: 1.3280 - val_acc: 0.6119\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7302\n",
      "Epoch 00016: val_loss did not improve from 1.26510\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.8379 - acc: 0.7302 - val_loss: 1.2695 - val_acc: 0.6259\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8074 - acc: 0.7372\n",
      "Epoch 00017: val_loss improved from 1.26510 to 1.24661, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/017-1.2466.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8074 - acc: 0.7372 - val_loss: 1.2466 - val_acc: 0.6359\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7791 - acc: 0.7488\n",
      "Epoch 00018: val_loss improved from 1.24661 to 1.22429, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/018-1.2243.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7792 - acc: 0.7488 - val_loss: 1.2243 - val_acc: 0.6420\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7485 - acc: 0.7586\n",
      "Epoch 00019: val_loss did not improve from 1.22429\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7485 - acc: 0.7586 - val_loss: 1.3048 - val_acc: 0.6177\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7644\n",
      "Epoch 00020: val_loss did not improve from 1.22429\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7275 - acc: 0.7644 - val_loss: 1.3218 - val_acc: 0.6194\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7088 - acc: 0.7693\n",
      "Epoch 00021: val_loss improved from 1.22429 to 1.22079, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/021-1.2208.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7089 - acc: 0.7692 - val_loss: 1.2208 - val_acc: 0.6478\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6820 - acc: 0.7785\n",
      "Epoch 00022: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.6821 - acc: 0.7785 - val_loss: 1.3152 - val_acc: 0.6194\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7820\n",
      "Epoch 00023: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6611 - acc: 0.7820 - val_loss: 1.2694 - val_acc: 0.6441\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.7911\n",
      "Epoch 00024: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6325 - acc: 0.7911 - val_loss: 1.3537 - val_acc: 0.6194\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6318 - acc: 0.7950\n",
      "Epoch 00025: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6319 - acc: 0.7950 - val_loss: 1.2649 - val_acc: 0.6362\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6077 - acc: 0.7991\n",
      "Epoch 00026: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6076 - acc: 0.7991 - val_loss: 1.3353 - val_acc: 0.6205\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.8028\n",
      "Epoch 00027: val_loss did not improve from 1.22079\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5965 - acc: 0.8027 - val_loss: 1.2312 - val_acc: 0.6513\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5922 - acc: 0.8029\n",
      "Epoch 00028: val_loss improved from 1.22079 to 1.19459, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/028-1.1946.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5922 - acc: 0.8029 - val_loss: 1.1946 - val_acc: 0.6683\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.8137\n",
      "Epoch 00029: val_loss did not improve from 1.19459\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5648 - acc: 0.8137 - val_loss: 1.4499 - val_acc: 0.6091\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.8177\n",
      "Epoch 00030: val_loss did not improve from 1.19459\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5575 - acc: 0.8177 - val_loss: 1.3684 - val_acc: 0.6310\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8183\n",
      "Epoch 00031: val_loss did not improve from 1.19459\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.5500 - acc: 0.8183 - val_loss: 1.1996 - val_acc: 0.6653\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8246\n",
      "Epoch 00032: val_loss improved from 1.19459 to 1.17154, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/032-1.1715.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5305 - acc: 0.8246 - val_loss: 1.1715 - val_acc: 0.6788\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.8282\n",
      "Epoch 00033: val_loss did not improve from 1.17154\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5158 - acc: 0.8282 - val_loss: 1.4334 - val_acc: 0.6168\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8307\n",
      "Epoch 00034: val_loss did not improve from 1.17154\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5030 - acc: 0.8307 - val_loss: 1.1848 - val_acc: 0.6702\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8328\n",
      "Epoch 00035: val_loss did not improve from 1.17154\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5037 - acc: 0.8328 - val_loss: 1.2717 - val_acc: 0.6548\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8338\n",
      "Epoch 00036: val_loss improved from 1.17154 to 1.14496, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/036-1.1450.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5020 - acc: 0.8337 - val_loss: 1.1450 - val_acc: 0.6930\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4885 - acc: 0.8367\n",
      "Epoch 00037: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4884 - acc: 0.8367 - val_loss: 1.3668 - val_acc: 0.6301\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8439\n",
      "Epoch 00038: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4710 - acc: 0.8439 - val_loss: 2.0057 - val_acc: 0.5090\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8411\n",
      "Epoch 00039: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4715 - acc: 0.8411 - val_loss: 1.2278 - val_acc: 0.6627\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8487\n",
      "Epoch 00040: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4573 - acc: 0.8487 - val_loss: 1.3891 - val_acc: 0.6331\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8483\n",
      "Epoch 00041: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4547 - acc: 0.8483 - val_loss: 1.2403 - val_acc: 0.6671\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8536\n",
      "Epoch 00042: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4435 - acc: 0.8536 - val_loss: 1.4416 - val_acc: 0.6208\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8543\n",
      "Epoch 00043: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4341 - acc: 0.8542 - val_loss: 1.1739 - val_acc: 0.6853\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.8594\n",
      "Epoch 00044: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4231 - acc: 0.8594 - val_loss: 1.2949 - val_acc: 0.6597\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4176 - acc: 0.8611\n",
      "Epoch 00045: val_loss did not improve from 1.14496\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4178 - acc: 0.8611 - val_loss: 1.3458 - val_acc: 0.6422\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8670\n",
      "Epoch 00046: val_loss improved from 1.14496 to 1.13903, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/046-1.1390.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4026 - acc: 0.8670 - val_loss: 1.1390 - val_acc: 0.6937\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8672\n",
      "Epoch 00047: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.4008 - acc: 0.8672 - val_loss: 1.3113 - val_acc: 0.6587\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8665\n",
      "Epoch 00048: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4030 - acc: 0.8664 - val_loss: 1.2321 - val_acc: 0.6594\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8701\n",
      "Epoch 00049: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3885 - acc: 0.8702 - val_loss: 1.2355 - val_acc: 0.6758\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8687\n",
      "Epoch 00050: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3938 - acc: 0.8687 - val_loss: 1.1543 - val_acc: 0.6953\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8717\n",
      "Epoch 00051: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3864 - acc: 0.8717 - val_loss: 1.1862 - val_acc: 0.6893\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8739\n",
      "Epoch 00052: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3797 - acc: 0.8739 - val_loss: 1.2393 - val_acc: 0.6739\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8750\n",
      "Epoch 00053: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3726 - acc: 0.8750 - val_loss: 1.2756 - val_acc: 0.6599\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8790\n",
      "Epoch 00054: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3635 - acc: 0.8790 - val_loss: 1.2189 - val_acc: 0.6867\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8804\n",
      "Epoch 00055: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3593 - acc: 0.8804 - val_loss: 1.1992 - val_acc: 0.6832\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8790\n",
      "Epoch 00056: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3635 - acc: 0.8790 - val_loss: 1.2888 - val_acc: 0.6685\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8818\n",
      "Epoch 00057: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3527 - acc: 0.8818 - val_loss: 1.1900 - val_acc: 0.7049\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8869\n",
      "Epoch 00058: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3459 - acc: 0.8869 - val_loss: 1.2153 - val_acc: 0.6944\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8874\n",
      "Epoch 00059: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3411 - acc: 0.8873 - val_loss: 1.5276 - val_acc: 0.6331\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8865\n",
      "Epoch 00060: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3399 - acc: 0.8865 - val_loss: 1.3127 - val_acc: 0.6487\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8907\n",
      "Epoch 00061: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3322 - acc: 0.8907 - val_loss: 1.1682 - val_acc: 0.7002\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8912\n",
      "Epoch 00062: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3298 - acc: 0.8911 - val_loss: 1.1745 - val_acc: 0.7007\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8902\n",
      "Epoch 00063: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3304 - acc: 0.8902 - val_loss: 1.3103 - val_acc: 0.6543\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8926\n",
      "Epoch 00064: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3261 - acc: 0.8926 - val_loss: 1.1706 - val_acc: 0.6962\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8946\n",
      "Epoch 00065: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3202 - acc: 0.8946 - val_loss: 1.1978 - val_acc: 0.6923\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8956\n",
      "Epoch 00066: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3138 - acc: 0.8956 - val_loss: 1.2047 - val_acc: 0.6935\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.8966\n",
      "Epoch 00067: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3162 - acc: 0.8966 - val_loss: 1.2172 - val_acc: 0.6967\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.8966\n",
      "Epoch 00068: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3097 - acc: 0.8966 - val_loss: 1.2934 - val_acc: 0.6823\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8985\n",
      "Epoch 00069: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3079 - acc: 0.8985 - val_loss: 1.3399 - val_acc: 0.6648\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9005\n",
      "Epoch 00070: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3022 - acc: 0.9005 - val_loss: 1.1873 - val_acc: 0.6893\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9005\n",
      "Epoch 00071: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3022 - acc: 0.9005 - val_loss: 1.2148 - val_acc: 0.6981\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9017\n",
      "Epoch 00072: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2956 - acc: 0.9017 - val_loss: 1.1661 - val_acc: 0.7028\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9035- ETA: 0s - loss: 0.2883 - a\n",
      "Epoch 00073: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2896 - acc: 0.9035 - val_loss: 1.2731 - val_acc: 0.6916\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9010\n",
      "Epoch 00074: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2942 - acc: 0.9010 - val_loss: 1.3377 - val_acc: 0.6601\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2882 - acc: 0.9049\n",
      "Epoch 00075: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2882 - acc: 0.9049 - val_loss: 1.1949 - val_acc: 0.6981\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9047\n",
      "Epoch 00076: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2872 - acc: 0.9046 - val_loss: 1.2084 - val_acc: 0.6965\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9072\n",
      "Epoch 00077: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2852 - acc: 0.9072 - val_loss: 1.1735 - val_acc: 0.7077\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9098\n",
      "Epoch 00078: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2739 - acc: 0.9098 - val_loss: 1.3313 - val_acc: 0.6550\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9082\n",
      "Epoch 00079: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2773 - acc: 0.9081 - val_loss: 1.3917 - val_acc: 0.6464\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9079\n",
      "Epoch 00080: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2775 - acc: 0.9079 - val_loss: 1.3373 - val_acc: 0.6713\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9114\n",
      "Epoch 00081: val_loss did not improve from 1.13903\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2701 - acc: 0.9114 - val_loss: 1.2558 - val_acc: 0.6939\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9104\n",
      "Epoch 00082: val_loss improved from 1.13903 to 1.12750, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/082-1.1275.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2735 - acc: 0.9103 - val_loss: 1.1275 - val_acc: 0.7249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9099\n",
      "Epoch 00083: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2702 - acc: 0.9098 - val_loss: 1.2927 - val_acc: 0.6699\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9115\n",
      "Epoch 00084: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2679 - acc: 0.9115 - val_loss: 1.4827 - val_acc: 0.6571\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.9113\n",
      "Epoch 00085: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2710 - acc: 0.9113 - val_loss: 1.2166 - val_acc: 0.6979\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9121\n",
      "Epoch 00086: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2613 - acc: 0.9121 - val_loss: 1.2276 - val_acc: 0.7049\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9151\n",
      "Epoch 00087: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2583 - acc: 0.9151 - val_loss: 1.5000 - val_acc: 0.6520\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9148\n",
      "Epoch 00088: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2586 - acc: 0.9148 - val_loss: 1.1757 - val_acc: 0.7144\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9164\n",
      "Epoch 00089: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2548 - acc: 0.9164 - val_loss: 1.3631 - val_acc: 0.6792\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9179\n",
      "Epoch 00090: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2526 - acc: 0.9179 - val_loss: 1.2606 - val_acc: 0.6967\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9141\n",
      "Epoch 00091: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2550 - acc: 0.9141 - val_loss: 1.3873 - val_acc: 0.6643\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9178- ETA: 1s\n",
      "Epoch 00092: val_loss did not improve from 1.12750\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2518 - acc: 0.9178 - val_loss: 1.2765 - val_acc: 0.6923\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9160\n",
      "Epoch 00093: val_loss improved from 1.12750 to 1.10373, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv_checkpoint/093-1.1037.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2556 - acc: 0.9159 - val_loss: 1.1037 - val_acc: 0.7265\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9155\n",
      "Epoch 00094: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2489 - acc: 0.9155 - val_loss: 1.4040 - val_acc: 0.6683\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9202\n",
      "Epoch 00095: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2403 - acc: 0.9202 - val_loss: 1.4000 - val_acc: 0.6723\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9190\n",
      "Epoch 00096: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2463 - acc: 0.9190 - val_loss: 1.3404 - val_acc: 0.6911\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9216\n",
      "Epoch 00097: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2400 - acc: 0.9216 - val_loss: 1.3255 - val_acc: 0.6911\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9200\n",
      "Epoch 00098: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2436 - acc: 0.9200 - val_loss: 1.2579 - val_acc: 0.6995\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9217\n",
      "Epoch 00099: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2378 - acc: 0.9217 - val_loss: 1.2758 - val_acc: 0.6979\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9207\n",
      "Epoch 00100: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2377 - acc: 0.9207 - val_loss: 1.2598 - val_acc: 0.6969\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9228\n",
      "Epoch 00101: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2327 - acc: 0.9227 - val_loss: 1.3622 - val_acc: 0.6755\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9260\n",
      "Epoch 00102: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2308 - acc: 0.9260 - val_loss: 1.4144 - val_acc: 0.6760\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9278\n",
      "Epoch 00103: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2258 - acc: 0.9278 - val_loss: 1.2272 - val_acc: 0.7205\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9237\n",
      "Epoch 00104: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2307 - acc: 0.9237 - val_loss: 1.2501 - val_acc: 0.7046\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9248\n",
      "Epoch 00105: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2294 - acc: 0.9248 - val_loss: 1.5652 - val_acc: 0.6438\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9282\n",
      "Epoch 00106: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2234 - acc: 0.9281 - val_loss: 1.2150 - val_acc: 0.7151\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9251\n",
      "Epoch 00107: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2262 - acc: 0.9251 - val_loss: 1.1687 - val_acc: 0.7326\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9280\n",
      "Epoch 00108: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2237 - acc: 0.9280 - val_loss: 1.3830 - val_acc: 0.6636\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9279\n",
      "Epoch 00109: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2219 - acc: 0.9278 - val_loss: 1.2959 - val_acc: 0.6993\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9248\n",
      "Epoch 00110: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2253 - acc: 0.9248 - val_loss: 1.6259 - val_acc: 0.6289\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9275\n",
      "Epoch 00111: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2246 - acc: 0.9275 - val_loss: 1.2738 - val_acc: 0.6988\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9295\n",
      "Epoch 00112: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2160 - acc: 0.9295 - val_loss: 1.1512 - val_acc: 0.7386\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9279\n",
      "Epoch 00113: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2161 - acc: 0.9279 - val_loss: 1.3951 - val_acc: 0.6928\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9286\n",
      "Epoch 00114: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2186 - acc: 0.9286 - val_loss: 1.2511 - val_acc: 0.7140\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9296\n",
      "Epoch 00115: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2126 - acc: 0.9296 - val_loss: 1.4094 - val_acc: 0.6804\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9293\n",
      "Epoch 00116: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2144 - acc: 0.9294 - val_loss: 1.1647 - val_acc: 0.7277\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9290\n",
      "Epoch 00117: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2124 - acc: 0.9290 - val_loss: 1.4479 - val_acc: 0.6853\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9320\n",
      "Epoch 00118: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2113 - acc: 0.9320 - val_loss: 1.3656 - val_acc: 0.6783\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9292\n",
      "Epoch 00119: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2154 - acc: 0.9292 - val_loss: 1.2798 - val_acc: 0.6965\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9288\n",
      "Epoch 00120: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2112 - acc: 0.9288 - val_loss: 1.4762 - val_acc: 0.6706\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9326\n",
      "Epoch 00121: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2091 - acc: 0.9326 - val_loss: 1.2000 - val_acc: 0.7063\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9305\n",
      "Epoch 00122: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2062 - acc: 0.9305 - val_loss: 1.2305 - val_acc: 0.7149\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9321\n",
      "Epoch 00123: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2074 - acc: 0.9322 - val_loss: 1.1993 - val_acc: 0.7214\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9338\n",
      "Epoch 00124: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2007 - acc: 0.9338 - val_loss: 1.3469 - val_acc: 0.6853\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9315\n",
      "Epoch 00125: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2046 - acc: 0.9315 - val_loss: 1.3942 - val_acc: 0.6697\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9327\n",
      "Epoch 00126: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2036 - acc: 0.9327 - val_loss: 1.2607 - val_acc: 0.7198\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9347\n",
      "Epoch 00127: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2030 - acc: 0.9347 - val_loss: 1.2331 - val_acc: 0.7160\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9335\n",
      "Epoch 00128: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2011 - acc: 0.9334 - val_loss: 1.2218 - val_acc: 0.7114\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9335\n",
      "Epoch 00129: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2030 - acc: 0.9335 - val_loss: 1.2618 - val_acc: 0.6967\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9370\n",
      "Epoch 00130: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1913 - acc: 0.9370 - val_loss: 1.4816 - val_acc: 0.6755\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9350\n",
      "Epoch 00131: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1977 - acc: 0.9350 - val_loss: 1.1891 - val_acc: 0.7226\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9370\n",
      "Epoch 00132: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1948 - acc: 0.9370 - val_loss: 1.1871 - val_acc: 0.7270\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9373\n",
      "Epoch 00133: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1935 - acc: 0.9373 - val_loss: 1.1466 - val_acc: 0.7345\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9372\n",
      "Epoch 00134: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1960 - acc: 0.9372 - val_loss: 1.2622 - val_acc: 0.6981\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9349\n",
      "Epoch 00135: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1988 - acc: 0.9350 - val_loss: 1.4956 - val_acc: 0.6809\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9371\n",
      "Epoch 00136: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1922 - acc: 0.9371 - val_loss: 1.1494 - val_acc: 0.7338\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9394\n",
      "Epoch 00137: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1867 - acc: 0.9394 - val_loss: 1.2576 - val_acc: 0.7067\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9382\n",
      "Epoch 00138: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1887 - acc: 0.9381 - val_loss: 1.7104 - val_acc: 0.6063\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9362\n",
      "Epoch 00139: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1974 - acc: 0.9361 - val_loss: 1.2797 - val_acc: 0.7081\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9374\n",
      "Epoch 00140: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1893 - acc: 0.9374 - val_loss: 1.5734 - val_acc: 0.6506\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9373\n",
      "Epoch 00141: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1874 - acc: 0.9373 - val_loss: 1.7082 - val_acc: 0.6716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9383\n",
      "Epoch 00142: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1897 - acc: 0.9383 - val_loss: 1.3544 - val_acc: 0.6990\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9376\n",
      "Epoch 00143: val_loss did not improve from 1.10373\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1889 - acc: 0.9376 - val_loss: 2.0796 - val_acc: 0.5518\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 453us/sample - loss: 1.2868 - acc: 0.6816\n",
      "Loss: 1.2867762950102983 Accuracy: 0.68161994\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3409 - acc: 0.1861\n",
      "Epoch 00001: val_loss improved from inf to 2.01857, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/001-2.0186.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 3.3410 - acc: 0.1860 - val_loss: 2.0186 - val_acc: 0.3750\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2216 - acc: 0.3574\n",
      "Epoch 00002: val_loss improved from 2.01857 to 1.54680, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/002-1.5468.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 2.2216 - acc: 0.3575 - val_loss: 1.5468 - val_acc: 0.5255\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8162 - acc: 0.4551\n",
      "Epoch 00003: val_loss improved from 1.54680 to 1.31099, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/003-1.3110.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.8161 - acc: 0.4551 - val_loss: 1.3110 - val_acc: 0.6101\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6092 - acc: 0.5077\n",
      "Epoch 00004: val_loss improved from 1.31099 to 1.20397, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/004-1.2040.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.6093 - acc: 0.5077 - val_loss: 1.2040 - val_acc: 0.6424\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4456 - acc: 0.5545\n",
      "Epoch 00005: val_loss improved from 1.20397 to 1.11983, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/005-1.1198.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.4457 - acc: 0.5545 - val_loss: 1.1198 - val_acc: 0.6627\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3181 - acc: 0.5932\n",
      "Epoch 00006: val_loss improved from 1.11983 to 1.05924, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/006-1.0592.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.3181 - acc: 0.5932 - val_loss: 1.0592 - val_acc: 0.6867\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2217 - acc: 0.6204\n",
      "Epoch 00007: val_loss improved from 1.05924 to 1.02724, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/007-1.0272.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2217 - acc: 0.6204 - val_loss: 1.0272 - val_acc: 0.6879\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1605 - acc: 0.6409\n",
      "Epoch 00008: val_loss improved from 1.02724 to 0.98942, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/008-0.9894.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1607 - acc: 0.6408 - val_loss: 0.9894 - val_acc: 0.7007\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1058 - acc: 0.6558\n",
      "Epoch 00009: val_loss improved from 0.98942 to 0.94095, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/009-0.9410.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.1058 - acc: 0.6558 - val_loss: 0.9410 - val_acc: 0.7135\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0638 - acc: 0.6696\n",
      "Epoch 00010: val_loss improved from 0.94095 to 0.93446, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/010-0.9345.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0638 - acc: 0.6696 - val_loss: 0.9345 - val_acc: 0.7284\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0217 - acc: 0.6845\n",
      "Epoch 00011: val_loss improved from 0.93446 to 0.93386, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/011-0.9339.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0217 - acc: 0.6844 - val_loss: 0.9339 - val_acc: 0.7233\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9856 - acc: 0.6960\n",
      "Epoch 00012: val_loss improved from 0.93386 to 0.91896, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/012-0.9190.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9856 - acc: 0.6960 - val_loss: 0.9190 - val_acc: 0.7247\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9496 - acc: 0.7068\n",
      "Epoch 00013: val_loss improved from 0.91896 to 0.87200, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/013-0.8720.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9495 - acc: 0.7068 - val_loss: 0.8720 - val_acc: 0.7498\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9256 - acc: 0.7136\n",
      "Epoch 00014: val_loss did not improve from 0.87200\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9258 - acc: 0.7136 - val_loss: 0.9704 - val_acc: 0.7007\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9093 - acc: 0.7184\n",
      "Epoch 00015: val_loss did not improve from 0.87200\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.9093 - acc: 0.7184 - val_loss: 0.8835 - val_acc: 0.7379\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8785 - acc: 0.7299\n",
      "Epoch 00016: val_loss improved from 0.87200 to 0.83930, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/016-0.8393.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8787 - acc: 0.7299 - val_loss: 0.8393 - val_acc: 0.7482\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8619 - acc: 0.7352\n",
      "Epoch 00017: val_loss did not improve from 0.83930\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8618 - acc: 0.7353 - val_loss: 1.2994 - val_acc: 0.6173\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7404\n",
      "Epoch 00018: val_loss improved from 0.83930 to 0.81898, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/018-0.8190.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8416 - acc: 0.7403 - val_loss: 0.8190 - val_acc: 0.7713\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7464\n",
      "Epoch 00019: val_loss improved from 0.81898 to 0.78850, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/019-0.7885.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8256 - acc: 0.7464 - val_loss: 0.7885 - val_acc: 0.7710\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7518\n",
      "Epoch 00020: val_loss did not improve from 0.78850\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8041 - acc: 0.7518 - val_loss: 0.9615 - val_acc: 0.6988\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7866 - acc: 0.7589\n",
      "Epoch 00021: val_loss improved from 0.78850 to 0.78553, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/021-0.7855.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7866 - acc: 0.7589 - val_loss: 0.7855 - val_acc: 0.7782\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7677 - acc: 0.7645\n",
      "Epoch 00022: val_loss did not improve from 0.78553\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7676 - acc: 0.7645 - val_loss: 0.9160 - val_acc: 0.7207\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.7650\n",
      "Epoch 00023: val_loss improved from 0.78553 to 0.78375, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/023-0.7838.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7562 - acc: 0.7650 - val_loss: 0.7838 - val_acc: 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7471 - acc: 0.7690\n",
      "Epoch 00024: val_loss did not improve from 0.78375\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7471 - acc: 0.7691 - val_loss: 0.8355 - val_acc: 0.7503\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7336 - acc: 0.7724\n",
      "Epoch 00025: val_loss did not improve from 0.78375\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7336 - acc: 0.7724 - val_loss: 0.9001 - val_acc: 0.7352\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7206 - acc: 0.7794\n",
      "Epoch 00026: val_loss did not improve from 0.78375\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7206 - acc: 0.7794 - val_loss: 0.8231 - val_acc: 0.7617\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7830\n",
      "Epoch 00027: val_loss did not improve from 0.78375\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6982 - acc: 0.7830 - val_loss: 0.7905 - val_acc: 0.7710\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.7868\n",
      "Epoch 00028: val_loss did not improve from 0.78375\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6932 - acc: 0.7868 - val_loss: 0.8946 - val_acc: 0.7405\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.7853\n",
      "Epoch 00029: val_loss improved from 0.78375 to 0.76821, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/029-0.7682.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6890 - acc: 0.7853 - val_loss: 0.7682 - val_acc: 0.7810\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6656 - acc: 0.7942\n",
      "Epoch 00030: val_loss did not improve from 0.76821\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6655 - acc: 0.7942 - val_loss: 0.9214 - val_acc: 0.7212\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6625 - acc: 0.7957\n",
      "Epoch 00031: val_loss improved from 0.76821 to 0.75560, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/031-0.7556.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6627 - acc: 0.7956 - val_loss: 0.7556 - val_acc: 0.7911\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6574 - acc: 0.7948\n",
      "Epoch 00032: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6573 - acc: 0.7948 - val_loss: 0.7866 - val_acc: 0.7803\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6475 - acc: 0.8002\n",
      "Epoch 00033: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6475 - acc: 0.8002 - val_loss: 0.9559 - val_acc: 0.7156\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6424 - acc: 0.7998\n",
      "Epoch 00034: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6424 - acc: 0.7998 - val_loss: 0.8131 - val_acc: 0.7624\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6263 - acc: 0.8048\n",
      "Epoch 00035: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6264 - acc: 0.8048 - val_loss: 0.8779 - val_acc: 0.7372\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.8078\n",
      "Epoch 00036: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6218 - acc: 0.8078 - val_loss: 0.8230 - val_acc: 0.7575\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8086\n",
      "Epoch 00037: val_loss did not improve from 0.75560\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6155 - acc: 0.8086 - val_loss: 0.8184 - val_acc: 0.7605\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6035 - acc: 0.8111\n",
      "Epoch 00038: val_loss improved from 0.75560 to 0.72576, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/038-0.7258.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6036 - acc: 0.8111 - val_loss: 0.7258 - val_acc: 0.7911\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5978 - acc: 0.8144\n",
      "Epoch 00039: val_loss did not improve from 0.72576\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5978 - acc: 0.8144 - val_loss: 0.7997 - val_acc: 0.7687\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5936 - acc: 0.8137\n",
      "Epoch 00040: val_loss did not improve from 0.72576\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5936 - acc: 0.8136 - val_loss: 0.8209 - val_acc: 0.7633\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8186\n",
      "Epoch 00041: val_loss did not improve from 0.72576\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5812 - acc: 0.8185 - val_loss: 0.8342 - val_acc: 0.7601\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5813 - acc: 0.8173\n",
      "Epoch 00042: val_loss did not improve from 0.72576\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5813 - acc: 0.8173 - val_loss: 0.8446 - val_acc: 0.7596\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8229\n",
      "Epoch 00043: val_loss did not improve from 0.72576\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5703 - acc: 0.8229 - val_loss: 0.7482 - val_acc: 0.7964\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8225\n",
      "Epoch 00044: val_loss improved from 0.72576 to 0.72309, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/044-0.7231.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5652 - acc: 0.8225 - val_loss: 0.7231 - val_acc: 0.7920\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.8258\n",
      "Epoch 00045: val_loss improved from 0.72309 to 0.69735, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/045-0.6973.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5593 - acc: 0.8258 - val_loss: 0.6973 - val_acc: 0.8083\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8280\n",
      "Epoch 00046: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5499 - acc: 0.8280 - val_loss: 0.7015 - val_acc: 0.7999\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.8298\n",
      "Epoch 00047: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5430 - acc: 0.8298 - val_loss: 0.7202 - val_acc: 0.7976\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8294\n",
      "Epoch 00048: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5408 - acc: 0.8294 - val_loss: 0.7697 - val_acc: 0.7785\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5394 - acc: 0.8313\n",
      "Epoch 00049: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5394 - acc: 0.8313 - val_loss: 0.7896 - val_acc: 0.7768\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8329\n",
      "Epoch 00050: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5329 - acc: 0.8328 - val_loss: 0.7187 - val_acc: 0.7980\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.8345\n",
      "Epoch 00051: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5258 - acc: 0.8345 - val_loss: 0.9070 - val_acc: 0.7384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8363\n",
      "Epoch 00052: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5198 - acc: 0.8362 - val_loss: 0.7413 - val_acc: 0.7932\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8388\n",
      "Epoch 00053: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5157 - acc: 0.8387 - val_loss: 0.7284 - val_acc: 0.7906\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.8377\n",
      "Epoch 00054: val_loss did not improve from 0.69735\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5106 - acc: 0.8377 - val_loss: 0.7505 - val_acc: 0.7887\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8397\n",
      "Epoch 00055: val_loss improved from 0.69735 to 0.69205, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/055-0.6921.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5059 - acc: 0.8397 - val_loss: 0.6921 - val_acc: 0.8039\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8416\n",
      "Epoch 00056: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5036 - acc: 0.8416 - val_loss: 0.7265 - val_acc: 0.7920\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8433\n",
      "Epoch 00057: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4934 - acc: 0.8433 - val_loss: 0.7251 - val_acc: 0.8022\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8454\n",
      "Epoch 00058: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4920 - acc: 0.8453 - val_loss: 0.7486 - val_acc: 0.7913\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8484\n",
      "Epoch 00059: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4829 - acc: 0.8484 - val_loss: 0.6933 - val_acc: 0.8099\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8482\n",
      "Epoch 00060: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4864 - acc: 0.8482 - val_loss: 0.7949 - val_acc: 0.7706\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4834 - acc: 0.8470\n",
      "Epoch 00061: val_loss did not improve from 0.69205\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4834 - acc: 0.8470 - val_loss: 0.7139 - val_acc: 0.7966\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8496\n",
      "Epoch 00062: val_loss improved from 0.69205 to 0.68658, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/062-0.6866.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4731 - acc: 0.8496 - val_loss: 0.6866 - val_acc: 0.8116\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8508\n",
      "Epoch 00063: val_loss improved from 0.68658 to 0.67830, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/063-0.6783.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4735 - acc: 0.8508 - val_loss: 0.6783 - val_acc: 0.8239\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4730 - acc: 0.8509\n",
      "Epoch 00064: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4730 - acc: 0.8509 - val_loss: 0.7691 - val_acc: 0.7892\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8535\n",
      "Epoch 00065: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4624 - acc: 0.8534 - val_loss: 0.7233 - val_acc: 0.7994\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8539\n",
      "Epoch 00066: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4620 - acc: 0.8539 - val_loss: 0.7956 - val_acc: 0.7836\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.8536\n",
      "Epoch 00067: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4607 - acc: 0.8535 - val_loss: 0.9840 - val_acc: 0.7133\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8542\n",
      "Epoch 00068: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4628 - acc: 0.8541 - val_loss: 0.7077 - val_acc: 0.8022\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4532 - acc: 0.8562\n",
      "Epoch 00069: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4532 - acc: 0.8563 - val_loss: 0.8919 - val_acc: 0.7452\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8579\n",
      "Epoch 00070: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4497 - acc: 0.8579 - val_loss: 0.7722 - val_acc: 0.7768\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8577\n",
      "Epoch 00071: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4501 - acc: 0.8577 - val_loss: 0.7874 - val_acc: 0.7794\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8589\n",
      "Epoch 00072: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4446 - acc: 0.8589 - val_loss: 0.7309 - val_acc: 0.8022\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8568\n",
      "Epoch 00073: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4459 - acc: 0.8568 - val_loss: 0.7409 - val_acc: 0.7985\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8612\n",
      "Epoch 00074: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4330 - acc: 0.8611 - val_loss: 0.7495 - val_acc: 0.7987\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8634\n",
      "Epoch 00075: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4354 - acc: 0.8634 - val_loss: 0.9778 - val_acc: 0.7386\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8644\n",
      "Epoch 00076: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4283 - acc: 0.8644 - val_loss: 0.6914 - val_acc: 0.8123\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8643\n",
      "Epoch 00077: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4281 - acc: 0.8643 - val_loss: 0.7720 - val_acc: 0.7901\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8649\n",
      "Epoch 00078: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4195 - acc: 0.8649 - val_loss: 0.9088 - val_acc: 0.7505\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8654\n",
      "Epoch 00079: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4237 - acc: 0.8654 - val_loss: 0.7207 - val_acc: 0.8053\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8671\n",
      "Epoch 00080: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4166 - acc: 0.8671 - val_loss: 0.7340 - val_acc: 0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8661\n",
      "Epoch 00081: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4179 - acc: 0.8661 - val_loss: 0.7193 - val_acc: 0.8057\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8682\n",
      "Epoch 00082: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4122 - acc: 0.8683 - val_loss: 0.6795 - val_acc: 0.8204\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8680\n",
      "Epoch 00083: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4101 - acc: 0.8680 - val_loss: 0.6800 - val_acc: 0.8199\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8689\n",
      "Epoch 00084: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4083 - acc: 0.8689 - val_loss: 0.7891 - val_acc: 0.7848\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4028 - acc: 0.8710\n",
      "Epoch 00085: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4028 - acc: 0.8710 - val_loss: 0.7355 - val_acc: 0.7997\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8707\n",
      "Epoch 00086: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4086 - acc: 0.8707 - val_loss: 0.7351 - val_acc: 0.7992\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8721\n",
      "Epoch 00087: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3953 - acc: 0.8721 - val_loss: 0.8374 - val_acc: 0.7727\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8716\n",
      "Epoch 00088: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3981 - acc: 0.8716 - val_loss: 0.7062 - val_acc: 0.8143\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8741\n",
      "Epoch 00089: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3979 - acc: 0.8741 - val_loss: 0.7335 - val_acc: 0.8011\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8740\n",
      "Epoch 00090: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3955 - acc: 0.8740 - val_loss: 0.7094 - val_acc: 0.8120\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8715\n",
      "Epoch 00091: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3926 - acc: 0.8716 - val_loss: 0.8166 - val_acc: 0.7694\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8753\n",
      "Epoch 00092: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3869 - acc: 0.8753 - val_loss: 0.7238 - val_acc: 0.8120\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8768\n",
      "Epoch 00093: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3872 - acc: 0.8768 - val_loss: 0.7378 - val_acc: 0.7994\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8785\n",
      "Epoch 00094: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3775 - acc: 0.8785 - val_loss: 0.7455 - val_acc: 0.8001\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8771\n",
      "Epoch 00095: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3850 - acc: 0.8772 - val_loss: 0.7447 - val_acc: 0.7978\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8761\n",
      "Epoch 00096: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3860 - acc: 0.8760 - val_loss: 0.7760 - val_acc: 0.7987\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8775\n",
      "Epoch 00097: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3791 - acc: 0.8775 - val_loss: 0.7819 - val_acc: 0.7964\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8764\n",
      "Epoch 00098: val_loss did not improve from 0.67830\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3844 - acc: 0.8764 - val_loss: 0.8937 - val_acc: 0.7652\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8804\n",
      "Epoch 00099: val_loss improved from 0.67830 to 0.67721, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/099-0.6772.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3744 - acc: 0.8804 - val_loss: 0.6772 - val_acc: 0.8239\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8784\n",
      "Epoch 00100: val_loss did not improve from 0.67721\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3743 - acc: 0.8784 - val_loss: 0.6785 - val_acc: 0.8248\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8823\n",
      "Epoch 00101: val_loss did not improve from 0.67721\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3715 - acc: 0.8824 - val_loss: 0.8790 - val_acc: 0.7661\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8805\n",
      "Epoch 00102: val_loss did not improve from 0.67721\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3689 - acc: 0.8805 - val_loss: 1.5701 - val_acc: 0.6522\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8812\n",
      "Epoch 00103: val_loss did not improve from 0.67721\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3682 - acc: 0.8812 - val_loss: 0.6874 - val_acc: 0.8260\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8829\n",
      "Epoch 00104: val_loss did not improve from 0.67721\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3666 - acc: 0.8829 - val_loss: 0.7732 - val_acc: 0.7966\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8820\n",
      "Epoch 00105: val_loss improved from 0.67721 to 0.67418, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/105-0.6742.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3661 - acc: 0.8820 - val_loss: 0.6742 - val_acc: 0.8260\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8854\n",
      "Epoch 00106: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3565 - acc: 0.8853 - val_loss: 0.7007 - val_acc: 0.8125\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8849\n",
      "Epoch 00107: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3607 - acc: 0.8849 - val_loss: 1.0134 - val_acc: 0.7365\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8854\n",
      "Epoch 00108: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3585 - acc: 0.8853 - val_loss: 0.6906 - val_acc: 0.8218\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8827\n",
      "Epoch 00109: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3586 - acc: 0.8827 - val_loss: 0.7442 - val_acc: 0.8018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8831\n",
      "Epoch 00110: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3585 - acc: 0.8830 - val_loss: 0.7433 - val_acc: 0.8013\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8876\n",
      "Epoch 00111: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3482 - acc: 0.8875 - val_loss: 0.7032 - val_acc: 0.8106\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8858\n",
      "Epoch 00112: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3525 - acc: 0.8859 - val_loss: 0.7606 - val_acc: 0.7985\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.8896\n",
      "Epoch 00113: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3458 - acc: 0.8896 - val_loss: 0.7547 - val_acc: 0.8060\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8868\n",
      "Epoch 00114: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3509 - acc: 0.8868 - val_loss: 0.7619 - val_acc: 0.8001\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8893\n",
      "Epoch 00115: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3474 - acc: 0.8893 - val_loss: 0.7483 - val_acc: 0.8018\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8866\n",
      "Epoch 00116: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3493 - acc: 0.8866 - val_loss: 0.8260 - val_acc: 0.7787\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8890\n",
      "Epoch 00117: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3436 - acc: 0.8891 - val_loss: 0.7787 - val_acc: 0.7906\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8880\n",
      "Epoch 00118: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3466 - acc: 0.8880 - val_loss: 0.7453 - val_acc: 0.8006\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8891\n",
      "Epoch 00119: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3477 - acc: 0.8891 - val_loss: 0.6913 - val_acc: 0.8169\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8908\n",
      "Epoch 00120: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3405 - acc: 0.8907 - val_loss: 0.7448 - val_acc: 0.8046\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8881\n",
      "Epoch 00121: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3481 - acc: 0.8881 - val_loss: 0.8658 - val_acc: 0.7703\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8912\n",
      "Epoch 00122: val_loss did not improve from 0.67418\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3343 - acc: 0.8912 - val_loss: 0.7058 - val_acc: 0.8157\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8899\n",
      "Epoch 00123: val_loss improved from 0.67418 to 0.65987, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/123-0.6599.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3361 - acc: 0.8899 - val_loss: 0.6599 - val_acc: 0.8281\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8908\n",
      "Epoch 00124: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3348 - acc: 0.8909 - val_loss: 0.8017 - val_acc: 0.7880\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8910\n",
      "Epoch 00125: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3325 - acc: 0.8910 - val_loss: 0.7736 - val_acc: 0.8008\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3330 - acc: 0.8910\n",
      "Epoch 00126: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3330 - acc: 0.8910 - val_loss: 0.7119 - val_acc: 0.8109\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8911\n",
      "Epoch 00127: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3325 - acc: 0.8911 - val_loss: 0.7189 - val_acc: 0.8074\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8930\n",
      "Epoch 00128: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3293 - acc: 0.8930 - val_loss: 0.6907 - val_acc: 0.8206\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8896\n",
      "Epoch 00129: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3318 - acc: 0.8896 - val_loss: 0.7889 - val_acc: 0.7957\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8898\n",
      "Epoch 00130: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3380 - acc: 0.8897 - val_loss: 0.7786 - val_acc: 0.7918\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8900\n",
      "Epoch 00131: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3309 - acc: 0.8900 - val_loss: 0.7592 - val_acc: 0.7994\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8935\n",
      "Epoch 00132: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3235 - acc: 0.8935 - val_loss: 0.7625 - val_acc: 0.7994\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8938\n",
      "Epoch 00133: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3254 - acc: 0.8938 - val_loss: 0.6867 - val_acc: 0.8160\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8950\n",
      "Epoch 00134: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3242 - acc: 0.8950 - val_loss: 0.7056 - val_acc: 0.8211\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8964\n",
      "Epoch 00135: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3158 - acc: 0.8964 - val_loss: 0.6892 - val_acc: 0.8248\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8968\n",
      "Epoch 00136: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3220 - acc: 0.8969 - val_loss: 0.6847 - val_acc: 0.8244\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8968\n",
      "Epoch 00137: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3229 - acc: 0.8968 - val_loss: 0.8907 - val_acc: 0.7775\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8967\n",
      "Epoch 00138: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3181 - acc: 0.8967 - val_loss: 0.7025 - val_acc: 0.8190\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8948\n",
      "Epoch 00139: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3193 - acc: 0.8948 - val_loss: 0.7373 - val_acc: 0.8095\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8971\n",
      "Epoch 00140: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3150 - acc: 0.8971 - val_loss: 0.7344 - val_acc: 0.8125\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.8994\n",
      "Epoch 00141: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3111 - acc: 0.8994 - val_loss: 0.7052 - val_acc: 0.8195\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8982\n",
      "Epoch 00142: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3148 - acc: 0.8982 - val_loss: 0.7385 - val_acc: 0.8099\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.8986\n",
      "Epoch 00143: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3161 - acc: 0.8986 - val_loss: 0.7071 - val_acc: 0.8171\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8976\n",
      "Epoch 00144: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3149 - acc: 0.8976 - val_loss: 0.6888 - val_acc: 0.8223\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.8994\n",
      "Epoch 00145: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3108 - acc: 0.8994 - val_loss: 0.7784 - val_acc: 0.8069\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.8996\n",
      "Epoch 00146: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3046 - acc: 0.8997 - val_loss: 0.6846 - val_acc: 0.8276\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8977\n",
      "Epoch 00147: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3110 - acc: 0.8977 - val_loss: 0.6783 - val_acc: 0.8216\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8979\n",
      "Epoch 00148: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3119 - acc: 0.8979 - val_loss: 0.6680 - val_acc: 0.8325\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.9002\n",
      "Epoch 00149: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3053 - acc: 0.9002 - val_loss: 0.6751 - val_acc: 0.8258\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.8984\n",
      "Epoch 00150: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3085 - acc: 0.8984 - val_loss: 0.6691 - val_acc: 0.8267\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9032\n",
      "Epoch 00151: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2961 - acc: 0.9032 - val_loss: 0.9573 - val_acc: 0.7587\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.8983\n",
      "Epoch 00152: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3108 - acc: 0.8983 - val_loss: 0.8867 - val_acc: 0.7738\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9002\n",
      "Epoch 00153: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2998 - acc: 0.9002 - val_loss: 0.7357 - val_acc: 0.8171\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8990\n",
      "Epoch 00154: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3082 - acc: 0.8990 - val_loss: 0.7273 - val_acc: 0.8071\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.8995\n",
      "Epoch 00155: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3092 - acc: 0.8994 - val_loss: 0.6976 - val_acc: 0.8195\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9009\n",
      "Epoch 00156: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3050 - acc: 0.9009 - val_loss: 0.6994 - val_acc: 0.8239\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9028\n",
      "Epoch 00157: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2968 - acc: 0.9028 - val_loss: 0.6834 - val_acc: 0.8295\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9041\n",
      "Epoch 00158: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2951 - acc: 0.9041 - val_loss: 0.7867 - val_acc: 0.7950\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9022\n",
      "Epoch 00159: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2987 - acc: 0.9022 - val_loss: 0.8860 - val_acc: 0.7692\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9027\n",
      "Epoch 00160: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2947 - acc: 0.9027 - val_loss: 0.8390 - val_acc: 0.7899\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9029\n",
      "Epoch 00161: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2971 - acc: 0.9028 - val_loss: 0.7063 - val_acc: 0.8213\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9029\n",
      "Epoch 00162: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2969 - acc: 0.9029 - val_loss: 0.8591 - val_acc: 0.7822\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9021\n",
      "Epoch 00163: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2996 - acc: 0.9021 - val_loss: 0.6640 - val_acc: 0.8381\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9042\n",
      "Epoch 00164: val_loss did not improve from 0.65987\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2920 - acc: 0.9042 - val_loss: 0.7895 - val_acc: 0.7987\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9020\n",
      "Epoch 00165: val_loss improved from 0.65987 to 0.65785, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv_checkpoint/165-0.6579.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2990 - acc: 0.9020 - val_loss: 0.6579 - val_acc: 0.8316\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9039\n",
      "Epoch 00166: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2925 - acc: 0.9038 - val_loss: 0.6614 - val_acc: 0.8267\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9053\n",
      "Epoch 00167: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2908 - acc: 0.9053 - val_loss: 0.7373 - val_acc: 0.8174\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9045\n",
      "Epoch 00168: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2889 - acc: 0.9045 - val_loss: 0.6829 - val_acc: 0.8281\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9064\n",
      "Epoch 00169: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2858 - acc: 0.9064 - val_loss: 0.6971 - val_acc: 0.8246\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9076\n",
      "Epoch 00170: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2866 - acc: 0.9076 - val_loss: 0.6928 - val_acc: 0.8283\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.9055\n",
      "Epoch 00171: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2883 - acc: 0.9055 - val_loss: 0.7415 - val_acc: 0.8085\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9067\n",
      "Epoch 00172: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2891 - acc: 0.9067 - val_loss: 0.7737 - val_acc: 0.8020\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9087\n",
      "Epoch 00173: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2833 - acc: 0.9087 - val_loss: 0.6967 - val_acc: 0.8260\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.9065\n",
      "Epoch 00174: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2852 - acc: 0.9065 - val_loss: 1.2063 - val_acc: 0.6986\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9066\n",
      "Epoch 00175: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2862 - acc: 0.9066 - val_loss: 0.6613 - val_acc: 0.8330\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9074\n",
      "Epoch 00176: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2814 - acc: 0.9074 - val_loss: 0.7248 - val_acc: 0.8160\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9066\n",
      "Epoch 00177: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2861 - acc: 0.9066 - val_loss: 0.8060 - val_acc: 0.7962\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.9068\n",
      "Epoch 00178: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2835 - acc: 0.9068 - val_loss: 0.8397 - val_acc: 0.7836\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9048\n",
      "Epoch 00179: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2880 - acc: 0.9048 - val_loss: 0.6959 - val_acc: 0.8244\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9079\n",
      "Epoch 00180: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2813 - acc: 0.9079 - val_loss: 0.7638 - val_acc: 0.8088\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9056\n",
      "Epoch 00181: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2841 - acc: 0.9056 - val_loss: 0.6826 - val_acc: 0.8314\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9075\n",
      "Epoch 00182: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2817 - acc: 0.9075 - val_loss: 0.7041 - val_acc: 0.8213\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9087\n",
      "Epoch 00183: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2786 - acc: 0.9087 - val_loss: 0.7008 - val_acc: 0.8181\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9091\n",
      "Epoch 00184: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2728 - acc: 0.9091 - val_loss: 0.6731 - val_acc: 0.8295\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9074\n",
      "Epoch 00185: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2812 - acc: 0.9073 - val_loss: 0.7389 - val_acc: 0.8146\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2775 - acc: 0.9102\n",
      "Epoch 00186: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2775 - acc: 0.9101 - val_loss: 0.7751 - val_acc: 0.8069\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9108\n",
      "Epoch 00187: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2740 - acc: 0.9108 - val_loss: 1.3717 - val_acc: 0.6897\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9102\n",
      "Epoch 00188: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2769 - acc: 0.9102 - val_loss: 0.6790 - val_acc: 0.8220\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9079\n",
      "Epoch 00189: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2776 - acc: 0.9079 - val_loss: 0.6729 - val_acc: 0.8348\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9123\n",
      "Epoch 00190: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2738 - acc: 0.9123 - val_loss: 0.8293 - val_acc: 0.7978\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9099\n",
      "Epoch 00191: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2753 - acc: 0.9099 - val_loss: 0.7590 - val_acc: 0.8143\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9091\n",
      "Epoch 00192: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2800 - acc: 0.9091 - val_loss: 0.7421 - val_acc: 0.8150\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9110\n",
      "Epoch 00193: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2693 - acc: 0.9110 - val_loss: 0.6723 - val_acc: 0.8337\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9088\n",
      "Epoch 00194: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2778 - acc: 0.9088 - val_loss: 0.7169 - val_acc: 0.8260\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9109\n",
      "Epoch 00195: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2704 - acc: 0.9109 - val_loss: 0.6792 - val_acc: 0.8265\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9103\n",
      "Epoch 00196: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2691 - acc: 0.9103 - val_loss: 0.6737 - val_acc: 0.8330\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2690 - acc: 0.9112\n",
      "Epoch 00197: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2690 - acc: 0.9112 - val_loss: 0.6918 - val_acc: 0.8334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9121\n",
      "Epoch 00198: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2674 - acc: 0.9121 - val_loss: 0.6898 - val_acc: 0.8316\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9105\n",
      "Epoch 00199: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2693 - acc: 0.9104 - val_loss: 0.7101 - val_acc: 0.8223\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9129\n",
      "Epoch 00200: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2681 - acc: 0.9129 - val_loss: 0.8052 - val_acc: 0.7985\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9137\n",
      "Epoch 00201: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2649 - acc: 0.9137 - val_loss: 0.7249 - val_acc: 0.8143\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.9114\n",
      "Epoch 00202: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2670 - acc: 0.9114 - val_loss: 0.7334 - val_acc: 0.8225\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9111\n",
      "Epoch 00203: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2679 - acc: 0.9111 - val_loss: 0.6851 - val_acc: 0.8346\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9117\n",
      "Epoch 00204: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2667 - acc: 0.9117 - val_loss: 0.7367 - val_acc: 0.8174\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9113\n",
      "Epoch 00205: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2700 - acc: 0.9113 - val_loss: 0.6825 - val_acc: 0.8237\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9123\n",
      "Epoch 00206: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2666 - acc: 0.9123 - val_loss: 0.7039 - val_acc: 0.8295\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9149\n",
      "Epoch 00207: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2604 - acc: 0.9149 - val_loss: 0.6882 - val_acc: 0.8232\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9146\n",
      "Epoch 00208: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2640 - acc: 0.9145 - val_loss: 0.7382 - val_acc: 0.8241\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9114\n",
      "Epoch 00209: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2656 - acc: 0.9114 - val_loss: 0.7094 - val_acc: 0.8302\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9154\n",
      "Epoch 00210: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2612 - acc: 0.9154 - val_loss: 0.7026 - val_acc: 0.8251\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9091\n",
      "Epoch 00211: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2734 - acc: 0.9091 - val_loss: 0.7887 - val_acc: 0.8022\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9130\n",
      "Epoch 00212: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2620 - acc: 0.9131 - val_loss: 0.6850 - val_acc: 0.8332\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9168\n",
      "Epoch 00213: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2559 - acc: 0.9168 - val_loss: 0.7802 - val_acc: 0.8064\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9133\n",
      "Epoch 00214: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2610 - acc: 0.9133 - val_loss: 0.7927 - val_acc: 0.8032\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9165\n",
      "Epoch 00215: val_loss did not improve from 0.65785\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2582 - acc: 0.9164 - val_loss: 0.8867 - val_acc: 0.7876\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 472us/sample - loss: 0.7213 - acc: 0.8019\n",
      "Loss: 0.721279725231238 Accuracy: 0.80186915\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.2662 - acc: 0.1808\n",
      "Epoch 00001: val_loss improved from inf to 1.91855, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/001-1.9186.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 3.2663 - acc: 0.1808 - val_loss: 1.9186 - val_acc: 0.3666\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1347 - acc: 0.3697\n",
      "Epoch 00002: val_loss improved from 1.91855 to 1.38745, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/002-1.3875.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 2.1346 - acc: 0.3697 - val_loss: 1.3875 - val_acc: 0.5756\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7358 - acc: 0.4721\n",
      "Epoch 00003: val_loss improved from 1.38745 to 1.19919, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/003-1.1992.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.7361 - acc: 0.4721 - val_loss: 1.1992 - val_acc: 0.6348\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5103 - acc: 0.5385\n",
      "Epoch 00004: val_loss improved from 1.19919 to 1.11693, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/004-1.1169.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.5102 - acc: 0.5385 - val_loss: 1.1169 - val_acc: 0.6569\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3365 - acc: 0.5908\n",
      "Epoch 00005: val_loss improved from 1.11693 to 1.00863, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/005-1.0086.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.3366 - acc: 0.5908 - val_loss: 1.0086 - val_acc: 0.6988\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2227 - acc: 0.6214\n",
      "Epoch 00006: val_loss improved from 1.00863 to 0.98953, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/006-0.9895.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2226 - acc: 0.6214 - val_loss: 0.9895 - val_acc: 0.7032\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1241 - acc: 0.6540\n",
      "Epoch 00007: val_loss improved from 0.98953 to 0.85897, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/007-0.8590.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1242 - acc: 0.6540 - val_loss: 0.8590 - val_acc: 0.7477\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0578 - acc: 0.6742\n",
      "Epoch 00008: val_loss did not improve from 0.85897\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.0579 - acc: 0.6742 - val_loss: 0.8694 - val_acc: 0.7463\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9992 - acc: 0.6923\n",
      "Epoch 00009: val_loss did not improve from 0.85897\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9993 - acc: 0.6923 - val_loss: 0.9485 - val_acc: 0.7142\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.7092\n",
      "Epoch 00010: val_loss improved from 0.85897 to 0.80518, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/010-0.8052.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9471 - acc: 0.7093 - val_loss: 0.8052 - val_acc: 0.7624\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9110 - acc: 0.7224\n",
      "Epoch 00011: val_loss improved from 0.80518 to 0.76181, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/011-0.7618.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9110 - acc: 0.7223 - val_loss: 0.7618 - val_acc: 0.7724\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8671 - acc: 0.7336\n",
      "Epoch 00012: val_loss did not improve from 0.76181\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8673 - acc: 0.7336 - val_loss: 0.7991 - val_acc: 0.7589\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8464 - acc: 0.7395\n",
      "Epoch 00013: val_loss did not improve from 0.76181\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8464 - acc: 0.7395 - val_loss: 0.7850 - val_acc: 0.7689\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8153 - acc: 0.7501\n",
      "Epoch 00014: val_loss improved from 0.76181 to 0.75636, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/014-0.7564.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8152 - acc: 0.7501 - val_loss: 0.7564 - val_acc: 0.7824\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7571\n",
      "Epoch 00015: val_loss improved from 0.75636 to 0.73259, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/015-0.7326.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7883 - acc: 0.7571 - val_loss: 0.7326 - val_acc: 0.7855\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7691 - acc: 0.7632\n",
      "Epoch 00016: val_loss improved from 0.73259 to 0.68556, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/016-0.6856.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7690 - acc: 0.7632 - val_loss: 0.6856 - val_acc: 0.8083\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7701\n",
      "Epoch 00017: val_loss did not improve from 0.68556\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7489 - acc: 0.7701 - val_loss: 0.7199 - val_acc: 0.7890\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7359 - acc: 0.7749\n",
      "Epoch 00018: val_loss improved from 0.68556 to 0.67447, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/018-0.6745.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7359 - acc: 0.7749 - val_loss: 0.6745 - val_acc: 0.8041\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7828\n",
      "Epoch 00019: val_loss did not improve from 0.67447\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7168 - acc: 0.7827 - val_loss: 0.7050 - val_acc: 0.8027\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.7866\n",
      "Epoch 00020: val_loss did not improve from 0.67447\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7015 - acc: 0.7866 - val_loss: 0.6897 - val_acc: 0.7983\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7908\n",
      "Epoch 00021: val_loss improved from 0.67447 to 0.64984, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/021-0.6498.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6918 - acc: 0.7908 - val_loss: 0.6498 - val_acc: 0.8104\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6729 - acc: 0.7940\n",
      "Epoch 00022: val_loss did not improve from 0.64984\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6729 - acc: 0.7939 - val_loss: 0.7344 - val_acc: 0.7841\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.7977\n",
      "Epoch 00023: val_loss improved from 0.64984 to 0.61092, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/023-0.6109.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6620 - acc: 0.7977 - val_loss: 0.6109 - val_acc: 0.8290\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.7999\n",
      "Epoch 00024: val_loss did not improve from 0.61092\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6502 - acc: 0.7998 - val_loss: 0.6306 - val_acc: 0.8148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6371 - acc: 0.8070\n",
      "Epoch 00025: val_loss improved from 0.61092 to 0.60792, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/025-0.6079.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6372 - acc: 0.8070 - val_loss: 0.6079 - val_acc: 0.8309\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.8083\n",
      "Epoch 00026: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6291 - acc: 0.8083 - val_loss: 0.8976 - val_acc: 0.7405\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.8112\n",
      "Epoch 00027: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6190 - acc: 0.8112 - val_loss: 0.7101 - val_acc: 0.7966\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6054 - acc: 0.8151\n",
      "Epoch 00028: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6053 - acc: 0.8151 - val_loss: 0.6347 - val_acc: 0.8169\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.8156\n",
      "Epoch 00029: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6013 - acc: 0.8156 - val_loss: 0.6461 - val_acc: 0.8132\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.8197\n",
      "Epoch 00030: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5885 - acc: 0.8197 - val_loss: 0.6483 - val_acc: 0.8218\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8230\n",
      "Epoch 00031: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5790 - acc: 0.8230 - val_loss: 0.6187 - val_acc: 0.8178\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8242\n",
      "Epoch 00032: val_loss did not improve from 0.60792\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5740 - acc: 0.8242 - val_loss: 0.6103 - val_acc: 0.8276\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.8263\n",
      "Epoch 00033: val_loss improved from 0.60792 to 0.59336, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/033-0.5934.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5632 - acc: 0.8263 - val_loss: 0.5934 - val_acc: 0.8258\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.8271\n",
      "Epoch 00034: val_loss did not improve from 0.59336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5629 - acc: 0.8271 - val_loss: 0.6253 - val_acc: 0.8248\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.8324\n",
      "Epoch 00035: val_loss did not improve from 0.59336\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5470 - acc: 0.8324 - val_loss: 0.5996 - val_acc: 0.8295\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8315\n",
      "Epoch 00036: val_loss improved from 0.59336 to 0.57829, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/036-0.5783.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5447 - acc: 0.8314 - val_loss: 0.5783 - val_acc: 0.8404\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.8345\n",
      "Epoch 00037: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5390 - acc: 0.8345 - val_loss: 0.7390 - val_acc: 0.7845\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8372\n",
      "Epoch 00038: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5315 - acc: 0.8372 - val_loss: 0.7027 - val_acc: 0.7957\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5250 - acc: 0.8390\n",
      "Epoch 00039: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5251 - acc: 0.8390 - val_loss: 0.6622 - val_acc: 0.8116\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8421\n",
      "Epoch 00040: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5107 - acc: 0.8421 - val_loss: 0.7312 - val_acc: 0.7927\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5104 - acc: 0.8437\n",
      "Epoch 00041: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5104 - acc: 0.8437 - val_loss: 0.6177 - val_acc: 0.8300\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.8455\n",
      "Epoch 00042: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5048 - acc: 0.8455 - val_loss: 0.6224 - val_acc: 0.8185\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8467\n",
      "Epoch 00043: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4970 - acc: 0.8467 - val_loss: 0.5859 - val_acc: 0.8274\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4958 - acc: 0.8474\n",
      "Epoch 00044: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4961 - acc: 0.8473 - val_loss: 0.7513 - val_acc: 0.7906\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4922 - acc: 0.8465\n",
      "Epoch 00045: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4924 - acc: 0.8464 - val_loss: 0.7104 - val_acc: 0.7990\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8479\n",
      "Epoch 00046: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4870 - acc: 0.8479 - val_loss: 0.5849 - val_acc: 0.8400\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8527\n",
      "Epoch 00047: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4751 - acc: 0.8527 - val_loss: 0.6280 - val_acc: 0.8104\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8537\n",
      "Epoch 00048: val_loss did not improve from 0.57829\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4734 - acc: 0.8536 - val_loss: 0.6064 - val_acc: 0.8332\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4678 - acc: 0.8555\n",
      "Epoch 00049: val_loss improved from 0.57829 to 0.56992, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/049-0.5699.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4679 - acc: 0.8555 - val_loss: 0.5699 - val_acc: 0.8390\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8569\n",
      "Epoch 00050: val_loss did not improve from 0.56992\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4664 - acc: 0.8569 - val_loss: 0.6066 - val_acc: 0.8232\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.8564\n",
      "Epoch 00051: val_loss did not improve from 0.56992\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4646 - acc: 0.8564 - val_loss: 0.5817 - val_acc: 0.8332\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.8606\n",
      "Epoch 00052: val_loss improved from 0.56992 to 0.56794, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/052-0.5679.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4552 - acc: 0.8606 - val_loss: 0.5679 - val_acc: 0.8460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8604\n",
      "Epoch 00053: val_loss did not improve from 0.56794\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4496 - acc: 0.8604 - val_loss: 0.7728 - val_acc: 0.7862\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8600\n",
      "Epoch 00054: val_loss improved from 0.56794 to 0.56761, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/054-0.5676.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4503 - acc: 0.8600 - val_loss: 0.5676 - val_acc: 0.8486\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8616\n",
      "Epoch 00055: val_loss improved from 0.56761 to 0.54141, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/055-0.5414.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4426 - acc: 0.8616 - val_loss: 0.5414 - val_acc: 0.8532\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4339 - acc: 0.8630\n",
      "Epoch 00056: val_loss did not improve from 0.54141\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4340 - acc: 0.8630 - val_loss: 0.5909 - val_acc: 0.8381\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8643\n",
      "Epoch 00057: val_loss did not improve from 0.54141\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4351 - acc: 0.8643 - val_loss: 0.5918 - val_acc: 0.8360\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8657\n",
      "Epoch 00058: val_loss did not improve from 0.54141\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4338 - acc: 0.8656 - val_loss: 0.5428 - val_acc: 0.8495\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8651\n",
      "Epoch 00059: val_loss improved from 0.54141 to 0.53518, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/059-0.5352.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4271 - acc: 0.8651 - val_loss: 0.5352 - val_acc: 0.8491\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4271 - acc: 0.8674\n",
      "Epoch 00060: val_loss did not improve from 0.53518\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4272 - acc: 0.8674 - val_loss: 0.5819 - val_acc: 0.8390\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8683\n",
      "Epoch 00061: val_loss did not improve from 0.53518\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4225 - acc: 0.8682 - val_loss: 0.5590 - val_acc: 0.8414\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8695\n",
      "Epoch 00062: val_loss did not improve from 0.53518\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4191 - acc: 0.8695 - val_loss: 0.5701 - val_acc: 0.8430\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8718\n",
      "Epoch 00063: val_loss did not improve from 0.53518\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4158 - acc: 0.8718 - val_loss: 0.5723 - val_acc: 0.8414\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8730\n",
      "Epoch 00064: val_loss improved from 0.53518 to 0.52362, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/064-0.5236.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4100 - acc: 0.8730 - val_loss: 0.5236 - val_acc: 0.8542\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8745\n",
      "Epoch 00065: val_loss did not improve from 0.52362\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4036 - acc: 0.8745 - val_loss: 0.5283 - val_acc: 0.8560\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8764\n",
      "Epoch 00066: val_loss did not improve from 0.52362\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4034 - acc: 0.8764 - val_loss: 0.5397 - val_acc: 0.8537\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8735\n",
      "Epoch 00067: val_loss did not improve from 0.52362\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4033 - acc: 0.8735 - val_loss: 0.5617 - val_acc: 0.8437\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8763\n",
      "Epoch 00068: val_loss improved from 0.52362 to 0.52186, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/068-0.5219.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3966 - acc: 0.8763 - val_loss: 0.5219 - val_acc: 0.8542\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8739\n",
      "Epoch 00069: val_loss did not improve from 0.52186\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3966 - acc: 0.8739 - val_loss: 0.5493 - val_acc: 0.8479\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8752\n",
      "Epoch 00070: val_loss did not improve from 0.52186\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3951 - acc: 0.8752 - val_loss: 0.5484 - val_acc: 0.8484\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8756\n",
      "Epoch 00071: val_loss did not improve from 0.52186\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3933 - acc: 0.8756 - val_loss: 0.5375 - val_acc: 0.8460\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8785\n",
      "Epoch 00072: val_loss did not improve from 0.52186\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3905 - acc: 0.8785 - val_loss: 0.5601 - val_acc: 0.8446\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8805\n",
      "Epoch 00073: val_loss improved from 0.52186 to 0.51444, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/073-0.5144.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3862 - acc: 0.8805 - val_loss: 0.5144 - val_acc: 0.8656\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8811\n",
      "Epoch 00074: val_loss did not improve from 0.51444\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3785 - acc: 0.8811 - val_loss: 0.5729 - val_acc: 0.8449\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8775\n",
      "Epoch 00075: val_loss did not improve from 0.51444\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3822 - acc: 0.8775 - val_loss: 0.5383 - val_acc: 0.8500\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8818\n",
      "Epoch 00076: val_loss did not improve from 0.51444\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3770 - acc: 0.8818 - val_loss: 0.5566 - val_acc: 0.8439\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8827\n",
      "Epoch 00077: val_loss did not improve from 0.51444\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3726 - acc: 0.8827 - val_loss: 0.5907 - val_acc: 0.8421\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8821\n",
      "Epoch 00078: val_loss did not improve from 0.51444\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3718 - acc: 0.8820 - val_loss: 0.5174 - val_acc: 0.8593\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8839\n",
      "Epoch 00079: val_loss improved from 0.51444 to 0.51384, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/079-0.5138.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3675 - acc: 0.8839 - val_loss: 0.5138 - val_acc: 0.8572\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8841\n",
      "Epoch 00080: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3692 - acc: 0.8840 - val_loss: 0.5340 - val_acc: 0.8535\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8833\n",
      "Epoch 00081: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3657 - acc: 0.8833 - val_loss: 0.5332 - val_acc: 0.8577\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8871\n",
      "Epoch 00082: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3573 - acc: 0.8872 - val_loss: 0.5527 - val_acc: 0.8479\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3640 - acc: 0.8847\n",
      "Epoch 00083: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3640 - acc: 0.8847 - val_loss: 0.5304 - val_acc: 0.8586\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3615 - acc: 0.8875\n",
      "Epoch 00084: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3615 - acc: 0.8875 - val_loss: 0.5921 - val_acc: 0.8374\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8848\n",
      "Epoch 00085: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3631 - acc: 0.8847 - val_loss: 0.5146 - val_acc: 0.8598\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8895\n",
      "Epoch 00086: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3533 - acc: 0.8895 - val_loss: 0.5422 - val_acc: 0.8542\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8874\n",
      "Epoch 00087: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3541 - acc: 0.8874 - val_loss: 0.5226 - val_acc: 0.8628\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8894\n",
      "Epoch 00088: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3482 - acc: 0.8894 - val_loss: 0.5241 - val_acc: 0.8626\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8888\n",
      "Epoch 00089: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3503 - acc: 0.8888 - val_loss: 0.5306 - val_acc: 0.8570\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8937\n",
      "Epoch 00090: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3369 - acc: 0.8937 - val_loss: 0.5540 - val_acc: 0.8512\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8915\n",
      "Epoch 00091: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3432 - acc: 0.8915 - val_loss: 0.5177 - val_acc: 0.8619\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8932\n",
      "Epoch 00092: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3388 - acc: 0.8931 - val_loss: 0.6008 - val_acc: 0.8330\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8917\n",
      "Epoch 00093: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3411 - acc: 0.8917 - val_loss: 0.5170 - val_acc: 0.8616\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8947\n",
      "Epoch 00094: val_loss did not improve from 0.51384\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3338 - acc: 0.8947 - val_loss: 0.5200 - val_acc: 0.8581\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8960\n",
      "Epoch 00095: val_loss improved from 0.51384 to 0.50019, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/095-0.5002.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3299 - acc: 0.8960 - val_loss: 0.5002 - val_acc: 0.8654\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.8950\n",
      "Epoch 00096: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3302 - acc: 0.8950 - val_loss: 0.5261 - val_acc: 0.8626\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8954\n",
      "Epoch 00097: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3294 - acc: 0.8954 - val_loss: 0.8210 - val_acc: 0.7894\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8959\n",
      "Epoch 00098: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3278 - acc: 0.8959 - val_loss: 0.5360 - val_acc: 0.8544\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8944\n",
      "Epoch 00099: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3271 - acc: 0.8944 - val_loss: 0.5275 - val_acc: 0.8593\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8972\n",
      "Epoch 00100: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3272 - acc: 0.8972 - val_loss: 0.5718 - val_acc: 0.8528\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8937\n",
      "Epoch 00101: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3299 - acc: 0.8937 - val_loss: 0.5624 - val_acc: 0.8470\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8968\n",
      "Epoch 00102: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3233 - acc: 0.8968 - val_loss: 0.5263 - val_acc: 0.8563\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8985\n",
      "Epoch 00103: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3229 - acc: 0.8985 - val_loss: 0.5277 - val_acc: 0.8593\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8991\n",
      "Epoch 00104: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3156 - acc: 0.8991 - val_loss: 0.5970 - val_acc: 0.8400\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9002\n",
      "Epoch 00105: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3108 - acc: 0.9003 - val_loss: 0.5257 - val_acc: 0.8619\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8982\n",
      "Epoch 00106: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3145 - acc: 0.8982 - val_loss: 0.5203 - val_acc: 0.8546\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.8994\n",
      "Epoch 00107: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3127 - acc: 0.8994 - val_loss: 0.5592 - val_acc: 0.8449\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.9012\n",
      "Epoch 00108: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3122 - acc: 0.9012 - val_loss: 0.5882 - val_acc: 0.8400\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9003\n",
      "Epoch 00109: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3134 - acc: 0.9003 - val_loss: 0.5250 - val_acc: 0.8661\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8980\n",
      "Epoch 00110: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3156 - acc: 0.8979 - val_loss: 0.5348 - val_acc: 0.8609\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.8998\n",
      "Epoch 00111: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3103 - acc: 0.8997 - val_loss: 0.5459 - val_acc: 0.8481\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.9036\n",
      "Epoch 00112: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3083 - acc: 0.9037 - val_loss: 0.5393 - val_acc: 0.8565\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9025\n",
      "Epoch 00113: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3073 - acc: 0.9025 - val_loss: 0.5528 - val_acc: 0.8465\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9028\n",
      "Epoch 00114: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2996 - acc: 0.9028 - val_loss: 0.8780 - val_acc: 0.7773\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9021\n",
      "Epoch 00115: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3032 - acc: 0.9021 - val_loss: 0.5775 - val_acc: 0.8542\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9061\n",
      "Epoch 00116: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2952 - acc: 0.9061 - val_loss: 0.5238 - val_acc: 0.8675\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9037\n",
      "Epoch 00117: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2999 - acc: 0.9037 - val_loss: 0.5160 - val_acc: 0.8675\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9060\n",
      "Epoch 00118: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2971 - acc: 0.9060 - val_loss: 0.5437 - val_acc: 0.8560\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2996 - acc: 0.9029\n",
      "Epoch 00119: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2997 - acc: 0.9028 - val_loss: 0.5669 - val_acc: 0.8542\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.9064\n",
      "Epoch 00120: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2956 - acc: 0.9064 - val_loss: 0.5455 - val_acc: 0.8491\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9054\n",
      "Epoch 00121: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2937 - acc: 0.9053 - val_loss: 0.5121 - val_acc: 0.8686\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9060\n",
      "Epoch 00122: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2949 - acc: 0.9060 - val_loss: 0.5983 - val_acc: 0.8374\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9065\n",
      "Epoch 00123: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2897 - acc: 0.9064 - val_loss: 0.5024 - val_acc: 0.8712\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9059\n",
      "Epoch 00124: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2914 - acc: 0.9059 - val_loss: 0.5152 - val_acc: 0.8696\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9083\n",
      "Epoch 00125: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2860 - acc: 0.9083 - val_loss: 0.5108 - val_acc: 0.8616\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9089\n",
      "Epoch 00126: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2841 - acc: 0.9089 - val_loss: 0.5291 - val_acc: 0.8581\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9078\n",
      "Epoch 00127: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2874 - acc: 0.9078 - val_loss: 0.5794 - val_acc: 0.8514\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9093\n",
      "Epoch 00128: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2843 - acc: 0.9092 - val_loss: 0.5316 - val_acc: 0.8609\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9074\n",
      "Epoch 00129: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2896 - acc: 0.9074 - val_loss: 0.5341 - val_acc: 0.8612\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9092\n",
      "Epoch 00130: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2805 - acc: 0.9092 - val_loss: 0.5604 - val_acc: 0.8556\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9099\n",
      "Epoch 00131: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2832 - acc: 0.9099 - val_loss: 0.5286 - val_acc: 0.8658\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9102\n",
      "Epoch 00132: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2785 - acc: 0.9102 - val_loss: 0.5105 - val_acc: 0.8661\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9107\n",
      "Epoch 00133: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2763 - acc: 0.9107 - val_loss: 0.7185 - val_acc: 0.8150\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9107\n",
      "Epoch 00134: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2766 - acc: 0.9107 - val_loss: 0.5026 - val_acc: 0.8710\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9104\n",
      "Epoch 00135: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2772 - acc: 0.9104 - val_loss: 0.5183 - val_acc: 0.8693\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9132\n",
      "Epoch 00136: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2737 - acc: 0.9132 - val_loss: 0.5486 - val_acc: 0.8530\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9110\n",
      "Epoch 00137: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2778 - acc: 0.9110 - val_loss: 0.5223 - val_acc: 0.8647\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9120\n",
      "Epoch 00138: val_loss did not improve from 0.50019\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2708 - acc: 0.9119 - val_loss: 0.5665 - val_acc: 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9112\n",
      "Epoch 00139: val_loss improved from 0.50019 to 0.49684, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv_checkpoint/139-0.4968.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2737 - acc: 0.9112 - val_loss: 0.4968 - val_acc: 0.8737\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.9101\n",
      "Epoch 00140: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2745 - acc: 0.9101 - val_loss: 0.5004 - val_acc: 0.8728\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9129\n",
      "Epoch 00141: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2680 - acc: 0.9129 - val_loss: 0.5526 - val_acc: 0.8577\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9120\n",
      "Epoch 00142: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2728 - acc: 0.9119 - val_loss: 0.5146 - val_acc: 0.8661\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9124\n",
      "Epoch 00143: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2691 - acc: 0.9124 - val_loss: 0.6881 - val_acc: 0.8241\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9142\n",
      "Epoch 00144: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2671 - acc: 0.9143 - val_loss: 0.5096 - val_acc: 0.8744\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9134\n",
      "Epoch 00145: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2647 - acc: 0.9134 - val_loss: 0.4992 - val_acc: 0.8733\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9147\n",
      "Epoch 00146: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2628 - acc: 0.9147 - val_loss: 0.5102 - val_acc: 0.8679\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9130\n",
      "Epoch 00147: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2670 - acc: 0.9130 - val_loss: 0.6118 - val_acc: 0.8346\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9146\n",
      "Epoch 00148: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2657 - acc: 0.9145 - val_loss: 0.5402 - val_acc: 0.8609\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9154\n",
      "Epoch 00149: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2623 - acc: 0.9154 - val_loss: 0.5157 - val_acc: 0.8700\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9146\n",
      "Epoch 00150: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2641 - acc: 0.9145 - val_loss: 0.5195 - val_acc: 0.8679\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9173\n",
      "Epoch 00151: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2550 - acc: 0.9172 - val_loss: 0.5219 - val_acc: 0.8665\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9167\n",
      "Epoch 00152: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2587 - acc: 0.9166 - val_loss: 0.7725 - val_acc: 0.8043\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9179\n",
      "Epoch 00153: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2551 - acc: 0.9179 - val_loss: 0.5780 - val_acc: 0.8509\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9156\n",
      "Epoch 00154: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2570 - acc: 0.9156 - val_loss: 0.6115 - val_acc: 0.8470\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9163\n",
      "Epoch 00155: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2589 - acc: 0.9163 - val_loss: 0.5771 - val_acc: 0.8470\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9166\n",
      "Epoch 00156: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2559 - acc: 0.9166 - val_loss: 0.6393 - val_acc: 0.8451\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9159\n",
      "Epoch 00157: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2570 - acc: 0.9159 - val_loss: 0.5059 - val_acc: 0.8626\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9196\n",
      "Epoch 00158: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2538 - acc: 0.9196 - val_loss: 0.5344 - val_acc: 0.8616\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9192\n",
      "Epoch 00159: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2521 - acc: 0.9191 - val_loss: 0.5122 - val_acc: 0.8689\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9214\n",
      "Epoch 00160: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2476 - acc: 0.9214 - val_loss: 0.5214 - val_acc: 0.8698\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9185\n",
      "Epoch 00161: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2547 - acc: 0.9185 - val_loss: 0.4992 - val_acc: 0.8677\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9199\n",
      "Epoch 00162: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2465 - acc: 0.9199 - val_loss: 0.5237 - val_acc: 0.8721\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9166\n",
      "Epoch 00163: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2554 - acc: 0.9166 - val_loss: 0.6406 - val_acc: 0.8348\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9210\n",
      "Epoch 00164: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2465 - acc: 0.9210 - val_loss: 0.5245 - val_acc: 0.8661\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9205\n",
      "Epoch 00165: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2483 - acc: 0.9205 - val_loss: 0.5713 - val_acc: 0.8493\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9184\n",
      "Epoch 00166: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2454 - acc: 0.9184 - val_loss: 0.5209 - val_acc: 0.8637\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9200\n",
      "Epoch 00167: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2492 - acc: 0.9200 - val_loss: 0.5383 - val_acc: 0.8602\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9200\n",
      "Epoch 00168: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2464 - acc: 0.9200 - val_loss: 0.5087 - val_acc: 0.8684\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9200\n",
      "Epoch 00169: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2450 - acc: 0.9200 - val_loss: 0.5205 - val_acc: 0.8726\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9218\n",
      "Epoch 00170: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2439 - acc: 0.9218 - val_loss: 0.7284 - val_acc: 0.8099\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9195\n",
      "Epoch 00171: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2456 - acc: 0.9195 - val_loss: 0.5581 - val_acc: 0.8602\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9207\n",
      "Epoch 00172: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2419 - acc: 0.9207 - val_loss: 0.5721 - val_acc: 0.8526\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9207\n",
      "Epoch 00173: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2436 - acc: 0.9207 - val_loss: 0.5012 - val_acc: 0.8835\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9195\n",
      "Epoch 00174: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2452 - acc: 0.9195 - val_loss: 0.5276 - val_acc: 0.8735\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9245\n",
      "Epoch 00175: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2369 - acc: 0.9244 - val_loss: 0.6420 - val_acc: 0.8402\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9215\n",
      "Epoch 00176: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2440 - acc: 0.9215 - val_loss: 0.5242 - val_acc: 0.8656\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9224\n",
      "Epoch 00177: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2368 - acc: 0.9224 - val_loss: 0.5106 - val_acc: 0.8726\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9207\n",
      "Epoch 00178: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2445 - acc: 0.9207 - val_loss: 0.5305 - val_acc: 0.8700\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9223\n",
      "Epoch 00179: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2399 - acc: 0.9223 - val_loss: 0.5575 - val_acc: 0.8593\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9214\n",
      "Epoch 00180: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2358 - acc: 0.9214 - val_loss: 0.5606 - val_acc: 0.8598\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9226\n",
      "Epoch 00181: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2330 - acc: 0.9226 - val_loss: 0.5244 - val_acc: 0.8744\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9235\n",
      "Epoch 00182: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2342 - acc: 0.9235 - val_loss: 0.5654 - val_acc: 0.8535\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9238\n",
      "Epoch 00183: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2362 - acc: 0.9238 - val_loss: 0.5739 - val_acc: 0.8612\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9213\n",
      "Epoch 00184: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2402 - acc: 0.9213 - val_loss: 0.5002 - val_acc: 0.8717\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9247\n",
      "Epoch 00185: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2349 - acc: 0.9247 - val_loss: 0.5826 - val_acc: 0.8546\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9249\n",
      "Epoch 00186: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2324 - acc: 0.9249 - val_loss: 0.4978 - val_acc: 0.8824\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9257\n",
      "Epoch 00187: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2302 - acc: 0.9257 - val_loss: 0.5876 - val_acc: 0.8449\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9232\n",
      "Epoch 00188: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2355 - acc: 0.9232 - val_loss: 0.5692 - val_acc: 0.8570\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9256\n",
      "Epoch 00189: val_loss did not improve from 0.49684\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2317 - acc: 0.9256 - val_loss: 0.5936 - val_acc: 0.8551\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 507us/sample - loss: 0.5648 - acc: 0.8461\n",
      "Loss: 0.5647518580947709 Accuracy: 0.84610593\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3059 - acc: 0.1417\n",
      "Epoch 00001: val_loss improved from inf to 2.13847, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/001-2.1385.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 3.3059 - acc: 0.1417 - val_loss: 2.1385 - val_acc: 0.3068\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2627 - acc: 0.3120\n",
      "Epoch 00002: val_loss improved from 2.13847 to 1.56266, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/002-1.5627.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 2.2627 - acc: 0.3120 - val_loss: 1.5627 - val_acc: 0.5208\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8207 - acc: 0.4250\n",
      "Epoch 00003: val_loss improved from 1.56266 to 1.28078, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/003-1.2808.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.8207 - acc: 0.4249 - val_loss: 1.2808 - val_acc: 0.6089\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5485 - acc: 0.5079\n",
      "Epoch 00004: val_loss improved from 1.28078 to 1.20182, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/004-1.2018.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.5485 - acc: 0.5079 - val_loss: 1.2018 - val_acc: 0.6259\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3545 - acc: 0.5715\n",
      "Epoch 00005: val_loss improved from 1.20182 to 0.99328, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/005-0.9933.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3545 - acc: 0.5715 - val_loss: 0.9933 - val_acc: 0.7046\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2133 - acc: 0.6154\n",
      "Epoch 00006: val_loss did not improve from 0.99328\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.2134 - acc: 0.6153 - val_loss: 1.2140 - val_acc: 0.6177\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.6513\n",
      "Epoch 00007: val_loss improved from 0.99328 to 0.84920, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/007-0.8492.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1042 - acc: 0.6513 - val_loss: 0.8492 - val_acc: 0.7575\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6799\n",
      "Epoch 00008: val_loss improved from 0.84920 to 0.81041, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/008-0.8104.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0299 - acc: 0.6799 - val_loss: 0.8104 - val_acc: 0.7720\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9632 - acc: 0.7035\n",
      "Epoch 00009: val_loss improved from 0.81041 to 0.76281, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/009-0.7628.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9631 - acc: 0.7036 - val_loss: 0.7628 - val_acc: 0.7813\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7247\n",
      "Epoch 00010: val_loss improved from 0.76281 to 0.71700, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/010-0.7170.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9022 - acc: 0.7247 - val_loss: 0.7170 - val_acc: 0.7922\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8609 - acc: 0.7376\n",
      "Epoch 00011: val_loss improved from 0.71700 to 0.69907, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/011-0.6991.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8609 - acc: 0.7376 - val_loss: 0.6991 - val_acc: 0.7994\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8194 - acc: 0.7526\n",
      "Epoch 00012: val_loss improved from 0.69907 to 0.66990, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/012-0.6699.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8195 - acc: 0.7526 - val_loss: 0.6699 - val_acc: 0.8109\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.7597\n",
      "Epoch 00013: val_loss improved from 0.66990 to 0.64794, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/013-0.6479.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7937 - acc: 0.7597 - val_loss: 0.6479 - val_acc: 0.8132\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.7724\n",
      "Epoch 00014: val_loss did not improve from 0.64794\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7626 - acc: 0.7724 - val_loss: 0.6610 - val_acc: 0.8146\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7400 - acc: 0.7770\n",
      "Epoch 00015: val_loss did not improve from 0.64794\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7400 - acc: 0.7770 - val_loss: 0.6568 - val_acc: 0.8153\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7194 - acc: 0.7847\n",
      "Epoch 00016: val_loss improved from 0.64794 to 0.62581, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/016-0.6258.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7194 - acc: 0.7847 - val_loss: 0.6258 - val_acc: 0.8199\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.7930\n",
      "Epoch 00017: val_loss improved from 0.62581 to 0.60670, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/017-0.6067.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6929 - acc: 0.7930 - val_loss: 0.6067 - val_acc: 0.8262\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6730 - acc: 0.7986\n",
      "Epoch 00018: val_loss improved from 0.60670 to 0.60281, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/018-0.6028.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6731 - acc: 0.7985 - val_loss: 0.6028 - val_acc: 0.8353\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.8060\n",
      "Epoch 00019: val_loss improved from 0.60281 to 0.54244, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/019-0.5424.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6534 - acc: 0.8060 - val_loss: 0.5424 - val_acc: 0.8446\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6373 - acc: 0.8114\n",
      "Epoch 00020: val_loss did not improve from 0.54244\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6373 - acc: 0.8114 - val_loss: 0.5906 - val_acc: 0.8309\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.8124\n",
      "Epoch 00021: val_loss improved from 0.54244 to 0.53834, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/021-0.5383.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6317 - acc: 0.8123 - val_loss: 0.5383 - val_acc: 0.8491\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6096 - acc: 0.8189\n",
      "Epoch 00022: val_loss improved from 0.53834 to 0.52003, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/022-0.5200.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6098 - acc: 0.8189 - val_loss: 0.5200 - val_acc: 0.8586\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.8204\n",
      "Epoch 00023: val_loss did not improve from 0.52003\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6034 - acc: 0.8204 - val_loss: 0.5291 - val_acc: 0.8516\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.8267\n",
      "Epoch 00024: val_loss did not improve from 0.52003\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5836 - acc: 0.8267 - val_loss: 0.5496 - val_acc: 0.8437\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5714 - acc: 0.8320\n",
      "Epoch 00025: val_loss did not improve from 0.52003\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5715 - acc: 0.8320 - val_loss: 0.5378 - val_acc: 0.8521\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5695 - acc: 0.8301\n",
      "Epoch 00026: val_loss did not improve from 0.52003\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5695 - acc: 0.8301 - val_loss: 0.5272 - val_acc: 0.8467\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5591 - acc: 0.8350\n",
      "Epoch 00027: val_loss improved from 0.52003 to 0.48796, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/027-0.4880.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5590 - acc: 0.8350 - val_loss: 0.4880 - val_acc: 0.8682\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8355\n",
      "Epoch 00028: val_loss improved from 0.48796 to 0.47739, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/028-0.4774.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5448 - acc: 0.8355 - val_loss: 0.4774 - val_acc: 0.8679\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8433\n",
      "Epoch 00029: val_loss did not improve from 0.47739\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5368 - acc: 0.8433 - val_loss: 0.4874 - val_acc: 0.8621\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8439\n",
      "Epoch 00030: val_loss improved from 0.47739 to 0.46242, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/030-0.4624.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5302 - acc: 0.8439 - val_loss: 0.4624 - val_acc: 0.8705\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5233 - acc: 0.8432\n",
      "Epoch 00031: val_loss improved from 0.46242 to 0.46032, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/031-0.4603.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5234 - acc: 0.8432 - val_loss: 0.4603 - val_acc: 0.8786\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8449\n",
      "Epoch 00032: val_loss did not improve from 0.46032\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5150 - acc: 0.8449 - val_loss: 0.4621 - val_acc: 0.8779\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5049 - acc: 0.8491\n",
      "Epoch 00033: val_loss improved from 0.46032 to 0.46021, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/033-0.4602.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5048 - acc: 0.8491 - val_loss: 0.4602 - val_acc: 0.8754\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8507\n",
      "Epoch 00034: val_loss did not improve from 0.46021\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4993 - acc: 0.8507 - val_loss: 0.4649 - val_acc: 0.8689\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8524\n",
      "Epoch 00035: val_loss did not improve from 0.46021\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4899 - acc: 0.8524 - val_loss: 0.4719 - val_acc: 0.8670\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8543\n",
      "Epoch 00036: val_loss did not improve from 0.46021\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4871 - acc: 0.8542 - val_loss: 0.4787 - val_acc: 0.8623\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8573\n",
      "Epoch 00037: val_loss improved from 0.46021 to 0.43849, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/037-0.4385.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4798 - acc: 0.8572 - val_loss: 0.4385 - val_acc: 0.8842\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8602\n",
      "Epoch 00038: val_loss improved from 0.43849 to 0.42472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/038-0.4247.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4745 - acc: 0.8602 - val_loss: 0.4247 - val_acc: 0.8838\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4676 - acc: 0.8607\n",
      "Epoch 00039: val_loss did not improve from 0.42472\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4677 - acc: 0.8606 - val_loss: 0.4885 - val_acc: 0.8612\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4669 - acc: 0.8612\n",
      "Epoch 00040: val_loss improved from 0.42472 to 0.41865, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/040-0.4187.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4669 - acc: 0.8612 - val_loss: 0.4187 - val_acc: 0.8840\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8636\n",
      "Epoch 00041: val_loss did not improve from 0.41865\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4567 - acc: 0.8636 - val_loss: 0.4851 - val_acc: 0.8672\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8632\n",
      "Epoch 00042: val_loss did not improve from 0.41865\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4474 - acc: 0.8632 - val_loss: 0.4384 - val_acc: 0.8849\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8632\n",
      "Epoch 00043: val_loss did not improve from 0.41865\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4547 - acc: 0.8631 - val_loss: 0.4237 - val_acc: 0.8803\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8675\n",
      "Epoch 00044: val_loss improved from 0.41865 to 0.41780, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/044-0.4178.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4403 - acc: 0.8675 - val_loss: 0.4178 - val_acc: 0.8921\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4434 - acc: 0.8672\n",
      "Epoch 00045: val_loss did not improve from 0.41780\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4437 - acc: 0.8671 - val_loss: 0.4293 - val_acc: 0.8854\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8679\n",
      "Epoch 00046: val_loss did not improve from 0.41780\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4399 - acc: 0.8679 - val_loss: 0.4502 - val_acc: 0.8758\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4281 - acc: 0.8721\n",
      "Epoch 00047: val_loss improved from 0.41780 to 0.39385, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/047-0.3939.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4280 - acc: 0.8722 - val_loss: 0.3939 - val_acc: 0.8938\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8756\n",
      "Epoch 00048: val_loss did not improve from 0.39385\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4217 - acc: 0.8756 - val_loss: 0.3972 - val_acc: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4185 - acc: 0.8721\n",
      "Epoch 00049: val_loss did not improve from 0.39385\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4185 - acc: 0.8721 - val_loss: 0.4005 - val_acc: 0.8915\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8738\n",
      "Epoch 00050: val_loss did not improve from 0.39385\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4170 - acc: 0.8738 - val_loss: 0.4379 - val_acc: 0.8840\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8761\n",
      "Epoch 00051: val_loss improved from 0.39385 to 0.37718, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/051-0.3772.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4113 - acc: 0.8761 - val_loss: 0.3772 - val_acc: 0.8945\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4071 - acc: 0.8755\n",
      "Epoch 00052: val_loss did not improve from 0.37718\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4073 - acc: 0.8755 - val_loss: 0.4100 - val_acc: 0.8891\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8774\n",
      "Epoch 00053: val_loss improved from 0.37718 to 0.36002, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/053-0.3600.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4113 - acc: 0.8774 - val_loss: 0.3600 - val_acc: 0.9038\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3991 - acc: 0.8800\n",
      "Epoch 00054: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3992 - acc: 0.8800 - val_loss: 0.4102 - val_acc: 0.8991\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8776\n",
      "Epoch 00055: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4014 - acc: 0.8775 - val_loss: 0.3705 - val_acc: 0.8961\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8804\n",
      "Epoch 00056: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3986 - acc: 0.8804 - val_loss: 0.4033 - val_acc: 0.8940\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3897 - acc: 0.8828\n",
      "Epoch 00057: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3897 - acc: 0.8828 - val_loss: 0.3753 - val_acc: 0.9066\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3892 - acc: 0.8816\n",
      "Epoch 00058: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3893 - acc: 0.8815 - val_loss: 0.3885 - val_acc: 0.9015\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8835\n",
      "Epoch 00059: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3833 - acc: 0.8835 - val_loss: 0.3853 - val_acc: 0.8996\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8853\n",
      "Epoch 00060: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3808 - acc: 0.8852 - val_loss: 0.3741 - val_acc: 0.8987\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8848\n",
      "Epoch 00061: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3805 - acc: 0.8849 - val_loss: 0.3732 - val_acc: 0.8975\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8868\n",
      "Epoch 00062: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3746 - acc: 0.8868 - val_loss: 0.4327 - val_acc: 0.8831\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8863\n",
      "Epoch 00063: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3780 - acc: 0.8863 - val_loss: 0.3795 - val_acc: 0.9045\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8878\n",
      "Epoch 00064: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3696 - acc: 0.8878 - val_loss: 0.3651 - val_acc: 0.9047\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8862\n",
      "Epoch 00065: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3709 - acc: 0.8862 - val_loss: 0.3657 - val_acc: 0.9003\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8894\n",
      "Epoch 00066: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3631 - acc: 0.8894 - val_loss: 0.3806 - val_acc: 0.9005\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.8917\n",
      "Epoch 00067: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3583 - acc: 0.8917 - val_loss: 0.3688 - val_acc: 0.8952\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8890\n",
      "Epoch 00068: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3628 - acc: 0.8890 - val_loss: 0.3886 - val_acc: 0.8942\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8911\n",
      "Epoch 00069: val_loss did not improve from 0.36002\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3560 - acc: 0.8911 - val_loss: 0.3769 - val_acc: 0.9003\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8915\n",
      "Epoch 00070: val_loss improved from 0.36002 to 0.35104, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/070-0.3510.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3531 - acc: 0.8915 - val_loss: 0.3510 - val_acc: 0.9033\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.8938\n",
      "Epoch 00071: val_loss did not improve from 0.35104\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3525 - acc: 0.8937 - val_loss: 0.3650 - val_acc: 0.9043\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8927\n",
      "Epoch 00072: val_loss did not improve from 0.35104\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3508 - acc: 0.8927 - val_loss: 0.3805 - val_acc: 0.8968\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8919\n",
      "Epoch 00073: val_loss did not improve from 0.35104\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3481 - acc: 0.8919 - val_loss: 0.3759 - val_acc: 0.9033\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8942\n",
      "Epoch 00074: val_loss improved from 0.35104 to 0.34694, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/074-0.3469.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3466 - acc: 0.8942 - val_loss: 0.3469 - val_acc: 0.9043\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8957\n",
      "Epoch 00075: val_loss did not improve from 0.34694\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3408 - acc: 0.8957 - val_loss: 0.3897 - val_acc: 0.8942\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8961\n",
      "Epoch 00076: val_loss did not improve from 0.34694\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3391 - acc: 0.8960 - val_loss: 0.3741 - val_acc: 0.9019\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8951\n",
      "Epoch 00077: val_loss did not improve from 0.34694\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3426 - acc: 0.8951 - val_loss: 0.3529 - val_acc: 0.9078\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8984\n",
      "Epoch 00078: val_loss did not improve from 0.34694\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3319 - acc: 0.8983 - val_loss: 0.3720 - val_acc: 0.9008\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8970\n",
      "Epoch 00079: val_loss did not improve from 0.34694\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3329 - acc: 0.8969 - val_loss: 0.3531 - val_acc: 0.9080\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8992\n",
      "Epoch 00080: val_loss improved from 0.34694 to 0.34609, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/080-0.3461.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3322 - acc: 0.8993 - val_loss: 0.3461 - val_acc: 0.9045\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8983\n",
      "Epoch 00081: val_loss did not improve from 0.34609\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3284 - acc: 0.8984 - val_loss: 0.3676 - val_acc: 0.9059\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9032\n",
      "Epoch 00082: val_loss did not improve from 0.34609\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3226 - acc: 0.9032 - val_loss: 0.3798 - val_acc: 0.8989\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9013\n",
      "Epoch 00083: val_loss did not improve from 0.34609\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3236 - acc: 0.9013 - val_loss: 0.4029 - val_acc: 0.8963\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.9014\n",
      "Epoch 00084: val_loss improved from 0.34609 to 0.34383, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/084-0.3438.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3254 - acc: 0.9014 - val_loss: 0.3438 - val_acc: 0.9036\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9030\n",
      "Epoch 00085: val_loss improved from 0.34383 to 0.33607, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/085-0.3361.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3179 - acc: 0.9030 - val_loss: 0.3361 - val_acc: 0.9124\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.9037\n",
      "Epoch 00086: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3160 - acc: 0.9037 - val_loss: 0.3414 - val_acc: 0.9106\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9015\n",
      "Epoch 00087: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3162 - acc: 0.9015 - val_loss: 0.3408 - val_acc: 0.9103\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9034\n",
      "Epoch 00088: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3141 - acc: 0.9034 - val_loss: 0.4014 - val_acc: 0.8905\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9030\n",
      "Epoch 00089: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3138 - acc: 0.9029 - val_loss: 0.3470 - val_acc: 0.9061\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9031\n",
      "Epoch 00090: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3066 - acc: 0.9031 - val_loss: 0.3995 - val_acc: 0.8942\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9038\n",
      "Epoch 00091: val_loss did not improve from 0.33607\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3076 - acc: 0.9038 - val_loss: 0.3616 - val_acc: 0.9078\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9057\n",
      "Epoch 00092: val_loss improved from 0.33607 to 0.33121, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/092-0.3312.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3028 - acc: 0.9057 - val_loss: 0.3312 - val_acc: 0.9106\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9063\n",
      "Epoch 00093: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3007 - acc: 0.9063 - val_loss: 0.3583 - val_acc: 0.9064\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.9070\n",
      "Epoch 00094: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3004 - acc: 0.9070 - val_loss: 0.3455 - val_acc: 0.9096\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9062\n",
      "Epoch 00095: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3059 - acc: 0.9062 - val_loss: 0.3405 - val_acc: 0.9094\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9066\n",
      "Epoch 00096: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3005 - acc: 0.9066 - val_loss: 0.3371 - val_acc: 0.9140\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9091\n",
      "Epoch 00097: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2939 - acc: 0.9091 - val_loss: 0.3936 - val_acc: 0.8982\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.9072\n",
      "Epoch 00098: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2970 - acc: 0.9072 - val_loss: 0.4149 - val_acc: 0.8933\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9077\n",
      "Epoch 00099: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2998 - acc: 0.9077 - val_loss: 0.3325 - val_acc: 0.9136\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9085\n",
      "Epoch 00100: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2889 - acc: 0.9085 - val_loss: 0.4655 - val_acc: 0.8724\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9085\n",
      "Epoch 00101: val_loss did not improve from 0.33121\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2916 - acc: 0.9085 - val_loss: 0.3490 - val_acc: 0.9075\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9111\n",
      "Epoch 00102: val_loss improved from 0.33121 to 0.32880, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/102-0.3288.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2855 - acc: 0.9110 - val_loss: 0.3288 - val_acc: 0.9136\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9115\n",
      "Epoch 00103: val_loss did not improve from 0.32880\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2813 - acc: 0.9115 - val_loss: 0.3338 - val_acc: 0.9159\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9097\n",
      "Epoch 00104: val_loss did not improve from 0.32880\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2873 - acc: 0.9097 - val_loss: 0.3541 - val_acc: 0.9103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9121\n",
      "Epoch 00105: val_loss did not improve from 0.32880\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2873 - acc: 0.9121 - val_loss: 0.3511 - val_acc: 0.9094\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9100\n",
      "Epoch 00106: val_loss did not improve from 0.32880\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2876 - acc: 0.9100 - val_loss: 0.3452 - val_acc: 0.9110\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9105\n",
      "Epoch 00107: val_loss improved from 0.32880 to 0.32258, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/107-0.3226.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2833 - acc: 0.9105 - val_loss: 0.3226 - val_acc: 0.9194\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9127\n",
      "Epoch 00108: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2773 - acc: 0.9127 - val_loss: 0.3404 - val_acc: 0.9115\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2799 - acc: 0.9119\n",
      "Epoch 00109: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2798 - acc: 0.9119 - val_loss: 0.3397 - val_acc: 0.9110\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9135\n",
      "Epoch 00110: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2738 - acc: 0.9135 - val_loss: 0.3555 - val_acc: 0.9113\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9148\n",
      "Epoch 00111: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2756 - acc: 0.9148 - val_loss: 0.3513 - val_acc: 0.9078\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9141\n",
      "Epoch 00112: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2738 - acc: 0.9141 - val_loss: 0.3375 - val_acc: 0.9080\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9156\n",
      "Epoch 00113: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2740 - acc: 0.9156 - val_loss: 0.3256 - val_acc: 0.9189\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2716 - acc: 0.9155\n",
      "Epoch 00114: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2719 - acc: 0.9155 - val_loss: 0.3495 - val_acc: 0.9054\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9142\n",
      "Epoch 00115: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2746 - acc: 0.9142 - val_loss: 0.3508 - val_acc: 0.9108\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9172\n",
      "Epoch 00116: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2633 - acc: 0.9172 - val_loss: 0.3811 - val_acc: 0.9124\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9165\n",
      "Epoch 00117: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2630 - acc: 0.9165 - val_loss: 0.3516 - val_acc: 0.9138\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9180\n",
      "Epoch 00118: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2635 - acc: 0.9180 - val_loss: 0.3342 - val_acc: 0.9154\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9175\n",
      "Epoch 00119: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2605 - acc: 0.9175 - val_loss: 0.3270 - val_acc: 0.9159\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9127\n",
      "Epoch 00120: val_loss did not improve from 0.32258\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2754 - acc: 0.9127 - val_loss: 0.3286 - val_acc: 0.9133\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9181\n",
      "Epoch 00121: val_loss improved from 0.32258 to 0.31705, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/121-0.3170.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2606 - acc: 0.9181 - val_loss: 0.3170 - val_acc: 0.9168\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9190\n",
      "Epoch 00122: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2571 - acc: 0.9190 - val_loss: 0.3408 - val_acc: 0.9154\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9180\n",
      "Epoch 00123: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2614 - acc: 0.9180 - val_loss: 0.3377 - val_acc: 0.9099\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9189\n",
      "Epoch 00124: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2562 - acc: 0.9189 - val_loss: 0.3187 - val_acc: 0.9187\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.9218\n",
      "Epoch 00125: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2498 - acc: 0.9218 - val_loss: 0.3259 - val_acc: 0.9192\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9175\n",
      "Epoch 00126: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2610 - acc: 0.9175 - val_loss: 0.3453 - val_acc: 0.9087\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9201\n",
      "Epoch 00127: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2560 - acc: 0.9200 - val_loss: 0.3216 - val_acc: 0.9119\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.9199\n",
      "Epoch 00128: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2553 - acc: 0.9198 - val_loss: 0.3267 - val_acc: 0.9096\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9208\n",
      "Epoch 00129: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2530 - acc: 0.9208 - val_loss: 0.3398 - val_acc: 0.9138\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9205\n",
      "Epoch 00130: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2497 - acc: 0.9205 - val_loss: 0.3222 - val_acc: 0.9187\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9225\n",
      "Epoch 00131: val_loss did not improve from 0.31705\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2451 - acc: 0.9225 - val_loss: 0.3298 - val_acc: 0.9168\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9205\n",
      "Epoch 00132: val_loss improved from 0.31705 to 0.31148, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/132-0.3115.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2536 - acc: 0.9205 - val_loss: 0.3115 - val_acc: 0.9208\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9239\n",
      "Epoch 00133: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2416 - acc: 0.9240 - val_loss: 0.3172 - val_acc: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9221\n",
      "Epoch 00134: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2490 - acc: 0.9221 - val_loss: 0.3344 - val_acc: 0.9147\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2467 - acc: 0.9230\n",
      "Epoch 00135: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2466 - acc: 0.9231 - val_loss: 0.3579 - val_acc: 0.9138\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9231\n",
      "Epoch 00136: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2447 - acc: 0.9231 - val_loss: 0.3118 - val_acc: 0.9236\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9227\n",
      "Epoch 00137: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2440 - acc: 0.9227 - val_loss: 0.3282 - val_acc: 0.9224\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9209\n",
      "Epoch 00138: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2470 - acc: 0.9209 - val_loss: 0.3770 - val_acc: 0.9031\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9242\n",
      "Epoch 00139: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2413 - acc: 0.9241 - val_loss: 0.3357 - val_acc: 0.9129\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9236\n",
      "Epoch 00140: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2456 - acc: 0.9236 - val_loss: 0.3262 - val_acc: 0.9150\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9239\n",
      "Epoch 00141: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2389 - acc: 0.9239 - val_loss: 0.3313 - val_acc: 0.9157\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9229\n",
      "Epoch 00142: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2408 - acc: 0.9229 - val_loss: 0.3817 - val_acc: 0.8987\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9264\n",
      "Epoch 00143: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2341 - acc: 0.9264 - val_loss: 0.3807 - val_acc: 0.9024\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9222\n",
      "Epoch 00144: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2390 - acc: 0.9222 - val_loss: 0.3468 - val_acc: 0.9171\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9280\n",
      "Epoch 00145: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2292 - acc: 0.9279 - val_loss: 0.3125 - val_acc: 0.9224\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9265\n",
      "Epoch 00146: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2357 - acc: 0.9265 - val_loss: 0.3172 - val_acc: 0.9206\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9260\n",
      "Epoch 00147: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2326 - acc: 0.9260 - val_loss: 0.3275 - val_acc: 0.9152\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9266\n",
      "Epoch 00148: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2331 - acc: 0.9266 - val_loss: 0.3779 - val_acc: 0.9050\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9261\n",
      "Epoch 00149: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2304 - acc: 0.9261 - val_loss: 0.3270 - val_acc: 0.9168\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9280\n",
      "Epoch 00150: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2259 - acc: 0.9280 - val_loss: 0.3141 - val_acc: 0.9196\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9257\n",
      "Epoch 00151: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2341 - acc: 0.9257 - val_loss: 0.3225 - val_acc: 0.9145\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9272\n",
      "Epoch 00152: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2311 - acc: 0.9272 - val_loss: 0.3482 - val_acc: 0.9138\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9264\n",
      "Epoch 00153: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2283 - acc: 0.9263 - val_loss: 0.3182 - val_acc: 0.9173\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9274\n",
      "Epoch 00154: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2305 - acc: 0.9273 - val_loss: 0.4014 - val_acc: 0.9012\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9274\n",
      "Epoch 00155: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2289 - acc: 0.9273 - val_loss: 0.3246 - val_acc: 0.9180\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9278\n",
      "Epoch 00156: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2279 - acc: 0.9278 - val_loss: 0.3194 - val_acc: 0.9234\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9298\n",
      "Epoch 00157: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2197 - acc: 0.9297 - val_loss: 0.4245 - val_acc: 0.8917\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9277\n",
      "Epoch 00158: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2259 - acc: 0.9276 - val_loss: 0.3888 - val_acc: 0.8987\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9274\n",
      "Epoch 00159: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2280 - acc: 0.9273 - val_loss: 0.3132 - val_acc: 0.9250\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9311\n",
      "Epoch 00160: val_loss did not improve from 0.31148\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2158 - acc: 0.9311 - val_loss: 0.3197 - val_acc: 0.9238\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9307\n",
      "Epoch 00161: val_loss improved from 0.31148 to 0.30779, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/161-0.3078.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2165 - acc: 0.9307 - val_loss: 0.3078 - val_acc: 0.9215\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9304\n",
      "Epoch 00162: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2196 - acc: 0.9304 - val_loss: 0.3165 - val_acc: 0.9213\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9290\n",
      "Epoch 00163: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2211 - acc: 0.9290 - val_loss: 0.3285 - val_acc: 0.9206\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9283\n",
      "Epoch 00164: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2198 - acc: 0.9283 - val_loss: 0.3326 - val_acc: 0.9147\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9315\n",
      "Epoch 00165: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2137 - acc: 0.9315 - val_loss: 0.5518 - val_acc: 0.8637\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9291\n",
      "Epoch 00166: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2205 - acc: 0.9290 - val_loss: 0.3368 - val_acc: 0.9189\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9311\n",
      "Epoch 00167: val_loss did not improve from 0.30779\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2186 - acc: 0.9311 - val_loss: 0.3107 - val_acc: 0.9238\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9316\n",
      "Epoch 00168: val_loss improved from 0.30779 to 0.30010, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv_checkpoint/168-0.3001.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2118 - acc: 0.9316 - val_loss: 0.3001 - val_acc: 0.9248\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9315\n",
      "Epoch 00169: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2116 - acc: 0.9315 - val_loss: 0.3217 - val_acc: 0.9196\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9294\n",
      "Epoch 00170: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2176 - acc: 0.9293 - val_loss: 0.3204 - val_acc: 0.9269\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9289\n",
      "Epoch 00171: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2189 - acc: 0.9289 - val_loss: 0.3478 - val_acc: 0.9178\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9335\n",
      "Epoch 00172: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2101 - acc: 0.9335 - val_loss: 0.3861 - val_acc: 0.9040\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9328\n",
      "Epoch 00173: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2110 - acc: 0.9328 - val_loss: 0.3264 - val_acc: 0.9210\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9320\n",
      "Epoch 00174: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2091 - acc: 0.9320 - val_loss: 0.3488 - val_acc: 0.9159\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9323\n",
      "Epoch 00175: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2130 - acc: 0.9323 - val_loss: 0.3732 - val_acc: 0.9054\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9325\n",
      "Epoch 00176: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2078 - acc: 0.9325 - val_loss: 0.3129 - val_acc: 0.9215\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9328\n",
      "Epoch 00177: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2100 - acc: 0.9328 - val_loss: 0.3417 - val_acc: 0.9157\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9320\n",
      "Epoch 00178: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2088 - acc: 0.9320 - val_loss: 0.3245 - val_acc: 0.9248\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9306\n",
      "Epoch 00179: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2112 - acc: 0.9306 - val_loss: 0.3287 - val_acc: 0.9199\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9348\n",
      "Epoch 00180: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2064 - acc: 0.9348 - val_loss: 0.3327 - val_acc: 0.9159\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9327\n",
      "Epoch 00181: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2060 - acc: 0.9326 - val_loss: 0.3200 - val_acc: 0.9224\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9330\n",
      "Epoch 00182: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2075 - acc: 0.9330 - val_loss: 0.3664 - val_acc: 0.9143\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9340\n",
      "Epoch 00183: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2052 - acc: 0.9340 - val_loss: 0.3206 - val_acc: 0.9175\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9364\n",
      "Epoch 00184: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1998 - acc: 0.9364 - val_loss: 0.3184 - val_acc: 0.9234\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9358\n",
      "Epoch 00185: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1986 - acc: 0.9358 - val_loss: 0.3175 - val_acc: 0.9210\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9359\n",
      "Epoch 00186: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2015 - acc: 0.9359 - val_loss: 0.3242 - val_acc: 0.9224\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9350\n",
      "Epoch 00187: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2027 - acc: 0.9350 - val_loss: 0.3183 - val_acc: 0.9231\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9359\n",
      "Epoch 00188: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2017 - acc: 0.9359 - val_loss: 0.3503 - val_acc: 0.9194\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9357\n",
      "Epoch 00189: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1995 - acc: 0.9357 - val_loss: 0.3343 - val_acc: 0.9222\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9370\n",
      "Epoch 00190: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1963 - acc: 0.9370 - val_loss: 0.3349 - val_acc: 0.9199\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9348\n",
      "Epoch 00191: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2030 - acc: 0.9347 - val_loss: 0.3323 - val_acc: 0.9175\n",
      "Epoch 192/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9336\n",
      "Epoch 00192: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2011 - acc: 0.9336 - val_loss: 0.3396 - val_acc: 0.9189\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9371\n",
      "Epoch 00193: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1958 - acc: 0.9370 - val_loss: 0.3080 - val_acc: 0.9229\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9362\n",
      "Epoch 00194: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1945 - acc: 0.9362 - val_loss: 0.3306 - val_acc: 0.9196\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9355\n",
      "Epoch 00195: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1963 - acc: 0.9355 - val_loss: 0.3083 - val_acc: 0.9217\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9361\n",
      "Epoch 00196: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1987 - acc: 0.9361 - val_loss: 0.3174 - val_acc: 0.9189\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9361\n",
      "Epoch 00197: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2002 - acc: 0.9361 - val_loss: 0.3133 - val_acc: 0.9257\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9365\n",
      "Epoch 00198: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1956 - acc: 0.9366 - val_loss: 0.4670 - val_acc: 0.8849\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9365\n",
      "Epoch 00199: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1941 - acc: 0.9366 - val_loss: 0.3236 - val_acc: 0.9255\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9378\n",
      "Epoch 00200: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1929 - acc: 0.9378 - val_loss: 0.3094 - val_acc: 0.9259\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9368\n",
      "Epoch 00201: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1919 - acc: 0.9368 - val_loss: 0.3218 - val_acc: 0.9222\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9382\n",
      "Epoch 00202: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1922 - acc: 0.9382 - val_loss: 0.3308 - val_acc: 0.9222\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9390\n",
      "Epoch 00203: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1916 - acc: 0.9390 - val_loss: 0.3463 - val_acc: 0.9092\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9404\n",
      "Epoch 00204: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1876 - acc: 0.9404 - val_loss: 0.3726 - val_acc: 0.9248\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9378\n",
      "Epoch 00205: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1897 - acc: 0.9378 - val_loss: 0.3362 - val_acc: 0.9189\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9381\n",
      "Epoch 00206: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1917 - acc: 0.9381 - val_loss: 0.3419 - val_acc: 0.9206\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9390\n",
      "Epoch 00207: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1877 - acc: 0.9390 - val_loss: 0.3518 - val_acc: 0.9196\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9377\n",
      "Epoch 00208: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1900 - acc: 0.9377 - val_loss: 0.3117 - val_acc: 0.9264\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9380\n",
      "Epoch 00209: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1932 - acc: 0.9380 - val_loss: 0.3389 - val_acc: 0.9166\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9404\n",
      "Epoch 00210: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1862 - acc: 0.9404 - val_loss: 0.3354 - val_acc: 0.9150\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9394\n",
      "Epoch 00211: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1892 - acc: 0.9394 - val_loss: 0.3380 - val_acc: 0.9126\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9366\n",
      "Epoch 00212: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1948 - acc: 0.9366 - val_loss: 0.3321 - val_acc: 0.9196\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9402\n",
      "Epoch 00213: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1834 - acc: 0.9402 - val_loss: 0.3427 - val_acc: 0.9173\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9423\n",
      "Epoch 00214: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1804 - acc: 0.9422 - val_loss: 0.3208 - val_acc: 0.9208\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9402\n",
      "Epoch 00215: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1837 - acc: 0.9401 - val_loss: 0.3681 - val_acc: 0.9150\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9406\n",
      "Epoch 00216: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1816 - acc: 0.9406 - val_loss: 0.3105 - val_acc: 0.9262\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9400\n",
      "Epoch 00217: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1851 - acc: 0.9400 - val_loss: 0.3124 - val_acc: 0.9222\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9397\n",
      "Epoch 00218: val_loss did not improve from 0.30010\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1829 - acc: 0.9397 - val_loss: 0.3217 - val_acc: 0.9252\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 543us/sample - loss: 0.3728 - acc: 0.8951\n",
      "Loss: 0.3728217562503904 Accuracy: 0.8951194\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.5703 - acc: 0.1176\n",
      "Epoch 00001: val_loss improved from inf to 2.28887, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/001-2.2889.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 3.5702 - acc: 0.1176 - val_loss: 2.2889 - val_acc: 0.2891\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5144 - acc: 0.2410\n",
      "Epoch 00002: val_loss improved from 2.28887 to 1.70528, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/002-1.7053.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.5144 - acc: 0.2409 - val_loss: 1.7053 - val_acc: 0.5010\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0041 - acc: 0.3585\n",
      "Epoch 00003: val_loss improved from 1.70528 to 1.34381, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/003-1.3438.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.0041 - acc: 0.3585 - val_loss: 1.3438 - val_acc: 0.6056\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6826 - acc: 0.4504\n",
      "Epoch 00004: val_loss improved from 1.34381 to 1.13608, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/004-1.1361.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.6828 - acc: 0.4503 - val_loss: 1.1361 - val_acc: 0.6741\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4557 - acc: 0.5302\n",
      "Epoch 00005: val_loss improved from 1.13608 to 0.97810, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/005-0.9781.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4557 - acc: 0.5302 - val_loss: 0.9781 - val_acc: 0.7195\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2915 - acc: 0.5838\n",
      "Epoch 00006: val_loss improved from 0.97810 to 0.87763, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/006-0.8776.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2916 - acc: 0.5838 - val_loss: 0.8776 - val_acc: 0.7498\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1581 - acc: 0.6365\n",
      "Epoch 00007: val_loss improved from 0.87763 to 0.78840, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/007-0.7884.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1580 - acc: 0.6365 - val_loss: 0.7884 - val_acc: 0.7796\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0472 - acc: 0.6754\n",
      "Epoch 00008: val_loss improved from 0.78840 to 0.74106, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/008-0.7411.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0472 - acc: 0.6754 - val_loss: 0.7411 - val_acc: 0.7899\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9680 - acc: 0.7030\n",
      "Epoch 00009: val_loss improved from 0.74106 to 0.67332, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/009-0.6733.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9680 - acc: 0.7030 - val_loss: 0.6733 - val_acc: 0.8113\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9042 - acc: 0.7221\n",
      "Epoch 00010: val_loss improved from 0.67332 to 0.64274, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/010-0.6427.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9042 - acc: 0.7221 - val_loss: 0.6427 - val_acc: 0.8195\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7405\n",
      "Epoch 00011: val_loss did not improve from 0.64274\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8453 - acc: 0.7405 - val_loss: 0.6460 - val_acc: 0.8199\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7613\n",
      "Epoch 00012: val_loss improved from 0.64274 to 0.54618, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/012-0.5462.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7896 - acc: 0.7613 - val_loss: 0.5462 - val_acc: 0.8493\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7704\n",
      "Epoch 00013: val_loss did not improve from 0.54618\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7531 - acc: 0.7704 - val_loss: 0.5624 - val_acc: 0.8397\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7163 - acc: 0.7832\n",
      "Epoch 00014: val_loss improved from 0.54618 to 0.51530, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/014-0.5153.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7163 - acc: 0.7833 - val_loss: 0.5153 - val_acc: 0.8602\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6853 - acc: 0.7930\n",
      "Epoch 00015: val_loss improved from 0.51530 to 0.51342, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/015-0.5134.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6852 - acc: 0.7931 - val_loss: 0.5134 - val_acc: 0.8584\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.8028\n",
      "Epoch 00016: val_loss improved from 0.51342 to 0.47068, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/016-0.4707.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6597 - acc: 0.8027 - val_loss: 0.4707 - val_acc: 0.8714\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.8082\n",
      "Epoch 00017: val_loss improved from 0.47068 to 0.44836, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/017-0.4484.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6291 - acc: 0.8082 - val_loss: 0.4484 - val_acc: 0.8803\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.8178\n",
      "Epoch 00018: val_loss improved from 0.44836 to 0.43859, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/018-0.4386.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6066 - acc: 0.8178 - val_loss: 0.4386 - val_acc: 0.8789\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.8224\n",
      "Epoch 00019: val_loss did not improve from 0.43859\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5873 - acc: 0.8224 - val_loss: 0.4396 - val_acc: 0.8793\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5642 - acc: 0.8311\n",
      "Epoch 00020: val_loss did not improve from 0.43859\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5642 - acc: 0.8311 - val_loss: 0.4881 - val_acc: 0.8551\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.8361\n",
      "Epoch 00021: val_loss improved from 0.43859 to 0.41561, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/021-0.4156.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5429 - acc: 0.8361 - val_loss: 0.4156 - val_acc: 0.8849\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.8386\n",
      "Epoch 00022: val_loss improved from 0.41561 to 0.39712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/022-0.3971.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5296 - acc: 0.8386 - val_loss: 0.3971 - val_acc: 0.8901\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5153 - acc: 0.8435\n",
      "Epoch 00023: val_loss improved from 0.39712 to 0.38254, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/023-0.3825.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5153 - acc: 0.8435 - val_loss: 0.3825 - val_acc: 0.8945\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.8483\n",
      "Epoch 00024: val_loss did not improve from 0.38254\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5031 - acc: 0.8483 - val_loss: 0.4048 - val_acc: 0.8908\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8532\n",
      "Epoch 00025: val_loss improved from 0.38254 to 0.35423, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/025-0.3542.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4904 - acc: 0.8532 - val_loss: 0.3542 - val_acc: 0.9029\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8557\n",
      "Epoch 00026: val_loss improved from 0.35423 to 0.34237, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/026-0.3424.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4784 - acc: 0.8557 - val_loss: 0.3424 - val_acc: 0.9089\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8598\n",
      "Epoch 00027: val_loss did not improve from 0.34237\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4672 - acc: 0.8597 - val_loss: 0.3813 - val_acc: 0.8928\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.8633\n",
      "Epoch 00028: val_loss improved from 0.34237 to 0.33005, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/028-0.3300.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4516 - acc: 0.8633 - val_loss: 0.3300 - val_acc: 0.9110\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8639\n",
      "Epoch 00029: val_loss did not improve from 0.33005\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4445 - acc: 0.8639 - val_loss: 0.3468 - val_acc: 0.9047\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8657\n",
      "Epoch 00030: val_loss improved from 0.33005 to 0.32664, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/030-0.3266.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4423 - acc: 0.8656 - val_loss: 0.3266 - val_acc: 0.9106\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8708\n",
      "Epoch 00031: val_loss did not improve from 0.32664\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4240 - acc: 0.8707 - val_loss: 0.3396 - val_acc: 0.9045\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8732\n",
      "Epoch 00032: val_loss did not improve from 0.32664\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4167 - acc: 0.8731 - val_loss: 0.3387 - val_acc: 0.9094\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8745\n",
      "Epoch 00033: val_loss improved from 0.32664 to 0.30488, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/033-0.3049.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4159 - acc: 0.8744 - val_loss: 0.3049 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8776\n",
      "Epoch 00034: val_loss did not improve from 0.30488\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4038 - acc: 0.8775 - val_loss: 0.3288 - val_acc: 0.9124\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3908 - acc: 0.8804\n",
      "Epoch 00035: val_loss improved from 0.30488 to 0.30143, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/035-0.3014.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3908 - acc: 0.8803 - val_loss: 0.3014 - val_acc: 0.9206\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8827\n",
      "Epoch 00036: val_loss did not improve from 0.30143\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3864 - acc: 0.8827 - val_loss: 0.3383 - val_acc: 0.9157\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8834\n",
      "Epoch 00037: val_loss did not improve from 0.30143\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3802 - acc: 0.8834 - val_loss: 0.3050 - val_acc: 0.9196\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8846\n",
      "Epoch 00038: val_loss did not improve from 0.30143\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3756 - acc: 0.8845 - val_loss: 0.3072 - val_acc: 0.9147\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8853\n",
      "Epoch 00039: val_loss improved from 0.30143 to 0.28015, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/039-0.2801.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3726 - acc: 0.8853 - val_loss: 0.2801 - val_acc: 0.9259\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8870\n",
      "Epoch 00040: val_loss improved from 0.28015 to 0.27405, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/040-0.2741.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3656 - acc: 0.8870 - val_loss: 0.2741 - val_acc: 0.9273\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.8891\n",
      "Epoch 00041: val_loss did not improve from 0.27405\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3603 - acc: 0.8891 - val_loss: 0.3536 - val_acc: 0.9059\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8915\n",
      "Epoch 00042: val_loss did not improve from 0.27405\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3575 - acc: 0.8915 - val_loss: 0.3000 - val_acc: 0.9227\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8939\n",
      "Epoch 00043: val_loss did not improve from 0.27405\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3435 - acc: 0.8939 - val_loss: 0.2946 - val_acc: 0.9259\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.8957\n",
      "Epoch 00044: val_loss improved from 0.27405 to 0.26108, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/044-0.2611.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3375 - acc: 0.8957 - val_loss: 0.2611 - val_acc: 0.9304\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8961\n",
      "Epoch 00045: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3402 - acc: 0.8962 - val_loss: 0.2913 - val_acc: 0.9215\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8962\n",
      "Epoch 00046: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3357 - acc: 0.8962 - val_loss: 0.2649 - val_acc: 0.9280\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8998\n",
      "Epoch 00047: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3276 - acc: 0.8998 - val_loss: 0.2917 - val_acc: 0.9220\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9013\n",
      "Epoch 00048: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3227 - acc: 0.9013 - val_loss: 0.2776 - val_acc: 0.9297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9015\n",
      "Epoch 00049: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3162 - acc: 0.9015 - val_loss: 0.2889 - val_acc: 0.9236\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.9031\n",
      "Epoch 00050: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3148 - acc: 0.9031 - val_loss: 0.2927 - val_acc: 0.9187\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9047\n",
      "Epoch 00051: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3078 - acc: 0.9047 - val_loss: 0.2791 - val_acc: 0.9276\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9031\n",
      "Epoch 00052: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3064 - acc: 0.9031 - val_loss: 0.2781 - val_acc: 0.9308\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9071\n",
      "Epoch 00053: val_loss did not improve from 0.26108\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3035 - acc: 0.9072 - val_loss: 0.2654 - val_acc: 0.9252\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9054\n",
      "Epoch 00054: val_loss improved from 0.26108 to 0.25540, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/054-0.2554.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3031 - acc: 0.9054 - val_loss: 0.2554 - val_acc: 0.9317\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9093\n",
      "Epoch 00055: val_loss did not improve from 0.25540\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2970 - acc: 0.9092 - val_loss: 0.2980 - val_acc: 0.9224\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9050\n",
      "Epoch 00056: val_loss did not improve from 0.25540\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3068 - acc: 0.9050 - val_loss: 0.2722 - val_acc: 0.9320\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9106\n",
      "Epoch 00057: val_loss did not improve from 0.25540\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2878 - acc: 0.9106 - val_loss: 0.2557 - val_acc: 0.9306\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9112\n",
      "Epoch 00058: val_loss improved from 0.25540 to 0.24681, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/058-0.2468.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2890 - acc: 0.9112 - val_loss: 0.2468 - val_acc: 0.9364\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9121\n",
      "Epoch 00059: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2844 - acc: 0.9121 - val_loss: 0.2661 - val_acc: 0.9283\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9113\n",
      "Epoch 00060: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2803 - acc: 0.9113 - val_loss: 0.2499 - val_acc: 0.9364\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9118\n",
      "Epoch 00061: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2793 - acc: 0.9118 - val_loss: 0.2510 - val_acc: 0.9348\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.9132\n",
      "Epoch 00062: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2756 - acc: 0.9132 - val_loss: 0.2608 - val_acc: 0.9278\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9145\n",
      "Epoch 00063: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2713 - acc: 0.9145 - val_loss: 0.2611 - val_acc: 0.9301\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9154\n",
      "Epoch 00064: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2699 - acc: 0.9154 - val_loss: 0.2492 - val_acc: 0.9324\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9124\n",
      "Epoch 00065: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2728 - acc: 0.9124 - val_loss: 0.2524 - val_acc: 0.9352\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9180\n",
      "Epoch 00066: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2608 - acc: 0.9180 - val_loss: 0.2550 - val_acc: 0.9299\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9174\n",
      "Epoch 00067: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2651 - acc: 0.9174 - val_loss: 0.2565 - val_acc: 0.9336\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9183\n",
      "Epoch 00068: val_loss did not improve from 0.24681\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2638 - acc: 0.9182 - val_loss: 0.2608 - val_acc: 0.9345\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9182\n",
      "Epoch 00069: val_loss improved from 0.24681 to 0.23412, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/069-0.2341.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2585 - acc: 0.9182 - val_loss: 0.2341 - val_acc: 0.9362\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9194\n",
      "Epoch 00070: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2518 - acc: 0.9194 - val_loss: 0.2489 - val_acc: 0.9371\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9210\n",
      "Epoch 00071: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2527 - acc: 0.9210 - val_loss: 0.2465 - val_acc: 0.9387\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9200\n",
      "Epoch 00072: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2512 - acc: 0.9200 - val_loss: 0.2680 - val_acc: 0.9278\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9227\n",
      "Epoch 00073: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2437 - acc: 0.9227 - val_loss: 0.2493 - val_acc: 0.9343\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9226\n",
      "Epoch 00074: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2429 - acc: 0.9226 - val_loss: 0.3136 - val_acc: 0.9194\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9240\n",
      "Epoch 00075: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2414 - acc: 0.9240 - val_loss: 0.2415 - val_acc: 0.9397\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9230\n",
      "Epoch 00076: val_loss did not improve from 0.23412\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2442 - acc: 0.9230 - val_loss: 0.2582 - val_acc: 0.9399\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9241\n",
      "Epoch 00077: val_loss improved from 0.23412 to 0.22585, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/077-0.2259.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2404 - acc: 0.9241 - val_loss: 0.2259 - val_acc: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9226\n",
      "Epoch 00078: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2434 - acc: 0.9226 - val_loss: 0.2444 - val_acc: 0.9387\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9262\n",
      "Epoch 00079: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2377 - acc: 0.9262 - val_loss: 0.2401 - val_acc: 0.9324\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9275\n",
      "Epoch 00080: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2318 - acc: 0.9275 - val_loss: 0.2350 - val_acc: 0.9357\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9249\n",
      "Epoch 00081: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2346 - acc: 0.9250 - val_loss: 0.2360 - val_acc: 0.9373\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9278\n",
      "Epoch 00082: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2255 - acc: 0.9277 - val_loss: 0.2372 - val_acc: 0.9380\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9261\n",
      "Epoch 00083: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2321 - acc: 0.9260 - val_loss: 0.2458 - val_acc: 0.9378\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9272\n",
      "Epoch 00084: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2285 - acc: 0.9272 - val_loss: 0.2371 - val_acc: 0.9408\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9280\n",
      "Epoch 00085: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2273 - acc: 0.9280 - val_loss: 0.2393 - val_acc: 0.9392\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9285\n",
      "Epoch 00086: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2212 - acc: 0.9285 - val_loss: 0.2474 - val_acc: 0.9371\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9309\n",
      "Epoch 00087: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2222 - acc: 0.9309 - val_loss: 0.2690 - val_acc: 0.9301\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9275\n",
      "Epoch 00088: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2242 - acc: 0.9275 - val_loss: 0.2302 - val_acc: 0.9399\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9290\n",
      "Epoch 00089: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2214 - acc: 0.9290 - val_loss: 0.2780 - val_acc: 0.9299\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9298\n",
      "Epoch 00090: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2169 - acc: 0.9297 - val_loss: 0.2297 - val_acc: 0.9429\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9310\n",
      "Epoch 00091: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2160 - acc: 0.9310 - val_loss: 0.2379 - val_acc: 0.9418\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9300\n",
      "Epoch 00092: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2208 - acc: 0.9300 - val_loss: 0.2691 - val_acc: 0.9343\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9323\n",
      "Epoch 00093: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2135 - acc: 0.9323 - val_loss: 0.2273 - val_acc: 0.9415\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9342\n",
      "Epoch 00094: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2092 - acc: 0.9342 - val_loss: 0.2409 - val_acc: 0.9385\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9327\n",
      "Epoch 00095: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2116 - acc: 0.9328 - val_loss: 0.2311 - val_acc: 0.9390\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9330\n",
      "Epoch 00096: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2078 - acc: 0.9329 - val_loss: 0.2474 - val_acc: 0.9357\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9340\n",
      "Epoch 00097: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2060 - acc: 0.9340 - val_loss: 0.2283 - val_acc: 0.9401\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9324\n",
      "Epoch 00098: val_loss did not improve from 0.22585\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2086 - acc: 0.9324 - val_loss: 0.2398 - val_acc: 0.9376\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9356\n",
      "Epoch 00099: val_loss improved from 0.22585 to 0.22054, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv_checkpoint/099-0.2205.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1994 - acc: 0.9356 - val_loss: 0.2205 - val_acc: 0.9460\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9322\n",
      "Epoch 00100: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2096 - acc: 0.9322 - val_loss: 0.2242 - val_acc: 0.9441\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9358\n",
      "Epoch 00101: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2018 - acc: 0.9357 - val_loss: 0.2551 - val_acc: 0.9373\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9347\n",
      "Epoch 00102: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2082 - acc: 0.9347 - val_loss: 0.2379 - val_acc: 0.9406\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9401\n",
      "Epoch 00103: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1908 - acc: 0.9401 - val_loss: 0.2245 - val_acc: 0.9394\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9371\n",
      "Epoch 00104: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1937 - acc: 0.9371 - val_loss: 0.2317 - val_acc: 0.9434\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9383\n",
      "Epoch 00105: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1941 - acc: 0.9383 - val_loss: 0.2364 - val_acc: 0.9404\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9357\n",
      "Epoch 00106: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1995 - acc: 0.9357 - val_loss: 0.2351 - val_acc: 0.9415\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9377\n",
      "Epoch 00107: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1914 - acc: 0.9378 - val_loss: 0.2324 - val_acc: 0.9404\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9383\n",
      "Epoch 00108: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1914 - acc: 0.9383 - val_loss: 0.2409 - val_acc: 0.9387\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9397\n",
      "Epoch 00109: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1881 - acc: 0.9397 - val_loss: 0.2674 - val_acc: 0.9338\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9374\n",
      "Epoch 00110: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1927 - acc: 0.9375 - val_loss: 0.2231 - val_acc: 0.9448\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9384\n",
      "Epoch 00111: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1894 - acc: 0.9384 - val_loss: 0.2212 - val_acc: 0.9446\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9404\n",
      "Epoch 00112: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1863 - acc: 0.9403 - val_loss: 0.2248 - val_acc: 0.9420\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9395\n",
      "Epoch 00113: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1880 - acc: 0.9395 - val_loss: 0.2434 - val_acc: 0.9399\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9398\n",
      "Epoch 00114: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1868 - acc: 0.9398 - val_loss: 0.2656 - val_acc: 0.9362\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9406\n",
      "Epoch 00115: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1823 - acc: 0.9406 - val_loss: 0.2335 - val_acc: 0.9397\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9414\n",
      "Epoch 00116: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1829 - acc: 0.9414 - val_loss: 0.2283 - val_acc: 0.9401\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9421\n",
      "Epoch 00117: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1822 - acc: 0.9421 - val_loss: 0.2563 - val_acc: 0.9329\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9406\n",
      "Epoch 00118: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1857 - acc: 0.9406 - val_loss: 0.2580 - val_acc: 0.9336\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9418\n",
      "Epoch 00119: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1805 - acc: 0.9418 - val_loss: 0.2477 - val_acc: 0.9366\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9411\n",
      "Epoch 00120: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1797 - acc: 0.9411 - val_loss: 0.2783 - val_acc: 0.9294\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9432\n",
      "Epoch 00121: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1757 - acc: 0.9431 - val_loss: 0.2408 - val_acc: 0.9420\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9425\n",
      "Epoch 00122: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1780 - acc: 0.9425 - val_loss: 0.2403 - val_acc: 0.9448\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9441\n",
      "Epoch 00123: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1745 - acc: 0.9441 - val_loss: 0.2610 - val_acc: 0.9378\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9405\n",
      "Epoch 00124: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1857 - acc: 0.9405 - val_loss: 0.2289 - val_acc: 0.9418\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9460\n",
      "Epoch 00125: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1695 - acc: 0.9460 - val_loss: 0.2280 - val_acc: 0.9425\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9442\n",
      "Epoch 00126: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1743 - acc: 0.9442 - val_loss: 0.2254 - val_acc: 0.9434\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9440\n",
      "Epoch 00127: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1719 - acc: 0.9440 - val_loss: 0.2220 - val_acc: 0.9450\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9444\n",
      "Epoch 00128: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1708 - acc: 0.9444 - val_loss: 0.2373 - val_acc: 0.9415\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9458\n",
      "Epoch 00129: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1689 - acc: 0.9458 - val_loss: 0.2258 - val_acc: 0.9441\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9430\n",
      "Epoch 00130: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1721 - acc: 0.9430 - val_loss: 0.2836 - val_acc: 0.9327\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9448\n",
      "Epoch 00131: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1686 - acc: 0.9448 - val_loss: 0.2238 - val_acc: 0.9446\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9448\n",
      "Epoch 00132: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1685 - acc: 0.9448 - val_loss: 0.2513 - val_acc: 0.9425\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9445\n",
      "Epoch 00133: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1682 - acc: 0.9445 - val_loss: 0.2371 - val_acc: 0.9429\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9458\n",
      "Epoch 00134: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1652 - acc: 0.9458 - val_loss: 0.2319 - val_acc: 0.9427\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9484\n",
      "Epoch 00135: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1615 - acc: 0.9483 - val_loss: 0.2417 - val_acc: 0.9432\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9482\n",
      "Epoch 00136: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1613 - acc: 0.9482 - val_loss: 0.2449 - val_acc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9486\n",
      "Epoch 00137: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1618 - acc: 0.9486 - val_loss: 0.2526 - val_acc: 0.9387\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9485\n",
      "Epoch 00138: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1562 - acc: 0.9485 - val_loss: 0.2569 - val_acc: 0.9378\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9471\n",
      "Epoch 00139: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1617 - acc: 0.9471 - val_loss: 0.2357 - val_acc: 0.9453\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9496\n",
      "Epoch 00140: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1584 - acc: 0.9496 - val_loss: 0.2482 - val_acc: 0.9420\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9485\n",
      "Epoch 00141: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1615 - acc: 0.9485 - val_loss: 0.2471 - val_acc: 0.9404\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9491\n",
      "Epoch 00142: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1565 - acc: 0.9491 - val_loss: 0.2314 - val_acc: 0.9457\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9479\n",
      "Epoch 00143: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1591 - acc: 0.9479 - val_loss: 0.2591 - val_acc: 0.9311\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9500\n",
      "Epoch 00144: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1562 - acc: 0.9500 - val_loss: 0.2766 - val_acc: 0.9338\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9501\n",
      "Epoch 00145: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1558 - acc: 0.9500 - val_loss: 0.2441 - val_acc: 0.9429\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9471\n",
      "Epoch 00146: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1647 - acc: 0.9471 - val_loss: 0.2389 - val_acc: 0.9422\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9497\n",
      "Epoch 00147: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1522 - acc: 0.9497 - val_loss: 0.2449 - val_acc: 0.9422\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9495\n",
      "Epoch 00148: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1560 - acc: 0.9495 - val_loss: 0.2330 - val_acc: 0.9413\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9507\n",
      "Epoch 00149: val_loss did not improve from 0.22054\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1522 - acc: 0.9507 - val_loss: 0.2688 - val_acc: 0.9401\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 574us/sample - loss: 0.2614 - acc: 0.9252\n",
      "Loss: 0.26139286952102914 Accuracy: 0.92523366\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.4718 - acc: 0.1192\n",
      "Epoch 00001: val_loss improved from inf to 2.32432, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/001-2.3243.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 3.4718 - acc: 0.1192 - val_loss: 2.3243 - val_acc: 0.2732\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5439 - acc: 0.2101\n",
      "Epoch 00002: val_loss improved from 2.32432 to 1.86477, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/002-1.8648.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.5439 - acc: 0.2101 - val_loss: 1.8648 - val_acc: 0.4510\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1625 - acc: 0.3020\n",
      "Epoch 00003: val_loss improved from 1.86477 to 1.54907, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/003-1.5491.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 2.1626 - acc: 0.3021 - val_loss: 1.5491 - val_acc: 0.5418\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8683 - acc: 0.3850\n",
      "Epoch 00004: val_loss improved from 1.54907 to 1.27653, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/004-1.2765.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.8683 - acc: 0.3850 - val_loss: 1.2765 - val_acc: 0.6324\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6229 - acc: 0.4687\n",
      "Epoch 00005: val_loss improved from 1.27653 to 1.06357, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/005-1.0636.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.6229 - acc: 0.4687 - val_loss: 1.0636 - val_acc: 0.7128\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4018 - acc: 0.5412\n",
      "Epoch 00006: val_loss improved from 1.06357 to 0.89546, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/006-0.8955.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.4018 - acc: 0.5413 - val_loss: 0.8955 - val_acc: 0.7584\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2054 - acc: 0.6131\n",
      "Epoch 00007: val_loss improved from 0.89546 to 0.76893, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/007-0.7689.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.2054 - acc: 0.6131 - val_loss: 0.7689 - val_acc: 0.7913\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0514 - acc: 0.6645\n",
      "Epoch 00008: val_loss improved from 0.76893 to 0.65506, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/008-0.6551.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0514 - acc: 0.6645 - val_loss: 0.6551 - val_acc: 0.8199\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9362 - acc: 0.7059\n",
      "Epoch 00009: val_loss improved from 0.65506 to 0.55812, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/009-0.5581.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9361 - acc: 0.7059 - val_loss: 0.5581 - val_acc: 0.8460\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8481 - acc: 0.7378\n",
      "Epoch 00010: val_loss improved from 0.55812 to 0.49824, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/010-0.4982.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8486 - acc: 0.7377 - val_loss: 0.4982 - val_acc: 0.8726\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7794 - acc: 0.7586\n",
      "Epoch 00011: val_loss improved from 0.49824 to 0.45453, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/011-0.4545.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7794 - acc: 0.7586 - val_loss: 0.4545 - val_acc: 0.8726\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.7832\n",
      "Epoch 00012: val_loss improved from 0.45453 to 0.42286, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/012-0.4229.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7013 - acc: 0.7832 - val_loss: 0.4229 - val_acc: 0.8821\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.8023\n",
      "Epoch 00013: val_loss improved from 0.42286 to 0.39364, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/013-0.3936.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6469 - acc: 0.8022 - val_loss: 0.3936 - val_acc: 0.8961\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6203 - acc: 0.8067\n",
      "Epoch 00014: val_loss did not improve from 0.39364\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6203 - acc: 0.8067 - val_loss: 0.4588 - val_acc: 0.8656\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.8252\n",
      "Epoch 00015: val_loss improved from 0.39364 to 0.32513, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/015-0.3251.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5780 - acc: 0.8252 - val_loss: 0.3251 - val_acc: 0.9115\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8337\n",
      "Epoch 00016: val_loss did not improve from 0.32513\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5475 - acc: 0.8337 - val_loss: 0.3263 - val_acc: 0.9136\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.8412\n",
      "Epoch 00017: val_loss improved from 0.32513 to 0.30303, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/017-0.3030.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5166 - acc: 0.8411 - val_loss: 0.3030 - val_acc: 0.9175\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4949 - acc: 0.8506\n",
      "Epoch 00018: val_loss improved from 0.30303 to 0.29586, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/018-0.2959.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4949 - acc: 0.8506 - val_loss: 0.2959 - val_acc: 0.9236\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8537\n",
      "Epoch 00019: val_loss did not improve from 0.29586\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4732 - acc: 0.8536 - val_loss: 0.4015 - val_acc: 0.8842\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.8643\n",
      "Epoch 00020: val_loss improved from 0.29586 to 0.29471, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/020-0.2947.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4514 - acc: 0.8643 - val_loss: 0.2947 - val_acc: 0.9178\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8685\n",
      "Epoch 00021: val_loss improved from 0.29471 to 0.27275, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/021-0.2728.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4383 - acc: 0.8685 - val_loss: 0.2728 - val_acc: 0.9229\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8702\n",
      "Epoch 00022: val_loss improved from 0.27275 to 0.26588, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/022-0.2659.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4263 - acc: 0.8702 - val_loss: 0.2659 - val_acc: 0.9283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8794\n",
      "Epoch 00023: val_loss improved from 0.26588 to 0.23978, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/023-0.2398.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4027 - acc: 0.8794 - val_loss: 0.2398 - val_acc: 0.9383\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8818\n",
      "Epoch 00024: val_loss improved from 0.23978 to 0.23678, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/024-0.2368.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3900 - acc: 0.8818 - val_loss: 0.2368 - val_acc: 0.9350\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3813 - acc: 0.8841\n",
      "Epoch 00025: val_loss did not improve from 0.23678\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3814 - acc: 0.8841 - val_loss: 0.2392 - val_acc: 0.9378\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8896\n",
      "Epoch 00026: val_loss did not improve from 0.23678\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3680 - acc: 0.8896 - val_loss: 0.2372 - val_acc: 0.9362\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8889\n",
      "Epoch 00027: val_loss did not improve from 0.23678\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3657 - acc: 0.8889 - val_loss: 0.2558 - val_acc: 0.9278\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8941\n",
      "Epoch 00028: val_loss did not improve from 0.23678\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3514 - acc: 0.8941 - val_loss: 0.2748 - val_acc: 0.9290\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8945\n",
      "Epoch 00029: val_loss improved from 0.23678 to 0.22575, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/029-0.2257.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3455 - acc: 0.8945 - val_loss: 0.2257 - val_acc: 0.9401\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8997\n",
      "Epoch 00030: val_loss improved from 0.22575 to 0.21216, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/030-0.2122.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3284 - acc: 0.8996 - val_loss: 0.2122 - val_acc: 0.9436\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.9029\n",
      "Epoch 00031: val_loss did not improve from 0.21216\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3220 - acc: 0.9028 - val_loss: 0.2144 - val_acc: 0.9429\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.9010\n",
      "Epoch 00032: val_loss did not improve from 0.21216\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3257 - acc: 0.9010 - val_loss: 0.2460 - val_acc: 0.9334\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9039\n",
      "Epoch 00033: val_loss did not improve from 0.21216\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3143 - acc: 0.9039 - val_loss: 0.2139 - val_acc: 0.9418\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9068\n",
      "Epoch 00034: val_loss improved from 0.21216 to 0.20169, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/034-0.2017.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3089 - acc: 0.9068 - val_loss: 0.2017 - val_acc: 0.9467\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9077\n",
      "Epoch 00035: val_loss did not improve from 0.20169\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2995 - acc: 0.9077 - val_loss: 0.2263 - val_acc: 0.9415\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9106\n",
      "Epoch 00036: val_loss improved from 0.20169 to 0.20073, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/036-0.2007.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2928 - acc: 0.9106 - val_loss: 0.2007 - val_acc: 0.9453\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9126\n",
      "Epoch 00037: val_loss improved from 0.20073 to 0.19530, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/037-0.1953.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2866 - acc: 0.9126 - val_loss: 0.1953 - val_acc: 0.9469\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9155\n",
      "Epoch 00038: val_loss did not improve from 0.19530\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2796 - acc: 0.9154 - val_loss: 0.2010 - val_acc: 0.9462\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9140\n",
      "Epoch 00039: val_loss improved from 0.19530 to 0.18910, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/039-0.1891.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2784 - acc: 0.9140 - val_loss: 0.1891 - val_acc: 0.9471\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9177\n",
      "Epoch 00040: val_loss improved from 0.18910 to 0.18479, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/040-0.1848.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2694 - acc: 0.9177 - val_loss: 0.1848 - val_acc: 0.9495\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9172\n",
      "Epoch 00041: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2654 - acc: 0.9172 - val_loss: 0.1923 - val_acc: 0.9481\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9216\n",
      "Epoch 00042: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2545 - acc: 0.9216 - val_loss: 0.1941 - val_acc: 0.9497\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9223\n",
      "Epoch 00043: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2558 - acc: 0.9222 - val_loss: 0.2037 - val_acc: 0.9476\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9229\n",
      "Epoch 00044: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2516 - acc: 0.9229 - val_loss: 0.1961 - val_acc: 0.9467\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9205\n",
      "Epoch 00045: val_loss improved from 0.18479 to 0.18212, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/045-0.1821.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2547 - acc: 0.9204 - val_loss: 0.1821 - val_acc: 0.9488\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9237\n",
      "Epoch 00046: val_loss did not improve from 0.18212\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2466 - acc: 0.9237 - val_loss: 0.1959 - val_acc: 0.9478\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9276\n",
      "Epoch 00047: val_loss did not improve from 0.18212\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2366 - acc: 0.9276 - val_loss: 0.2065 - val_acc: 0.9457\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9269\n",
      "Epoch 00048: val_loss improved from 0.18212 to 0.17668, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/048-0.1767.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2387 - acc: 0.9269 - val_loss: 0.1767 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9263\n",
      "Epoch 00049: val_loss did not improve from 0.17668\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2378 - acc: 0.9263 - val_loss: 0.1794 - val_acc: 0.9525\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9276\n",
      "Epoch 00050: val_loss did not improve from 0.17668\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2281 - acc: 0.9276 - val_loss: 0.1786 - val_acc: 0.9495\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9313\n",
      "Epoch 00051: val_loss did not improve from 0.17668\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2243 - acc: 0.9313 - val_loss: 0.2162 - val_acc: 0.9401\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9319\n",
      "Epoch 00052: val_loss did not improve from 0.17668\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2207 - acc: 0.9319 - val_loss: 0.1795 - val_acc: 0.9483\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9329\n",
      "Epoch 00053: val_loss improved from 0.17668 to 0.17241, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/053-0.1724.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2157 - acc: 0.9328 - val_loss: 0.1724 - val_acc: 0.9525\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9328\n",
      "Epoch 00054: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2163 - acc: 0.9328 - val_loss: 0.1984 - val_acc: 0.9446\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9340\n",
      "Epoch 00055: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2101 - acc: 0.9341 - val_loss: 0.1775 - val_acc: 0.9513\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9342\n",
      "Epoch 00056: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2093 - acc: 0.9342 - val_loss: 0.2006 - val_acc: 0.9481\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9347\n",
      "Epoch 00057: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2110 - acc: 0.9347 - val_loss: 0.1771 - val_acc: 0.9518\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.9382\n",
      "Epoch 00058: val_loss did not improve from 0.17241\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2017 - acc: 0.9382 - val_loss: 0.1755 - val_acc: 0.9509\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9365\n",
      "Epoch 00059: val_loss improved from 0.17241 to 0.16008, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/059-0.1601.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2009 - acc: 0.9365 - val_loss: 0.1601 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9369\n",
      "Epoch 00060: val_loss did not improve from 0.16008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2012 - acc: 0.9369 - val_loss: 0.1825 - val_acc: 0.9522\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9386\n",
      "Epoch 00061: val_loss did not improve from 0.16008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1942 - acc: 0.9386 - val_loss: 0.1889 - val_acc: 0.9481\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9404\n",
      "Epoch 00062: val_loss improved from 0.16008 to 0.15935, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/062-0.1593.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1896 - acc: 0.9404 - val_loss: 0.1593 - val_acc: 0.9553\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9390\n",
      "Epoch 00063: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1911 - acc: 0.9391 - val_loss: 0.1945 - val_acc: 0.9488\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9422\n",
      "Epoch 00064: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1818 - acc: 0.9422 - val_loss: 0.1670 - val_acc: 0.9548\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9423\n",
      "Epoch 00065: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1831 - acc: 0.9422 - val_loss: 0.2119 - val_acc: 0.9446\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9426\n",
      "Epoch 00066: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1830 - acc: 0.9426 - val_loss: 0.1653 - val_acc: 0.9534\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9444\n",
      "Epoch 00067: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1784 - acc: 0.9443 - val_loss: 0.2024 - val_acc: 0.9408\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9422\n",
      "Epoch 00068: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1825 - acc: 0.9422 - val_loss: 0.1759 - val_acc: 0.9536\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9432\n",
      "Epoch 00069: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1771 - acc: 0.9432 - val_loss: 0.1774 - val_acc: 0.9509\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9451\n",
      "Epoch 00070: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1734 - acc: 0.9451 - val_loss: 0.1802 - val_acc: 0.9502\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9459\n",
      "Epoch 00071: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1722 - acc: 0.9458 - val_loss: 0.1925 - val_acc: 0.9453\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9462\n",
      "Epoch 00072: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1690 - acc: 0.9461 - val_loss: 0.1984 - val_acc: 0.9441\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9451\n",
      "Epoch 00073: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1693 - acc: 0.9451 - val_loss: 0.1716 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9483\n",
      "Epoch 00074: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1670 - acc: 0.9482 - val_loss: 0.1611 - val_acc: 0.9541\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9465\n",
      "Epoch 00075: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1653 - acc: 0.9464 - val_loss: 0.1872 - val_acc: 0.9534\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9485\n",
      "Epoch 00076: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1621 - acc: 0.9484 - val_loss: 0.1633 - val_acc: 0.9564\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9496\n",
      "Epoch 00077: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1626 - acc: 0.9497 - val_loss: 0.1945 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9498\n",
      "Epoch 00078: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1555 - acc: 0.9498 - val_loss: 0.1718 - val_acc: 0.9515\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9486\n",
      "Epoch 00079: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1573 - acc: 0.9486 - val_loss: 0.1962 - val_acc: 0.9471\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9493\n",
      "Epoch 00080: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1581 - acc: 0.9493 - val_loss: 0.1770 - val_acc: 0.9555\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9509\n",
      "Epoch 00081: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1536 - acc: 0.9508 - val_loss: 0.1655 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9519\n",
      "Epoch 00082: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1497 - acc: 0.9519 - val_loss: 0.1805 - val_acc: 0.9564\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9528\n",
      "Epoch 00083: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1487 - acc: 0.9527 - val_loss: 0.1744 - val_acc: 0.9564\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9515\n",
      "Epoch 00084: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1522 - acc: 0.9514 - val_loss: 0.1622 - val_acc: 0.9546\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9519\n",
      "Epoch 00085: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1510 - acc: 0.9518 - val_loss: 0.1681 - val_acc: 0.9550\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9521\n",
      "Epoch 00086: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1478 - acc: 0.9521 - val_loss: 0.1779 - val_acc: 0.9522\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9535\n",
      "Epoch 00087: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1451 - acc: 0.9535 - val_loss: 0.1885 - val_acc: 0.9527\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9548\n",
      "Epoch 00088: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1422 - acc: 0.9548 - val_loss: 0.1866 - val_acc: 0.9532\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9526\n",
      "Epoch 00089: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1439 - acc: 0.9526 - val_loss: 0.2247 - val_acc: 0.9387\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9564\n",
      "Epoch 00090: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1356 - acc: 0.9563 - val_loss: 0.1830 - val_acc: 0.9548\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9568\n",
      "Epoch 00091: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1347 - acc: 0.9569 - val_loss: 0.1710 - val_acc: 0.9546\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9565\n",
      "Epoch 00092: val_loss did not improve from 0.15935\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1371 - acc: 0.9566 - val_loss: 0.1983 - val_acc: 0.9515\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9560\n",
      "Epoch 00093: val_loss improved from 0.15935 to 0.15820, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv_checkpoint/093-0.1582.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1364 - acc: 0.9560 - val_loss: 0.1582 - val_acc: 0.9564\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9556\n",
      "Epoch 00094: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1357 - acc: 0.9555 - val_loss: 0.1839 - val_acc: 0.9520\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9553\n",
      "Epoch 00095: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1402 - acc: 0.9553 - val_loss: 0.1872 - val_acc: 0.9532\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9581\n",
      "Epoch 00096: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1295 - acc: 0.9580 - val_loss: 0.1738 - val_acc: 0.9576\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9586\n",
      "Epoch 00097: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1280 - acc: 0.9586 - val_loss: 0.1793 - val_acc: 0.9518\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9594\n",
      "Epoch 00098: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1267 - acc: 0.9594 - val_loss: 0.1919 - val_acc: 0.9495\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9601\n",
      "Epoch 00099: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1279 - acc: 0.9601 - val_loss: 0.1696 - val_acc: 0.9550\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9589\n",
      "Epoch 00100: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1257 - acc: 0.9588 - val_loss: 0.1585 - val_acc: 0.9571\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9558\n",
      "Epoch 00101: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1384 - acc: 0.9558 - val_loss: 0.1886 - val_acc: 0.9548\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9610\n",
      "Epoch 00102: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1195 - acc: 0.9610 - val_loss: 0.1697 - val_acc: 0.9562\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9587\n",
      "Epoch 00103: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1231 - acc: 0.9587 - val_loss: 0.1700 - val_acc: 0.9578\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9602\n",
      "Epoch 00104: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1206 - acc: 0.9602 - val_loss: 0.1904 - val_acc: 0.9532\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9611\n",
      "Epoch 00105: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1200 - acc: 0.9610 - val_loss: 0.1657 - val_acc: 0.9527\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9604\n",
      "Epoch 00106: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1195 - acc: 0.9604 - val_loss: 0.1699 - val_acc: 0.9571\n",
      "Epoch 107/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9620\n",
      "Epoch 00107: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1192 - acc: 0.9620 - val_loss: 0.1706 - val_acc: 0.9548\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9619\n",
      "Epoch 00108: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1155 - acc: 0.9619 - val_loss: 0.1801 - val_acc: 0.9543\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9630\n",
      "Epoch 00109: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1161 - acc: 0.9630 - val_loss: 0.1797 - val_acc: 0.9490\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9611\n",
      "Epoch 00110: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1195 - acc: 0.9611 - val_loss: 0.1725 - val_acc: 0.9548\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9647\n",
      "Epoch 00111: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1108 - acc: 0.9647 - val_loss: 0.1805 - val_acc: 0.9564\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9649\n",
      "Epoch 00112: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1087 - acc: 0.9649 - val_loss: 0.1719 - val_acc: 0.9574\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9640\n",
      "Epoch 00113: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1112 - acc: 0.9640 - val_loss: 0.1931 - val_acc: 0.9525\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9633\n",
      "Epoch 00114: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1137 - acc: 0.9633 - val_loss: 0.1771 - val_acc: 0.9541\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9630\n",
      "Epoch 00115: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1135 - acc: 0.9630 - val_loss: 0.1861 - val_acc: 0.9553\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9664\n",
      "Epoch 00116: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1031 - acc: 0.9664 - val_loss: 0.1729 - val_acc: 0.9588\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9657\n",
      "Epoch 00117: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1059 - acc: 0.9657 - val_loss: 0.1705 - val_acc: 0.9592\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9658\n",
      "Epoch 00118: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1053 - acc: 0.9658 - val_loss: 0.1820 - val_acc: 0.9550\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9642\n",
      "Epoch 00119: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1084 - acc: 0.9642 - val_loss: 0.1685 - val_acc: 0.9599\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9667\n",
      "Epoch 00120: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1046 - acc: 0.9666 - val_loss: 0.1833 - val_acc: 0.9564\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9635\n",
      "Epoch 00121: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1139 - acc: 0.9635 - val_loss: 0.1859 - val_acc: 0.9562\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9683\n",
      "Epoch 00122: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0968 - acc: 0.9683 - val_loss: 0.1892 - val_acc: 0.9560\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9644\n",
      "Epoch 00123: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1094 - acc: 0.9644 - val_loss: 0.1812 - val_acc: 0.9541\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9657\n",
      "Epoch 00124: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1055 - acc: 0.9657 - val_loss: 0.2033 - val_acc: 0.9536\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9666\n",
      "Epoch 00125: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1046 - acc: 0.9666 - val_loss: 0.1703 - val_acc: 0.9574\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9671\n",
      "Epoch 00126: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0998 - acc: 0.9671 - val_loss: 0.1839 - val_acc: 0.9562\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9692\n",
      "Epoch 00127: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0981 - acc: 0.9692 - val_loss: 0.1730 - val_acc: 0.9550\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9680\n",
      "Epoch 00128: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0993 - acc: 0.9680 - val_loss: 0.1885 - val_acc: 0.9557\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9676\n",
      "Epoch 00129: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0983 - acc: 0.9676 - val_loss: 0.1621 - val_acc: 0.9578\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9702\n",
      "Epoch 00130: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0923 - acc: 0.9701 - val_loss: 0.2020 - val_acc: 0.9529\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9693\n",
      "Epoch 00131: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0981 - acc: 0.9693 - val_loss: 0.1815 - val_acc: 0.9571\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9711\n",
      "Epoch 00132: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0887 - acc: 0.9711 - val_loss: 0.1772 - val_acc: 0.9560\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9670\n",
      "Epoch 00133: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1044 - acc: 0.9669 - val_loss: 0.2099 - val_acc: 0.9525\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9706\n",
      "Epoch 00134: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0920 - acc: 0.9705 - val_loss: 0.1822 - val_acc: 0.9548\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9709\n",
      "Epoch 00135: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0914 - acc: 0.9709 - val_loss: 0.1932 - val_acc: 0.9578\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9720\n",
      "Epoch 00136: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0850 - acc: 0.9720 - val_loss: 0.1852 - val_acc: 0.9543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9719\n",
      "Epoch 00137: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0862 - acc: 0.9719 - val_loss: 0.1795 - val_acc: 0.9569\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9684\n",
      "Epoch 00138: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0955 - acc: 0.9684 - val_loss: 0.1700 - val_acc: 0.9606\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9676\n",
      "Epoch 00139: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0964 - acc: 0.9676 - val_loss: 0.1983 - val_acc: 0.9543\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9713\n",
      "Epoch 00140: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0913 - acc: 0.9713 - val_loss: 0.1942 - val_acc: 0.9548\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9696\n",
      "Epoch 00141: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0950 - acc: 0.9696 - val_loss: 0.1897 - val_acc: 0.9562\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9706\n",
      "Epoch 00142: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0875 - acc: 0.9705 - val_loss: 0.2049 - val_acc: 0.9560\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9684\n",
      "Epoch 00143: val_loss did not improve from 0.15820\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0956 - acc: 0.9684 - val_loss: 0.1866 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 670us/sample - loss: 0.1825 - acc: 0.9458\n",
      "Loss: 0.18254458988752692 Accuracy: 0.9457944\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3686 - acc: 0.1536\n",
      "Epoch 00001: val_loss improved from inf to 2.20298, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/001-2.2030.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 3.3686 - acc: 0.1536 - val_loss: 2.2030 - val_acc: 0.3296\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2539 - acc: 0.3159\n",
      "Epoch 00002: val_loss improved from 2.20298 to 1.36932, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/002-1.3693.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.2540 - acc: 0.3159 - val_loss: 1.3693 - val_acc: 0.5966\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7114 - acc: 0.4614\n",
      "Epoch 00003: val_loss improved from 1.36932 to 0.97162, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/003-0.9716.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.7114 - acc: 0.4614 - val_loss: 0.9716 - val_acc: 0.7193\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3472 - acc: 0.5734\n",
      "Epoch 00004: val_loss improved from 0.97162 to 0.75593, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/004-0.7559.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3476 - acc: 0.5733 - val_loss: 0.7559 - val_acc: 0.7813\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0761 - acc: 0.6625\n",
      "Epoch 00005: val_loss improved from 0.75593 to 0.61335, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/005-0.6133.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0763 - acc: 0.6624 - val_loss: 0.6133 - val_acc: 0.8290\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8919 - acc: 0.7240\n",
      "Epoch 00006: val_loss improved from 0.61335 to 0.47139, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/006-0.4714.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8920 - acc: 0.7240 - val_loss: 0.4714 - val_acc: 0.8747\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7646\n",
      "Epoch 00007: val_loss improved from 0.47139 to 0.39470, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/007-0.3947.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7711 - acc: 0.7646 - val_loss: 0.3947 - val_acc: 0.8940\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.7975\n",
      "Epoch 00008: val_loss improved from 0.39470 to 0.36730, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/008-0.3673.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6684 - acc: 0.7974 - val_loss: 0.3673 - val_acc: 0.8973\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6014 - acc: 0.8165\n",
      "Epoch 00009: val_loss improved from 0.36730 to 0.35891, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/009-0.3589.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6015 - acc: 0.8164 - val_loss: 0.3589 - val_acc: 0.8970\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.8387\n",
      "Epoch 00010: val_loss improved from 0.35891 to 0.28282, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/010-0.2828.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5361 - acc: 0.8387 - val_loss: 0.2828 - val_acc: 0.9217\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4921 - acc: 0.8513\n",
      "Epoch 00011: val_loss did not improve from 0.28282\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4922 - acc: 0.8513 - val_loss: 0.3054 - val_acc: 0.9161\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8616\n",
      "Epoch 00012: val_loss improved from 0.28282 to 0.27003, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/012-0.2700.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4575 - acc: 0.8615 - val_loss: 0.2700 - val_acc: 0.9210\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8729\n",
      "Epoch 00013: val_loss improved from 0.27003 to 0.23932, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/013-0.2393.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4267 - acc: 0.8729 - val_loss: 0.2393 - val_acc: 0.9327\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8807\n",
      "Epoch 00014: val_loss did not improve from 0.23932\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3949 - acc: 0.8807 - val_loss: 0.2509 - val_acc: 0.9273\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8896\n",
      "Epoch 00015: val_loss improved from 0.23932 to 0.21728, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/015-0.2173.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3648 - acc: 0.8897 - val_loss: 0.2173 - val_acc: 0.9362\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8973\n",
      "Epoch 00016: val_loss improved from 0.21728 to 0.20720, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/016-0.2072.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3417 - acc: 0.8972 - val_loss: 0.2072 - val_acc: 0.9399\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.9008\n",
      "Epoch 00017: val_loss did not improve from 0.20720\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3311 - acc: 0.9009 - val_loss: 0.2111 - val_acc: 0.9373\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9092\n",
      "Epoch 00018: val_loss improved from 0.20720 to 0.20674, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/018-0.2067.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3043 - acc: 0.9092 - val_loss: 0.2067 - val_acc: 0.9371\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9113\n",
      "Epoch 00019: val_loss improved from 0.20674 to 0.19525, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/019-0.1953.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2945 - acc: 0.9113 - val_loss: 0.1953 - val_acc: 0.9457\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9139\n",
      "Epoch 00020: val_loss improved from 0.19525 to 0.17863, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/020-0.1786.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2837 - acc: 0.9139 - val_loss: 0.1786 - val_acc: 0.9469\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9201\n",
      "Epoch 00021: val_loss did not improve from 0.17863\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2669 - acc: 0.9200 - val_loss: 0.1800 - val_acc: 0.9478\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9199\n",
      "Epoch 00022: val_loss did not improve from 0.17863\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2619 - acc: 0.9199 - val_loss: 0.1833 - val_acc: 0.9478\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9240\n",
      "Epoch 00023: val_loss improved from 0.17863 to 0.17008, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/023-0.1701.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2496 - acc: 0.9240 - val_loss: 0.1701 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9286\n",
      "Epoch 00024: val_loss did not improve from 0.17008\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2361 - acc: 0.9286 - val_loss: 0.1850 - val_acc: 0.9434\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9283\n",
      "Epoch 00025: val_loss improved from 0.17008 to 0.16563, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/025-0.1656.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2337 - acc: 0.9282 - val_loss: 0.1656 - val_acc: 0.9462\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9325\n",
      "Epoch 00026: val_loss improved from 0.16563 to 0.15103, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/026-0.1510.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2221 - acc: 0.9325 - val_loss: 0.1510 - val_acc: 0.9555\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9355\n",
      "Epoch 00027: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2100 - acc: 0.9355 - val_loss: 0.1624 - val_acc: 0.9532\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9379\n",
      "Epoch 00028: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2052 - acc: 0.9379 - val_loss: 0.1679 - val_acc: 0.9511\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9384\n",
      "Epoch 00029: val_loss did not improve from 0.15103\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1966 - acc: 0.9384 - val_loss: 0.1634 - val_acc: 0.9529\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9415\n",
      "Epoch 00030: val_loss improved from 0.15103 to 0.14320, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/030-0.1432.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1922 - acc: 0.9414 - val_loss: 0.1432 - val_acc: 0.9590\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9434\n",
      "Epoch 00031: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1817 - acc: 0.9434 - val_loss: 0.1662 - val_acc: 0.9509\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9448\n",
      "Epoch 00032: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1799 - acc: 0.9448 - val_loss: 0.1821 - val_acc: 0.9495\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9467\n",
      "Epoch 00033: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1725 - acc: 0.9467 - val_loss: 0.1602 - val_acc: 0.9527\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9470\n",
      "Epoch 00034: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1685 - acc: 0.9470 - val_loss: 0.1433 - val_acc: 0.9590\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9495\n",
      "Epoch 00035: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1634 - acc: 0.9495 - val_loss: 0.1674 - val_acc: 0.9485\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9511\n",
      "Epoch 00036: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1596 - acc: 0.9511 - val_loss: 0.1504 - val_acc: 0.9567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9514\n",
      "Epoch 00037: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1558 - acc: 0.9514 - val_loss: 0.1680 - val_acc: 0.9502\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9536\n",
      "Epoch 00038: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1494 - acc: 0.9535 - val_loss: 0.1550 - val_acc: 0.9543\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9518\n",
      "Epoch 00039: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1526 - acc: 0.9518 - val_loss: 0.1516 - val_acc: 0.9536\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9539\n",
      "Epoch 00040: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1490 - acc: 0.9539 - val_loss: 0.1778 - val_acc: 0.9443\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9571\n",
      "Epoch 00041: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1346 - acc: 0.9572 - val_loss: 0.1675 - val_acc: 0.9492\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9584\n",
      "Epoch 00042: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1319 - acc: 0.9583 - val_loss: 0.1665 - val_acc: 0.9495\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9611\n",
      "Epoch 00043: val_loss improved from 0.14320 to 0.14123, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/043-0.1412.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1247 - acc: 0.9610 - val_loss: 0.1412 - val_acc: 0.9618\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9597\n",
      "Epoch 00044: val_loss did not improve from 0.14123\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1285 - acc: 0.9597 - val_loss: 0.1621 - val_acc: 0.9550\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9590\n",
      "Epoch 00045: val_loss improved from 0.14123 to 0.14110, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/045-0.1411.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1289 - acc: 0.9590 - val_loss: 0.1411 - val_acc: 0.9571\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9630\n",
      "Epoch 00046: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1223 - acc: 0.9629 - val_loss: 0.1452 - val_acc: 0.9581\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9581\n",
      "Epoch 00047: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1299 - acc: 0.9581 - val_loss: 0.1608 - val_acc: 0.9557\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9621\n",
      "Epoch 00048: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1225 - acc: 0.9620 - val_loss: 0.1492 - val_acc: 0.9578\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9653\n",
      "Epoch 00049: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1110 - acc: 0.9653 - val_loss: 0.1507 - val_acc: 0.9550\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9661\n",
      "Epoch 00050: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1078 - acc: 0.9661 - val_loss: 0.1573 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9673\n",
      "Epoch 00051: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1046 - acc: 0.9673 - val_loss: 0.1531 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9649\n",
      "Epoch 00052: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1111 - acc: 0.9648 - val_loss: 0.1476 - val_acc: 0.9576\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9668\n",
      "Epoch 00053: val_loss improved from 0.14110 to 0.13466, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/053-0.1347.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1097 - acc: 0.9668 - val_loss: 0.1347 - val_acc: 0.9618\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9689\n",
      "Epoch 00054: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0987 - acc: 0.9689 - val_loss: 0.1774 - val_acc: 0.9515\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9695\n",
      "Epoch 00055: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1002 - acc: 0.9694 - val_loss: 0.1607 - val_acc: 0.9583\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9699\n",
      "Epoch 00056: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0947 - acc: 0.9699 - val_loss: 0.1776 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9691\n",
      "Epoch 00057: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0981 - acc: 0.9691 - val_loss: 0.1424 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9695\n",
      "Epoch 00058: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0986 - acc: 0.9695 - val_loss: 0.1442 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9714\n",
      "Epoch 00059: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0893 - acc: 0.9714 - val_loss: 0.1582 - val_acc: 0.9539\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9725\n",
      "Epoch 00060: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0874 - acc: 0.9725 - val_loss: 0.1448 - val_acc: 0.9602\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9722\n",
      "Epoch 00061: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0895 - acc: 0.9722 - val_loss: 0.1458 - val_acc: 0.9609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9727\n",
      "Epoch 00062: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0861 - acc: 0.9727 - val_loss: 0.1460 - val_acc: 0.9597\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9743\n",
      "Epoch 00063: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0808 - acc: 0.9743 - val_loss: 0.1443 - val_acc: 0.9611\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9736\n",
      "Epoch 00064: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0814 - acc: 0.9736 - val_loss: 0.1368 - val_acc: 0.9599\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9747\n",
      "Epoch 00065: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0796 - acc: 0.9747 - val_loss: 0.1792 - val_acc: 0.9548\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9740\n",
      "Epoch 00066: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0801 - acc: 0.9740 - val_loss: 0.1547 - val_acc: 0.9546\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9751\n",
      "Epoch 00067: val_loss did not improve from 0.13466\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0822 - acc: 0.9750 - val_loss: 0.1497 - val_acc: 0.9574\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9760\n",
      "Epoch 00068: val_loss improved from 0.13466 to 0.13309, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv_checkpoint/068-0.1331.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0755 - acc: 0.9760 - val_loss: 0.1331 - val_acc: 0.9616\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9773\n",
      "Epoch 00069: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0710 - acc: 0.9773 - val_loss: 0.1799 - val_acc: 0.9562\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9748\n",
      "Epoch 00070: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0789 - acc: 0.9747 - val_loss: 0.1579 - val_acc: 0.9597\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9733\n",
      "Epoch 00071: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0855 - acc: 0.9732 - val_loss: 0.1404 - val_acc: 0.9620\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9776\n",
      "Epoch 00072: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0711 - acc: 0.9776 - val_loss: 0.1337 - val_acc: 0.9606\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9794\n",
      "Epoch 00073: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0668 - acc: 0.9794 - val_loss: 0.1527 - val_acc: 0.9597\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9774\n",
      "Epoch 00074: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0697 - acc: 0.9773 - val_loss: 0.1634 - val_acc: 0.9578\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9774\n",
      "Epoch 00075: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0738 - acc: 0.9774 - val_loss: 0.1417 - val_acc: 0.9604\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9785\n",
      "Epoch 00076: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0669 - acc: 0.9785 - val_loss: 0.1626 - val_acc: 0.9578\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9776\n",
      "Epoch 00077: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0693 - acc: 0.9776 - val_loss: 0.1401 - val_acc: 0.9618\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9795\n",
      "Epoch 00078: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0622 - acc: 0.9795 - val_loss: 0.1486 - val_acc: 0.9588\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9785\n",
      "Epoch 00079: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0692 - acc: 0.9785 - val_loss: 0.1442 - val_acc: 0.9599\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9790\n",
      "Epoch 00080: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0635 - acc: 0.9790 - val_loss: 0.1630 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9819\n",
      "Epoch 00081: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0592 - acc: 0.9819 - val_loss: 0.1658 - val_acc: 0.9609\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9774\n",
      "Epoch 00082: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0686 - acc: 0.9774 - val_loss: 0.1503 - val_acc: 0.9613\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9817\n",
      "Epoch 00083: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0576 - acc: 0.9817 - val_loss: 0.1807 - val_acc: 0.9497\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9792\n",
      "Epoch 00084: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0671 - acc: 0.9792 - val_loss: 0.1708 - val_acc: 0.9576\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9806\n",
      "Epoch 00085: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0608 - acc: 0.9806 - val_loss: 0.1511 - val_acc: 0.9618\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9840\n",
      "Epoch 00086: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0518 - acc: 0.9839 - val_loss: 0.1543 - val_acc: 0.9597\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9822\n",
      "Epoch 00087: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0566 - acc: 0.9822 - val_loss: 0.1409 - val_acc: 0.9620\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9832\n",
      "Epoch 00088: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0532 - acc: 0.9832 - val_loss: 0.1682 - val_acc: 0.9574\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9828\n",
      "Epoch 00089: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0523 - acc: 0.9828 - val_loss: 0.2057 - val_acc: 0.9474\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9830\n",
      "Epoch 00090: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0544 - acc: 0.9830 - val_loss: 0.1793 - val_acc: 0.9578\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9838\n",
      "Epoch 00091: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0511 - acc: 0.9838 - val_loss: 0.1626 - val_acc: 0.9564\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9851\n",
      "Epoch 00092: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0490 - acc: 0.9850 - val_loss: 0.2158 - val_acc: 0.9462\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9775\n",
      "Epoch 00093: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0689 - acc: 0.9775 - val_loss: 0.1678 - val_acc: 0.9576\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9839\n",
      "Epoch 00094: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0516 - acc: 0.9838 - val_loss: 0.1905 - val_acc: 0.9541\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9795\n",
      "Epoch 00095: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0654 - acc: 0.9795 - val_loss: 0.1540 - val_acc: 0.9613\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9855\n",
      "Epoch 00096: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0462 - acc: 0.9855 - val_loss: 0.1744 - val_acc: 0.9576\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9857\n",
      "Epoch 00097: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0475 - acc: 0.9857 - val_loss: 0.1694 - val_acc: 0.9616\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9867\n",
      "Epoch 00098: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0440 - acc: 0.9867 - val_loss: 0.1685 - val_acc: 0.9581\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9857\n",
      "Epoch 00099: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0459 - acc: 0.9857 - val_loss: 0.1658 - val_acc: 0.9585\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9870\n",
      "Epoch 00100: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0418 - acc: 0.9869 - val_loss: 0.2058 - val_acc: 0.9529\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9805\n",
      "Epoch 00101: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0615 - acc: 0.9805 - val_loss: 0.1596 - val_acc: 0.9632\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9839\n",
      "Epoch 00102: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0511 - acc: 0.9839 - val_loss: 0.1601 - val_acc: 0.9597\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9867\n",
      "Epoch 00103: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0423 - acc: 0.9867 - val_loss: 0.1661 - val_acc: 0.9606\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9827\n",
      "Epoch 00104: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0528 - acc: 0.9827 - val_loss: 0.1600 - val_acc: 0.9618\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9888\n",
      "Epoch 00105: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0370 - acc: 0.9888 - val_loss: 0.1552 - val_acc: 0.9602\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9851\n",
      "Epoch 00106: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0452 - acc: 0.9851 - val_loss: 0.1693 - val_acc: 0.9644\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9870\n",
      "Epoch 00107: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0402 - acc: 0.9870 - val_loss: 0.1631 - val_acc: 0.9604\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9866\n",
      "Epoch 00108: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0409 - acc: 0.9865 - val_loss: 0.1870 - val_acc: 0.9525\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9845\n",
      "Epoch 00109: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0517 - acc: 0.9845 - val_loss: 0.1669 - val_acc: 0.9597\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9868\n",
      "Epoch 00110: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0418 - acc: 0.9868 - val_loss: 0.1758 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9876\n",
      "Epoch 00111: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0413 - acc: 0.9876 - val_loss: 0.1549 - val_acc: 0.9616\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9890\n",
      "Epoch 00112: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0365 - acc: 0.9891 - val_loss: 0.1786 - val_acc: 0.9618\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9877\n",
      "Epoch 00113: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0403 - acc: 0.9877 - val_loss: 0.1751 - val_acc: 0.9592\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9868\n",
      "Epoch 00114: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0406 - acc: 0.9868 - val_loss: 0.1630 - val_acc: 0.9604\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9876\n",
      "Epoch 00115: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0397 - acc: 0.9876 - val_loss: 0.1836 - val_acc: 0.9571\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9882\n",
      "Epoch 00116: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0385 - acc: 0.9882 - val_loss: 0.1944 - val_acc: 0.9546\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9881\n",
      "Epoch 00117: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0386 - acc: 0.9880 - val_loss: 0.1858 - val_acc: 0.9611\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9857\n",
      "Epoch 00118: val_loss did not improve from 0.13309\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0476 - acc: 0.9857 - val_loss: 0.1597 - val_acc: 0.9648\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 689us/sample - loss: 0.1838 - acc: 0.9491\n",
      "Loss: 0.18378769090459107 Accuracy: 0.94911736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO_075_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 645us/sample - loss: 1.2868 - acc: 0.6816\n",
      "Loss: 1.2867762950102983 Accuracy: 0.68161994\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 319,280\n",
      "Trainable params: 319,024\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 664us/sample - loss: 0.7213 - acc: 0.8019\n",
      "Loss: 0.721279725231238 Accuracy: 0.80186915\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 228,464\n",
      "Trainable params: 228,080\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 662us/sample - loss: 0.5648 - acc: 0.8461\n",
      "Loss: 0.5647518580947709 Accuracy: 0.84610593\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 114,096\n",
      "Trainable params: 113,584\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 687us/sample - loss: 0.3728 - acc: 0.8951\n",
      "Loss: 0.3728217562503904 Accuracy: 0.8951194\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 89,840\n",
      "Trainable params: 89,200\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 702us/sample - loss: 0.2614 - acc: 0.9252\n",
      "Loss: 0.26139286952102914 Accuracy: 0.92523366\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 96,304\n",
      "Trainable params: 95,536\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 754us/sample - loss: 0.1825 - acc: 0.9458\n",
      "Loss: 0.18254458988752692 Accuracy: 0.9457944\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 134,832\n",
      "Trainable params: 133,808\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 790us/sample - loss: 0.1838 - acc: 0.9491\n",
      "Loss: 0.18378769090459107 Accuracy: 0.94911736\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO_075_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 775us/sample - loss: 2.2935 - acc: 0.5188\n",
      "Loss: 2.2935126847814797 Accuracy: 0.51879543\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 319,280\n",
      "Trainable params: 319,024\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 760us/sample - loss: 1.0185 - acc: 0.7412\n",
      "Loss: 1.0185046268029376 Accuracy: 0.74122536\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 228,464\n",
      "Trainable params: 228,080\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 796us/sample - loss: 0.6775 - acc: 0.8243\n",
      "Loss: 0.6774843389371474 Accuracy: 0.82429904\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 114,096\n",
      "Trainable params: 113,584\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 798us/sample - loss: 0.3783 - acc: 0.9007\n",
      "Loss: 0.37834709975578334 Accuracy: 0.9007269\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 89,840\n",
      "Trainable params: 89,200\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 820us/sample - loss: 0.2828 - acc: 0.9219\n",
      "Loss: 0.28284206380168225 Accuracy: 0.9219107\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 96,304\n",
      "Trainable params: 95,536\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 899us/sample - loss: 0.1933 - acc: 0.9445\n",
      "Loss: 0.19327436757572808 Accuracy: 0.9445483\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 134,832\n",
      "Trainable params: 133,808\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 878us/sample - loss: 0.2170 - acc: 0.9466\n",
      "Loss: 0.21696156144939047 Accuracy: 0.9466251\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
