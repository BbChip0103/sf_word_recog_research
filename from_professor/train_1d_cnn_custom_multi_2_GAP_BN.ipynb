{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 128)          512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 128)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 192)          768         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           3088        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 256)          1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 256)          1024        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 256)          1024        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8480 - acc: 0.4356\n",
      "Epoch 00001: val_loss improved from inf to 2.01416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/001-2.0142.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.8479 - acc: 0.4356 - val_loss: 2.0142 - val_acc: 0.3338\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4595 - acc: 0.5618\n",
      "Epoch 00002: val_loss improved from 2.01416 to 1.59156, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/002-1.5916.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4596 - acc: 0.5618 - val_loss: 1.5916 - val_acc: 0.4747\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3207 - acc: 0.6096\n",
      "Epoch 00003: val_loss improved from 1.59156 to 1.42783, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/003-1.4278.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3208 - acc: 0.6096 - val_loss: 1.4278 - val_acc: 0.5590\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2265 - acc: 0.6395\n",
      "Epoch 00004: val_loss improved from 1.42783 to 1.40208, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/004-1.4021.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2264 - acc: 0.6395 - val_loss: 1.4021 - val_acc: 0.5304\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6590\n",
      "Epoch 00005: val_loss did not improve from 1.40208\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1610 - acc: 0.6590 - val_loss: 1.6778 - val_acc: 0.4703\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1097 - acc: 0.6758\n",
      "Epoch 00006: val_loss improved from 1.40208 to 1.21797, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/006-1.2180.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1097 - acc: 0.6758 - val_loss: 1.2180 - val_acc: 0.6098\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0720 - acc: 0.6866\n",
      "Epoch 00007: val_loss did not improve from 1.21797\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0720 - acc: 0.6866 - val_loss: 1.3222 - val_acc: 0.5802\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0330 - acc: 0.6996\n",
      "Epoch 00008: val_loss did not improve from 1.21797\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0330 - acc: 0.6996 - val_loss: 1.4111 - val_acc: 0.5535\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0072 - acc: 0.7036\n",
      "Epoch 00009: val_loss did not improve from 1.21797\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0073 - acc: 0.7035 - val_loss: 1.4294 - val_acc: 0.5225\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9808 - acc: 0.7126\n",
      "Epoch 00010: val_loss improved from 1.21797 to 1.03420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/010-1.0342.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9808 - acc: 0.7126 - val_loss: 1.0342 - val_acc: 0.6741\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.7189\n",
      "Epoch 00011: val_loss improved from 1.03420 to 1.01932, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/011-1.0193.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9614 - acc: 0.7189 - val_loss: 1.0193 - val_acc: 0.7063\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9413 - acc: 0.7255\n",
      "Epoch 00012: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9413 - acc: 0.7256 - val_loss: 1.1730 - val_acc: 0.6387\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9270 - acc: 0.7279\n",
      "Epoch 00013: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9272 - acc: 0.7278 - val_loss: 1.0390 - val_acc: 0.6776\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9121 - acc: 0.7348\n",
      "Epoch 00014: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9122 - acc: 0.7347 - val_loss: 1.6023 - val_acc: 0.5036\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8974 - acc: 0.7395\n",
      "Epoch 00015: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8974 - acc: 0.7395 - val_loss: 1.2383 - val_acc: 0.6129\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8838 - acc: 0.7423\n",
      "Epoch 00016: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8838 - acc: 0.7423 - val_loss: 1.2920 - val_acc: 0.5998\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7479\n",
      "Epoch 00017: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8714 - acc: 0.7479 - val_loss: 1.0925 - val_acc: 0.6359\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8576 - acc: 0.7513\n",
      "Epoch 00018: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8577 - acc: 0.7513 - val_loss: 1.1106 - val_acc: 0.6520\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8512 - acc: 0.7516\n",
      "Epoch 00019: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8512 - acc: 0.7516 - val_loss: 1.0218 - val_acc: 0.6792\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8394 - acc: 0.7558\n",
      "Epoch 00020: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8395 - acc: 0.7557 - val_loss: 1.0511 - val_acc: 0.6879\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8315 - acc: 0.7588\n",
      "Epoch 00021: val_loss did not improve from 1.01932\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8315 - acc: 0.7588 - val_loss: 1.4990 - val_acc: 0.5413\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8195 - acc: 0.7616\n",
      "Epoch 00022: val_loss improved from 1.01932 to 0.86595, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/022-0.8660.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8196 - acc: 0.7615 - val_loss: 0.8660 - val_acc: 0.7491\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8137 - acc: 0.7645\n",
      "Epoch 00023: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8138 - acc: 0.7644 - val_loss: 1.8995 - val_acc: 0.4962\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8061 - acc: 0.7653\n",
      "Epoch 00024: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8060 - acc: 0.7654 - val_loss: 1.0308 - val_acc: 0.6783\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7689\n",
      "Epoch 00025: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7944 - acc: 0.7688 - val_loss: 0.9309 - val_acc: 0.7228\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.7692\n",
      "Epoch 00026: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7935 - acc: 0.7692 - val_loss: 1.1243 - val_acc: 0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7812 - acc: 0.7748\n",
      "Epoch 00027: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7813 - acc: 0.7747 - val_loss: 1.0044 - val_acc: 0.6942\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7777\n",
      "Epoch 00028: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7736 - acc: 0.7778 - val_loss: 1.1187 - val_acc: 0.6487\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.7758\n",
      "Epoch 00029: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7717 - acc: 0.7757 - val_loss: 1.1422 - val_acc: 0.6564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.7776\n",
      "Epoch 00030: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7649 - acc: 0.7776 - val_loss: 1.0487 - val_acc: 0.6872\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.7800\n",
      "Epoch 00031: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7553 - acc: 0.7799 - val_loss: 1.0028 - val_acc: 0.6848\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7802\n",
      "Epoch 00032: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7514 - acc: 0.7801 - val_loss: 1.4101 - val_acc: 0.5737\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7449 - acc: 0.7818\n",
      "Epoch 00033: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7449 - acc: 0.7818 - val_loss: 1.3956 - val_acc: 0.5730\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7837\n",
      "Epoch 00034: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7423 - acc: 0.7837 - val_loss: 1.2977 - val_acc: 0.5933\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7862\n",
      "Epoch 00035: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7360 - acc: 0.7862 - val_loss: 1.2001 - val_acc: 0.6406\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7324 - acc: 0.7842\n",
      "Epoch 00036: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7324 - acc: 0.7842 - val_loss: 1.1881 - val_acc: 0.6487\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7268 - acc: 0.7885\n",
      "Epoch 00037: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7268 - acc: 0.7885 - val_loss: 1.2030 - val_acc: 0.6250\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7895\n",
      "Epoch 00038: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7165 - acc: 0.7895 - val_loss: 0.9128 - val_acc: 0.7149\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.7935\n",
      "Epoch 00039: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7170 - acc: 0.7935 - val_loss: 1.4470 - val_acc: 0.5621\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.7921\n",
      "Epoch 00040: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7168 - acc: 0.7921 - val_loss: 1.3431 - val_acc: 0.6091\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7957\n",
      "Epoch 00041: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7062 - acc: 0.7956 - val_loss: 1.3987 - val_acc: 0.5889\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7072 - acc: 0.7941\n",
      "Epoch 00042: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7072 - acc: 0.7941 - val_loss: 1.1471 - val_acc: 0.6597\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7019 - acc: 0.7965\n",
      "Epoch 00043: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7021 - acc: 0.7965 - val_loss: 1.2895 - val_acc: 0.5956\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6954 - acc: 0.7975\n",
      "Epoch 00044: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6954 - acc: 0.7975 - val_loss: 0.9120 - val_acc: 0.7272\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7977\n",
      "Epoch 00045: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6917 - acc: 0.7977 - val_loss: 0.9205 - val_acc: 0.7331\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6879 - acc: 0.7993\n",
      "Epoch 00046: val_loss did not improve from 0.86595\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6881 - acc: 0.7992 - val_loss: 1.6336 - val_acc: 0.5045\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6853 - acc: 0.7999\n",
      "Epoch 00047: val_loss improved from 0.86595 to 0.79618, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/047-0.7962.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6853 - acc: 0.7999 - val_loss: 0.7962 - val_acc: 0.7682\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6835 - acc: 0.8013\n",
      "Epoch 00048: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6835 - acc: 0.8012 - val_loss: 1.2246 - val_acc: 0.6189\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.8036\n",
      "Epoch 00049: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6795 - acc: 0.8035 - val_loss: 1.0154 - val_acc: 0.6846\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6773 - acc: 0.8011\n",
      "Epoch 00050: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6772 - acc: 0.8012 - val_loss: 1.7851 - val_acc: 0.5167\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.8057\n",
      "Epoch 00051: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6666 - acc: 0.8058 - val_loss: 1.1999 - val_acc: 0.6266\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8069\n",
      "Epoch 00052: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6657 - acc: 0.8069 - val_loss: 1.4352 - val_acc: 0.6068\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6610 - acc: 0.8075\n",
      "Epoch 00053: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6612 - acc: 0.8075 - val_loss: 1.0832 - val_acc: 0.6641\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6642 - acc: 0.8061\n",
      "Epoch 00054: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6642 - acc: 0.8061 - val_loss: 1.6852 - val_acc: 0.5497\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.8064\n",
      "Epoch 00055: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6608 - acc: 0.8063 - val_loss: 1.6278 - val_acc: 0.5376\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6570 - acc: 0.8081\n",
      "Epoch 00056: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6569 - acc: 0.8081 - val_loss: 1.5943 - val_acc: 0.5837\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8100\n",
      "Epoch 00057: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6512 - acc: 0.8099 - val_loss: 1.5491 - val_acc: 0.5709\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.8122\n",
      "Epoch 00058: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6515 - acc: 0.8122 - val_loss: 0.9715 - val_acc: 0.7160\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.8135\n",
      "Epoch 00059: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6442 - acc: 0.8134 - val_loss: 1.4503 - val_acc: 0.5744\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.8120\n",
      "Epoch 00060: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6417 - acc: 0.8120 - val_loss: 1.5223 - val_acc: 0.5523\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.8129\n",
      "Epoch 00061: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6426 - acc: 0.8128 - val_loss: 2.1511 - val_acc: 0.5057\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.8121\n",
      "Epoch 00062: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6446 - acc: 0.8121 - val_loss: 1.0005 - val_acc: 0.7065\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6354 - acc: 0.8164\n",
      "Epoch 00063: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6354 - acc: 0.8164 - val_loss: 2.0881 - val_acc: 0.4684\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6361 - acc: 0.8145\n",
      "Epoch 00064: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6361 - acc: 0.8144 - val_loss: 1.3681 - val_acc: 0.5800\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.8151\n",
      "Epoch 00065: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6289 - acc: 0.8151 - val_loss: 0.9906 - val_acc: 0.7070\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.8174\n",
      "Epoch 00066: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6300 - acc: 0.8174 - val_loss: 1.4392 - val_acc: 0.6033\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6251 - acc: 0.8175\n",
      "Epoch 00067: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6251 - acc: 0.8175 - val_loss: 3.9077 - val_acc: 0.3613\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6248 - acc: 0.8162\n",
      "Epoch 00068: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6249 - acc: 0.8162 - val_loss: 3.9746 - val_acc: 0.3312\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6217 - acc: 0.8180\n",
      "Epoch 00069: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6219 - acc: 0.8180 - val_loss: 1.2520 - val_acc: 0.6322\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8169\n",
      "Epoch 00070: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6254 - acc: 0.8169 - val_loss: 1.1911 - val_acc: 0.6310\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.8220\n",
      "Epoch 00071: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6088 - acc: 0.8220 - val_loss: 2.2829 - val_acc: 0.4528\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.8216\n",
      "Epoch 00072: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6133 - acc: 0.8216 - val_loss: 2.5636 - val_acc: 0.4095\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8220\n",
      "Epoch 00073: val_loss did not improve from 0.79618\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6104 - acc: 0.8221 - val_loss: 0.8228 - val_acc: 0.7522\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.8216\n",
      "Epoch 00074: val_loss improved from 0.79618 to 0.79594, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/074-0.7959.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6101 - acc: 0.8216 - val_loss: 0.7959 - val_acc: 0.7678\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.8242\n",
      "Epoch 00075: val_loss improved from 0.79594 to 0.76749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_3_conv_checkpoint/075-0.7675.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6024 - acc: 0.8241 - val_loss: 0.7675 - val_acc: 0.7796\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6037 - acc: 0.8237\n",
      "Epoch 00076: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6039 - acc: 0.8236 - val_loss: 1.5836 - val_acc: 0.5476\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6048 - acc: 0.8254\n",
      "Epoch 00077: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6052 - acc: 0.8253 - val_loss: 1.2066 - val_acc: 0.6201\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.8244\n",
      "Epoch 00078: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6013 - acc: 0.8243 - val_loss: 0.8143 - val_acc: 0.7668\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8254\n",
      "Epoch 00079: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6003 - acc: 0.8254 - val_loss: 1.1430 - val_acc: 0.6394\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8279\n",
      "Epoch 00080: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5912 - acc: 0.8279 - val_loss: 2.6469 - val_acc: 0.4272\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5924 - acc: 0.8254\n",
      "Epoch 00081: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5924 - acc: 0.8255 - val_loss: 2.8361 - val_acc: 0.4319\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8282\n",
      "Epoch 00082: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5863 - acc: 0.8282 - val_loss: 0.9556 - val_acc: 0.7317\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8281\n",
      "Epoch 00083: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5902 - acc: 0.8280 - val_loss: 1.8332 - val_acc: 0.5015\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8292\n",
      "Epoch 00084: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5867 - acc: 0.8292 - val_loss: 2.7349 - val_acc: 0.4915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5889 - acc: 0.8274\n",
      "Epoch 00085: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5889 - acc: 0.8274 - val_loss: 1.0263 - val_acc: 0.6942\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.8296\n",
      "Epoch 00086: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5824 - acc: 0.8296 - val_loss: 1.3537 - val_acc: 0.6161\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5799 - acc: 0.8301\n",
      "Epoch 00087: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5799 - acc: 0.8301 - val_loss: 1.4561 - val_acc: 0.5877\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.8324\n",
      "Epoch 00088: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5762 - acc: 0.8324 - val_loss: 1.0891 - val_acc: 0.6886\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8335\n",
      "Epoch 00089: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5726 - acc: 0.8334 - val_loss: 1.7290 - val_acc: 0.5928\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5806 - acc: 0.8305\n",
      "Epoch 00090: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5805 - acc: 0.8305 - val_loss: 1.9337 - val_acc: 0.5008\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5732 - acc: 0.8330\n",
      "Epoch 00091: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5731 - acc: 0.8330 - val_loss: 1.4025 - val_acc: 0.5959\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5724 - acc: 0.8338\n",
      "Epoch 00092: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5724 - acc: 0.8338 - val_loss: 0.8669 - val_acc: 0.7480\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5711 - acc: 0.8338\n",
      "Epoch 00093: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5710 - acc: 0.8338 - val_loss: 1.0340 - val_acc: 0.7200\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5668 - acc: 0.8346\n",
      "Epoch 00094: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5670 - acc: 0.8346 - val_loss: 1.3055 - val_acc: 0.6243\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.8347\n",
      "Epoch 00095: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5666 - acc: 0.8348 - val_loss: 1.0360 - val_acc: 0.6706\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8368\n",
      "Epoch 00096: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5666 - acc: 0.8368 - val_loss: 1.8632 - val_acc: 0.5127\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8362\n",
      "Epoch 00097: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5635 - acc: 0.8361 - val_loss: 2.1514 - val_acc: 0.5246\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.8349\n",
      "Epoch 00098: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5626 - acc: 0.8349 - val_loss: 1.9471 - val_acc: 0.4978\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5605 - acc: 0.8374\n",
      "Epoch 00099: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5605 - acc: 0.8374 - val_loss: 1.3004 - val_acc: 0.6219\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5558 - acc: 0.8376\n",
      "Epoch 00100: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5560 - acc: 0.8375 - val_loss: 1.0180 - val_acc: 0.6962\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5533 - acc: 0.8387\n",
      "Epoch 00101: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5533 - acc: 0.8387 - val_loss: 1.2806 - val_acc: 0.6229\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.8377\n",
      "Epoch 00102: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5555 - acc: 0.8377 - val_loss: 0.8713 - val_acc: 0.7501\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5501 - acc: 0.8406\n",
      "Epoch 00103: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5501 - acc: 0.8406 - val_loss: 1.1325 - val_acc: 0.6762\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8404\n",
      "Epoch 00104: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5505 - acc: 0.8403 - val_loss: 1.3195 - val_acc: 0.6608\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5510 - acc: 0.8389\n",
      "Epoch 00105: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5509 - acc: 0.8389 - val_loss: 1.2279 - val_acc: 0.6394\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8413\n",
      "Epoch 00106: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5442 - acc: 0.8412 - val_loss: 0.9211 - val_acc: 0.7375\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8413\n",
      "Epoch 00107: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5456 - acc: 0.8413 - val_loss: 0.9480 - val_acc: 0.7240\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.8424\n",
      "Epoch 00108: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5401 - acc: 0.8424 - val_loss: 1.0152 - val_acc: 0.7209\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.8429\n",
      "Epoch 00109: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5397 - acc: 0.8429 - val_loss: 1.1091 - val_acc: 0.6949\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.8399\n",
      "Epoch 00110: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5447 - acc: 0.8399 - val_loss: 1.5091 - val_acc: 0.5986\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.8428\n",
      "Epoch 00111: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5374 - acc: 0.8428 - val_loss: 1.0360 - val_acc: 0.6990\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5372 - acc: 0.8443\n",
      "Epoch 00112: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5372 - acc: 0.8443 - val_loss: 1.4248 - val_acc: 0.5980\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8438\n",
      "Epoch 00113: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5334 - acc: 0.8439 - val_loss: 1.2951 - val_acc: 0.6629\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8445\n",
      "Epoch 00114: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5308 - acc: 0.8445 - val_loss: 1.5210 - val_acc: 0.5912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8458\n",
      "Epoch 00115: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5314 - acc: 0.8457 - val_loss: 1.0239 - val_acc: 0.7035\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8436\n",
      "Epoch 00116: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5335 - acc: 0.8436 - val_loss: 1.3245 - val_acc: 0.6224\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8451\n",
      "Epoch 00117: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5298 - acc: 0.8451 - val_loss: 0.9647 - val_acc: 0.7345\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5269 - acc: 0.8452\n",
      "Epoch 00118: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5270 - acc: 0.8452 - val_loss: 1.8533 - val_acc: 0.5290\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.8460\n",
      "Epoch 00119: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5264 - acc: 0.8459 - val_loss: 1.4177 - val_acc: 0.6026\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.8491\n",
      "Epoch 00120: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5220 - acc: 0.8491 - val_loss: 0.9766 - val_acc: 0.7212\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8484\n",
      "Epoch 00121: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5243 - acc: 0.8484 - val_loss: 1.3828 - val_acc: 0.6103\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5197 - acc: 0.8488\n",
      "Epoch 00122: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5197 - acc: 0.8488 - val_loss: 1.2288 - val_acc: 0.6580\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.8449\n",
      "Epoch 00123: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5252 - acc: 0.8448 - val_loss: 0.8224 - val_acc: 0.7563\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8470\n",
      "Epoch 00124: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5230 - acc: 0.8470 - val_loss: 1.7970 - val_acc: 0.5542\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8500\n",
      "Epoch 00125: val_loss did not improve from 0.76749\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.5156 - acc: 0.8500 - val_loss: 0.9443 - val_acc: 0.7347\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 553us/sample - loss: 0.8653 - acc: 0.7337\n",
      "Loss: 0.8652985211102019 Accuracy: 0.7337487\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6994 - acc: 0.4801\n",
      "Epoch 00001: val_loss improved from inf to 1.74680, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/001-1.7468.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.6994 - acc: 0.4801 - val_loss: 1.7468 - val_acc: 0.4687\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2802 - acc: 0.6187\n",
      "Epoch 00002: val_loss improved from 1.74680 to 1.30801, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/002-1.3080.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.2802 - acc: 0.6187 - val_loss: 1.3080 - val_acc: 0.5919\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1346 - acc: 0.6659\n",
      "Epoch 00003: val_loss improved from 1.30801 to 1.22033, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/003-1.2203.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.1347 - acc: 0.6659 - val_loss: 1.2203 - val_acc: 0.6382\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0441 - acc: 0.6954\n",
      "Epoch 00004: val_loss did not improve from 1.22033\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0441 - acc: 0.6954 - val_loss: 1.3844 - val_acc: 0.5544\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.7153\n",
      "Epoch 00005: val_loss improved from 1.22033 to 1.15721, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/005-1.1572.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9758 - acc: 0.7153 - val_loss: 1.1572 - val_acc: 0.6331\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9269 - acc: 0.7306\n",
      "Epoch 00006: val_loss did not improve from 1.15721\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9269 - acc: 0.7306 - val_loss: 1.2663 - val_acc: 0.6089\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8844 - acc: 0.7449\n",
      "Epoch 00007: val_loss improved from 1.15721 to 0.97829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/007-0.9783.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8846 - acc: 0.7448 - val_loss: 0.9783 - val_acc: 0.6993\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8557 - acc: 0.7518\n",
      "Epoch 00008: val_loss did not improve from 0.97829\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8556 - acc: 0.7519 - val_loss: 1.0583 - val_acc: 0.6641\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8200 - acc: 0.7630\n",
      "Epoch 00009: val_loss improved from 0.97829 to 0.96350, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/009-0.9635.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8200 - acc: 0.7629 - val_loss: 0.9635 - val_acc: 0.7002\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7982 - acc: 0.7688\n",
      "Epoch 00010: val_loss did not improve from 0.96350\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7983 - acc: 0.7688 - val_loss: 1.1353 - val_acc: 0.6543\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7719 - acc: 0.7773\n",
      "Epoch 00011: val_loss did not improve from 0.96350\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7720 - acc: 0.7772 - val_loss: 1.1659 - val_acc: 0.6483\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7577 - acc: 0.7836\n",
      "Epoch 00012: val_loss improved from 0.96350 to 0.80390, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/012-0.8039.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7578 - acc: 0.7835 - val_loss: 0.8039 - val_acc: 0.7736\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7418 - acc: 0.7867\n",
      "Epoch 00013: val_loss did not improve from 0.80390\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7420 - acc: 0.7866 - val_loss: 0.9672 - val_acc: 0.6890\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7942\n",
      "Epoch 00014: val_loss did not improve from 0.80390\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7207 - acc: 0.7941 - val_loss: 1.4863 - val_acc: 0.5674\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.7958\n",
      "Epoch 00015: val_loss improved from 0.80390 to 0.75266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/015-0.7527.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7092 - acc: 0.7958 - val_loss: 0.7527 - val_acc: 0.7927\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.7977\n",
      "Epoch 00016: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6940 - acc: 0.7976 - val_loss: 1.1009 - val_acc: 0.6606\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.8027\n",
      "Epoch 00017: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6874 - acc: 0.8027 - val_loss: 0.9277 - val_acc: 0.7209\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6707 - acc: 0.8076\n",
      "Epoch 00018: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6706 - acc: 0.8076 - val_loss: 0.7628 - val_acc: 0.7706\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6601 - acc: 0.8101\n",
      "Epoch 00019: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6603 - acc: 0.8101 - val_loss: 0.8468 - val_acc: 0.7510\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6497 - acc: 0.8138\n",
      "Epoch 00020: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6498 - acc: 0.8138 - val_loss: 1.0553 - val_acc: 0.6767\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6393 - acc: 0.8176\n",
      "Epoch 00021: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6393 - acc: 0.8176 - val_loss: 0.8593 - val_acc: 0.7340\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.8212\n",
      "Epoch 00022: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6305 - acc: 0.8212 - val_loss: 1.0958 - val_acc: 0.6601\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.8234\n",
      "Epoch 00023: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6211 - acc: 0.8234 - val_loss: 0.8661 - val_acc: 0.7473\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8245\n",
      "Epoch 00024: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6104 - acc: 0.8245 - val_loss: 0.7934 - val_acc: 0.7559\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8270\n",
      "Epoch 00025: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6023 - acc: 0.8270 - val_loss: 0.8127 - val_acc: 0.7596\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8302\n",
      "Epoch 00026: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5933 - acc: 0.8302 - val_loss: 0.7761 - val_acc: 0.7808\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8323\n",
      "Epoch 00027: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5869 - acc: 0.8323 - val_loss: 0.8135 - val_acc: 0.7587\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.8373\n",
      "Epoch 00028: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5764 - acc: 0.8372 - val_loss: 0.9084 - val_acc: 0.7289\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8343\n",
      "Epoch 00029: val_loss did not improve from 0.75266\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5735 - acc: 0.8342 - val_loss: 0.8974 - val_acc: 0.7216\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8372\n",
      "Epoch 00030: val_loss improved from 0.75266 to 0.67530, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/030-0.6753.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5645 - acc: 0.8372 - val_loss: 0.6753 - val_acc: 0.8153\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.8427\n",
      "Epoch 00031: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5534 - acc: 0.8427 - val_loss: 0.6965 - val_acc: 0.8032\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8438\n",
      "Epoch 00032: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5482 - acc: 0.8437 - val_loss: 1.1549 - val_acc: 0.6571\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.8476\n",
      "Epoch 00033: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5397 - acc: 0.8475 - val_loss: 0.7782 - val_acc: 0.7720\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8464\n",
      "Epoch 00034: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5358 - acc: 0.8465 - val_loss: 1.0615 - val_acc: 0.6848\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5283 - acc: 0.8481\n",
      "Epoch 00035: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5282 - acc: 0.8481 - val_loss: 0.8407 - val_acc: 0.7454\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8520\n",
      "Epoch 00036: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5200 - acc: 0.8519 - val_loss: 0.7501 - val_acc: 0.7850\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8507\n",
      "Epoch 00037: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5199 - acc: 0.8507 - val_loss: 0.9629 - val_acc: 0.6758\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5090 - acc: 0.8546\n",
      "Epoch 00038: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5090 - acc: 0.8546 - val_loss: 1.0222 - val_acc: 0.6872\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8583\n",
      "Epoch 00039: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5020 - acc: 0.8583 - val_loss: 1.2877 - val_acc: 0.6150\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8564\n",
      "Epoch 00040: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5039 - acc: 0.8564 - val_loss: 1.0010 - val_acc: 0.7265\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4918 - acc: 0.8591\n",
      "Epoch 00041: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4918 - acc: 0.8591 - val_loss: 0.7442 - val_acc: 0.7831\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8597\n",
      "Epoch 00042: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4891 - acc: 0.8597 - val_loss: 0.7369 - val_acc: 0.7841\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4837 - acc: 0.8619\n",
      "Epoch 00043: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4836 - acc: 0.8619 - val_loss: 0.8946 - val_acc: 0.7328\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8639\n",
      "Epoch 00044: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4783 - acc: 0.8639 - val_loss: 1.2128 - val_acc: 0.6375\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8668\n",
      "Epoch 00045: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4706 - acc: 0.8668 - val_loss: 0.8592 - val_acc: 0.7459\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8680\n",
      "Epoch 00046: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4677 - acc: 0.8680 - val_loss: 0.8725 - val_acc: 0.7524\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8664\n",
      "Epoch 00047: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4632 - acc: 0.8663 - val_loss: 0.7073 - val_acc: 0.8050\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8675\n",
      "Epoch 00048: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4602 - acc: 0.8675 - val_loss: 0.8332 - val_acc: 0.7515\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4521 - acc: 0.8723\n",
      "Epoch 00049: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4523 - acc: 0.8723 - val_loss: 1.1247 - val_acc: 0.6771\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8728\n",
      "Epoch 00050: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4466 - acc: 0.8728 - val_loss: 0.7022 - val_acc: 0.8041\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8745\n",
      "Epoch 00051: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4423 - acc: 0.8745 - val_loss: 0.8274 - val_acc: 0.7741\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8739\n",
      "Epoch 00052: val_loss did not improve from 0.67530\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4412 - acc: 0.8739 - val_loss: 0.7837 - val_acc: 0.7766\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.8782\n",
      "Epoch 00053: val_loss improved from 0.67530 to 0.64451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/053-0.6445.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4322 - acc: 0.8782 - val_loss: 0.6445 - val_acc: 0.8185\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.8758\n",
      "Epoch 00054: val_loss did not improve from 0.64451\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4333 - acc: 0.8758 - val_loss: 0.9089 - val_acc: 0.7160\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8783\n",
      "Epoch 00055: val_loss did not improve from 0.64451\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4275 - acc: 0.8782 - val_loss: 0.7375 - val_acc: 0.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8796\n",
      "Epoch 00056: val_loss did not improve from 0.64451\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4239 - acc: 0.8796 - val_loss: 0.9410 - val_acc: 0.7275\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8816\n",
      "Epoch 00057: val_loss did not improve from 0.64451\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4129 - acc: 0.8816 - val_loss: 1.0764 - val_acc: 0.6816\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8818\n",
      "Epoch 00058: val_loss did not improve from 0.64451\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4149 - acc: 0.8818 - val_loss: 0.9267 - val_acc: 0.7230\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4087 - acc: 0.8825\n",
      "Epoch 00059: val_loss improved from 0.64451 to 0.61841, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_4_conv_checkpoint/059-0.6184.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4087 - acc: 0.8825 - val_loss: 0.6184 - val_acc: 0.8227\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4045 - acc: 0.8839\n",
      "Epoch 00060: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4047 - acc: 0.8838 - val_loss: 1.1278 - val_acc: 0.6916\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4050 - acc: 0.8845\n",
      "Epoch 00061: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4050 - acc: 0.8845 - val_loss: 0.8641 - val_acc: 0.7573\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8884\n",
      "Epoch 00062: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3957 - acc: 0.8884 - val_loss: 0.7599 - val_acc: 0.7848\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8869\n",
      "Epoch 00063: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3935 - acc: 0.8869 - val_loss: 0.9637 - val_acc: 0.7240\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8902\n",
      "Epoch 00064: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3912 - acc: 0.8902 - val_loss: 0.6722 - val_acc: 0.8106\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8887\n",
      "Epoch 00065: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3861 - acc: 0.8887 - val_loss: 0.7550 - val_acc: 0.7857\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8915\n",
      "Epoch 00066: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3809 - acc: 0.8915 - val_loss: 0.6511 - val_acc: 0.8164\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8917\n",
      "Epoch 00067: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3816 - acc: 0.8916 - val_loss: 0.8319 - val_acc: 0.7633\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8931\n",
      "Epoch 00068: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3753 - acc: 0.8931 - val_loss: 0.8122 - val_acc: 0.7605\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8935\n",
      "Epoch 00069: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3736 - acc: 0.8935 - val_loss: 0.7103 - val_acc: 0.7992\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8955\n",
      "Epoch 00070: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3708 - acc: 0.8955 - val_loss: 0.8681 - val_acc: 0.7610\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8958\n",
      "Epoch 00071: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3674 - acc: 0.8957 - val_loss: 1.2140 - val_acc: 0.6655\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8977\n",
      "Epoch 00072: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3591 - acc: 0.8977 - val_loss: 0.6786 - val_acc: 0.8150\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8972\n",
      "Epoch 00073: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3584 - acc: 0.8972 - val_loss: 0.8003 - val_acc: 0.7654\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8978\n",
      "Epoch 00074: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3575 - acc: 0.8978 - val_loss: 0.7222 - val_acc: 0.7885\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8995\n",
      "Epoch 00075: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3538 - acc: 0.8995 - val_loss: 0.6805 - val_acc: 0.8099\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3461 - acc: 0.9007\n",
      "Epoch 00076: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3462 - acc: 0.9007 - val_loss: 0.8789 - val_acc: 0.7407\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.9005\n",
      "Epoch 00077: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3514 - acc: 0.9004 - val_loss: 0.8603 - val_acc: 0.7421\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.9016\n",
      "Epoch 00078: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3442 - acc: 0.9016 - val_loss: 0.9152 - val_acc: 0.7473\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.9032\n",
      "Epoch 00079: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3388 - acc: 0.9032 - val_loss: 0.8844 - val_acc: 0.7400\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.9038\n",
      "Epoch 00080: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3374 - acc: 0.9038 - val_loss: 1.0944 - val_acc: 0.7007\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.9046\n",
      "Epoch 00081: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3379 - acc: 0.9046 - val_loss: 0.7309 - val_acc: 0.7864\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.9048\n",
      "Epoch 00082: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3318 - acc: 0.9047 - val_loss: 0.7642 - val_acc: 0.7913\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.9075\n",
      "Epoch 00083: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3285 - acc: 0.9075 - val_loss: 0.6853 - val_acc: 0.8050\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.9069\n",
      "Epoch 00084: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3287 - acc: 0.9069 - val_loss: 0.6741 - val_acc: 0.8043\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9093\n",
      "Epoch 00085: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3186 - acc: 0.9093 - val_loss: 0.8270 - val_acc: 0.7689\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.9087\n",
      "Epoch 00086: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3242 - acc: 0.9087 - val_loss: 0.7755 - val_acc: 0.7822\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.9098\n",
      "Epoch 00087: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3196 - acc: 0.9097 - val_loss: 0.8181 - val_acc: 0.7657\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9096\n",
      "Epoch 00088: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3143 - acc: 0.9096 - val_loss: 0.7290 - val_acc: 0.7980\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9123\n",
      "Epoch 00089: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3068 - acc: 0.9123 - val_loss: 0.7775 - val_acc: 0.7829\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9120\n",
      "Epoch 00090: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3065 - acc: 0.9119 - val_loss: 0.7398 - val_acc: 0.7964\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9126\n",
      "Epoch 00091: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3073 - acc: 0.9126 - val_loss: 0.8295 - val_acc: 0.7822\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9142\n",
      "Epoch 00092: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3064 - acc: 0.9142 - val_loss: 0.9051 - val_acc: 0.7559\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.9144\n",
      "Epoch 00093: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3013 - acc: 0.9143 - val_loss: 0.7044 - val_acc: 0.8078\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9149\n",
      "Epoch 00094: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3008 - acc: 0.9149 - val_loss: 0.8889 - val_acc: 0.7442\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.9166\n",
      "Epoch 00095: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2974 - acc: 0.9166 - val_loss: 0.9147 - val_acc: 0.7368\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9152\n",
      "Epoch 00096: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2938 - acc: 0.9151 - val_loss: 0.7120 - val_acc: 0.7952\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9153\n",
      "Epoch 00097: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2989 - acc: 0.9152 - val_loss: 0.8165 - val_acc: 0.7696\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9187\n",
      "Epoch 00098: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2886 - acc: 0.9187 - val_loss: 0.9586 - val_acc: 0.7338\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9174\n",
      "Epoch 00099: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2890 - acc: 0.9174 - val_loss: 0.7000 - val_acc: 0.8106\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9174\n",
      "Epoch 00100: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2847 - acc: 0.9173 - val_loss: 1.0807 - val_acc: 0.7233\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9195\n",
      "Epoch 00101: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2825 - acc: 0.9195 - val_loss: 0.8331 - val_acc: 0.7631\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9220\n",
      "Epoch 00102: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2784 - acc: 0.9220 - val_loss: 0.8178 - val_acc: 0.7687\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9195\n",
      "Epoch 00103: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2791 - acc: 0.9195 - val_loss: 0.7644 - val_acc: 0.7945\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2677 - acc: 0.9239\n",
      "Epoch 00104: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2677 - acc: 0.9239 - val_loss: 0.7139 - val_acc: 0.8022\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9239\n",
      "Epoch 00105: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2710 - acc: 0.9239 - val_loss: 0.9342 - val_acc: 0.7389\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9231\n",
      "Epoch 00106: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2733 - acc: 0.9230 - val_loss: 0.7403 - val_acc: 0.7943\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9256\n",
      "Epoch 00107: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2640 - acc: 0.9256 - val_loss: 0.8198 - val_acc: 0.7801\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9241\n",
      "Epoch 00108: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2671 - acc: 0.9241 - val_loss: 1.0258 - val_acc: 0.7389\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.9252\n",
      "Epoch 00109: val_loss did not improve from 0.61841\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2650 - acc: 0.9252 - val_loss: 0.7572 - val_acc: 0.7939\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 582us/sample - loss: 0.7107 - acc: 0.7956\n",
      "Loss: 0.7106986285865246 Accuracy: 0.7956386\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5511 - acc: 0.5318\n",
      "Epoch 00001: val_loss improved from inf to 1.85642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/001-1.8564.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 1.5511 - acc: 0.5317 - val_loss: 1.8564 - val_acc: 0.3704\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0454 - acc: 0.6983\n",
      "Epoch 00002: val_loss improved from 1.85642 to 1.29965, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/002-1.2996.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.0454 - acc: 0.6982 - val_loss: 1.2996 - val_acc: 0.5837\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7523\n",
      "Epoch 00003: val_loss improved from 1.29965 to 0.86010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/003-0.8601.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.8774 - acc: 0.7523 - val_loss: 0.8601 - val_acc: 0.7601\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7655 - acc: 0.7873\n",
      "Epoch 00004: val_loss improved from 0.86010 to 0.83002, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/004-0.8300.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7654 - acc: 0.7874 - val_loss: 0.8300 - val_acc: 0.7615\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6843 - acc: 0.8110\n",
      "Epoch 00005: val_loss improved from 0.83002 to 0.78429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/005-0.7843.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6842 - acc: 0.8110 - val_loss: 0.7843 - val_acc: 0.7645\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6219 - acc: 0.8313\n",
      "Epoch 00006: val_loss improved from 0.78429 to 0.76043, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/006-0.7604.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6219 - acc: 0.8313 - val_loss: 0.7604 - val_acc: 0.7829\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8445\n",
      "Epoch 00007: val_loss did not improve from 0.76043\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5717 - acc: 0.8446 - val_loss: 0.9372 - val_acc: 0.7226\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5267 - acc: 0.8568\n",
      "Epoch 00008: val_loss did not improve from 0.76043\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5268 - acc: 0.8568 - val_loss: 0.8733 - val_acc: 0.7386\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8653\n",
      "Epoch 00009: val_loss improved from 0.76043 to 0.73995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/009-0.7399.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4911 - acc: 0.8652 - val_loss: 0.7399 - val_acc: 0.7768\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8741\n",
      "Epoch 00010: val_loss improved from 0.73995 to 0.57441, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/010-0.5744.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4625 - acc: 0.8741 - val_loss: 0.5744 - val_acc: 0.8362\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8791\n",
      "Epoch 00011: val_loss improved from 0.57441 to 0.57410, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/011-0.5741.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4411 - acc: 0.8791 - val_loss: 0.5741 - val_acc: 0.8321\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8868\n",
      "Epoch 00012: val_loss improved from 0.57410 to 0.47177, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/012-0.4718.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4144 - acc: 0.8868 - val_loss: 0.4718 - val_acc: 0.8665\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8903\n",
      "Epoch 00013: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3988 - acc: 0.8903 - val_loss: 0.6991 - val_acc: 0.7908\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8955\n",
      "Epoch 00014: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3798 - acc: 0.8955 - val_loss: 0.5277 - val_acc: 0.8512\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8996\n",
      "Epoch 00015: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3644 - acc: 0.8996 - val_loss: 0.4953 - val_acc: 0.8546\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.9038\n",
      "Epoch 00016: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3493 - acc: 0.9038 - val_loss: 0.5084 - val_acc: 0.8665\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.9086\n",
      "Epoch 00017: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3329 - acc: 0.9086 - val_loss: 0.7492 - val_acc: 0.7780\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9097\n",
      "Epoch 00018: val_loss did not improve from 0.47177\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3263 - acc: 0.9096 - val_loss: 0.6220 - val_acc: 0.8164\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.9138\n",
      "Epoch 00019: val_loss improved from 0.47177 to 0.45617, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/019-0.4562.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3153 - acc: 0.9138 - val_loss: 0.4562 - val_acc: 0.8744\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9163\n",
      "Epoch 00020: val_loss did not improve from 0.45617\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3011 - acc: 0.9163 - val_loss: 0.5428 - val_acc: 0.8481\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9189\n",
      "Epoch 00021: val_loss did not improve from 0.45617\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2943 - acc: 0.9189 - val_loss: 0.4740 - val_acc: 0.8705\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9201\n",
      "Epoch 00022: val_loss improved from 0.45617 to 0.45430, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/022-0.4543.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2828 - acc: 0.9201 - val_loss: 0.4543 - val_acc: 0.8779\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9262\n",
      "Epoch 00023: val_loss improved from 0.45430 to 0.40009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/023-0.4001.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2726 - acc: 0.9262 - val_loss: 0.4001 - val_acc: 0.8891\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9249\n",
      "Epoch 00024: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2690 - acc: 0.9249 - val_loss: 0.4298 - val_acc: 0.8835\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9273\n",
      "Epoch 00025: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2585 - acc: 0.9272 - val_loss: 0.4599 - val_acc: 0.8675\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9298\n",
      "Epoch 00026: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2490 - acc: 0.9297 - val_loss: 0.4203 - val_acc: 0.8798\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9318\n",
      "Epoch 00027: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2462 - acc: 0.9318 - val_loss: 0.4553 - val_acc: 0.8682\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9349\n",
      "Epoch 00028: val_loss did not improve from 0.40009\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2332 - acc: 0.9349 - val_loss: 0.7048 - val_acc: 0.7955\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9328\n",
      "Epoch 00029: val_loss improved from 0.40009 to 0.39811, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/029-0.3981.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2356 - acc: 0.9328 - val_loss: 0.3981 - val_acc: 0.8875\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9395\n",
      "Epoch 00030: val_loss did not improve from 0.39811\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2214 - acc: 0.9394 - val_loss: 0.5381 - val_acc: 0.8535\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9376\n",
      "Epoch 00031: val_loss did not improve from 0.39811\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2195 - acc: 0.9376 - val_loss: 0.4801 - val_acc: 0.8612\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9419\n",
      "Epoch 00032: val_loss did not improve from 0.39811\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2089 - acc: 0.9419 - val_loss: 0.4794 - val_acc: 0.8693\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9419\n",
      "Epoch 00033: val_loss improved from 0.39811 to 0.39037, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/033-0.3904.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2082 - acc: 0.9419 - val_loss: 0.3904 - val_acc: 0.8894\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9413\n",
      "Epoch 00034: val_loss did not improve from 0.39037\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2092 - acc: 0.9413 - val_loss: 0.4206 - val_acc: 0.8835\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9461\n",
      "Epoch 00035: val_loss improved from 0.39037 to 0.38624, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/035-0.3862.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1979 - acc: 0.9461 - val_loss: 0.3862 - val_acc: 0.8912\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9482\n",
      "Epoch 00036: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1889 - acc: 0.9482 - val_loss: 0.4264 - val_acc: 0.8835\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9480\n",
      "Epoch 00037: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1868 - acc: 0.9479 - val_loss: 0.3999 - val_acc: 0.8873\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9492\n",
      "Epoch 00038: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1822 - acc: 0.9492 - val_loss: 0.4497 - val_acc: 0.8791\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9502\n",
      "Epoch 00039: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1818 - acc: 0.9501 - val_loss: 0.4885 - val_acc: 0.8679\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9524\n",
      "Epoch 00040: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1721 - acc: 0.9524 - val_loss: 0.4514 - val_acc: 0.8784\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9533\n",
      "Epoch 00041: val_loss did not improve from 0.38624\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1695 - acc: 0.9533 - val_loss: 0.4350 - val_acc: 0.8784\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9541\n",
      "Epoch 00042: val_loss improved from 0.38624 to 0.37266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/042-0.3727.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1626 - acc: 0.9541 - val_loss: 0.3727 - val_acc: 0.8994\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9550\n",
      "Epoch 00043: val_loss did not improve from 0.37266\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1617 - acc: 0.9550 - val_loss: 0.4775 - val_acc: 0.8672\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9568\n",
      "Epoch 00044: val_loss improved from 0.37266 to 0.35408, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_5_conv_checkpoint/044-0.3541.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1583 - acc: 0.9568 - val_loss: 0.3541 - val_acc: 0.9022\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9578\n",
      "Epoch 00045: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1519 - acc: 0.9578 - val_loss: 0.4176 - val_acc: 0.8845\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9587\n",
      "Epoch 00046: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1504 - acc: 0.9586 - val_loss: 0.3757 - val_acc: 0.9017\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9613\n",
      "Epoch 00047: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1443 - acc: 0.9613 - val_loss: 0.5701 - val_acc: 0.8467\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9625\n",
      "Epoch 00048: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1392 - acc: 0.9625 - val_loss: 0.5806 - val_acc: 0.8386\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9624\n",
      "Epoch 00049: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1403 - acc: 0.9624 - val_loss: 0.4749 - val_acc: 0.8658\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9633\n",
      "Epoch 00050: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1356 - acc: 0.9633 - val_loss: 0.4867 - val_acc: 0.8684\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9636\n",
      "Epoch 00051: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1334 - acc: 0.9636 - val_loss: 0.5163 - val_acc: 0.8675\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9667\n",
      "Epoch 00052: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1254 - acc: 0.9667 - val_loss: 0.4259 - val_acc: 0.8789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9659\n",
      "Epoch 00053: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1262 - acc: 0.9659 - val_loss: 0.4609 - val_acc: 0.8737\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9692\n",
      "Epoch 00054: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1190 - acc: 0.9692 - val_loss: 0.3743 - val_acc: 0.9017\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9691\n",
      "Epoch 00055: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1186 - acc: 0.9691 - val_loss: 0.3807 - val_acc: 0.8940\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9701\n",
      "Epoch 00056: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1155 - acc: 0.9701 - val_loss: 0.6711 - val_acc: 0.8248\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9709\n",
      "Epoch 00057: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1126 - acc: 0.9709 - val_loss: 0.5187 - val_acc: 0.8537\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9699\n",
      "Epoch 00058: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1114 - acc: 0.9699 - val_loss: 0.4366 - val_acc: 0.8800\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9735\n",
      "Epoch 00059: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1058 - acc: 0.9734 - val_loss: 0.4602 - val_acc: 0.8810\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9701\n",
      "Epoch 00060: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1092 - acc: 0.9701 - val_loss: 0.5926 - val_acc: 0.8376\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9729\n",
      "Epoch 00061: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1040 - acc: 0.9729 - val_loss: 0.4566 - val_acc: 0.8763\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9737\n",
      "Epoch 00062: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1010 - acc: 0.9737 - val_loss: 0.4317 - val_acc: 0.8842\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9742\n",
      "Epoch 00063: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0988 - acc: 0.9741 - val_loss: 0.5605 - val_acc: 0.8560\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9719\n",
      "Epoch 00064: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1042 - acc: 0.9719 - val_loss: 0.3847 - val_acc: 0.8959\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9746\n",
      "Epoch 00065: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0982 - acc: 0.9745 - val_loss: 0.4555 - val_acc: 0.8728\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9750\n",
      "Epoch 00066: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0936 - acc: 0.9750 - val_loss: 0.4297 - val_acc: 0.8882\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9786\n",
      "Epoch 00067: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0855 - acc: 0.9786 - val_loss: 0.4141 - val_acc: 0.8956\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9778\n",
      "Epoch 00068: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0881 - acc: 0.9778 - val_loss: 0.6135 - val_acc: 0.8453\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9780\n",
      "Epoch 00069: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0862 - acc: 0.9780 - val_loss: 0.4183 - val_acc: 0.8921\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9792\n",
      "Epoch 00070: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0837 - acc: 0.9792 - val_loss: 0.3986 - val_acc: 0.8989\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9811\n",
      "Epoch 00071: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0782 - acc: 0.9811 - val_loss: 0.4308 - val_acc: 0.8859\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9801\n",
      "Epoch 00072: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0815 - acc: 0.9801 - val_loss: 0.4689 - val_acc: 0.8861\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9823\n",
      "Epoch 00073: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0732 - acc: 0.9823 - val_loss: 0.3785 - val_acc: 0.9010\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9786\n",
      "Epoch 00074: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0837 - acc: 0.9786 - val_loss: 0.4299 - val_acc: 0.8935\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9827\n",
      "Epoch 00075: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0711 - acc: 0.9827 - val_loss: 0.3785 - val_acc: 0.9092\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9823\n",
      "Epoch 00076: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0724 - acc: 0.9823 - val_loss: 0.4188 - val_acc: 0.8952\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9828\n",
      "Epoch 00077: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0691 - acc: 0.9828 - val_loss: 0.4541 - val_acc: 0.8819\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9809\n",
      "Epoch 00078: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0749 - acc: 0.9809 - val_loss: 0.5533 - val_acc: 0.8633\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9825\n",
      "Epoch 00079: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0710 - acc: 0.9825 - val_loss: 0.3990 - val_acc: 0.8959\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9835\n",
      "Epoch 00080: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0676 - acc: 0.9834 - val_loss: 0.4588 - val_acc: 0.8810\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9842\n",
      "Epoch 00081: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0662 - acc: 0.9841 - val_loss: 0.4507 - val_acc: 0.8896\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9829\n",
      "Epoch 00082: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0705 - acc: 0.9828 - val_loss: 0.4379 - val_acc: 0.8968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9857\n",
      "Epoch 00083: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0612 - acc: 0.9857 - val_loss: 0.4400 - val_acc: 0.8859\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9849\n",
      "Epoch 00084: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0616 - acc: 0.9849 - val_loss: 0.4738 - val_acc: 0.8863\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9872\n",
      "Epoch 00085: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0561 - acc: 0.9872 - val_loss: 0.5838 - val_acc: 0.8619\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9838\n",
      "Epoch 00086: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0657 - acc: 0.9838 - val_loss: 0.4458 - val_acc: 0.8915\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9882\n",
      "Epoch 00087: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0523 - acc: 0.9882 - val_loss: 0.3573 - val_acc: 0.9122\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9871\n",
      "Epoch 00088: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0550 - acc: 0.9871 - val_loss: 0.4595 - val_acc: 0.8942\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9862\n",
      "Epoch 00089: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0571 - acc: 0.9862 - val_loss: 0.4362 - val_acc: 0.8898\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9885\n",
      "Epoch 00090: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0522 - acc: 0.9884 - val_loss: 0.8477 - val_acc: 0.8018\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9861\n",
      "Epoch 00091: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0579 - acc: 0.9861 - val_loss: 0.5937 - val_acc: 0.8567\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9898\n",
      "Epoch 00092: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0466 - acc: 0.9898 - val_loss: 0.5056 - val_acc: 0.8798\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9894\n",
      "Epoch 00093: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0508 - acc: 0.9894 - val_loss: 0.4003 - val_acc: 0.9031\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9888\n",
      "Epoch 00094: val_loss did not improve from 0.35408\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0468 - acc: 0.9888 - val_loss: 0.4263 - val_acc: 0.8947\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 633us/sample - loss: 0.4003 - acc: 0.8804\n",
      "Loss: 0.4003377884038388 Accuracy: 0.88037384\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3284 - acc: 0.6073\n",
      "Epoch 00001: val_loss improved from inf to 1.45541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/001-1.4554.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.3283 - acc: 0.6073 - val_loss: 1.4554 - val_acc: 0.5572\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7301 - acc: 0.8006\n",
      "Epoch 00002: val_loss improved from 1.45541 to 0.71465, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/002-0.7147.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7301 - acc: 0.8006 - val_loss: 0.7147 - val_acc: 0.7936\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8572\n",
      "Epoch 00003: val_loss improved from 0.71465 to 0.51920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/003-0.5192.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5422 - acc: 0.8572 - val_loss: 0.5192 - val_acc: 0.8549\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8805\n",
      "Epoch 00004: val_loss improved from 0.51920 to 0.45884, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/004-0.4588.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4438 - acc: 0.8805 - val_loss: 0.4588 - val_acc: 0.8735\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8979\n",
      "Epoch 00005: val_loss improved from 0.45884 to 0.39554, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/005-0.3955.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3797 - acc: 0.8979 - val_loss: 0.3955 - val_acc: 0.8928\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.9090\n",
      "Epoch 00006: val_loss improved from 0.39554 to 0.35184, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/006-0.3518.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3353 - acc: 0.9090 - val_loss: 0.3518 - val_acc: 0.9071\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9177\n",
      "Epoch 00007: val_loss improved from 0.35184 to 0.32532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/007-0.3253.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3051 - acc: 0.9177 - val_loss: 0.3253 - val_acc: 0.9096\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9251\n",
      "Epoch 00008: val_loss improved from 0.32532 to 0.28417, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/008-0.2842.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2741 - acc: 0.9251 - val_loss: 0.2842 - val_acc: 0.9206\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9310\n",
      "Epoch 00009: val_loss did not improve from 0.28417\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2513 - acc: 0.9309 - val_loss: 0.3989 - val_acc: 0.8817\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9351\n",
      "Epoch 00010: val_loss did not improve from 0.28417\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2367 - acc: 0.9351 - val_loss: 0.2856 - val_acc: 0.9159\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9400\n",
      "Epoch 00011: val_loss improved from 0.28417 to 0.27503, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/011-0.2750.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2153 - acc: 0.9400 - val_loss: 0.2750 - val_acc: 0.9192\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9435\n",
      "Epoch 00012: val_loss did not improve from 0.27503\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2030 - acc: 0.9435 - val_loss: 0.3384 - val_acc: 0.9059\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9470\n",
      "Epoch 00013: val_loss improved from 0.27503 to 0.25074, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/013-0.2507.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1908 - acc: 0.9469 - val_loss: 0.2507 - val_acc: 0.9269\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9502\n",
      "Epoch 00014: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1802 - acc: 0.9502 - val_loss: 0.2674 - val_acc: 0.9259\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9523\n",
      "Epoch 00015: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1677 - acc: 0.9523 - val_loss: 0.2674 - val_acc: 0.9283\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9550\n",
      "Epoch 00016: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1587 - acc: 0.9550 - val_loss: 0.4331 - val_acc: 0.8793\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9564\n",
      "Epoch 00017: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1550 - acc: 0.9563 - val_loss: 0.2776 - val_acc: 0.9215\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9611\n",
      "Epoch 00018: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1401 - acc: 0.9611 - val_loss: 0.2664 - val_acc: 0.9315\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9628\n",
      "Epoch 00019: val_loss did not improve from 0.25074\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1337 - acc: 0.9628 - val_loss: 0.2956 - val_acc: 0.9234\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9652\n",
      "Epoch 00020: val_loss improved from 0.25074 to 0.24929, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/020-0.2493.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1252 - acc: 0.9652 - val_loss: 0.2493 - val_acc: 0.9280\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9656\n",
      "Epoch 00021: val_loss improved from 0.24929 to 0.24394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/021-0.2439.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1222 - acc: 0.9656 - val_loss: 0.2439 - val_acc: 0.9320\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9685\n",
      "Epoch 00022: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1161 - acc: 0.9685 - val_loss: 0.2470 - val_acc: 0.9324\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9711\n",
      "Epoch 00023: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1067 - acc: 0.9711 - val_loss: 0.2562 - val_acc: 0.9322\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9737\n",
      "Epoch 00024: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0987 - acc: 0.9736 - val_loss: 0.2555 - val_acc: 0.9329\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9733\n",
      "Epoch 00025: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0997 - acc: 0.9733 - val_loss: 0.2540 - val_acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9751\n",
      "Epoch 00026: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0938 - acc: 0.9751 - val_loss: 0.2881 - val_acc: 0.9241\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9769\n",
      "Epoch 00027: val_loss did not improve from 0.24394\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0868 - acc: 0.9769 - val_loss: 0.2778 - val_acc: 0.9266\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9780\n",
      "Epoch 00028: val_loss improved from 0.24394 to 0.22895, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/028-0.2289.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0816 - acc: 0.9780 - val_loss: 0.2289 - val_acc: 0.9306\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9819\n",
      "Epoch 00029: val_loss did not improve from 0.22895\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0708 - acc: 0.9819 - val_loss: 0.2893 - val_acc: 0.9315\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9800\n",
      "Epoch 00030: val_loss did not improve from 0.22895\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0763 - acc: 0.9800 - val_loss: 0.2613 - val_acc: 0.9297\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9820\n",
      "Epoch 00031: val_loss did not improve from 0.22895\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0712 - acc: 0.9820 - val_loss: 0.3079 - val_acc: 0.9199\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9832\n",
      "Epoch 00032: val_loss improved from 0.22895 to 0.22490, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/032-0.2249.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0656 - acc: 0.9832 - val_loss: 0.2249 - val_acc: 0.9434\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9861\n",
      "Epoch 00033: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0586 - acc: 0.9861 - val_loss: 0.2523 - val_acc: 0.9320\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9852\n",
      "Epoch 00034: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0607 - acc: 0.9852 - val_loss: 0.2871 - val_acc: 0.9227\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9868\n",
      "Epoch 00035: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0555 - acc: 0.9867 - val_loss: 0.2778 - val_acc: 0.9292\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9842\n",
      "Epoch 00036: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0587 - acc: 0.9842 - val_loss: 0.2909 - val_acc: 0.9245\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9887\n",
      "Epoch 00037: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0481 - acc: 0.9888 - val_loss: 0.2500 - val_acc: 0.9350\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9874\n",
      "Epoch 00038: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0518 - acc: 0.9874 - val_loss: 0.2803 - val_acc: 0.9266\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9902\n",
      "Epoch 00039: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0418 - acc: 0.9901 - val_loss: 0.2620 - val_acc: 0.9276\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9859\n",
      "Epoch 00040: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0559 - acc: 0.9859 - val_loss: 0.2917 - val_acc: 0.9262\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9870\n",
      "Epoch 00041: val_loss did not improve from 0.22490\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0526 - acc: 0.9870 - val_loss: 0.2496 - val_acc: 0.9350\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9944\n",
      "Epoch 00042: val_loss improved from 0.22490 to 0.22398, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/042-0.2240.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0310 - acc: 0.9944 - val_loss: 0.2240 - val_acc: 0.9425\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9895\n",
      "Epoch 00043: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0432 - acc: 0.9895 - val_loss: 0.3165 - val_acc: 0.9234\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9920\n",
      "Epoch 00044: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0342 - acc: 0.9920 - val_loss: 0.3018 - val_acc: 0.9292\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9924\n",
      "Epoch 00045: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0337 - acc: 0.9924 - val_loss: 0.3507 - val_acc: 0.9192\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9912\n",
      "Epoch 00046: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0369 - acc: 0.9912 - val_loss: 0.2684 - val_acc: 0.9343\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9940\n",
      "Epoch 00047: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0309 - acc: 0.9940 - val_loss: 0.2466 - val_acc: 0.9359\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9917\n",
      "Epoch 00048: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0367 - acc: 0.9917 - val_loss: 0.2361 - val_acc: 0.9404\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.22398\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0309 - acc: 0.9932 - val_loss: 0.2667 - val_acc: 0.9311\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9921\n",
      "Epoch 00050: val_loss improved from 0.22398 to 0.21763, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_6_conv_checkpoint/050-0.2176.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0341 - acc: 0.9921 - val_loss: 0.2176 - val_acc: 0.9457\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9948\n",
      "Epoch 00051: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0267 - acc: 0.9948 - val_loss: 0.2746 - val_acc: 0.9345\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9941\n",
      "Epoch 00052: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0290 - acc: 0.9941 - val_loss: 0.2381 - val_acc: 0.9429\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9959\n",
      "Epoch 00053: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0225 - acc: 0.9959 - val_loss: 0.3124 - val_acc: 0.9245\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9939\n",
      "Epoch 00054: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0285 - acc: 0.9938 - val_loss: 0.2468 - val_acc: 0.9418\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9936\n",
      "Epoch 00055: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0278 - acc: 0.9935 - val_loss: 0.2602 - val_acc: 0.9364\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9924\n",
      "Epoch 00056: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0323 - acc: 0.9924 - val_loss: 0.2283 - val_acc: 0.9422\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9966\n",
      "Epoch 00057: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0187 - acc: 0.9966 - val_loss: 0.2675 - val_acc: 0.9429\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9964\n",
      "Epoch 00058: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0194 - acc: 0.9964 - val_loss: 0.3005 - val_acc: 0.9238\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9949\n",
      "Epoch 00059: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0242 - acc: 0.9949 - val_loss: 0.3320 - val_acc: 0.9269\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9965\n",
      "Epoch 00060: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0167 - acc: 0.9965 - val_loss: 0.2563 - val_acc: 0.9406\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9926\n",
      "Epoch 00061: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0306 - acc: 0.9926 - val_loss: 0.2839 - val_acc: 0.9336\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9958\n",
      "Epoch 00062: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0200 - acc: 0.9958 - val_loss: 0.2384 - val_acc: 0.9460\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9953\n",
      "Epoch 00063: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0219 - acc: 0.9953 - val_loss: 0.2466 - val_acc: 0.9392\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9963\n",
      "Epoch 00064: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0202 - acc: 0.9963 - val_loss: 0.4779 - val_acc: 0.8968\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9952\n",
      "Epoch 00065: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0242 - acc: 0.9952 - val_loss: 0.2712 - val_acc: 0.9348\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9970\n",
      "Epoch 00066: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0162 - acc: 0.9970 - val_loss: 0.2634 - val_acc: 0.9420\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9955\n",
      "Epoch 00067: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0216 - acc: 0.9955 - val_loss: 0.2320 - val_acc: 0.9418\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9965\n",
      "Epoch 00068: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0174 - acc: 0.9965 - val_loss: 0.2960 - val_acc: 0.9308\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0189 - acc: 0.9963 - val_loss: 0.2962 - val_acc: 0.9285\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9968\n",
      "Epoch 00070: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0165 - acc: 0.9968 - val_loss: 0.2574 - val_acc: 0.9408\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9958\n",
      "Epoch 00071: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0184 - acc: 0.9958 - val_loss: 0.3034 - val_acc: 0.9343\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9949\n",
      "Epoch 00072: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0230 - acc: 0.9949 - val_loss: 0.2621 - val_acc: 0.9448\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9980\n",
      "Epoch 00073: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0122 - acc: 0.9980 - val_loss: 0.4068 - val_acc: 0.9159\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9924\n",
      "Epoch 00074: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0294 - acc: 0.9924 - val_loss: 0.2551 - val_acc: 0.9432\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 00075: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0103 - acc: 0.9984 - val_loss: 0.2783 - val_acc: 0.9357\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9975\n",
      "Epoch 00076: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0131 - acc: 0.9974 - val_loss: 0.3077 - val_acc: 0.9322\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9960\n",
      "Epoch 00077: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0189 - acc: 0.9960 - val_loss: 0.2602 - val_acc: 0.9366\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9975\n",
      "Epoch 00078: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0132 - acc: 0.9975 - val_loss: 0.2710 - val_acc: 0.9394\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9937\n",
      "Epoch 00079: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0271 - acc: 0.9937 - val_loss: 0.2977 - val_acc: 0.9355\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9968\n",
      "Epoch 00080: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0152 - acc: 0.9968 - val_loss: 0.2627 - val_acc: 0.9406\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9990\n",
      "Epoch 00081: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0086 - acc: 0.9990 - val_loss: 0.2345 - val_acc: 0.9471\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9945\n",
      "Epoch 00082: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0211 - acc: 0.9945 - val_loss: 0.2396 - val_acc: 0.9434\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9949\n",
      "Epoch 00083: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0206 - acc: 0.9949 - val_loss: 0.2337 - val_acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9985\n",
      "Epoch 00084: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0090 - acc: 0.9985 - val_loss: 0.3375 - val_acc: 0.9243\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9988\n",
      "Epoch 00085: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0084 - acc: 0.9988 - val_loss: 0.2653 - val_acc: 0.9427\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9978\n",
      "Epoch 00086: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0112 - acc: 0.9978 - val_loss: 0.2837 - val_acc: 0.9401\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9977\n",
      "Epoch 00087: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0136 - acc: 0.9977 - val_loss: 0.4069 - val_acc: 0.9215\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9969\n",
      "Epoch 00088: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0151 - acc: 0.9969 - val_loss: 0.2731 - val_acc: 0.9411\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9936\n",
      "Epoch 00089: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0240 - acc: 0.9936 - val_loss: 0.2265 - val_acc: 0.9504\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9989\n",
      "Epoch 00090: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0069 - acc: 0.9989 - val_loss: 0.2508 - val_acc: 0.9441\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9983\n",
      "Epoch 00091: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0099 - acc: 0.9982 - val_loss: 0.2630 - val_acc: 0.9422\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9952\n",
      "Epoch 00092: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0185 - acc: 0.9952 - val_loss: 0.2357 - val_acc: 0.9467\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9980\n",
      "Epoch 00093: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0105 - acc: 0.9980 - val_loss: 0.2869 - val_acc: 0.9392\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9979\n",
      "Epoch 00094: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0109 - acc: 0.9979 - val_loss: 0.2360 - val_acc: 0.9495\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9966\n",
      "Epoch 00095: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0151 - acc: 0.9966 - val_loss: 0.2363 - val_acc: 0.9495\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9984\n",
      "Epoch 00096: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0093 - acc: 0.9984 - val_loss: 0.2962 - val_acc: 0.9371\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9975\n",
      "Epoch 00097: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0123 - acc: 0.9975 - val_loss: 0.3054 - val_acc: 0.9350\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9983\n",
      "Epoch 00098: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0098 - acc: 0.9982 - val_loss: 0.3063 - val_acc: 0.9287\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9943\n",
      "Epoch 00099: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0224 - acc: 0.9943 - val_loss: 0.2602 - val_acc: 0.9432\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9993\n",
      "Epoch 00100: val_loss did not improve from 0.21763\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0060 - acc: 0.9993 - val_loss: 0.2807 - val_acc: 0.9448\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 663us/sample - loss: 0.2719 - acc: 0.9281\n",
      "Loss: 0.27192329813572474 Accuracy: 0.92814124\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1172 - acc: 0.6693\n",
      "Epoch 00001: val_loss improved from inf to 1.01726, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/001-1.0173.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 1.1171 - acc: 0.6693 - val_loss: 1.0173 - val_acc: 0.7156\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.8626\n",
      "Epoch 00002: val_loss improved from 1.01726 to 0.38810, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/002-0.3881.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4998 - acc: 0.8626 - val_loss: 0.3881 - val_acc: 0.8966\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.9015\n",
      "Epoch 00003: val_loss improved from 0.38810 to 0.38802, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/003-0.3880.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3550 - acc: 0.9015 - val_loss: 0.3880 - val_acc: 0.8847\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9205\n",
      "Epoch 00004: val_loss improved from 0.38802 to 0.30374, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/004-0.3037.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2847 - acc: 0.9205 - val_loss: 0.3037 - val_acc: 0.9143\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9339\n",
      "Epoch 00005: val_loss improved from 0.30374 to 0.28109, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/005-0.2811.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2372 - acc: 0.9339 - val_loss: 0.2811 - val_acc: 0.9236\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9425\n",
      "Epoch 00006: val_loss improved from 0.28109 to 0.25854, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/006-0.2585.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2062 - acc: 0.9425 - val_loss: 0.2585 - val_acc: 0.9255\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9505\n",
      "Epoch 00007: val_loss improved from 0.25854 to 0.21343, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/007-0.2134.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1801 - acc: 0.9505 - val_loss: 0.2134 - val_acc: 0.9373\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9563\n",
      "Epoch 00008: val_loss did not improve from 0.21343\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1582 - acc: 0.9563 - val_loss: 0.2233 - val_acc: 0.9380\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9613\n",
      "Epoch 00009: val_loss did not improve from 0.21343\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1426 - acc: 0.9613 - val_loss: 0.2266 - val_acc: 0.9329\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9644\n",
      "Epoch 00010: val_loss did not improve from 0.21343\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1295 - acc: 0.9644 - val_loss: 0.2176 - val_acc: 0.9343\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9676\n",
      "Epoch 00011: val_loss did not improve from 0.21343\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1184 - acc: 0.9676 - val_loss: 0.2269 - val_acc: 0.9336\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9713\n",
      "Epoch 00012: val_loss improved from 0.21343 to 0.20664, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/012-0.2066.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1044 - acc: 0.9713 - val_loss: 0.2066 - val_acc: 0.9418\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9752\n",
      "Epoch 00013: val_loss did not improve from 0.20664\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0951 - acc: 0.9752 - val_loss: 0.2166 - val_acc: 0.9362\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9767\n",
      "Epoch 00014: val_loss improved from 0.20664 to 0.19627, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/014-0.1963.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0866 - acc: 0.9766 - val_loss: 0.1963 - val_acc: 0.9453\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9785\n",
      "Epoch 00015: val_loss improved from 0.19627 to 0.17542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/015-0.1754.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0816 - acc: 0.9785 - val_loss: 0.1754 - val_acc: 0.9499\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9825\n",
      "Epoch 00016: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0710 - acc: 0.9824 - val_loss: 0.2378 - val_acc: 0.9311\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9813\n",
      "Epoch 00017: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0750 - acc: 0.9813 - val_loss: 0.2194 - val_acc: 0.9385\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9846\n",
      "Epoch 00018: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0611 - acc: 0.9846 - val_loss: 0.1874 - val_acc: 0.9481\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9889\n",
      "Epoch 00019: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0488 - acc: 0.9889 - val_loss: 0.1921 - val_acc: 0.9469\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9897\n",
      "Epoch 00020: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0451 - acc: 0.9897 - val_loss: 0.2292 - val_acc: 0.9385\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9874\n",
      "Epoch 00021: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0507 - acc: 0.9874 - val_loss: 0.2578 - val_acc: 0.9331\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9898\n",
      "Epoch 00022: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0433 - acc: 0.9898 - val_loss: 0.1959 - val_acc: 0.9476\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9920\n",
      "Epoch 00023: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0354 - acc: 0.9920 - val_loss: 0.1824 - val_acc: 0.9539\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9929\n",
      "Epoch 00024: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0334 - acc: 0.9928 - val_loss: 0.1878 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9915\n",
      "Epoch 00025: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0367 - acc: 0.9915 - val_loss: 0.1901 - val_acc: 0.9483\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9911\n",
      "Epoch 00026: val_loss did not improve from 0.17542\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0374 - acc: 0.9910 - val_loss: 0.2244 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9926\n",
      "Epoch 00027: val_loss improved from 0.17542 to 0.16677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/027-0.1668.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0337 - acc: 0.9926 - val_loss: 0.1668 - val_acc: 0.9562\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9956\n",
      "Epoch 00028: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9956 - val_loss: 0.1974 - val_acc: 0.9450\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9948\n",
      "Epoch 00029: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0257 - acc: 0.9948 - val_loss: 0.2261 - val_acc: 0.9448\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9950\n",
      "Epoch 00030: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0248 - acc: 0.9950 - val_loss: 0.1760 - val_acc: 0.9548\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9946\n",
      "Epoch 00031: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0256 - acc: 0.9946 - val_loss: 0.2356 - val_acc: 0.9422\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9962\n",
      "Epoch 00032: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0201 - acc: 0.9962 - val_loss: 0.1992 - val_acc: 0.9490\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9939\n",
      "Epoch 00033: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0263 - acc: 0.9939 - val_loss: 0.1798 - val_acc: 0.9534\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9971\n",
      "Epoch 00034: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0164 - acc: 0.9971 - val_loss: 0.2126 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9929\n",
      "Epoch 00035: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0296 - acc: 0.9929 - val_loss: 0.1905 - val_acc: 0.9560\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9979\n",
      "Epoch 00036: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0139 - acc: 0.9979 - val_loss: 0.2197 - val_acc: 0.9511\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9980\n",
      "Epoch 00037: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0127 - acc: 0.9980 - val_loss: 0.2396 - val_acc: 0.9411\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9957\n",
      "Epoch 00038: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0185 - acc: 0.9957 - val_loss: 0.2607 - val_acc: 0.9408\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9952\n",
      "Epoch 00039: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0203 - acc: 0.9952 - val_loss: 0.2113 - val_acc: 0.9441\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9974\n",
      "Epoch 00040: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0144 - acc: 0.9974 - val_loss: 0.1911 - val_acc: 0.9525\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9959\n",
      "Epoch 00041: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0185 - acc: 0.9959 - val_loss: 0.2206 - val_acc: 0.9478\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9969\n",
      "Epoch 00042: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0146 - acc: 0.9969 - val_loss: 0.2443 - val_acc: 0.9411\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9980\n",
      "Epoch 00043: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0119 - acc: 0.9979 - val_loss: 0.2256 - val_acc: 0.9446\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9943\n",
      "Epoch 00044: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0222 - acc: 0.9943 - val_loss: 0.1731 - val_acc: 0.9606\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9985\n",
      "Epoch 00045: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0096 - acc: 0.9985 - val_loss: 0.1966 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9979\n",
      "Epoch 00046: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0116 - acc: 0.9979 - val_loss: 0.2057 - val_acc: 0.9481\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9980\n",
      "Epoch 00047: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0110 - acc: 0.9979 - val_loss: 0.2179 - val_acc: 0.9460\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9922\n",
      "Epoch 00048: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0278 - acc: 0.9922 - val_loss: 0.1815 - val_acc: 0.9560\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9976\n",
      "Epoch 00049: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0115 - acc: 0.9976 - val_loss: 0.1897 - val_acc: 0.9532\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9989\n",
      "Epoch 00050: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9989 - val_loss: 0.2045 - val_acc: 0.9499\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9983\n",
      "Epoch 00051: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0097 - acc: 0.9983 - val_loss: 0.2024 - val_acc: 0.9513\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9961\n",
      "Epoch 00052: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0157 - acc: 0.9961 - val_loss: 0.2424 - val_acc: 0.9457\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9989\n",
      "Epoch 00053: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0071 - acc: 0.9989 - val_loss: 0.2544 - val_acc: 0.9460\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9957\n",
      "Epoch 00054: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.1926 - val_acc: 0.9527\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9983\n",
      "Epoch 00055: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0094 - acc: 0.9983 - val_loss: 0.1841 - val_acc: 0.9532\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9974\n",
      "Epoch 00056: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0127 - acc: 0.9974 - val_loss: 0.1913 - val_acc: 0.9557\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 00057: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.1891 - val_acc: 0.9539\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9993\n",
      "Epoch 00058: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0059 - acc: 0.9993 - val_loss: 0.2253 - val_acc: 0.9483\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9967\n",
      "Epoch 00059: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0141 - acc: 0.9967 - val_loss: 0.2232 - val_acc: 0.9497\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 00060: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0121 - acc: 0.9973 - val_loss: 0.1869 - val_acc: 0.9502\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9992\n",
      "Epoch 00061: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0053 - acc: 0.9992 - val_loss: 0.2024 - val_acc: 0.9562\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9953\n",
      "Epoch 00062: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0162 - acc: 0.9953 - val_loss: 0.1900 - val_acc: 0.9555\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 00063: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0058 - acc: 0.9992 - val_loss: 0.2112 - val_acc: 0.9513\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00064: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0045 - acc: 0.9993 - val_loss: 0.2846 - val_acc: 0.9371\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9966\n",
      "Epoch 00065: val_loss did not improve from 0.16677\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0141 - acc: 0.9966 - val_loss: 0.2139 - val_acc: 0.9485\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9960\n",
      "Epoch 00066: val_loss improved from 0.16677 to 0.16281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_7_conv_checkpoint/066-0.1628.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0167 - acc: 0.9960 - val_loss: 0.1628 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9994\n",
      "Epoch 00067: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0046 - acc: 0.9994 - val_loss: 0.2197 - val_acc: 0.9504\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9974\n",
      "Epoch 00068: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0104 - acc: 0.9974 - val_loss: 0.2099 - val_acc: 0.9518\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9994\n",
      "Epoch 00069: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0043 - acc: 0.9994 - val_loss: 0.2104 - val_acc: 0.9520\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9983\n",
      "Epoch 00070: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0077 - acc: 0.9983 - val_loss: 0.1950 - val_acc: 0.9515\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9989\n",
      "Epoch 00071: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0066 - acc: 0.9989 - val_loss: 0.2177 - val_acc: 0.9476\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9964\n",
      "Epoch 00072: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0145 - acc: 0.9964 - val_loss: 0.1985 - val_acc: 0.9564\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00073: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.1705 - val_acc: 0.9585\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9996\n",
      "Epoch 00074: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0032 - acc: 0.9996 - val_loss: 0.2000 - val_acc: 0.9567\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9960\n",
      "Epoch 00075: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0152 - acc: 0.9960 - val_loss: 0.2106 - val_acc: 0.9513\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 00076: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.2879 - val_acc: 0.9376\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9991\n",
      "Epoch 00077: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.2271 - val_acc: 0.9518\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00078: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.2122 - val_acc: 0.9534\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00079: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1854 - val_acc: 0.9585\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 00080: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.2375 - val_acc: 0.9464\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9978\n",
      "Epoch 00081: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0093 - acc: 0.9978 - val_loss: 0.2025 - val_acc: 0.9527\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0134 - acc: 0.9966 - val_loss: 0.2170 - val_acc: 0.9502\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 00083: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.1990 - val_acc: 0.9550\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9958\n",
      "Epoch 00084: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0156 - acc: 0.9958 - val_loss: 0.2211 - val_acc: 0.9511\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9962\n",
      "Epoch 00085: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0144 - acc: 0.9962 - val_loss: 0.1733 - val_acc: 0.9613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00086: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1668 - val_acc: 0.9606\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00087: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1666 - val_acc: 0.9616\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00088: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.2262 - val_acc: 0.9518\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 00089: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.1975 - val_acc: 0.9569\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9953\n",
      "Epoch 00090: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0170 - acc: 0.9953 - val_loss: 0.2092 - val_acc: 0.9567\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9958\n",
      "Epoch 00091: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0158 - acc: 0.9958 - val_loss: 0.1966 - val_acc: 0.9511\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00092: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.1671 - val_acc: 0.9604\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00093: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.1732 - val_acc: 0.9588\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 00094: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0031 - acc: 0.9997 - val_loss: 0.1791 - val_acc: 0.9604\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00095: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0042 - acc: 0.9993 - val_loss: 0.2904 - val_acc: 0.9401\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9965\n",
      "Epoch 00096: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0121 - acc: 0.9964 - val_loss: 0.2827 - val_acc: 0.9448\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9961\n",
      "Epoch 00097: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0168 - acc: 0.9960 - val_loss: 0.1952 - val_acc: 0.9602\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9984\n",
      "Epoch 00098: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0075 - acc: 0.9984 - val_loss: 0.1847 - val_acc: 0.9599\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 00099: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0021 - acc: 0.9999 - val_loss: 0.1884 - val_acc: 0.9599\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00100: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.2164 - val_acc: 0.9546\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9976\n",
      "Epoch 00101: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0090 - acc: 0.9976 - val_loss: 0.2164 - val_acc: 0.9525\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 00102: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0092 - acc: 0.9977 - val_loss: 0.1870 - val_acc: 0.9597\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 00103: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0021 - acc: 0.9997 - val_loss: 0.2264 - val_acc: 0.9506\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9971\n",
      "Epoch 00104: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0111 - acc: 0.9971 - val_loss: 0.1858 - val_acc: 0.9560\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00105: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.1893 - val_acc: 0.9599\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 00106: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0045 - acc: 0.9993 - val_loss: 0.2006 - val_acc: 0.9588\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9987\n",
      "Epoch 00107: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0056 - acc: 0.9987 - val_loss: 0.1848 - val_acc: 0.9602\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9988\n",
      "Epoch 00108: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0058 - acc: 0.9988 - val_loss: 0.2530 - val_acc: 0.9481\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
      "Epoch 00109: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0052 - acc: 0.9989 - val_loss: 0.2134 - val_acc: 0.9532\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9989\n",
      "Epoch 00110: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0056 - acc: 0.9989 - val_loss: 0.3003 - val_acc: 0.9415\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00111: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0050 - acc: 0.9989 - val_loss: 0.2020 - val_acc: 0.9595\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9989\n",
      "Epoch 00112: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0056 - acc: 0.9989 - val_loss: 0.2626 - val_acc: 0.9457\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 00113: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0064 - acc: 0.9983 - val_loss: 0.4044 - val_acc: 0.9131\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9972\n",
      "Epoch 00114: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0109 - acc: 0.9972 - val_loss: 0.2040 - val_acc: 0.9567\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 00115: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1869 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00116: val_loss did not improve from 0.16281\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1810 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 692us/sample - loss: 0.1985 - acc: 0.9493\n",
      "Loss: 0.19854257594821115 Accuracy: 0.949325\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0323 - acc: 0.6876\n",
      "Epoch 00001: val_loss improved from inf to 0.81100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/001-0.8110.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.0322 - acc: 0.6876 - val_loss: 0.8110 - val_acc: 0.7689\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8759\n",
      "Epoch 00002: val_loss improved from 0.81100 to 0.34005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/002-0.3400.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4207 - acc: 0.8759 - val_loss: 0.3400 - val_acc: 0.8991\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9149\n",
      "Epoch 00003: val_loss improved from 0.34005 to 0.26871, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/003-0.2687.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2913 - acc: 0.9149 - val_loss: 0.2687 - val_acc: 0.9159\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9333\n",
      "Epoch 00004: val_loss improved from 0.26871 to 0.22088, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/004-0.2209.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2259 - acc: 0.9334 - val_loss: 0.2209 - val_acc: 0.9322\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9451\n",
      "Epoch 00005: val_loss improved from 0.22088 to 0.21121, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/005-0.2112.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1864 - acc: 0.9451 - val_loss: 0.2112 - val_acc: 0.9387\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9541\n",
      "Epoch 00006: val_loss did not improve from 0.21121\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1593 - acc: 0.9541 - val_loss: 0.2426 - val_acc: 0.9287\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9618\n",
      "Epoch 00007: val_loss improved from 0.21121 to 0.19352, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/007-0.1935.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1380 - acc: 0.9618 - val_loss: 0.1935 - val_acc: 0.9429\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9670\n",
      "Epoch 00008: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1189 - acc: 0.9670 - val_loss: 0.2017 - val_acc: 0.9373\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9694\n",
      "Epoch 00009: val_loss did not improve from 0.19352\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1094 - acc: 0.9694 - val_loss: 0.2255 - val_acc: 0.9317\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9739\n",
      "Epoch 00010: val_loss improved from 0.19352 to 0.16824, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/010-0.1682.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0968 - acc: 0.9739 - val_loss: 0.1682 - val_acc: 0.9467\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9801\n",
      "Epoch 00011: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0758 - acc: 0.9800 - val_loss: 0.1704 - val_acc: 0.9478\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9812\n",
      "Epoch 00012: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0726 - acc: 0.9812 - val_loss: 0.1741 - val_acc: 0.9478\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9842\n",
      "Epoch 00013: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0620 - acc: 0.9842 - val_loss: 0.1758 - val_acc: 0.9460\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9866\n",
      "Epoch 00014: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0524 - acc: 0.9866 - val_loss: 0.2198 - val_acc: 0.9359\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9846\n",
      "Epoch 00015: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0566 - acc: 0.9846 - val_loss: 0.1774 - val_acc: 0.9506\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9911\n",
      "Epoch 00016: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0382 - acc: 0.9910 - val_loss: 0.2085 - val_acc: 0.9341\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9874\n",
      "Epoch 00017: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0474 - acc: 0.9874 - val_loss: 0.2081 - val_acc: 0.9397\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9908\n",
      "Epoch 00018: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0381 - acc: 0.9908 - val_loss: 0.1859 - val_acc: 0.9497\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9945\n",
      "Epoch 00019: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0270 - acc: 0.9944 - val_loss: 0.1862 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9917\n",
      "Epoch 00020: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0341 - acc: 0.9917 - val_loss: 0.2365 - val_acc: 0.9334\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9901\n",
      "Epoch 00021: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0392 - acc: 0.9901 - val_loss: 0.1873 - val_acc: 0.9499\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9962\n",
      "Epoch 00022: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0211 - acc: 0.9962 - val_loss: 0.1832 - val_acc: 0.9527\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9961\n",
      "Epoch 00023: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0207 - acc: 0.9961 - val_loss: 0.2143 - val_acc: 0.9427\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9950\n",
      "Epoch 00024: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0220 - acc: 0.9950 - val_loss: 0.2263 - val_acc: 0.9415\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9950\n",
      "Epoch 00025: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0207 - acc: 0.9950 - val_loss: 0.2424 - val_acc: 0.9397\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9938\n",
      "Epoch 00026: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0262 - acc: 0.9938 - val_loss: 0.1928 - val_acc: 0.9485\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9940\n",
      "Epoch 00027: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0231 - acc: 0.9939 - val_loss: 0.1941 - val_acc: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9923\n",
      "Epoch 00028: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0290 - acc: 0.9923 - val_loss: 0.1686 - val_acc: 0.9532\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9978\n",
      "Epoch 00029: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0112 - acc: 0.9977 - val_loss: 0.2012 - val_acc: 0.9488\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9946\n",
      "Epoch 00030: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0218 - acc: 0.9945 - val_loss: 0.2045 - val_acc: 0.9436\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9933\n",
      "Epoch 00031: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0253 - acc: 0.9933 - val_loss: 0.1866 - val_acc: 0.9471\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9980\n",
      "Epoch 00032: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0098 - acc: 0.9980 - val_loss: 0.1985 - val_acc: 0.9495\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9986\n",
      "Epoch 00033: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.2136 - val_acc: 0.9476\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9960\n",
      "Epoch 00034: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0162 - acc: 0.9960 - val_loss: 0.1788 - val_acc: 0.9518\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9945\n",
      "Epoch 00035: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0197 - acc: 0.9945 - val_loss: 0.1842 - val_acc: 0.9532\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9964\n",
      "Epoch 00036: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0146 - acc: 0.9964 - val_loss: 0.1948 - val_acc: 0.9490\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9979\n",
      "Epoch 00037: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0093 - acc: 0.9979 - val_loss: 0.2140 - val_acc: 0.9462\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9986\n",
      "Epoch 00038: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0082 - acc: 0.9986 - val_loss: 0.2155 - val_acc: 0.9497\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9947\n",
      "Epoch 00039: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0189 - acc: 0.9947 - val_loss: 0.2423 - val_acc: 0.9415\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9959\n",
      "Epoch 00040: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0165 - acc: 0.9959 - val_loss: 0.2308 - val_acc: 0.9450\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9989\n",
      "Epoch 00041: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0066 - acc: 0.9988 - val_loss: 0.2457 - val_acc: 0.9406\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9924\n",
      "Epoch 00042: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0260 - acc: 0.9924 - val_loss: 0.2021 - val_acc: 0.9511\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9994\n",
      "Epoch 00043: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0061 - acc: 0.9994 - val_loss: 0.1735 - val_acc: 0.9588\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00044: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.2097 - val_acc: 0.9504\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9974\n",
      "Epoch 00045: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0114 - acc: 0.9974 - val_loss: 0.2531 - val_acc: 0.9425\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9933\n",
      "Epoch 00046: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0220 - acc: 0.9933 - val_loss: 0.1996 - val_acc: 0.9525\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9950\n",
      "Epoch 00047: val_loss did not improve from 0.16824\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0194 - acc: 0.9950 - val_loss: 0.1766 - val_acc: 0.9576\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9993\n",
      "Epoch 00048: val_loss improved from 0.16824 to 0.16479, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_BN_8_conv_checkpoint/048-0.1648.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0049 - acc: 0.9993 - val_loss: 0.1648 - val_acc: 0.9604\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00049: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.1913 - val_acc: 0.9567\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9980\n",
      "Epoch 00050: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0082 - acc: 0.9980 - val_loss: 0.2574 - val_acc: 0.9411\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0159 - acc: 0.9958 - val_loss: 0.2183 - val_acc: 0.9485\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00052: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0087 - acc: 0.9977 - val_loss: 0.2225 - val_acc: 0.9478\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9940\n",
      "Epoch 00053: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0206 - acc: 0.9940 - val_loss: 0.1698 - val_acc: 0.9578\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 00054: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0054 - acc: 0.9987 - val_loss: 0.2007 - val_acc: 0.9548\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9992\n",
      "Epoch 00055: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.1932 - val_acc: 0.9576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00056: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1851 - val_acc: 0.9581\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9968\n",
      "Epoch 00057: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0115 - acc: 0.9968 - val_loss: 0.2095 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9981\n",
      "Epoch 00058: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0088 - acc: 0.9981 - val_loss: 0.1935 - val_acc: 0.9539\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9953\n",
      "Epoch 00059: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0162 - acc: 0.9953 - val_loss: 0.2064 - val_acc: 0.9562\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00060: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.1781 - val_acc: 0.9567\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 00061: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0035 - acc: 0.9994 - val_loss: 0.1932 - val_acc: 0.9541\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9970\n",
      "Epoch 00062: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0110 - acc: 0.9970 - val_loss: 0.2165 - val_acc: 0.9488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9986\n",
      "Epoch 00063: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.2154 - val_acc: 0.9525\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9979\n",
      "Epoch 00064: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 0.2205 - val_acc: 0.9515\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00065: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.1801 - val_acc: 0.9576\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9994\n",
      "Epoch 00066: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0039 - acc: 0.9994 - val_loss: 0.1916 - val_acc: 0.9571\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9958\n",
      "Epoch 00067: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0149 - acc: 0.9958 - val_loss: 0.1981 - val_acc: 0.9550\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00068: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1969 - val_acc: 0.9548\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9993\n",
      "Epoch 00069: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.1892 - val_acc: 0.9578\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9976\n",
      "Epoch 00070: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0092 - acc: 0.9976 - val_loss: 0.2006 - val_acc: 0.9553\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00071: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0035 - acc: 0.9993 - val_loss: 0.2160 - val_acc: 0.9511\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9967\n",
      "Epoch 00072: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0111 - acc: 0.9966 - val_loss: 0.2326 - val_acc: 0.9483\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0136 - acc: 0.9965 - val_loss: 0.1688 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00074: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 0.1772 - val_acc: 0.9569\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00075: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.2201 - val_acc: 0.9485\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 00076: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0067 - acc: 0.9983 - val_loss: 0.2371 - val_acc: 0.9467\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9965\n",
      "Epoch 00077: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 0.1959 - val_acc: 0.9539\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9972\n",
      "Epoch 00078: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.1661 - val_acc: 0.9606\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00079: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1856 - val_acc: 0.9571\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00080: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.1721 - val_acc: 0.9576\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00081: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.3140 - val_acc: 0.9383\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9989\n",
      "Epoch 00082: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0054 - acc: 0.9989 - val_loss: 0.2165 - val_acc: 0.9548\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9947\n",
      "Epoch 00083: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0175 - acc: 0.9947 - val_loss: 0.2077 - val_acc: 0.9518\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9992\n",
      "Epoch 00084: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0040 - acc: 0.9992 - val_loss: 0.2240 - val_acc: 0.9492\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00085: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2032 - val_acc: 0.9541\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0034 - acc: 0.9991 - val_loss: 0.2121 - val_acc: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 00087: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.2146 - val_acc: 0.9539\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00088: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1863 - val_acc: 0.9555\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9969\n",
      "Epoch 00089: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0106 - acc: 0.9969 - val_loss: 0.1971 - val_acc: 0.9553\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9993\n",
      "Epoch 00090: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0033 - acc: 0.9993 - val_loss: 0.1976 - val_acc: 0.9585\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9961\n",
      "Epoch 00091: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0129 - acc: 0.9961 - val_loss: 0.1902 - val_acc: 0.9585\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00092: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.1913 - val_acc: 0.9583\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00093: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1849 - val_acc: 0.9627\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00094: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.2399 - val_acc: 0.9469\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 00095: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9988 - val_loss: 0.2668 - val_acc: 0.9453\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9970\n",
      "Epoch 00096: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0093 - acc: 0.9970 - val_loss: 0.2046 - val_acc: 0.9576\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00097: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0033 - acc: 0.9994 - val_loss: 0.2123 - val_acc: 0.9534\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9995\n",
      "Epoch 00098: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.2239 - val_acc: 0.9529\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 763us/sample - loss: 0.2102 - acc: 0.9470\n",
      "Loss: 0.2101627866953209 Accuracy: 0.9470405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 128)          512         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 683us/sample - loss: 0.8653 - acc: 0.7337\n",
      "Loss: 0.8652985211102019 Accuracy: 0.7337487\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 128)          512         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 739us/sample - loss: 0.7107 - acc: 0.7956\n",
      "Loss: 0.7106986285865246 Accuracy: 0.7956386\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 192)          768         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 749us/sample - loss: 0.4003 - acc: 0.8804\n",
      "Loss: 0.4003377884038388 Accuracy: 0.88037384\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 256)          1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 791us/sample - loss: 0.2719 - acc: 0.9281\n",
      "Loss: 0.27192329813572474 Accuracy: 0.92814124\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 256)          1024        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 827us/sample - loss: 0.1985 - acc: 0.9493\n",
      "Loss: 0.19854257594821115 Accuracy: 0.949325\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 256)          1024        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 847us/sample - loss: 0.2102 - acc: 0.9470\n",
      "Loss: 0.2101627866953209 Accuracy: 0.9470405\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 128)          512         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 44,816\n",
      "Trainable params: 44,176\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 795us/sample - loss: 1.0987 - acc: 0.6960\n",
      "Loss: 1.0987274745666844 Accuracy: 0.69595015\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 128)          512         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 65,616\n",
      "Trainable params: 64,848\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 884us/sample - loss: 0.8275 - acc: 0.7674\n",
      "Loss: 0.8274626008075345 Accuracy: 0.7673936\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 192)          768         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 108,496\n",
      "Trainable params: 107,344\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 880us/sample - loss: 0.5297 - acc: 0.8627\n",
      "Loss: 0.5297019189391924 Accuracy: 0.86272067\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 256)          1024        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 192,336\n",
      "Trainable params: 190,800\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 901us/sample - loss: 0.2937 - acc: 0.9335\n",
      "Loss: 0.29370298198852707 Accuracy: 0.933541\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 256)          1024        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 274,896\n",
      "Trainable params: 273,104\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 943us/sample - loss: 0.2211 - acc: 0.9516\n",
      "Loss: 0.2211026437905533 Accuracy: 0.95160955\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 256)          1024        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 357,456\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 965us/sample - loss: 0.2952 - acc: 0.9383\n",
      "Loss: 0.2952482807042845 Accuracy: 0.9383178\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
