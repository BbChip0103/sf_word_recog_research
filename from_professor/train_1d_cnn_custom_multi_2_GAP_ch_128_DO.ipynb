{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4112        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           6160        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 256)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 256)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           8208        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 256)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 256)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           8208        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 256)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           8208        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6642 - acc: 0.1377\n",
      "Epoch 00001: val_loss improved from inf to 2.47907, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/001-2.4791.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 2.6642 - acc: 0.1376 - val_loss: 2.4791 - val_acc: 0.2409\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3718 - acc: 0.2324\n",
      "Epoch 00002: val_loss improved from 2.47907 to 2.16337, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/002-2.1634.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.3719 - acc: 0.2324 - val_loss: 2.1634 - val_acc: 0.3149\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1835 - acc: 0.2731\n",
      "Epoch 00003: val_loss improved from 2.16337 to 2.00038, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/003-2.0004.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.1835 - acc: 0.2731 - val_loss: 2.0004 - val_acc: 0.3722\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0690 - acc: 0.3117\n",
      "Epoch 00004: val_loss improved from 2.00038 to 1.90597, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/004-1.9060.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 2.0690 - acc: 0.3117 - val_loss: 1.9060 - val_acc: 0.4207\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9909 - acc: 0.3407\n",
      "Epoch 00005: val_loss improved from 1.90597 to 1.82117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/005-1.8212.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.9910 - acc: 0.3407 - val_loss: 1.8212 - val_acc: 0.4456\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9200 - acc: 0.3675\n",
      "Epoch 00006: val_loss improved from 1.82117 to 1.74413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/006-1.7441.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.9200 - acc: 0.3674 - val_loss: 1.7441 - val_acc: 0.4710\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8632 - acc: 0.3888\n",
      "Epoch 00007: val_loss improved from 1.74413 to 1.69106, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/007-1.6911.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.8632 - acc: 0.3888 - val_loss: 1.6911 - val_acc: 0.4887\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8096 - acc: 0.4105- ETA\n",
      "Epoch 00008: val_loss improved from 1.69106 to 1.65313, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/008-1.6531.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.8095 - acc: 0.4106 - val_loss: 1.6531 - val_acc: 0.5136\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7619 - acc: 0.4280\n",
      "Epoch 00009: val_loss improved from 1.65313 to 1.59739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/009-1.5974.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.7619 - acc: 0.4279 - val_loss: 1.5974 - val_acc: 0.5302\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7247 - acc: 0.4401\n",
      "Epoch 00010: val_loss improved from 1.59739 to 1.55831, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/010-1.5583.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.7246 - acc: 0.4401 - val_loss: 1.5583 - val_acc: 0.5430\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6887 - acc: 0.4539\n",
      "Epoch 00011: val_loss improved from 1.55831 to 1.52006, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/011-1.5201.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6886 - acc: 0.4539 - val_loss: 1.5201 - val_acc: 0.5530\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6587 - acc: 0.4649\n",
      "Epoch 00012: val_loss improved from 1.52006 to 1.48482, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/012-1.4848.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6586 - acc: 0.4649 - val_loss: 1.4848 - val_acc: 0.5588\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6295 - acc: 0.4777\n",
      "Epoch 00013: val_loss improved from 1.48482 to 1.46945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/013-1.4694.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.6299 - acc: 0.4777 - val_loss: 1.4694 - val_acc: 0.5623\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5989 - acc: 0.4889\n",
      "Epoch 00014: val_loss improved from 1.46945 to 1.43229, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/014-1.4323.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5989 - acc: 0.4888 - val_loss: 1.4323 - val_acc: 0.5800\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5764 - acc: 0.4977\n",
      "Epoch 00015: val_loss improved from 1.43229 to 1.39835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/015-1.3984.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5765 - acc: 0.4976 - val_loss: 1.3984 - val_acc: 0.5828\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5509 - acc: 0.5064\n",
      "Epoch 00016: val_loss improved from 1.39835 to 1.37844, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/016-1.3784.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5509 - acc: 0.5064 - val_loss: 1.3784 - val_acc: 0.5882\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5315 - acc: 0.5129\n",
      "Epoch 00017: val_loss improved from 1.37844 to 1.34822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/017-1.3482.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5314 - acc: 0.5130 - val_loss: 1.3482 - val_acc: 0.5991\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5043 - acc: 0.5258\n",
      "Epoch 00018: val_loss improved from 1.34822 to 1.33016, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/018-1.3302.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5043 - acc: 0.5258 - val_loss: 1.3302 - val_acc: 0.5982\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4910 - acc: 0.5315\n",
      "Epoch 00019: val_loss improved from 1.33016 to 1.29801, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/019-1.2980.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4911 - acc: 0.5315 - val_loss: 1.2980 - val_acc: 0.6129\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4579 - acc: 0.5374\n",
      "Epoch 00020: val_loss improved from 1.29801 to 1.29402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/020-1.2940.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4580 - acc: 0.5374 - val_loss: 1.2940 - val_acc: 0.6101\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4433 - acc: 0.5427\n",
      "Epoch 00021: val_loss improved from 1.29402 to 1.25495, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/021-1.2549.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4434 - acc: 0.5427 - val_loss: 1.2549 - val_acc: 0.6303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4229 - acc: 0.5523\n",
      "Epoch 00022: val_loss improved from 1.25495 to 1.24108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/022-1.2411.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4228 - acc: 0.5523 - val_loss: 1.2411 - val_acc: 0.6310\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4051 - acc: 0.5574\n",
      "Epoch 00023: val_loss improved from 1.24108 to 1.22140, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/023-1.2214.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.4051 - acc: 0.5575 - val_loss: 1.2214 - val_acc: 0.6401\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3842 - acc: 0.5673\n",
      "Epoch 00024: val_loss improved from 1.22140 to 1.20995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/024-1.2099.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3841 - acc: 0.5673 - val_loss: 1.2099 - val_acc: 0.6410\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3666 - acc: 0.5727\n",
      "Epoch 00025: val_loss improved from 1.20995 to 1.19544, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/025-1.1954.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3665 - acc: 0.5727 - val_loss: 1.1954 - val_acc: 0.6441\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3480 - acc: 0.5810\n",
      "Epoch 00026: val_loss improved from 1.19544 to 1.17013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/026-1.1701.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3479 - acc: 0.5810 - val_loss: 1.1701 - val_acc: 0.6543\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3307 - acc: 0.5839\n",
      "Epoch 00027: val_loss improved from 1.17013 to 1.14974, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/027-1.1497.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3307 - acc: 0.5839 - val_loss: 1.1497 - val_acc: 0.6632\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3156 - acc: 0.5912\n",
      "Epoch 00028: val_loss improved from 1.14974 to 1.14465, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/028-1.1446.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.3155 - acc: 0.5912 - val_loss: 1.1446 - val_acc: 0.6604\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2984 - acc: 0.5942\n",
      "Epoch 00029: val_loss improved from 1.14465 to 1.11786, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/029-1.1179.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2983 - acc: 0.5942 - val_loss: 1.1179 - val_acc: 0.6718\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2926 - acc: 0.6001\n",
      "Epoch 00030: val_loss improved from 1.11786 to 1.09879, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/030-1.0988.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2928 - acc: 0.6001 - val_loss: 1.0988 - val_acc: 0.6792\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2778 - acc: 0.6044\n",
      "Epoch 00031: val_loss improved from 1.09879 to 1.08316, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/031-1.0832.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2777 - acc: 0.6044 - val_loss: 1.0832 - val_acc: 0.6823\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2616 - acc: 0.6083\n",
      "Epoch 00032: val_loss improved from 1.08316 to 1.07637, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/032-1.0764.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2615 - acc: 0.6084 - val_loss: 1.0764 - val_acc: 0.6820\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2516 - acc: 0.6107\n",
      "Epoch 00033: val_loss improved from 1.07637 to 1.06378, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/033-1.0638.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2517 - acc: 0.6107 - val_loss: 1.0638 - val_acc: 0.6862\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2373 - acc: 0.6171\n",
      "Epoch 00034: val_loss improved from 1.06378 to 1.05349, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/034-1.0535.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2373 - acc: 0.6171 - val_loss: 1.0535 - val_acc: 0.6904\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2253 - acc: 0.6196\n",
      "Epoch 00035: val_loss improved from 1.05349 to 1.03748, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/035-1.0375.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2252 - acc: 0.6196 - val_loss: 1.0375 - val_acc: 0.6962\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2162 - acc: 0.6251\n",
      "Epoch 00036: val_loss improved from 1.03748 to 1.02933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/036-1.0293.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2161 - acc: 0.6251 - val_loss: 1.0293 - val_acc: 0.6972\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2029 - acc: 0.6310\n",
      "Epoch 00037: val_loss improved from 1.02933 to 1.01779, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/037-1.0178.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.2027 - acc: 0.6310 - val_loss: 1.0178 - val_acc: 0.6986\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1875 - acc: 0.6352\n",
      "Epoch 00038: val_loss improved from 1.01779 to 1.01454, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/038-1.0145.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1874 - acc: 0.6353 - val_loss: 1.0145 - val_acc: 0.6990\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1903 - acc: 0.6352\n",
      "Epoch 00039: val_loss improved from 1.01454 to 0.99924, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/039-0.9992.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1902 - acc: 0.6351 - val_loss: 0.9992 - val_acc: 0.7053\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1794 - acc: 0.6392\n",
      "Epoch 00040: val_loss improved from 0.99924 to 0.98669, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/040-0.9867.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1793 - acc: 0.6392 - val_loss: 0.9867 - val_acc: 0.7098\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1686 - acc: 0.6423\n",
      "Epoch 00041: val_loss improved from 0.98669 to 0.97953, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/041-0.9795.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1686 - acc: 0.6423 - val_loss: 0.9795 - val_acc: 0.7116\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1608 - acc: 0.6457\n",
      "Epoch 00042: val_loss improved from 0.97953 to 0.97873, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/042-0.9787.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1609 - acc: 0.6457 - val_loss: 0.9787 - val_acc: 0.7133\n",
      "Epoch 43/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1494 - acc: 0.6483\n",
      "Epoch 00043: val_loss improved from 0.97873 to 0.96153, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/043-0.9615.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1495 - acc: 0.6482 - val_loss: 0.9615 - val_acc: 0.7193\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1441 - acc: 0.6490\n",
      "Epoch 00044: val_loss improved from 0.96153 to 0.95485, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/044-0.9548.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1441 - acc: 0.6490 - val_loss: 0.9548 - val_acc: 0.7200\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1317 - acc: 0.6532\n",
      "Epoch 00045: val_loss improved from 0.95485 to 0.95383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/045-0.9538.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1319 - acc: 0.6532 - val_loss: 0.9538 - val_acc: 0.7244\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1236 - acc: 0.6589\n",
      "Epoch 00046: val_loss improved from 0.95383 to 0.94203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/046-0.9420.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1237 - acc: 0.6589 - val_loss: 0.9420 - val_acc: 0.7247\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1223 - acc: 0.6575\n",
      "Epoch 00047: val_loss improved from 0.94203 to 0.93851, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/047-0.9385.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1222 - acc: 0.6575 - val_loss: 0.9385 - val_acc: 0.7230\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1155 - acc: 0.6589\n",
      "Epoch 00048: val_loss improved from 0.93851 to 0.93450, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/048-0.9345.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1155 - acc: 0.6590 - val_loss: 0.9345 - val_acc: 0.7303\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1092 - acc: 0.6640\n",
      "Epoch 00049: val_loss improved from 0.93450 to 0.91888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/049-0.9189.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1092 - acc: 0.6641 - val_loss: 0.9189 - val_acc: 0.7338\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1059 - acc: 0.6614\n",
      "Epoch 00050: val_loss did not improve from 0.91888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.1059 - acc: 0.6615 - val_loss: 0.9320 - val_acc: 0.7256\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.6687\n",
      "Epoch 00051: val_loss did not improve from 0.91888\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0972 - acc: 0.6687 - val_loss: 0.9258 - val_acc: 0.7237\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0873 - acc: 0.6689\n",
      "Epoch 00052: val_loss improved from 0.91888 to 0.91603, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/052-0.9160.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0873 - acc: 0.6688 - val_loss: 0.9160 - val_acc: 0.7277\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0832 - acc: 0.6699\n",
      "Epoch 00053: val_loss improved from 0.91603 to 0.90782, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/053-0.9078.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0832 - acc: 0.6699 - val_loss: 0.9078 - val_acc: 0.7335\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0757 - acc: 0.6723\n",
      "Epoch 00054: val_loss improved from 0.90782 to 0.89388, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/054-0.8939.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0757 - acc: 0.6724 - val_loss: 0.8939 - val_acc: 0.7442\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0703 - acc: 0.6745\n",
      "Epoch 00055: val_loss did not improve from 0.89388\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0702 - acc: 0.6745 - val_loss: 0.8980 - val_acc: 0.7419\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0682 - acc: 0.6743\n",
      "Epoch 00056: val_loss improved from 0.89388 to 0.88374, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/056-0.8837.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0682 - acc: 0.6743 - val_loss: 0.8837 - val_acc: 0.7438\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0628 - acc: 0.6789\n",
      "Epoch 00057: val_loss improved from 0.88374 to 0.88269, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/057-0.8827.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0629 - acc: 0.6788 - val_loss: 0.8827 - val_acc: 0.7468\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0608 - acc: 0.6768\n",
      "Epoch 00058: val_loss improved from 0.88269 to 0.87054, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/058-0.8705.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0607 - acc: 0.6769 - val_loss: 0.8705 - val_acc: 0.7487\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0515 - acc: 0.6804\n",
      "Epoch 00059: val_loss improved from 0.87054 to 0.86418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/059-0.8642.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0515 - acc: 0.6803 - val_loss: 0.8642 - val_acc: 0.7484\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0514 - acc: 0.6802\n",
      "Epoch 00060: val_loss improved from 0.86418 to 0.85936, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/060-0.8594.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0514 - acc: 0.6803 - val_loss: 0.8594 - val_acc: 0.7543\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0444 - acc: 0.6863- ETA: 0s - loss: 1.0454 - acc: 0\n",
      "Epoch 00061: val_loss did not improve from 0.85936\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0445 - acc: 0.6863 - val_loss: 0.8612 - val_acc: 0.7545\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0367 - acc: 0.6880\n",
      "Epoch 00062: val_loss improved from 0.85936 to 0.84872, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/062-0.8487.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0366 - acc: 0.6880 - val_loss: 0.8487 - val_acc: 0.7601\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0353 - acc: 0.6879\n",
      "Epoch 00063: val_loss did not improve from 0.84872\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0352 - acc: 0.6880 - val_loss: 0.8514 - val_acc: 0.7573\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0327 - acc: 0.6907\n",
      "Epoch 00064: val_loss did not improve from 0.84872\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0326 - acc: 0.6907 - val_loss: 0.8523 - val_acc: 0.7570\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0267 - acc: 0.6917\n",
      "Epoch 00065: val_loss improved from 0.84872 to 0.83418, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/065-0.8342.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0266 - acc: 0.6918 - val_loss: 0.8342 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0226 - acc: 0.6948\n",
      "Epoch 00066: val_loss did not improve from 0.83418\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0225 - acc: 0.6948 - val_loss: 0.8385 - val_acc: 0.7624\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0207 - acc: 0.6935\n",
      "Epoch 00067: val_loss improved from 0.83418 to 0.83323, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/067-0.8332.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0206 - acc: 0.6935 - val_loss: 0.8332 - val_acc: 0.7638\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0156 - acc: 0.6938\n",
      "Epoch 00068: val_loss improved from 0.83323 to 0.82847, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/068-0.8285.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0155 - acc: 0.6938 - val_loss: 0.8285 - val_acc: 0.7633\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0103 - acc: 0.6986\n",
      "Epoch 00069: val_loss improved from 0.82847 to 0.82771, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/069-0.8277.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0102 - acc: 0.6987 - val_loss: 0.8277 - val_acc: 0.7673\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.7005\n",
      "Epoch 00070: val_loss improved from 0.82771 to 0.81812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/070-0.8181.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0076 - acc: 0.7004 - val_loss: 0.8181 - val_acc: 0.7738\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0038 - acc: 0.6957\n",
      "Epoch 00071: val_loss did not improve from 0.81812\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.0038 - acc: 0.6957 - val_loss: 0.8206 - val_acc: 0.7680\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9977 - acc: 0.6992\n",
      "Epoch 00072: val_loss improved from 0.81812 to 0.81561, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/072-0.8156.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9976 - acc: 0.6992 - val_loss: 0.8156 - val_acc: 0.7692\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9967 - acc: 0.7001\n",
      "Epoch 00073: val_loss improved from 0.81561 to 0.81289, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/073-0.8129.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9966 - acc: 0.7001 - val_loss: 0.8129 - val_acc: 0.7715\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9949 - acc: 0.7016\n",
      "Epoch 00074: val_loss did not improve from 0.81289\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9949 - acc: 0.7016 - val_loss: 0.8239 - val_acc: 0.7706\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9906 - acc: 0.7013\n",
      "Epoch 00075: val_loss improved from 0.81289 to 0.81161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/075-0.8116.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9906 - acc: 0.7013 - val_loss: 0.8116 - val_acc: 0.7710\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9853 - acc: 0.7041\n",
      "Epoch 00076: val_loss improved from 0.81161 to 0.80333, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/076-0.8033.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9853 - acc: 0.7041 - val_loss: 0.8033 - val_acc: 0.7743\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9850 - acc: 0.7024\n",
      "Epoch 00077: val_loss improved from 0.80333 to 0.80217, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/077-0.8022.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9850 - acc: 0.7024 - val_loss: 0.8022 - val_acc: 0.7803\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9792 - acc: 0.7042\n",
      "Epoch 00078: val_loss improved from 0.80217 to 0.79683, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/078-0.7968.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9793 - acc: 0.7041 - val_loss: 0.7968 - val_acc: 0.7768\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9726 - acc: 0.7090\n",
      "Epoch 00079: val_loss improved from 0.79683 to 0.79356, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/079-0.7936.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9725 - acc: 0.7090 - val_loss: 0.7936 - val_acc: 0.7754\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9709 - acc: 0.7092\n",
      "Epoch 00080: val_loss improved from 0.79356 to 0.78706, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/080-0.7871.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9710 - acc: 0.7091 - val_loss: 0.7871 - val_acc: 0.7815\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9680 - acc: 0.7101\n",
      "Epoch 00081: val_loss did not improve from 0.78706\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9683 - acc: 0.7101 - val_loss: 0.7872 - val_acc: 0.7822\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9630 - acc: 0.7115\n",
      "Epoch 00082: val_loss did not improve from 0.78706\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9629 - acc: 0.7115 - val_loss: 0.7874 - val_acc: 0.7820\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.7107\n",
      "Epoch 00083: val_loss improved from 0.78706 to 0.77977, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/083-0.7798.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9631 - acc: 0.7107 - val_loss: 0.7798 - val_acc: 0.7836\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.7128\n",
      "Epoch 00084: val_loss improved from 0.77977 to 0.77531, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/084-0.7753.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9608 - acc: 0.7128 - val_loss: 0.7753 - val_acc: 0.7869\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9598 - acc: 0.7156\n",
      "Epoch 00085: val_loss improved from 0.77531 to 0.77346, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/085-0.7735.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9598 - acc: 0.7156 - val_loss: 0.7735 - val_acc: 0.7908\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9545 - acc: 0.7167\n",
      "Epoch 00086: val_loss did not improve from 0.77346\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9546 - acc: 0.7167 - val_loss: 0.7742 - val_acc: 0.7815\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9542 - acc: 0.7129\n",
      "Epoch 00087: val_loss did not improve from 0.77346\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9542 - acc: 0.7129 - val_loss: 0.7783 - val_acc: 0.7813\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9485 - acc: 0.7161\n",
      "Epoch 00088: val_loss improved from 0.77346 to 0.77294, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/088-0.7729.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9486 - acc: 0.7161 - val_loss: 0.7729 - val_acc: 0.7810\n",
      "Epoch 89/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9474 - acc: 0.7174\n",
      "Epoch 00089: val_loss improved from 0.77294 to 0.76545, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/089-0.7655.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9474 - acc: 0.7175 - val_loss: 0.7655 - val_acc: 0.7848\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9454 - acc: 0.7180\n",
      "Epoch 00090: val_loss did not improve from 0.76545\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9454 - acc: 0.7180 - val_loss: 0.7665 - val_acc: 0.7883\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.7213- ETA: 0s - loss: 0.9425 - ac\n",
      "Epoch 00091: val_loss did not improve from 0.76545\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9406 - acc: 0.7213 - val_loss: 0.7684 - val_acc: 0.7806\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9466 - acc: 0.7177\n",
      "Epoch 00092: val_loss improved from 0.76545 to 0.75192, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/092-0.7519.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9466 - acc: 0.7177 - val_loss: 0.7519 - val_acc: 0.7932\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9419 - acc: 0.7174\n",
      "Epoch 00093: val_loss did not improve from 0.75192\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9421 - acc: 0.7174 - val_loss: 0.7528 - val_acc: 0.7901\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9413 - acc: 0.7192\n",
      "Epoch 00094: val_loss did not improve from 0.75192\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9416 - acc: 0.7192 - val_loss: 0.7556 - val_acc: 0.7936\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.7230\n",
      "Epoch 00095: val_loss improved from 0.75192 to 0.74638, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/095-0.7464.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9337 - acc: 0.7230 - val_loss: 0.7464 - val_acc: 0.7922\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9368 - acc: 0.7217\n",
      "Epoch 00096: val_loss did not improve from 0.74638\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9368 - acc: 0.7217 - val_loss: 0.7470 - val_acc: 0.7952\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9286 - acc: 0.7232\n",
      "Epoch 00097: val_loss improved from 0.74638 to 0.73807, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/097-0.7381.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9285 - acc: 0.7232 - val_loss: 0.7381 - val_acc: 0.7932\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9324 - acc: 0.7256\n",
      "Epoch 00098: val_loss did not improve from 0.73807\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9324 - acc: 0.7256 - val_loss: 0.7409 - val_acc: 0.7978\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9309 - acc: 0.7240\n",
      "Epoch 00099: val_loss did not improve from 0.73807\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9309 - acc: 0.7240 - val_loss: 0.7418 - val_acc: 0.7969\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.7260\n",
      "Epoch 00100: val_loss did not improve from 0.73807\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9254 - acc: 0.7260 - val_loss: 0.7410 - val_acc: 0.7969\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9239 - acc: 0.7263\n",
      "Epoch 00101: val_loss did not improve from 0.73807\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9239 - acc: 0.7263 - val_loss: 0.7435 - val_acc: 0.7911\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.7270\n",
      "Epoch 00102: val_loss improved from 0.73807 to 0.73617, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/102-0.7362.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9202 - acc: 0.7269 - val_loss: 0.7362 - val_acc: 0.7950\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9236 - acc: 0.7243\n",
      "Epoch 00103: val_loss improved from 0.73617 to 0.72954, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/103-0.7295.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9236 - acc: 0.7244 - val_loss: 0.7295 - val_acc: 0.7945\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9140 - acc: 0.7279\n",
      "Epoch 00104: val_loss did not improve from 0.72954\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9139 - acc: 0.7279 - val_loss: 0.7358 - val_acc: 0.7987\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9136 - acc: 0.7267\n",
      "Epoch 00105: val_loss did not improve from 0.72954\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9136 - acc: 0.7267 - val_loss: 0.7307 - val_acc: 0.7992\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9082 - acc: 0.7308\n",
      "Epoch 00106: val_loss did not improve from 0.72954\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9082 - acc: 0.7308 - val_loss: 0.7354 - val_acc: 0.7973\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7293\n",
      "Epoch 00107: val_loss improved from 0.72954 to 0.72614, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/107-0.7261.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9075 - acc: 0.7294 - val_loss: 0.7261 - val_acc: 0.8022\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9080 - acc: 0.7289\n",
      "Epoch 00108: val_loss improved from 0.72614 to 0.72222, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/108-0.7222.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9081 - acc: 0.7288 - val_loss: 0.7222 - val_acc: 0.8015\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9072 - acc: 0.7322\n",
      "Epoch 00109: val_loss did not improve from 0.72222\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9071 - acc: 0.7322 - val_loss: 0.7225 - val_acc: 0.8006\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9080 - acc: 0.7298\n",
      "Epoch 00110: val_loss did not improve from 0.72222\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.9080 - acc: 0.7298 - val_loss: 0.7354 - val_acc: 0.7969\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.7335\n",
      "Epoch 00111: val_loss improved from 0.72222 to 0.71732, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/111-0.7173.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8999 - acc: 0.7335 - val_loss: 0.7173 - val_acc: 0.8036\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7365\n",
      "Epoch 00112: val_loss did not improve from 0.71732\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8978 - acc: 0.7366 - val_loss: 0.7223 - val_acc: 0.8020\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8948 - acc: 0.7374\n",
      "Epoch 00113: val_loss did not improve from 0.71732\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8948 - acc: 0.7374 - val_loss: 0.7220 - val_acc: 0.7997\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8985 - acc: 0.7327\n",
      "Epoch 00114: val_loss did not improve from 0.71732\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8984 - acc: 0.7327 - val_loss: 0.7218 - val_acc: 0.7997\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7346\n",
      "Epoch 00115: val_loss improved from 0.71732 to 0.71013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/115-0.7101.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8962 - acc: 0.7346 - val_loss: 0.7101 - val_acc: 0.8085\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8916 - acc: 0.7371\n",
      "Epoch 00116: val_loss did not improve from 0.71013\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8916 - acc: 0.7371 - val_loss: 0.7190 - val_acc: 0.7971\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.7356\n",
      "Epoch 00117: val_loss improved from 0.71013 to 0.70971, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/117-0.7097.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8909 - acc: 0.7356 - val_loss: 0.7097 - val_acc: 0.7976\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7356\n",
      "Epoch 00118: val_loss improved from 0.70971 to 0.70885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/118-0.7088.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8907 - acc: 0.7356 - val_loss: 0.7088 - val_acc: 0.8008\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8892 - acc: 0.7385\n",
      "Epoch 00119: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8892 - acc: 0.7385 - val_loss: 0.7100 - val_acc: 0.7980\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8885 - acc: 0.7370\n",
      "Epoch 00120: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8885 - acc: 0.7370 - val_loss: 0.7095 - val_acc: 0.8020\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7405\n",
      "Epoch 00121: val_loss did not improve from 0.70885\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8853 - acc: 0.7404 - val_loss: 0.7128 - val_acc: 0.8057\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8851 - acc: 0.7376\n",
      "Epoch 00122: val_loss improved from 0.70885 to 0.69897, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/122-0.6990.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8851 - acc: 0.7376 - val_loss: 0.6990 - val_acc: 0.8088\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8826 - acc: 0.7402\n",
      "Epoch 00123: val_loss did not improve from 0.69897\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8825 - acc: 0.7402 - val_loss: 0.6996 - val_acc: 0.8113\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8763 - acc: 0.7376- ETA: 0s - loss: 0.8768 - acc: \n",
      "Epoch 00124: val_loss improved from 0.69897 to 0.69143, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/124-0.6914.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8762 - acc: 0.7376 - val_loss: 0.6914 - val_acc: 0.8132\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8794 - acc: 0.7396\n",
      "Epoch 00125: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8794 - acc: 0.7396 - val_loss: 0.6990 - val_acc: 0.8074\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8810 - acc: 0.7375\n",
      "Epoch 00126: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8810 - acc: 0.7376 - val_loss: 0.7004 - val_acc: 0.8071\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8757 - acc: 0.7397\n",
      "Epoch 00127: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8757 - acc: 0.7397 - val_loss: 0.6923 - val_acc: 0.8125\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8767 - acc: 0.7420\n",
      "Epoch 00128: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8767 - acc: 0.7420 - val_loss: 0.6922 - val_acc: 0.8148\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7411\n",
      "Epoch 00129: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8735 - acc: 0.7411 - val_loss: 0.6924 - val_acc: 0.8125\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8705 - acc: 0.7432\n",
      "Epoch 00130: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8705 - acc: 0.7432 - val_loss: 0.6950 - val_acc: 0.8039\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8706 - acc: 0.7437\n",
      "Epoch 00131: val_loss did not improve from 0.69143\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8706 - acc: 0.7437 - val_loss: 0.6974 - val_acc: 0.8032\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8631 - acc: 0.7439\n",
      "Epoch 00132: val_loss improved from 0.69143 to 0.67856, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/132-0.6786.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8630 - acc: 0.7439 - val_loss: 0.6786 - val_acc: 0.8150\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8665 - acc: 0.7461\n",
      "Epoch 00133: val_loss did not improve from 0.67856\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8664 - acc: 0.7461 - val_loss: 0.6846 - val_acc: 0.8169\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7435\n",
      "Epoch 00134: val_loss did not improve from 0.67856\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8659 - acc: 0.7435 - val_loss: 0.6969 - val_acc: 0.8074\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8642 - acc: 0.7441- ETA: 2s - l\n",
      "Epoch 00135: val_loss did not improve from 0.67856\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8644 - acc: 0.7441 - val_loss: 0.6866 - val_acc: 0.8085\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7457\n",
      "Epoch 00136: val_loss did not improve from 0.67856\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8652 - acc: 0.7457 - val_loss: 0.6814 - val_acc: 0.8127\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8617 - acc: 0.7455\n",
      "Epoch 00137: val_loss did not improve from 0.67856\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8617 - acc: 0.7456 - val_loss: 0.6796 - val_acc: 0.8181\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7471\n",
      "Epoch 00138: val_loss improved from 0.67856 to 0.67630, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/138-0.6763.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8601 - acc: 0.7471 - val_loss: 0.6763 - val_acc: 0.8190\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7469\n",
      "Epoch 00139: val_loss improved from 0.67630 to 0.67495, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/139-0.6750.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8604 - acc: 0.7469 - val_loss: 0.6750 - val_acc: 0.8197\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8542 - acc: 0.7449\n",
      "Epoch 00140: val_loss did not improve from 0.67495\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8543 - acc: 0.7449 - val_loss: 0.6763 - val_acc: 0.8127\n",
      "Epoch 141/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8557 - acc: 0.7449\n",
      "Epoch 00141: val_loss did not improve from 0.67495\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8556 - acc: 0.7449 - val_loss: 0.6787 - val_acc: 0.8150\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8561 - acc: 0.7477\n",
      "Epoch 00142: val_loss did not improve from 0.67495\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8561 - acc: 0.7477 - val_loss: 0.6768 - val_acc: 0.8157\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8553 - acc: 0.7465\n",
      "Epoch 00143: val_loss did not improve from 0.67495\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8553 - acc: 0.7464 - val_loss: 0.6861 - val_acc: 0.8116\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8554 - acc: 0.7477\n",
      "Epoch 00144: val_loss improved from 0.67495 to 0.67039, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/144-0.6704.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8553 - acc: 0.7477 - val_loss: 0.6704 - val_acc: 0.8218\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8551 - acc: 0.7482\n",
      "Epoch 00145: val_loss improved from 0.67039 to 0.66778, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/145-0.6678.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8551 - acc: 0.7482 - val_loss: 0.6678 - val_acc: 0.8164\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8518 - acc: 0.7512\n",
      "Epoch 00146: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8517 - acc: 0.7512 - val_loss: 0.6799 - val_acc: 0.8123\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8449 - acc: 0.7508\n",
      "Epoch 00147: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8449 - acc: 0.7508 - val_loss: 0.6693 - val_acc: 0.8197\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8494 - acc: 0.7495\n",
      "Epoch 00148: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8498 - acc: 0.7495 - val_loss: 0.6682 - val_acc: 0.8218\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8449 - acc: 0.7525\n",
      "Epoch 00149: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8448 - acc: 0.7525 - val_loss: 0.6686 - val_acc: 0.8202\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8488 - acc: 0.7483\n",
      "Epoch 00150: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8488 - acc: 0.7483 - val_loss: 0.6678 - val_acc: 0.8216\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8459 - acc: 0.7508\n",
      "Epoch 00151: val_loss did not improve from 0.66778\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8459 - acc: 0.7508 - val_loss: 0.6682 - val_acc: 0.8188\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8464 - acc: 0.7490\n",
      "Epoch 00152: val_loss improved from 0.66778 to 0.66392, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/152-0.6639.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8466 - acc: 0.7490 - val_loss: 0.6639 - val_acc: 0.8216\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.7528\n",
      "Epoch 00153: val_loss did not improve from 0.66392\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8427 - acc: 0.7528 - val_loss: 0.6674 - val_acc: 0.8188\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8425 - acc: 0.7488\n",
      "Epoch 00154: val_loss did not improve from 0.66392\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8425 - acc: 0.7488 - val_loss: 0.6706 - val_acc: 0.8174\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8429 - acc: 0.7533\n",
      "Epoch 00155: val_loss improved from 0.66392 to 0.65656, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/155-0.6566.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8428 - acc: 0.7534 - val_loss: 0.6566 - val_acc: 0.8199\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7539\n",
      "Epoch 00156: val_loss did not improve from 0.65656\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8384 - acc: 0.7539 - val_loss: 0.6606 - val_acc: 0.8162\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8383 - acc: 0.7533\n",
      "Epoch 00157: val_loss improved from 0.65656 to 0.65451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/157-0.6545.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8384 - acc: 0.7533 - val_loss: 0.6545 - val_acc: 0.8267\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8349 - acc: 0.7529\n",
      "Epoch 00158: val_loss did not improve from 0.65451\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8349 - acc: 0.7529 - val_loss: 0.6560 - val_acc: 0.8234\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7562\n",
      "Epoch 00159: val_loss did not improve from 0.65451\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8327 - acc: 0.7562 - val_loss: 0.6586 - val_acc: 0.8234\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8306 - acc: 0.7541\n",
      "Epoch 00160: val_loss improved from 0.65451 to 0.65346, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/160-0.6535.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8307 - acc: 0.7541 - val_loss: 0.6535 - val_acc: 0.8234\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8337 - acc: 0.7516\n",
      "Epoch 00161: val_loss improved from 0.65346 to 0.65325, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/161-0.6532.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8337 - acc: 0.7516 - val_loss: 0.6532 - val_acc: 0.8255\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8299 - acc: 0.7559\n",
      "Epoch 00162: val_loss improved from 0.65325 to 0.65239, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/162-0.6524.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8299 - acc: 0.7559 - val_loss: 0.6524 - val_acc: 0.8225\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8288 - acc: 0.7581\n",
      "Epoch 00163: val_loss did not improve from 0.65239\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8290 - acc: 0.7580 - val_loss: 0.6530 - val_acc: 0.8241\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7542\n",
      "Epoch 00164: val_loss did not improve from 0.65239\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8270 - acc: 0.7542 - val_loss: 0.6591 - val_acc: 0.8218\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8346 - acc: 0.7573\n",
      "Epoch 00165: val_loss did not improve from 0.65239\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8346 - acc: 0.7573 - val_loss: 0.6547 - val_acc: 0.8272\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.7545\n",
      "Epoch 00166: val_loss improved from 0.65239 to 0.65032, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/166-0.6503.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8339 - acc: 0.7545 - val_loss: 0.6503 - val_acc: 0.8246\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.7529\n",
      "Epoch 00167: val_loss did not improve from 0.65032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8312 - acc: 0.7528 - val_loss: 0.6521 - val_acc: 0.8195\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8280 - acc: 0.7560\n",
      "Epoch 00168: val_loss improved from 0.65032 to 0.64682, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/168-0.6468.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8280 - acc: 0.7560 - val_loss: 0.6468 - val_acc: 0.8290\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7557\n",
      "Epoch 00169: val_loss did not improve from 0.64682\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8314 - acc: 0.7557 - val_loss: 0.6468 - val_acc: 0.8251\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8262 - acc: 0.7564\n",
      "Epoch 00170: val_loss did not improve from 0.64682\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8261 - acc: 0.7564 - val_loss: 0.6508 - val_acc: 0.8211\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7587\n",
      "Epoch 00171: val_loss improved from 0.64682 to 0.64112, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/171-0.6411.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8210 - acc: 0.7587 - val_loss: 0.6411 - val_acc: 0.8262\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8219 - acc: 0.7576\n",
      "Epoch 00172: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8219 - acc: 0.7576 - val_loss: 0.6415 - val_acc: 0.8295\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8292 - acc: 0.7568\n",
      "Epoch 00173: val_loss improved from 0.64112 to 0.63917, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/173-0.6392.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8292 - acc: 0.7568 - val_loss: 0.6392 - val_acc: 0.8262\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8216 - acc: 0.7606\n",
      "Epoch 00174: val_loss did not improve from 0.63917\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8215 - acc: 0.7606 - val_loss: 0.6451 - val_acc: 0.8251\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8200 - acc: 0.7594\n",
      "Epoch 00175: val_loss did not improve from 0.63917\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8199 - acc: 0.7594 - val_loss: 0.6424 - val_acc: 0.8255\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8225 - acc: 0.7566\n",
      "Epoch 00176: val_loss improved from 0.63917 to 0.63876, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/176-0.6388.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8227 - acc: 0.7566 - val_loss: 0.6388 - val_acc: 0.8269\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7571- ETA: 2s - los\n",
      "Epoch 00177: val_loss improved from 0.63876 to 0.63533, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/177-0.6353.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8144 - acc: 0.7571 - val_loss: 0.6353 - val_acc: 0.8288\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8212 - acc: 0.7577\n",
      "Epoch 00178: val_loss did not improve from 0.63533\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8211 - acc: 0.7577 - val_loss: 0.6445 - val_acc: 0.8283\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8138 - acc: 0.7590\n",
      "Epoch 00179: val_loss improved from 0.63533 to 0.63264, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/179-0.6326.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8137 - acc: 0.7591 - val_loss: 0.6326 - val_acc: 0.8330\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8192 - acc: 0.7593\n",
      "Epoch 00180: val_loss did not improve from 0.63264\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8194 - acc: 0.7592 - val_loss: 0.6427 - val_acc: 0.8253\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8185 - acc: 0.7575\n",
      "Epoch 00181: val_loss improved from 0.63264 to 0.62869, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/181-0.6287.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8184 - acc: 0.7576 - val_loss: 0.6287 - val_acc: 0.8351\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.7613\n",
      "Epoch 00182: val_loss did not improve from 0.62869\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8088 - acc: 0.7613 - val_loss: 0.6386 - val_acc: 0.8288\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8156 - acc: 0.7593\n",
      "Epoch 00183: val_loss did not improve from 0.62869\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8157 - acc: 0.7593 - val_loss: 0.6417 - val_acc: 0.8216\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8120 - acc: 0.7605\n",
      "Epoch 00184: val_loss did not improve from 0.62869\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8120 - acc: 0.7604 - val_loss: 0.6365 - val_acc: 0.8304\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8134 - acc: 0.7599\n",
      "Epoch 00185: val_loss did not improve from 0.62869\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8134 - acc: 0.7599 - val_loss: 0.6312 - val_acc: 0.8323\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.7591\n",
      "Epoch 00186: val_loss improved from 0.62869 to 0.62686, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/186-0.6269.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8089 - acc: 0.7591 - val_loss: 0.6269 - val_acc: 0.8344\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8107 - acc: 0.7587\n",
      "Epoch 00187: val_loss did not improve from 0.62686\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8107 - acc: 0.7587 - val_loss: 0.6352 - val_acc: 0.8293\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8098 - acc: 0.7613\n",
      "Epoch 00188: val_loss did not improve from 0.62686\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8098 - acc: 0.7614 - val_loss: 0.6319 - val_acc: 0.8286\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.7628\n",
      "Epoch 00189: val_loss did not improve from 0.62686\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8116 - acc: 0.7628 - val_loss: 0.6422 - val_acc: 0.8195\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8028 - acc: 0.7638\n",
      "Epoch 00190: val_loss improved from 0.62686 to 0.62459, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/190-0.6246.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8028 - acc: 0.7638 - val_loss: 0.6246 - val_acc: 0.8337\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.7621\n",
      "Epoch 00191: val_loss did not improve from 0.62459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8066 - acc: 0.7621 - val_loss: 0.6278 - val_acc: 0.8321\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8064 - acc: 0.7624\n",
      "Epoch 00192: val_loss did not improve from 0.62459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8064 - acc: 0.7624 - val_loss: 0.6276 - val_acc: 0.8353\n",
      "Epoch 193/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7607\n",
      "Epoch 00193: val_loss did not improve from 0.62459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8109 - acc: 0.7607 - val_loss: 0.6324 - val_acc: 0.8267\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8025 - acc: 0.7622\n",
      "Epoch 00194: val_loss did not improve from 0.62459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8025 - acc: 0.7622 - val_loss: 0.6256 - val_acc: 0.8318\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8046 - acc: 0.7644\n",
      "Epoch 00195: val_loss improved from 0.62459 to 0.62260, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/195-0.6226.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8046 - acc: 0.7644 - val_loss: 0.6226 - val_acc: 0.8304\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7660\n",
      "Epoch 00196: val_loss did not improve from 0.62260\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8028 - acc: 0.7659 - val_loss: 0.6301 - val_acc: 0.8286\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7640\n",
      "Epoch 00197: val_loss did not improve from 0.62260\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8057 - acc: 0.7640 - val_loss: 0.6282 - val_acc: 0.8307\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8049 - acc: 0.7608\n",
      "Epoch 00198: val_loss did not improve from 0.62260\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.8049 - acc: 0.7607 - val_loss: 0.6389 - val_acc: 0.8269\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7652\n",
      "Epoch 00199: val_loss improved from 0.62260 to 0.62117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/199-0.6212.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7973 - acc: 0.7652 - val_loss: 0.6212 - val_acc: 0.8376\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7991 - acc: 0.7669\n",
      "Epoch 00200: val_loss did not improve from 0.62117\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7990 - acc: 0.7669 - val_loss: 0.6284 - val_acc: 0.8286\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7962 - acc: 0.7664\n",
      "Epoch 00201: val_loss improved from 0.62117 to 0.62103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/201-0.6210.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7963 - acc: 0.7664 - val_loss: 0.6210 - val_acc: 0.8379\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7965 - acc: 0.7644\n",
      "Epoch 00202: val_loss improved from 0.62103 to 0.61738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/202-0.6174.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7965 - acc: 0.7644 - val_loss: 0.6174 - val_acc: 0.8318\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7959 - acc: 0.7658\n",
      "Epoch 00203: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7960 - acc: 0.7657 - val_loss: 0.6253 - val_acc: 0.8316\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7983 - acc: 0.7654\n",
      "Epoch 00204: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7984 - acc: 0.7654 - val_loss: 0.6300 - val_acc: 0.8314\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.7655\n",
      "Epoch 00205: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7934 - acc: 0.7654 - val_loss: 0.6211 - val_acc: 0.8346\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7982 - acc: 0.7634\n",
      "Epoch 00206: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7984 - acc: 0.7634 - val_loss: 0.6198 - val_acc: 0.8307\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7689\n",
      "Epoch 00207: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7906 - acc: 0.7689 - val_loss: 0.6257 - val_acc: 0.8325\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7909 - acc: 0.7680\n",
      "Epoch 00208: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7909 - acc: 0.7680 - val_loss: 0.6199 - val_acc: 0.8339\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7967 - acc: 0.7642\n",
      "Epoch 00209: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7967 - acc: 0.7642 - val_loss: 0.6202 - val_acc: 0.8374\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7964 - acc: 0.7645\n",
      "Epoch 00210: val_loss did not improve from 0.61738\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7964 - acc: 0.7645 - val_loss: 0.6255 - val_acc: 0.8316\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7884 - acc: 0.7684\n",
      "Epoch 00211: val_loss improved from 0.61738 to 0.61110, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/211-0.6111.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7885 - acc: 0.7684 - val_loss: 0.6111 - val_acc: 0.8365\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.7675\n",
      "Epoch 00212: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7935 - acc: 0.7674 - val_loss: 0.6189 - val_acc: 0.8346\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7892 - acc: 0.7681\n",
      "Epoch 00213: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7891 - acc: 0.7681 - val_loss: 0.6188 - val_acc: 0.8341\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7905 - acc: 0.7651\n",
      "Epoch 00214: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7905 - acc: 0.7651 - val_loss: 0.6185 - val_acc: 0.8365\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7684\n",
      "Epoch 00215: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7882 - acc: 0.7684 - val_loss: 0.6226 - val_acc: 0.8330\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7680\n",
      "Epoch 00216: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7830 - acc: 0.7680 - val_loss: 0.6163 - val_acc: 0.8376\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7858 - acc: 0.7687\n",
      "Epoch 00217: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7858 - acc: 0.7687 - val_loss: 0.6183 - val_acc: 0.8369\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7803 - acc: 0.7708\n",
      "Epoch 00218: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7802 - acc: 0.7708 - val_loss: 0.6162 - val_acc: 0.8383\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7670\n",
      "Epoch 00219: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7847 - acc: 0.7670 - val_loss: 0.6245 - val_acc: 0.8323\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7848 - acc: 0.7677\n",
      "Epoch 00220: val_loss did not improve from 0.61110\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7849 - acc: 0.7677 - val_loss: 0.6141 - val_acc: 0.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7670\n",
      "Epoch 00221: val_loss improved from 0.61110 to 0.60707, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/221-0.6071.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7871 - acc: 0.7670 - val_loss: 0.6071 - val_acc: 0.8404\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7831 - acc: 0.7719\n",
      "Epoch 00222: val_loss did not improve from 0.60707\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7831 - acc: 0.7719 - val_loss: 0.6154 - val_acc: 0.8355\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7711\n",
      "Epoch 00223: val_loss did not improve from 0.60707\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7839 - acc: 0.7711 - val_loss: 0.6152 - val_acc: 0.8344\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7866 - acc: 0.7682\n",
      "Epoch 00224: val_loss did not improve from 0.60707\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7866 - acc: 0.7682 - val_loss: 0.6168 - val_acc: 0.8365\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7837 - acc: 0.7686\n",
      "Epoch 00225: val_loss improved from 0.60707 to 0.60021, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/225-0.6002.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7837 - acc: 0.7686 - val_loss: 0.6002 - val_acc: 0.8430\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7698\n",
      "Epoch 00226: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7803 - acc: 0.7699 - val_loss: 0.6032 - val_acc: 0.8423\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7841 - acc: 0.7697\n",
      "Epoch 00227: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7842 - acc: 0.7697 - val_loss: 0.6181 - val_acc: 0.8355\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7857 - acc: 0.7697\n",
      "Epoch 00228: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7857 - acc: 0.7697 - val_loss: 0.6170 - val_acc: 0.8372\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.7698\n",
      "Epoch 00229: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7789 - acc: 0.7698 - val_loss: 0.6105 - val_acc: 0.8393\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7714\n",
      "Epoch 00230: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7773 - acc: 0.7714 - val_loss: 0.6126 - val_acc: 0.8379\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7768 - acc: 0.7715\n",
      "Epoch 00231: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7768 - acc: 0.7715 - val_loss: 0.6029 - val_acc: 0.8360\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7721\n",
      "Epoch 00232: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7759 - acc: 0.7721 - val_loss: 0.6015 - val_acc: 0.8430\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7721\n",
      "Epoch 00233: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7736 - acc: 0.7721 - val_loss: 0.6051 - val_acc: 0.8404\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7701 - acc: 0.7727\n",
      "Epoch 00234: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7701 - acc: 0.7727 - val_loss: 0.6054 - val_acc: 0.8414\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7750 - acc: 0.7706\n",
      "Epoch 00235: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7750 - acc: 0.7706 - val_loss: 0.6079 - val_acc: 0.8390\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7767 - acc: 0.7714\n",
      "Epoch 00236: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7767 - acc: 0.7714 - val_loss: 0.6037 - val_acc: 0.8397\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7719\n",
      "Epoch 00237: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7769 - acc: 0.7719 - val_loss: 0.6056 - val_acc: 0.8395\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7761 - acc: 0.7724\n",
      "Epoch 00238: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7761 - acc: 0.7724 - val_loss: 0.6108 - val_acc: 0.8353\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7723\n",
      "Epoch 00239: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7725 - acc: 0.7724 - val_loss: 0.6053 - val_acc: 0.8383\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.7730\n",
      "Epoch 00240: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7694 - acc: 0.7729 - val_loss: 0.6067 - val_acc: 0.8409\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7717 - acc: 0.7742\n",
      "Epoch 00241: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7717 - acc: 0.7742 - val_loss: 0.6052 - val_acc: 0.8393\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7707 - acc: 0.7710\n",
      "Epoch 00242: val_loss did not improve from 0.60021\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7707 - acc: 0.7710 - val_loss: 0.6072 - val_acc: 0.8428\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7695 - acc: 0.7746\n",
      "Epoch 00243: val_loss improved from 0.60021 to 0.60001, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/243-0.6000.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7696 - acc: 0.7746 - val_loss: 0.6000 - val_acc: 0.8428\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7727\n",
      "Epoch 00244: val_loss improved from 0.60001 to 0.59991, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/244-0.5999.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7703 - acc: 0.7727 - val_loss: 0.5999 - val_acc: 0.8439\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.7727\n",
      "Epoch 00245: val_loss did not improve from 0.59991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7664 - acc: 0.7726 - val_loss: 0.6007 - val_acc: 0.8404\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7693 - acc: 0.7744\n",
      "Epoch 00246: val_loss improved from 0.59991 to 0.59469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/246-0.5947.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7693 - acc: 0.7744 - val_loss: 0.5947 - val_acc: 0.8442\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7721 - acc: 0.7736\n",
      "Epoch 00247: val_loss improved from 0.59469 to 0.59293, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/247-0.5929.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7721 - acc: 0.7736 - val_loss: 0.5929 - val_acc: 0.8439\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.7745\n",
      "Epoch 00248: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7647 - acc: 0.7745 - val_loss: 0.5995 - val_acc: 0.8428\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.7736\n",
      "Epoch 00249: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7699 - acc: 0.7736 - val_loss: 0.6006 - val_acc: 0.8416\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.7770\n",
      "Epoch 00250: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7616 - acc: 0.7770 - val_loss: 0.5996 - val_acc: 0.8365\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7651 - acc: 0.7750\n",
      "Epoch 00251: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7650 - acc: 0.7750 - val_loss: 0.5959 - val_acc: 0.8397\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7650 - acc: 0.7754\n",
      "Epoch 00252: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7650 - acc: 0.7754 - val_loss: 0.5937 - val_acc: 0.8425\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.7747\n",
      "Epoch 00253: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7643 - acc: 0.7747 - val_loss: 0.5949 - val_acc: 0.8423\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7656 - acc: 0.7748- ETA: 1s - loss: 0.7652 \n",
      "Epoch 00254: val_loss did not improve from 0.59293\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7655 - acc: 0.7748 - val_loss: 0.5938 - val_acc: 0.8437\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7677 - acc: 0.7747\n",
      "Epoch 00255: val_loss improved from 0.59293 to 0.59008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/255-0.5901.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7677 - acc: 0.7747 - val_loss: 0.5901 - val_acc: 0.8432\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7569 - acc: 0.7757\n",
      "Epoch 00256: val_loss did not improve from 0.59008\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7568 - acc: 0.7757 - val_loss: 0.5958 - val_acc: 0.8463\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7761\n",
      "Epoch 00257: val_loss did not improve from 0.59008\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7598 - acc: 0.7761 - val_loss: 0.5990 - val_acc: 0.8435\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7753\n",
      "Epoch 00258: val_loss did not improve from 0.59008\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7609 - acc: 0.7753 - val_loss: 0.5907 - val_acc: 0.8460\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7780\n",
      "Epoch 00259: val_loss did not improve from 0.59008\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7575 - acc: 0.7780 - val_loss: 0.6024 - val_acc: 0.8353\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7606 - acc: 0.7763\n",
      "Epoch 00260: val_loss did not improve from 0.59008\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7608 - acc: 0.7763 - val_loss: 0.5928 - val_acc: 0.8432\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7591 - acc: 0.7755\n",
      "Epoch 00261: val_loss improved from 0.59008 to 0.58612, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/261-0.5861.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7591 - acc: 0.7755 - val_loss: 0.5861 - val_acc: 0.8414\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7607 - acc: 0.7757\n",
      "Epoch 00262: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7608 - acc: 0.7756 - val_loss: 0.5921 - val_acc: 0.8449\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7753\n",
      "Epoch 00263: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7631 - acc: 0.7753 - val_loss: 0.5932 - val_acc: 0.8432\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7588 - acc: 0.7778\n",
      "Epoch 00264: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7587 - acc: 0.7778 - val_loss: 0.5948 - val_acc: 0.8407\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7767\n",
      "Epoch 00265: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7574 - acc: 0.7767 - val_loss: 0.5878 - val_acc: 0.8467\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7620 - acc: 0.7771\n",
      "Epoch 00266: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7621 - acc: 0.7771 - val_loss: 0.5876 - val_acc: 0.8416\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.7760\n",
      "Epoch 00267: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7578 - acc: 0.7761 - val_loss: 0.5897 - val_acc: 0.8470\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7813\n",
      "Epoch 00268: val_loss did not improve from 0.58612\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7513 - acc: 0.7814 - val_loss: 0.5924 - val_acc: 0.8430\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.7793\n",
      "Epoch 00269: val_loss improved from 0.58612 to 0.58381, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/269-0.5838.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7489 - acc: 0.7793 - val_loss: 0.5838 - val_acc: 0.8498\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7560 - acc: 0.7780\n",
      "Epoch 00270: val_loss did not improve from 0.58381\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7560 - acc: 0.7780 - val_loss: 0.5871 - val_acc: 0.8437\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7782\n",
      "Epoch 00271: val_loss improved from 0.58381 to 0.58320, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/271-0.5832.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7560 - acc: 0.7782 - val_loss: 0.5832 - val_acc: 0.8484\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.7760\n",
      "Epoch 00272: val_loss improved from 0.58320 to 0.58180, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/272-0.5818.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7561 - acc: 0.7760 - val_loss: 0.5818 - val_acc: 0.8437\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.7779\n",
      "Epoch 00273: val_loss did not improve from 0.58180\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7562 - acc: 0.7779 - val_loss: 0.5859 - val_acc: 0.8432\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7552 - acc: 0.7765\n",
      "Epoch 00274: val_loss did not improve from 0.58180\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7553 - acc: 0.7765 - val_loss: 0.5878 - val_acc: 0.8423\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7525 - acc: 0.7791\n",
      "Epoch 00275: val_loss improved from 0.58180 to 0.58162, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/275-0.5816.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7525 - acc: 0.7791 - val_loss: 0.5816 - val_acc: 0.8463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7521 - acc: 0.7793\n",
      "Epoch 00276: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7522 - acc: 0.7793 - val_loss: 0.5854 - val_acc: 0.8444\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7456 - acc: 0.7789\n",
      "Epoch 00277: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7457 - acc: 0.7788 - val_loss: 0.5911 - val_acc: 0.8444\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7541 - acc: 0.7785\n",
      "Epoch 00278: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7541 - acc: 0.7785 - val_loss: 0.6039 - val_acc: 0.8435\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7507 - acc: 0.7797\n",
      "Epoch 00279: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7507 - acc: 0.7797 - val_loss: 0.5914 - val_acc: 0.8460\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7792\n",
      "Epoch 00280: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7500 - acc: 0.7793 - val_loss: 0.5879 - val_acc: 0.8488\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7787- ETA: 6 - E\n",
      "Epoch 00281: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7490 - acc: 0.7787 - val_loss: 0.5824 - val_acc: 0.8474\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7451 - acc: 0.7815\n",
      "Epoch 00282: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7452 - acc: 0.7816 - val_loss: 0.5828 - val_acc: 0.8416\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7441 - acc: 0.7808\n",
      "Epoch 00283: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7440 - acc: 0.7808 - val_loss: 0.5824 - val_acc: 0.8442\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7815\n",
      "Epoch 00284: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7482 - acc: 0.7815 - val_loss: 0.5832 - val_acc: 0.8444\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7796\n",
      "Epoch 00285: val_loss did not improve from 0.58162\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7513 - acc: 0.7796 - val_loss: 0.5911 - val_acc: 0.8435\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7459 - acc: 0.7815\n",
      "Epoch 00286: val_loss improved from 0.58162 to 0.57631, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/286-0.5763.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7460 - acc: 0.7814 - val_loss: 0.5763 - val_acc: 0.8474\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7465 - acc: 0.7828\n",
      "Epoch 00287: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7467 - acc: 0.7827 - val_loss: 0.5829 - val_acc: 0.8465\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7433 - acc: 0.7824\n",
      "Epoch 00288: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7433 - acc: 0.7824 - val_loss: 0.5790 - val_acc: 0.8453\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7404 - acc: 0.7813\n",
      "Epoch 00289: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7404 - acc: 0.7813 - val_loss: 0.5810 - val_acc: 0.8486\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7443 - acc: 0.7805\n",
      "Epoch 00290: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7443 - acc: 0.7805 - val_loss: 0.5832 - val_acc: 0.8519\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7439 - acc: 0.7820\n",
      "Epoch 00291: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7439 - acc: 0.7819 - val_loss: 0.5767 - val_acc: 0.8502\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7466 - acc: 0.7803\n",
      "Epoch 00292: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7465 - acc: 0.7803 - val_loss: 0.5874 - val_acc: 0.8393\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7458 - acc: 0.7815\n",
      "Epoch 00293: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7459 - acc: 0.7815 - val_loss: 0.5787 - val_acc: 0.8535\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7478 - acc: 0.7815\n",
      "Epoch 00294: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7478 - acc: 0.7816 - val_loss: 0.5764 - val_acc: 0.8502\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7380 - acc: 0.7830\n",
      "Epoch 00295: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7379 - acc: 0.7830 - val_loss: 0.5813 - val_acc: 0.8463\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7820\n",
      "Epoch 00296: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7398 - acc: 0.7820 - val_loss: 0.5789 - val_acc: 0.8451\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7451 - acc: 0.7788\n",
      "Epoch 00297: val_loss did not improve from 0.57631\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7451 - acc: 0.7788 - val_loss: 0.5828 - val_acc: 0.8498\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7437 - acc: 0.7814\n",
      "Epoch 00298: val_loss improved from 0.57631 to 0.57259, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/298-0.5726.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7437 - acc: 0.7813 - val_loss: 0.5726 - val_acc: 0.8474\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7832\n",
      "Epoch 00299: val_loss improved from 0.57259 to 0.57138, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/299-0.5714.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7404 - acc: 0.7832 - val_loss: 0.5714 - val_acc: 0.8491\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7428 - acc: 0.7808\n",
      "Epoch 00300: val_loss did not improve from 0.57138\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7427 - acc: 0.7808 - val_loss: 0.5782 - val_acc: 0.8532\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7381 - acc: 0.7839\n",
      "Epoch 00301: val_loss did not improve from 0.57138\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7381 - acc: 0.7840 - val_loss: 0.5725 - val_acc: 0.8484\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.7829\n",
      "Epoch 00302: val_loss improved from 0.57138 to 0.57135, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/302-0.5713.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7430 - acc: 0.7829 - val_loss: 0.5713 - val_acc: 0.8500\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7837\n",
      "Epoch 00303: val_loss did not improve from 0.57135\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7413 - acc: 0.7837 - val_loss: 0.5763 - val_acc: 0.8491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7347 - acc: 0.7830\n",
      "Epoch 00304: val_loss did not improve from 0.57135\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7346 - acc: 0.7830 - val_loss: 0.5727 - val_acc: 0.8512\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7383 - acc: 0.7831\n",
      "Epoch 00305: val_loss improved from 0.57135 to 0.57032, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/305-0.5703.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7383 - acc: 0.7831 - val_loss: 0.5703 - val_acc: 0.8505\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7845\n",
      "Epoch 00306: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7358 - acc: 0.7845 - val_loss: 0.5776 - val_acc: 0.8498\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7825\n",
      "Epoch 00307: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7359 - acc: 0.7824 - val_loss: 0.5740 - val_acc: 0.8481\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7831\n",
      "Epoch 00308: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7370 - acc: 0.7831 - val_loss: 0.5760 - val_acc: 0.8537\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7348 - acc: 0.7829\n",
      "Epoch 00309: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7350 - acc: 0.7828 - val_loss: 0.5719 - val_acc: 0.8505\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7376 - acc: 0.7835\n",
      "Epoch 00310: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7376 - acc: 0.7835 - val_loss: 0.5854 - val_acc: 0.8479\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7857\n",
      "Epoch 00311: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7322 - acc: 0.7857 - val_loss: 0.5716 - val_acc: 0.8512\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7345 - acc: 0.7840\n",
      "Epoch 00312: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7345 - acc: 0.7840 - val_loss: 0.5733 - val_acc: 0.8502\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7336 - acc: 0.7850\n",
      "Epoch 00313: val_loss did not improve from 0.57032\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7337 - acc: 0.7849 - val_loss: 0.5732 - val_acc: 0.8481\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7264 - acc: 0.7872\n",
      "Epoch 00314: val_loss improved from 0.57032 to 0.56971, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/314-0.5697.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7264 - acc: 0.7872 - val_loss: 0.5697 - val_acc: 0.8521\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7374 - acc: 0.7819\n",
      "Epoch 00315: val_loss improved from 0.56971 to 0.56960, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/315-0.5696.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7374 - acc: 0.7819 - val_loss: 0.5696 - val_acc: 0.8519\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7330 - acc: 0.7851\n",
      "Epoch 00316: val_loss did not improve from 0.56960\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7330 - acc: 0.7851 - val_loss: 0.5734 - val_acc: 0.8484\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7300 - acc: 0.7855\n",
      "Epoch 00317: val_loss improved from 0.56960 to 0.56946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/317-0.5695.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7301 - acc: 0.7855 - val_loss: 0.5695 - val_acc: 0.8532\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7293 - acc: 0.7858\n",
      "Epoch 00318: val_loss did not improve from 0.56946\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7292 - acc: 0.7859 - val_loss: 0.5793 - val_acc: 0.8481\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7326 - acc: 0.7846\n",
      "Epoch 00319: val_loss did not improve from 0.56946\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7326 - acc: 0.7846 - val_loss: 0.5723 - val_acc: 0.8505\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7317 - acc: 0.7827\n",
      "Epoch 00320: val_loss improved from 0.56946 to 0.56567, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/320-0.5657.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7317 - acc: 0.7827 - val_loss: 0.5657 - val_acc: 0.8523\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7278 - acc: 0.7857\n",
      "Epoch 00321: val_loss improved from 0.56567 to 0.56459, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/321-0.5646.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7278 - acc: 0.7857 - val_loss: 0.5646 - val_acc: 0.8539\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7313 - acc: 0.7829\n",
      "Epoch 00322: val_loss did not improve from 0.56459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7313 - acc: 0.7829 - val_loss: 0.5663 - val_acc: 0.8530\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7321 - acc: 0.7831\n",
      "Epoch 00323: val_loss did not improve from 0.56459\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7321 - acc: 0.7832 - val_loss: 0.5701 - val_acc: 0.8512\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7257 - acc: 0.7868- ETA: 2s -\n",
      "Epoch 00324: val_loss improved from 0.56459 to 0.56379, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/324-0.5638.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7258 - acc: 0.7867 - val_loss: 0.5638 - val_acc: 0.8542\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7852\n",
      "Epoch 00325: val_loss did not improve from 0.56379\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7322 - acc: 0.7852 - val_loss: 0.5664 - val_acc: 0.8532\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7296 - acc: 0.7859\n",
      "Epoch 00326: val_loss did not improve from 0.56379\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7296 - acc: 0.7859 - val_loss: 0.5686 - val_acc: 0.8488\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7853\n",
      "Epoch 00327: val_loss did not improve from 0.56379\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7250 - acc: 0.7852 - val_loss: 0.5652 - val_acc: 0.8567\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.7870\n",
      "Epoch 00328: val_loss improved from 0.56379 to 0.55993, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/328-0.5599.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7245 - acc: 0.7870 - val_loss: 0.5599 - val_acc: 0.8549\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7307 - acc: 0.7854\n",
      "Epoch 00329: val_loss did not improve from 0.55993\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7307 - acc: 0.7854 - val_loss: 0.5783 - val_acc: 0.8507\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7253 - acc: 0.7865\n",
      "Epoch 00330: val_loss did not improve from 0.55993\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7252 - acc: 0.7866 - val_loss: 0.5623 - val_acc: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7872\n",
      "Epoch 00331: val_loss did not improve from 0.55993\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7273 - acc: 0.7872 - val_loss: 0.5625 - val_acc: 0.8563\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7237 - acc: 0.7874\n",
      "Epoch 00332: val_loss improved from 0.55993 to 0.55802, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/332-0.5580.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7236 - acc: 0.7874 - val_loss: 0.5580 - val_acc: 0.8535\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7257 - acc: 0.7871\n",
      "Epoch 00333: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7256 - acc: 0.7871 - val_loss: 0.5687 - val_acc: 0.8507\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7868\n",
      "Epoch 00334: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7219 - acc: 0.7867 - val_loss: 0.5658 - val_acc: 0.8516\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7222 - acc: 0.7887\n",
      "Epoch 00335: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7223 - acc: 0.7888 - val_loss: 0.5599 - val_acc: 0.8558\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7228 - acc: 0.7867\n",
      "Epoch 00336: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7228 - acc: 0.7867 - val_loss: 0.5675 - val_acc: 0.8477\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7881\n",
      "Epoch 00337: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7200 - acc: 0.7882 - val_loss: 0.5651 - val_acc: 0.8537\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7235 - acc: 0.7871\n",
      "Epoch 00338: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7234 - acc: 0.7871 - val_loss: 0.5614 - val_acc: 0.8523\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7873\n",
      "Epoch 00339: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7242 - acc: 0.7873 - val_loss: 0.5652 - val_acc: 0.8502\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.7867\n",
      "Epoch 00340: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7272 - acc: 0.7867 - val_loss: 0.5647 - val_acc: 0.8486\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7188 - acc: 0.7876\n",
      "Epoch 00341: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7187 - acc: 0.7876 - val_loss: 0.5636 - val_acc: 0.8514\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7893\n",
      "Epoch 00342: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7187 - acc: 0.7893 - val_loss: 0.5716 - val_acc: 0.8481\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7180 - acc: 0.7861\n",
      "Epoch 00343: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7179 - acc: 0.7861 - val_loss: 0.5637 - val_acc: 0.8535\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7888\n",
      "Epoch 00344: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7204 - acc: 0.7888 - val_loss: 0.5624 - val_acc: 0.8535\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7214 - acc: 0.7876\n",
      "Epoch 00345: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7213 - acc: 0.7876 - val_loss: 0.5667 - val_acc: 0.8498\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7874\n",
      "Epoch 00346: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7215 - acc: 0.7874 - val_loss: 0.5595 - val_acc: 0.8537\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7882- ETA: 1s - loss: 0.7143\n",
      "Epoch 00347: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7165 - acc: 0.7882 - val_loss: 0.5641 - val_acc: 0.8500\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7192 - acc: 0.7873\n",
      "Epoch 00348: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7192 - acc: 0.7873 - val_loss: 0.5618 - val_acc: 0.8546\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7225 - acc: 0.7863\n",
      "Epoch 00349: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7226 - acc: 0.7863 - val_loss: 0.5615 - val_acc: 0.8574\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.7911\n",
      "Epoch 00350: val_loss did not improve from 0.55802\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7204 - acc: 0.7911 - val_loss: 0.5708 - val_acc: 0.8551\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7167 - acc: 0.7882\n",
      "Epoch 00351: val_loss improved from 0.55802 to 0.55456, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/351-0.5546.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7166 - acc: 0.7883 - val_loss: 0.5546 - val_acc: 0.8572\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.7904\n",
      "Epoch 00352: val_loss did not improve from 0.55456\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7169 - acc: 0.7905 - val_loss: 0.5570 - val_acc: 0.8530\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7901\n",
      "Epoch 00353: val_loss did not improve from 0.55456\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7128 - acc: 0.7901 - val_loss: 0.5726 - val_acc: 0.8428\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7174 - acc: 0.7886\n",
      "Epoch 00354: val_loss did not improve from 0.55456\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7174 - acc: 0.7886 - val_loss: 0.5566 - val_acc: 0.8542\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.7901\n",
      "Epoch 00355: val_loss did not improve from 0.55456\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7113 - acc: 0.7901 - val_loss: 0.5561 - val_acc: 0.8565\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7127 - acc: 0.7914\n",
      "Epoch 00356: val_loss did not improve from 0.55456\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7129 - acc: 0.7914 - val_loss: 0.5592 - val_acc: 0.8532\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7109 - acc: 0.7900\n",
      "Epoch 00357: val_loss improved from 0.55456 to 0.55323, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/357-0.5532.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7108 - acc: 0.7900 - val_loss: 0.5532 - val_acc: 0.8556\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7921\n",
      "Epoch 00358: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7138 - acc: 0.7922 - val_loss: 0.5536 - val_acc: 0.8539\n",
      "Epoch 359/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7901- ETA: 2s \n",
      "Epoch 00359: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7142 - acc: 0.7901 - val_loss: 0.5685 - val_acc: 0.8495\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7079 - acc: 0.7928\n",
      "Epoch 00360: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7078 - acc: 0.7929 - val_loss: 0.5558 - val_acc: 0.8514\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7132 - acc: 0.7904\n",
      "Epoch 00361: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7133 - acc: 0.7904 - val_loss: 0.5579 - val_acc: 0.8519\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7148 - acc: 0.7882\n",
      "Epoch 00362: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7149 - acc: 0.7882 - val_loss: 0.5555 - val_acc: 0.8574\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7162 - acc: 0.7909\n",
      "Epoch 00363: val_loss did not improve from 0.55323\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7165 - acc: 0.7909 - val_loss: 0.5645 - val_acc: 0.8442\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7141 - acc: 0.7870\n",
      "Epoch 00364: val_loss improved from 0.55323 to 0.55296, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/364-0.5530.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7140 - acc: 0.7870 - val_loss: 0.5530 - val_acc: 0.8584\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7111 - acc: 0.7881\n",
      "Epoch 00365: val_loss did not improve from 0.55296\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7111 - acc: 0.7881 - val_loss: 0.5568 - val_acc: 0.8542\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7916\n",
      "Epoch 00366: val_loss did not improve from 0.55296\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7118 - acc: 0.7916 - val_loss: 0.5565 - val_acc: 0.8532\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7132 - acc: 0.7912\n",
      "Epoch 00367: val_loss did not improve from 0.55296\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7131 - acc: 0.7913 - val_loss: 0.5569 - val_acc: 0.8558\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7107 - acc: 0.7929\n",
      "Epoch 00368: val_loss improved from 0.55296 to 0.54772, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/368-0.5477.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7107 - acc: 0.7929 - val_loss: 0.5477 - val_acc: 0.8572\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7906\n",
      "Epoch 00369: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7095 - acc: 0.7906 - val_loss: 0.5579 - val_acc: 0.8530\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7093 - acc: 0.7918\n",
      "Epoch 00370: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7093 - acc: 0.7918 - val_loss: 0.5588 - val_acc: 0.8544\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.7911\n",
      "Epoch 00371: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7079 - acc: 0.7911 - val_loss: 0.5560 - val_acc: 0.8526\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7069 - acc: 0.7902\n",
      "Epoch 00372: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7070 - acc: 0.7902 - val_loss: 0.5570 - val_acc: 0.8551\n",
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.7915\n",
      "Epoch 00373: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7116 - acc: 0.7915 - val_loss: 0.5546 - val_acc: 0.8519\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.7935\n",
      "Epoch 00374: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7014 - acc: 0.7934 - val_loss: 0.5597 - val_acc: 0.8528\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.7930\n",
      "Epoch 00375: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7117 - acc: 0.7930 - val_loss: 0.5595 - val_acc: 0.8539\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.7920- E\n",
      "Epoch 00376: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7050 - acc: 0.7920 - val_loss: 0.5496 - val_acc: 0.8565\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7079 - acc: 0.7923\n",
      "Epoch 00377: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7080 - acc: 0.7923 - val_loss: 0.5610 - val_acc: 0.8516\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7080 - acc: 0.7913\n",
      "Epoch 00378: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7081 - acc: 0.7913 - val_loss: 0.5492 - val_acc: 0.8577\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7126 - acc: 0.7929- ETA: 1s - loss: 0.71\n",
      "Epoch 00379: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7127 - acc: 0.7929 - val_loss: 0.5538 - val_acc: 0.8565\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7037 - acc: 0.7918\n",
      "Epoch 00380: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7037 - acc: 0.7918 - val_loss: 0.5537 - val_acc: 0.8537\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7914\n",
      "Epoch 00381: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7060 - acc: 0.7914 - val_loss: 0.5599 - val_acc: 0.8509\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.7913\n",
      "Epoch 00382: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7093 - acc: 0.7913 - val_loss: 0.5521 - val_acc: 0.8574\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7013 - acc: 0.7930\n",
      "Epoch 00383: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7013 - acc: 0.7930 - val_loss: 0.5532 - val_acc: 0.8542\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7051 - acc: 0.7923\n",
      "Epoch 00384: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7051 - acc: 0.7923 - val_loss: 0.5521 - val_acc: 0.8577\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7043 - acc: 0.7920\n",
      "Epoch 00385: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7043 - acc: 0.7919 - val_loss: 0.5492 - val_acc: 0.8584\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7035 - acc: 0.7914\n",
      "Epoch 00386: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7035 - acc: 0.7914 - val_loss: 0.5636 - val_acc: 0.8574\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7020 - acc: 0.7932\n",
      "Epoch 00387: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7021 - acc: 0.7932 - val_loss: 0.5542 - val_acc: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7951\n",
      "Epoch 00388: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7034 - acc: 0.7950 - val_loss: 0.5506 - val_acc: 0.8514\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.7930\n",
      "Epoch 00389: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6982 - acc: 0.7930 - val_loss: 0.5595 - val_acc: 0.8477\n",
      "Epoch 390/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6994 - acc: 0.7932\n",
      "Epoch 00390: val_loss did not improve from 0.54772\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6994 - acc: 0.7932 - val_loss: 0.5506 - val_acc: 0.8567\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7033 - acc: 0.7929\n",
      "Epoch 00391: val_loss improved from 0.54772 to 0.54603, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/391-0.5460.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7032 - acc: 0.7929 - val_loss: 0.5460 - val_acc: 0.8549\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7058 - acc: 0.7932\n",
      "Epoch 00392: val_loss did not improve from 0.54603\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7060 - acc: 0.7932 - val_loss: 0.5541 - val_acc: 0.8526\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7028 - acc: 0.7935\n",
      "Epoch 00393: val_loss did not improve from 0.54603\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7028 - acc: 0.7935 - val_loss: 0.5494 - val_acc: 0.8602\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.7922\n",
      "Epoch 00394: val_loss did not improve from 0.54603\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7003 - acc: 0.7922 - val_loss: 0.5495 - val_acc: 0.8537\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6979 - acc: 0.7958\n",
      "Epoch 00395: val_loss improved from 0.54603 to 0.54530, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/395-0.5453.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6980 - acc: 0.7958 - val_loss: 0.5453 - val_acc: 0.8612\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6994 - acc: 0.7927\n",
      "Epoch 00396: val_loss improved from 0.54530 to 0.54179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/396-0.5418.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6994 - acc: 0.7927 - val_loss: 0.5418 - val_acc: 0.8595\n",
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7018 - acc: 0.7936\n",
      "Epoch 00397: val_loss did not improve from 0.54179\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7018 - acc: 0.7936 - val_loss: 0.5475 - val_acc: 0.8558\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7952\n",
      "Epoch 00398: val_loss did not improve from 0.54179\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6958 - acc: 0.7952 - val_loss: 0.5429 - val_acc: 0.8598\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7936\n",
      "Epoch 00399: val_loss did not improve from 0.54179\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6965 - acc: 0.7936 - val_loss: 0.5501 - val_acc: 0.8535\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.7962\n",
      "Epoch 00400: val_loss did not improve from 0.54179\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6967 - acc: 0.7962 - val_loss: 0.5490 - val_acc: 0.8551\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6983 - acc: 0.7938\n",
      "Epoch 00401: val_loss improved from 0.54179 to 0.54082, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/401-0.5408.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6982 - acc: 0.7939 - val_loss: 0.5408 - val_acc: 0.8595\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.7954\n",
      "Epoch 00402: val_loss did not improve from 0.54082\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6997 - acc: 0.7955 - val_loss: 0.5412 - val_acc: 0.8579\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6983 - acc: 0.7958\n",
      "Epoch 00403: val_loss did not improve from 0.54082\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6982 - acc: 0.7958 - val_loss: 0.5487 - val_acc: 0.8528\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.7948\n",
      "Epoch 00404: val_loss did not improve from 0.54082\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.7003 - acc: 0.7949 - val_loss: 0.5466 - val_acc: 0.8553\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6988 - acc: 0.7948\n",
      "Epoch 00405: val_loss did not improve from 0.54082\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6988 - acc: 0.7948 - val_loss: 0.5488 - val_acc: 0.8556\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7952\n",
      "Epoch 00406: val_loss did not improve from 0.54082\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6941 - acc: 0.7952 - val_loss: 0.5498 - val_acc: 0.8558\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.7940\n",
      "Epoch 00407: val_loss improved from 0.54082 to 0.53991, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/407-0.5399.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6945 - acc: 0.7940 - val_loss: 0.5399 - val_acc: 0.8565\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6944 - acc: 0.7959\n",
      "Epoch 00408: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6944 - acc: 0.7959 - val_loss: 0.5468 - val_acc: 0.8586\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.7955\n",
      "Epoch 00409: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6938 - acc: 0.7956 - val_loss: 0.5465 - val_acc: 0.8567\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6947 - acc: 0.7932\n",
      "Epoch 00410: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6947 - acc: 0.7932 - val_loss: 0.5436 - val_acc: 0.8558\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.7973\n",
      "Epoch 00411: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6933 - acc: 0.7973 - val_loss: 0.5468 - val_acc: 0.8577\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6895 - acc: 0.7955\n",
      "Epoch 00412: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6895 - acc: 0.7955 - val_loss: 0.5451 - val_acc: 0.8595\n",
      "Epoch 413/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6910 - acc: 0.7947\n",
      "Epoch 00413: val_loss did not improve from 0.53991\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6910 - acc: 0.7947 - val_loss: 0.5542 - val_acc: 0.8546\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.7956\n",
      "Epoch 00414: val_loss improved from 0.53991 to 0.53868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/414-0.5387.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6904 - acc: 0.7957 - val_loss: 0.5387 - val_acc: 0.8558\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.7952\n",
      "Epoch 00415: val_loss improved from 0.53868 to 0.53817, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/415-0.5382.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6908 - acc: 0.7952 - val_loss: 0.5382 - val_acc: 0.8572\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.7955\n",
      "Epoch 00416: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6904 - acc: 0.7955 - val_loss: 0.5500 - val_acc: 0.8539\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.7981- ETA: 1s - loss: 0.69\n",
      "Epoch 00417: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6907 - acc: 0.7982 - val_loss: 0.5440 - val_acc: 0.8607\n",
      "Epoch 418/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7947\n",
      "Epoch 00418: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6957 - acc: 0.7947 - val_loss: 0.5416 - val_acc: 0.8595\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.7967\n",
      "Epoch 00419: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6928 - acc: 0.7967 - val_loss: 0.5419 - val_acc: 0.8567\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6900 - acc: 0.7983\n",
      "Epoch 00420: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6900 - acc: 0.7983 - val_loss: 0.5456 - val_acc: 0.8567\n",
      "Epoch 421/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6910 - acc: 0.7969\n",
      "Epoch 00421: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6909 - acc: 0.7969 - val_loss: 0.5404 - val_acc: 0.8577\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7951\n",
      "Epoch 00422: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6931 - acc: 0.7951 - val_loss: 0.5455 - val_acc: 0.8558\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.7987\n",
      "Epoch 00423: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6897 - acc: 0.7987 - val_loss: 0.5424 - val_acc: 0.8586\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7964\n",
      "Epoch 00424: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6926 - acc: 0.7964 - val_loss: 0.5440 - val_acc: 0.8598\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6891 - acc: 0.7963\n",
      "Epoch 00425: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6890 - acc: 0.7963 - val_loss: 0.5439 - val_acc: 0.8593\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.7973\n",
      "Epoch 00426: val_loss did not improve from 0.53817\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6884 - acc: 0.7973 - val_loss: 0.5395 - val_acc: 0.8614\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6880 - acc: 0.8002\n",
      "Epoch 00427: val_loss improved from 0.53817 to 0.53745, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/427-0.5374.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6880 - acc: 0.8002 - val_loss: 0.5374 - val_acc: 0.8579\n",
      "Epoch 428/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.7992\n",
      "Epoch 00428: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6856 - acc: 0.7992 - val_loss: 0.5383 - val_acc: 0.8591\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.7968\n",
      "Epoch 00429: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6929 - acc: 0.7968 - val_loss: 0.5498 - val_acc: 0.8572\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6868 - acc: 0.7948\n",
      "Epoch 00430: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6870 - acc: 0.7947 - val_loss: 0.5385 - val_acc: 0.8600\n",
      "Epoch 431/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6909 - acc: 0.7972\n",
      "Epoch 00431: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6910 - acc: 0.7972 - val_loss: 0.5523 - val_acc: 0.8553\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6879 - acc: 0.7977\n",
      "Epoch 00432: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6878 - acc: 0.7977 - val_loss: 0.5387 - val_acc: 0.8619\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6844 - acc: 0.7972\n",
      "Epoch 00433: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6844 - acc: 0.7972 - val_loss: 0.5444 - val_acc: 0.8579\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.7963\n",
      "Epoch 00434: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6918 - acc: 0.7963 - val_loss: 0.5437 - val_acc: 0.8577\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6897 - acc: 0.7988\n",
      "Epoch 00435: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6897 - acc: 0.7988 - val_loss: 0.5451 - val_acc: 0.8560\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6877 - acc: 0.7980\n",
      "Epoch 00436: val_loss did not improve from 0.53745\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6879 - acc: 0.7980 - val_loss: 0.5469 - val_acc: 0.8593\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6855 - acc: 0.8002\n",
      "Epoch 00437: val_loss improved from 0.53745 to 0.53275, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/437-0.5328.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6854 - acc: 0.8002 - val_loss: 0.5328 - val_acc: 0.8633\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.7951\n",
      "Epoch 00438: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6931 - acc: 0.7951 - val_loss: 0.5385 - val_acc: 0.8567\n",
      "Epoch 439/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.7987\n",
      "Epoch 00439: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6857 - acc: 0.7987 - val_loss: 0.5392 - val_acc: 0.8614\n",
      "Epoch 440/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6822 - acc: 0.7966\n",
      "Epoch 00440: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6822 - acc: 0.7966 - val_loss: 0.5384 - val_acc: 0.8612\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.7987\n",
      "Epoch 00441: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6805 - acc: 0.7987 - val_loss: 0.5428 - val_acc: 0.8626\n",
      "Epoch 442/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6827 - acc: 0.7988\n",
      "Epoch 00442: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6827 - acc: 0.7988 - val_loss: 0.5437 - val_acc: 0.8574\n",
      "Epoch 443/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.7964\n",
      "Epoch 00443: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6856 - acc: 0.7964 - val_loss: 0.5465 - val_acc: 0.8558\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6860 - acc: 0.7971\n",
      "Epoch 00444: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6859 - acc: 0.7971 - val_loss: 0.5387 - val_acc: 0.8584\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6872 - acc: 0.7976\n",
      "Epoch 00445: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6875 - acc: 0.7975 - val_loss: 0.5334 - val_acc: 0.8623\n",
      "Epoch 446/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6820 - acc: 0.7969\n",
      "Epoch 00446: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6821 - acc: 0.7968 - val_loss: 0.5367 - val_acc: 0.8598\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6811 - acc: 0.7996\n",
      "Epoch 00447: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6811 - acc: 0.7995 - val_loss: 0.5421 - val_acc: 0.8579\n",
      "Epoch 448/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6798 - acc: 0.8006\n",
      "Epoch 00448: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6797 - acc: 0.8007 - val_loss: 0.5418 - val_acc: 0.8605\n",
      "Epoch 449/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6838 - acc: 0.7974\n",
      "Epoch 00449: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6837 - acc: 0.7975 - val_loss: 0.5371 - val_acc: 0.8605\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6833 - acc: 0.7995\n",
      "Epoch 00450: val_loss did not improve from 0.53275\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6834 - acc: 0.7994 - val_loss: 0.5347 - val_acc: 0.8595\n",
      "Epoch 451/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.7998\n",
      "Epoch 00451: val_loss improved from 0.53275 to 0.53238, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/451-0.5324.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6797 - acc: 0.7998 - val_loss: 0.5324 - val_acc: 0.8609\n",
      "Epoch 452/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.7982\n",
      "Epoch 00452: val_loss did not improve from 0.53238\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6808 - acc: 0.7982 - val_loss: 0.5346 - val_acc: 0.8619\n",
      "Epoch 453/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.8000\n",
      "Epoch 00453: val_loss did not improve from 0.53238\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6792 - acc: 0.8000 - val_loss: 0.5396 - val_acc: 0.8616\n",
      "Epoch 454/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.8001\n",
      "Epoch 00454: val_loss did not improve from 0.53238\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6764 - acc: 0.8001 - val_loss: 0.5357 - val_acc: 0.8607\n",
      "Epoch 455/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6771 - acc: 0.7991\n",
      "Epoch 00455: val_loss did not improve from 0.53238\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6771 - acc: 0.7991 - val_loss: 0.5330 - val_acc: 0.8628\n",
      "Epoch 456/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6764 - acc: 0.7998\n",
      "Epoch 00456: val_loss improved from 0.53238 to 0.53163, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/456-0.5316.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6764 - acc: 0.7998 - val_loss: 0.5316 - val_acc: 0.8635\n",
      "Epoch 457/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6811 - acc: 0.7986- ETA: 6s - los\n",
      "Epoch 00457: val_loss did not improve from 0.53163\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6810 - acc: 0.7986 - val_loss: 0.5428 - val_acc: 0.8605\n",
      "Epoch 458/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6805 - acc: 0.7982\n",
      "Epoch 00458: val_loss did not improve from 0.53163\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6805 - acc: 0.7981 - val_loss: 0.5443 - val_acc: 0.8565\n",
      "Epoch 459/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6766 - acc: 0.8014\n",
      "Epoch 00459: val_loss improved from 0.53163 to 0.53106, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/459-0.5311.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6769 - acc: 0.8013 - val_loss: 0.5311 - val_acc: 0.8623\n",
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6799 - acc: 0.7985\n",
      "Epoch 00460: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6800 - acc: 0.7985 - val_loss: 0.5365 - val_acc: 0.8630\n",
      "Epoch 461/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6776 - acc: 0.7996\n",
      "Epoch 00461: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6776 - acc: 0.7996 - val_loss: 0.5467 - val_acc: 0.8558\n",
      "Epoch 462/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.8009\n",
      "Epoch 00462: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6770 - acc: 0.8008 - val_loss: 0.5394 - val_acc: 0.8581\n",
      "Epoch 463/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.8004\n",
      "Epoch 00463: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6808 - acc: 0.8004 - val_loss: 0.5426 - val_acc: 0.8553\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6740 - acc: 0.7997\n",
      "Epoch 00464: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6740 - acc: 0.7997 - val_loss: 0.5379 - val_acc: 0.8551\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6780 - acc: 0.8004\n",
      "Epoch 00465: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6779 - acc: 0.8004 - val_loss: 0.5397 - val_acc: 0.8558\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.7996\n",
      "Epoch 00466: val_loss did not improve from 0.53106\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6754 - acc: 0.7995 - val_loss: 0.5363 - val_acc: 0.8605\n",
      "Epoch 467/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6758 - acc: 0.7996\n",
      "Epoch 00467: val_loss improved from 0.53106 to 0.52938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/467-0.5294.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6758 - acc: 0.7996 - val_loss: 0.5294 - val_acc: 0.8619\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6697 - acc: 0.8022\n",
      "Epoch 00468: val_loss improved from 0.52938 to 0.52904, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/468-0.5290.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6698 - acc: 0.8021 - val_loss: 0.5290 - val_acc: 0.8623\n",
      "Epoch 469/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6759 - acc: 0.8002\n",
      "Epoch 00469: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6759 - acc: 0.8002 - val_loss: 0.5309 - val_acc: 0.8628\n",
      "Epoch 470/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6702 - acc: 0.8016\n",
      "Epoch 00470: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6702 - acc: 0.8016 - val_loss: 0.5354 - val_acc: 0.8609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6797 - acc: 0.8015\n",
      "Epoch 00471: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6797 - acc: 0.8016 - val_loss: 0.5414 - val_acc: 0.8581\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.7986\n",
      "Epoch 00472: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6759 - acc: 0.7986 - val_loss: 0.5336 - val_acc: 0.8616\n",
      "Epoch 473/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6846 - acc: 0.7994\n",
      "Epoch 00473: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6846 - acc: 0.7994 - val_loss: 0.5389 - val_acc: 0.8612\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6754 - acc: 0.8016\n",
      "Epoch 00474: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6754 - acc: 0.8016 - val_loss: 0.5356 - val_acc: 0.8581\n",
      "Epoch 475/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.8021\n",
      "Epoch 00475: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6756 - acc: 0.8021 - val_loss: 0.5433 - val_acc: 0.8591\n",
      "Epoch 476/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6750 - acc: 0.8002\n",
      "Epoch 00476: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6750 - acc: 0.8002 - val_loss: 0.5299 - val_acc: 0.8661\n",
      "Epoch 477/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.8017\n",
      "Epoch 00477: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6725 - acc: 0.8016 - val_loss: 0.5361 - val_acc: 0.8588\n",
      "Epoch 478/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.8021\n",
      "Epoch 00478: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6710 - acc: 0.8021 - val_loss: 0.5334 - val_acc: 0.8602\n",
      "Epoch 479/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.8012\n",
      "Epoch 00479: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6735 - acc: 0.8012 - val_loss: 0.5327 - val_acc: 0.8642\n",
      "Epoch 480/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6725 - acc: 0.8022\n",
      "Epoch 00480: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6727 - acc: 0.8021 - val_loss: 0.5354 - val_acc: 0.8621\n",
      "Epoch 481/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6723 - acc: 0.7999\n",
      "Epoch 00481: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6724 - acc: 0.7999 - val_loss: 0.5327 - val_acc: 0.8591\n",
      "Epoch 482/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6753 - acc: 0.8004\n",
      "Epoch 00482: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6753 - acc: 0.8004 - val_loss: 0.5419 - val_acc: 0.8591\n",
      "Epoch 483/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6717 - acc: 0.8031\n",
      "Epoch 00483: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6716 - acc: 0.8031 - val_loss: 0.5322 - val_acc: 0.8607\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.8020- ETA: 1s - loss: 0.669\n",
      "Epoch 00484: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6700 - acc: 0.8020 - val_loss: 0.5293 - val_acc: 0.8630\n",
      "Epoch 485/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6734 - acc: 0.8017\n",
      "Epoch 00485: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6734 - acc: 0.8017 - val_loss: 0.5325 - val_acc: 0.8607\n",
      "Epoch 486/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6684 - acc: 0.8002\n",
      "Epoch 00486: val_loss did not improve from 0.52904\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6684 - acc: 0.8002 - val_loss: 0.5429 - val_acc: 0.8551\n",
      "Epoch 487/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6670 - acc: 0.8019\n",
      "Epoch 00487: val_loss improved from 0.52904 to 0.52897, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/487-0.5290.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6674 - acc: 0.8019 - val_loss: 0.5290 - val_acc: 0.8602\n",
      "Epoch 488/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.8025\n",
      "Epoch 00488: val_loss did not improve from 0.52897\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6675 - acc: 0.8025 - val_loss: 0.5406 - val_acc: 0.8572\n",
      "Epoch 489/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6711 - acc: 0.8014\n",
      "Epoch 00489: val_loss did not improve from 0.52897\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6711 - acc: 0.8015 - val_loss: 0.5313 - val_acc: 0.8630\n",
      "Epoch 490/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6678 - acc: 0.8032\n",
      "Epoch 00490: val_loss improved from 0.52897 to 0.52694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/490-0.5269.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6677 - acc: 0.8033 - val_loss: 0.5269 - val_acc: 0.8675\n",
      "Epoch 491/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6677 - acc: 0.8007\n",
      "Epoch 00491: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6677 - acc: 0.8007 - val_loss: 0.5309 - val_acc: 0.8637\n",
      "Epoch 492/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8046\n",
      "Epoch 00492: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6657 - acc: 0.8046 - val_loss: 0.5392 - val_acc: 0.8593\n",
      "Epoch 493/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.8018\n",
      "Epoch 00493: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6686 - acc: 0.8018 - val_loss: 0.5359 - val_acc: 0.8612\n",
      "Epoch 494/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6639 - acc: 0.8045\n",
      "Epoch 00494: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6638 - acc: 0.8045 - val_loss: 0.5287 - val_acc: 0.8649\n",
      "Epoch 495/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6670 - acc: 0.8048\n",
      "Epoch 00495: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6670 - acc: 0.8048 - val_loss: 0.5335 - val_acc: 0.8588\n",
      "Epoch 496/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6672 - acc: 0.8049\n",
      "Epoch 00496: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6672 - acc: 0.8049 - val_loss: 0.5313 - val_acc: 0.8619\n",
      "Epoch 497/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8018\n",
      "Epoch 00497: val_loss did not improve from 0.52694\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6659 - acc: 0.8018 - val_loss: 0.5289 - val_acc: 0.8626\n",
      "Epoch 498/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.8020- ETA: 0s - loss: 0.6658 - acc: \n",
      "Epoch 00498: val_loss improved from 0.52694 to 0.52521, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv_checkpoint/498-0.5252.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6675 - acc: 0.8020 - val_loss: 0.5252 - val_acc: 0.8647\n",
      "Epoch 499/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6640 - acc: 0.8053\n",
      "Epoch 00499: val_loss did not improve from 0.52521\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6640 - acc: 0.8053 - val_loss: 0.5284 - val_acc: 0.8633\n",
      "Epoch 500/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.8035\n",
      "Epoch 00500: val_loss did not improve from 0.52521\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 0.6690 - acc: 0.8035 - val_loss: 0.5329 - val_acc: 0.8602\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 653us/sample - loss: 0.6031 - acc: 0.8297\n",
      "Loss: 0.6031385346736492 Accuracy: 0.82969886\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5137 - acc: 0.1822\n",
      "Epoch 00001: val_loss improved from inf to 2.08777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/001-2.0878.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 2.5137 - acc: 0.1823 - val_loss: 2.0878 - val_acc: 0.3361\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0829 - acc: 0.3087\n",
      "Epoch 00002: val_loss improved from 2.08777 to 1.84638, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/002-1.8464.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 2.0829 - acc: 0.3087 - val_loss: 1.8464 - val_acc: 0.4253\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9290 - acc: 0.3586\n",
      "Epoch 00003: val_loss improved from 1.84638 to 1.70195, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/003-1.7020.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.9290 - acc: 0.3586 - val_loss: 1.7020 - val_acc: 0.4677\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8067 - acc: 0.4025\n",
      "Epoch 00004: val_loss improved from 1.70195 to 1.62027, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/004-1.6203.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.8066 - acc: 0.4025 - val_loss: 1.6203 - val_acc: 0.5171\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7033 - acc: 0.4443\n",
      "Epoch 00005: val_loss improved from 1.62027 to 1.49189, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/005-1.4919.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.7032 - acc: 0.4443 - val_loss: 1.4919 - val_acc: 0.5623\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6223 - acc: 0.4711\n",
      "Epoch 00006: val_loss improved from 1.49189 to 1.42260, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/006-1.4226.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.6223 - acc: 0.4711 - val_loss: 1.4226 - val_acc: 0.5816\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5458 - acc: 0.5024\n",
      "Epoch 00007: val_loss improved from 1.42260 to 1.34826, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/007-1.3483.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.5458 - acc: 0.5024 - val_loss: 1.3483 - val_acc: 0.5984\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4781 - acc: 0.5257\n",
      "Epoch 00008: val_loss improved from 1.34826 to 1.27266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/008-1.2727.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.4781 - acc: 0.5257 - val_loss: 1.2727 - val_acc: 0.6273\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4236 - acc: 0.5425\n",
      "Epoch 00009: val_loss improved from 1.27266 to 1.23765, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/009-1.2377.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.4237 - acc: 0.5425 - val_loss: 1.2377 - val_acc: 0.6345\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3734 - acc: 0.5619\n",
      "Epoch 00010: val_loss improved from 1.23765 to 1.18871, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/010-1.1887.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.3734 - acc: 0.5619 - val_loss: 1.1887 - val_acc: 0.6487\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3383 - acc: 0.5733\n",
      "Epoch 00011: val_loss improved from 1.18871 to 1.15911, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/011-1.1591.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.3383 - acc: 0.5733 - val_loss: 1.1591 - val_acc: 0.6557\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2993 - acc: 0.5898\n",
      "Epoch 00012: val_loss improved from 1.15911 to 1.09560, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/012-1.0956.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.2994 - acc: 0.5898 - val_loss: 1.0956 - val_acc: 0.6790\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2638 - acc: 0.6000\n",
      "Epoch 00013: val_loss improved from 1.09560 to 1.08581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/013-1.0858.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.2640 - acc: 0.5999 - val_loss: 1.0858 - val_acc: 0.6804\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2282 - acc: 0.6130\n",
      "Epoch 00014: val_loss improved from 1.08581 to 1.03556, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/014-1.0356.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.2282 - acc: 0.6130 - val_loss: 1.0356 - val_acc: 0.6956\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2011 - acc: 0.6231\n",
      "Epoch 00015: val_loss improved from 1.03556 to 1.01094, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/015-1.0109.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.2011 - acc: 0.6230 - val_loss: 1.0109 - val_acc: 0.6995\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1766 - acc: 0.6291\n",
      "Epoch 00016: val_loss improved from 1.01094 to 0.97959, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/016-0.9796.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1766 - acc: 0.6291 - val_loss: 0.9796 - val_acc: 0.7102\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1524 - acc: 0.6430\n",
      "Epoch 00017: val_loss improved from 0.97959 to 0.96006, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/017-0.9601.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1524 - acc: 0.6430 - val_loss: 0.9601 - val_acc: 0.7121\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1282 - acc: 0.6520\n",
      "Epoch 00018: val_loss improved from 0.96006 to 0.93489, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/018-0.9349.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1282 - acc: 0.6520 - val_loss: 0.9349 - val_acc: 0.7286\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1039 - acc: 0.6582\n",
      "Epoch 00019: val_loss improved from 0.93489 to 0.91183, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/019-0.9118.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.1040 - acc: 0.6582 - val_loss: 0.9118 - val_acc: 0.7324\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0893 - acc: 0.6626\n",
      "Epoch 00020: val_loss did not improve from 0.91183\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0895 - acc: 0.6626 - val_loss: 0.9126 - val_acc: 0.7319\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0735 - acc: 0.6677\n",
      "Epoch 00021: val_loss improved from 0.91183 to 0.88716, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/021-0.8872.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0734 - acc: 0.6677 - val_loss: 0.8872 - val_acc: 0.7391\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0462 - acc: 0.6765\n",
      "Epoch 00022: val_loss improved from 0.88716 to 0.86600, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/022-0.8660.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0461 - acc: 0.6765 - val_loss: 0.8660 - val_acc: 0.7466\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0325 - acc: 0.6801\n",
      "Epoch 00023: val_loss improved from 0.86600 to 0.84959, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/023-0.8496.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0325 - acc: 0.6801 - val_loss: 0.8496 - val_acc: 0.7540\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0092 - acc: 0.6879\n",
      "Epoch 00024: val_loss improved from 0.84959 to 0.84065, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/024-0.8407.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0092 - acc: 0.6879 - val_loss: 0.8407 - val_acc: 0.7503\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0053 - acc: 0.6923\n",
      "Epoch 00025: val_loss improved from 0.84065 to 0.82795, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/025-0.8280.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 1.0054 - acc: 0.6923 - val_loss: 0.8280 - val_acc: 0.7589\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9909 - acc: 0.6966\n",
      "Epoch 00026: val_loss improved from 0.82795 to 0.81439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/026-0.8144.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9909 - acc: 0.6966 - val_loss: 0.8144 - val_acc: 0.7596\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9764 - acc: 0.6982\n",
      "Epoch 00027: val_loss improved from 0.81439 to 0.80953, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/027-0.8095.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9764 - acc: 0.6982 - val_loss: 0.8095 - val_acc: 0.7636\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9591 - acc: 0.7058\n",
      "Epoch 00028: val_loss improved from 0.80953 to 0.80402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/028-0.8040.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9592 - acc: 0.7057 - val_loss: 0.8040 - val_acc: 0.7673\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9518 - acc: 0.7122\n",
      "Epoch 00029: val_loss improved from 0.80402 to 0.78642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/029-0.7864.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9522 - acc: 0.7122 - val_loss: 0.7864 - val_acc: 0.7701\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.7138\n",
      "Epoch 00030: val_loss improved from 0.78642 to 0.77276, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/030-0.7728.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9392 - acc: 0.7138 - val_loss: 0.7728 - val_acc: 0.7727\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9243 - acc: 0.7184\n",
      "Epoch 00031: val_loss improved from 0.77276 to 0.76443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/031-0.7644.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9243 - acc: 0.7184 - val_loss: 0.7644 - val_acc: 0.7827\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9162 - acc: 0.7206\n",
      "Epoch 00032: val_loss did not improve from 0.76443\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9163 - acc: 0.7206 - val_loss: 0.7650 - val_acc: 0.7864\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9077 - acc: 0.7239\n",
      "Epoch 00033: val_loss did not improve from 0.76443\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.9078 - acc: 0.7238 - val_loss: 0.7867 - val_acc: 0.7741\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7306\n",
      "Epoch 00034: val_loss improved from 0.76443 to 0.73416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/034-0.7342.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8967 - acc: 0.7306 - val_loss: 0.7342 - val_acc: 0.7906\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8890 - acc: 0.7312\n",
      "Epoch 00035: val_loss did not improve from 0.73416\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8890 - acc: 0.7312 - val_loss: 0.7356 - val_acc: 0.7836\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8796 - acc: 0.7325\n",
      "Epoch 00036: val_loss improved from 0.73416 to 0.71204, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/036-0.7120.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8796 - acc: 0.7325 - val_loss: 0.7120 - val_acc: 0.7997\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8661 - acc: 0.7392\n",
      "Epoch 00037: val_loss did not improve from 0.71204\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8660 - acc: 0.7392 - val_loss: 0.7229 - val_acc: 0.7908\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8631 - acc: 0.7409\n",
      "Epoch 00038: val_loss did not improve from 0.71204\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8630 - acc: 0.7409 - val_loss: 0.7138 - val_acc: 0.8001\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8547 - acc: 0.7419\n",
      "Epoch 00039: val_loss improved from 0.71204 to 0.70153, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/039-0.7015.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8547 - acc: 0.7419 - val_loss: 0.7015 - val_acc: 0.8032\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8514 - acc: 0.7447\n",
      "Epoch 00040: val_loss did not improve from 0.70153\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8513 - acc: 0.7448 - val_loss: 0.7234 - val_acc: 0.7952\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8381 - acc: 0.7487\n",
      "Epoch 00041: val_loss did not improve from 0.70153\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8381 - acc: 0.7487 - val_loss: 0.7097 - val_acc: 0.7966\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8329 - acc: 0.7536\n",
      "Epoch 00042: val_loss improved from 0.70153 to 0.69307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/042-0.6931.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8330 - acc: 0.7535 - val_loss: 0.6931 - val_acc: 0.8046\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8285 - acc: 0.7517\n",
      "Epoch 00043: val_loss did not improve from 0.69307\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8285 - acc: 0.7517 - val_loss: 0.6938 - val_acc: 0.8043\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7536\n",
      "Epoch 00044: val_loss improved from 0.69307 to 0.68881, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/044-0.6888.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8174 - acc: 0.7537 - val_loss: 0.6888 - val_acc: 0.8034\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.7575\n",
      "Epoch 00045: val_loss improved from 0.68881 to 0.67108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/045-0.6711.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8149 - acc: 0.7575 - val_loss: 0.6711 - val_acc: 0.8143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8061 - acc: 0.7592\n",
      "Epoch 00046: val_loss did not improve from 0.67108\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8062 - acc: 0.7592 - val_loss: 0.6841 - val_acc: 0.8104\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8021 - acc: 0.7594\n",
      "Epoch 00047: val_loss improved from 0.67108 to 0.65425, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/047-0.6543.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.8022 - acc: 0.7593 - val_loss: 0.6543 - val_acc: 0.8232\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7930 - acc: 0.7641\n",
      "Epoch 00048: val_loss did not improve from 0.65425\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7930 - acc: 0.7641 - val_loss: 0.6703 - val_acc: 0.8137\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7911 - acc: 0.7656\n",
      "Epoch 00049: val_loss improved from 0.65425 to 0.64771, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/049-0.6477.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7912 - acc: 0.7656 - val_loss: 0.6477 - val_acc: 0.8220\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7884 - acc: 0.7634\n",
      "Epoch 00050: val_loss improved from 0.64771 to 0.64469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/050-0.6447.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7884 - acc: 0.7634 - val_loss: 0.6447 - val_acc: 0.8269\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7782 - acc: 0.7688\n",
      "Epoch 00051: val_loss improved from 0.64469 to 0.63695, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/051-0.6369.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7782 - acc: 0.7687 - val_loss: 0.6369 - val_acc: 0.8258\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7696\n",
      "Epoch 00052: val_loss improved from 0.63695 to 0.63252, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/052-0.6325.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7737 - acc: 0.7696 - val_loss: 0.6325 - val_acc: 0.8234\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7685 - acc: 0.7733\n",
      "Epoch 00053: val_loss did not improve from 0.63252\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7688 - acc: 0.7733 - val_loss: 0.6380 - val_acc: 0.8262\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7739\n",
      "Epoch 00054: val_loss improved from 0.63252 to 0.62654, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/054-0.6265.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7704 - acc: 0.7739 - val_loss: 0.6265 - val_acc: 0.8325\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7632 - acc: 0.7742\n",
      "Epoch 00055: val_loss did not improve from 0.62654\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7633 - acc: 0.7742 - val_loss: 0.6358 - val_acc: 0.8197\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7772\n",
      "Epoch 00056: val_loss improved from 0.62654 to 0.61833, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/056-0.6183.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7551 - acc: 0.7772 - val_loss: 0.6183 - val_acc: 0.8314\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7533 - acc: 0.7782\n",
      "Epoch 00057: val_loss improved from 0.61833 to 0.61080, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/057-0.6108.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7533 - acc: 0.7782 - val_loss: 0.6108 - val_acc: 0.8358\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7474 - acc: 0.7806\n",
      "Epoch 00058: val_loss did not improve from 0.61080\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7473 - acc: 0.7807 - val_loss: 0.6158 - val_acc: 0.8302\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7393 - acc: 0.7804\n",
      "Epoch 00059: val_loss improved from 0.61080 to 0.60975, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/059-0.6097.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7393 - acc: 0.7805 - val_loss: 0.6097 - val_acc: 0.8372\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7809\n",
      "Epoch 00060: val_loss improved from 0.60975 to 0.60358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/060-0.6036.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7434 - acc: 0.7808 - val_loss: 0.6036 - val_acc: 0.8376\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7337 - acc: 0.7826\n",
      "Epoch 00061: val_loss did not improve from 0.60358\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7336 - acc: 0.7826 - val_loss: 0.6042 - val_acc: 0.8351\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7856\n",
      "Epoch 00062: val_loss did not improve from 0.60358\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7316 - acc: 0.7856 - val_loss: 0.6196 - val_acc: 0.8260\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7278 - acc: 0.7847\n",
      "Epoch 00063: val_loss improved from 0.60358 to 0.59849, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/063-0.5985.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7277 - acc: 0.7847 - val_loss: 0.5985 - val_acc: 0.8386\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7238 - acc: 0.7854\n",
      "Epoch 00064: val_loss improved from 0.59849 to 0.59185, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/064-0.5919.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7238 - acc: 0.7854 - val_loss: 0.5919 - val_acc: 0.8379\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7202 - acc: 0.7878\n",
      "Epoch 00065: val_loss improved from 0.59185 to 0.58415, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/065-0.5841.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7202 - acc: 0.7877 - val_loss: 0.5841 - val_acc: 0.8421\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.7886\n",
      "Epoch 00066: val_loss did not improve from 0.58415\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7156 - acc: 0.7886 - val_loss: 0.5882 - val_acc: 0.8442\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7151 - acc: 0.7912\n",
      "Epoch 00067: val_loss did not improve from 0.58415\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7150 - acc: 0.7912 - val_loss: 0.5915 - val_acc: 0.8400\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7095 - acc: 0.7939\n",
      "Epoch 00068: val_loss improved from 0.58415 to 0.58262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/068-0.5826.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7094 - acc: 0.7939 - val_loss: 0.5826 - val_acc: 0.8418\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7057 - acc: 0.7938\n",
      "Epoch 00069: val_loss improved from 0.58262 to 0.57526, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/069-0.5753.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7057 - acc: 0.7939 - val_loss: 0.5753 - val_acc: 0.8451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7066 - acc: 0.7923\n",
      "Epoch 00070: val_loss did not improve from 0.57526\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.7065 - acc: 0.7923 - val_loss: 0.5796 - val_acc: 0.8477\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6989 - acc: 0.7927\n",
      "Epoch 00071: val_loss did not improve from 0.57526\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6989 - acc: 0.7927 - val_loss: 0.5753 - val_acc: 0.8477\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6957 - acc: 0.7958\n",
      "Epoch 00072: val_loss improved from 0.57526 to 0.56705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/072-0.5671.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6957 - acc: 0.7957 - val_loss: 0.5671 - val_acc: 0.8472\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6910 - acc: 0.7995\n",
      "Epoch 00073: val_loss did not improve from 0.56705\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6910 - acc: 0.7995 - val_loss: 0.5688 - val_acc: 0.8481\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6856 - acc: 0.8004\n",
      "Epoch 00074: val_loss did not improve from 0.56705\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6856 - acc: 0.8004 - val_loss: 0.5704 - val_acc: 0.8488\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.8009\n",
      "Epoch 00075: val_loss did not improve from 0.56705\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6821 - acc: 0.8009 - val_loss: 0.5861 - val_acc: 0.8383\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6771 - acc: 0.8036\n",
      "Epoch 00076: val_loss improved from 0.56705 to 0.56009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/076-0.5601.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6770 - acc: 0.8036 - val_loss: 0.5601 - val_acc: 0.8444\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6821 - acc: 0.8032\n",
      "Epoch 00077: val_loss improved from 0.56009 to 0.55706, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/077-0.5571.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6821 - acc: 0.8032 - val_loss: 0.5571 - val_acc: 0.8484\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6730 - acc: 0.8037\n",
      "Epoch 00078: val_loss improved from 0.55706 to 0.55261, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/078-0.5526.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6729 - acc: 0.8037 - val_loss: 0.5526 - val_acc: 0.8488\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6666 - acc: 0.8053\n",
      "Epoch 00079: val_loss improved from 0.55261 to 0.54989, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/079-0.5499.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6666 - acc: 0.8053 - val_loss: 0.5499 - val_acc: 0.8523\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.8022\n",
      "Epoch 00080: val_loss did not improve from 0.54989\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6700 - acc: 0.8023 - val_loss: 0.5524 - val_acc: 0.8505\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.8065\n",
      "Epoch 00081: val_loss did not improve from 0.54989\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6672 - acc: 0.8065 - val_loss: 0.5536 - val_acc: 0.8549\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6587 - acc: 0.8062\n",
      "Epoch 00082: val_loss did not improve from 0.54989\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6586 - acc: 0.8062 - val_loss: 0.5583 - val_acc: 0.8465\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6581 - acc: 0.8063\n",
      "Epoch 00083: val_loss did not improve from 0.54989\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6581 - acc: 0.8063 - val_loss: 0.5506 - val_acc: 0.8521\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6573 - acc: 0.8080\n",
      "Epoch 00084: val_loss improved from 0.54989 to 0.54488, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/084-0.5449.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6573 - acc: 0.8080 - val_loss: 0.5449 - val_acc: 0.8505\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6558 - acc: 0.8107\n",
      "Epoch 00085: val_loss improved from 0.54488 to 0.53698, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/085-0.5370.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6558 - acc: 0.8107 - val_loss: 0.5370 - val_acc: 0.8556\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8101\n",
      "Epoch 00086: val_loss improved from 0.53698 to 0.53502, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/086-0.5350.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6512 - acc: 0.8101 - val_loss: 0.5350 - val_acc: 0.8581\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6502 - acc: 0.8095\n",
      "Epoch 00087: val_loss improved from 0.53502 to 0.53018, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/087-0.5302.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6502 - acc: 0.8095 - val_loss: 0.5302 - val_acc: 0.8586\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.8109\n",
      "Epoch 00088: val_loss improved from 0.53018 to 0.52008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/088-0.5201.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6533 - acc: 0.8109 - val_loss: 0.5201 - val_acc: 0.8588\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6445 - acc: 0.8126\n",
      "Epoch 00089: val_loss did not improve from 0.52008\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6444 - acc: 0.8126 - val_loss: 0.5250 - val_acc: 0.8637\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6397 - acc: 0.8141\n",
      "Epoch 00090: val_loss did not improve from 0.52008\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6398 - acc: 0.8141 - val_loss: 0.5303 - val_acc: 0.8630\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.8131\n",
      "Epoch 00091: val_loss did not improve from 0.52008\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6392 - acc: 0.8131 - val_loss: 0.5313 - val_acc: 0.8605\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6339 - acc: 0.8135\n",
      "Epoch 00092: val_loss improved from 0.52008 to 0.51416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/092-0.5142.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6338 - acc: 0.8134 - val_loss: 0.5142 - val_acc: 0.8614\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.8161\n",
      "Epoch 00093: val_loss did not improve from 0.51416\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6256 - acc: 0.8161 - val_loss: 0.5346 - val_acc: 0.8486\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.8172\n",
      "Epoch 00094: val_loss did not improve from 0.51416\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6282 - acc: 0.8172 - val_loss: 0.5221 - val_acc: 0.8612\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.8173\n",
      "Epoch 00095: val_loss did not improve from 0.51416\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6278 - acc: 0.8173 - val_loss: 0.5251 - val_acc: 0.8598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6259 - acc: 0.8193\n",
      "Epoch 00096: val_loss did not improve from 0.51416\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6258 - acc: 0.8193 - val_loss: 0.5162 - val_acc: 0.8640\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8189\n",
      "Epoch 00097: val_loss improved from 0.51416 to 0.51173, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/097-0.5117.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6176 - acc: 0.8189 - val_loss: 0.5117 - val_acc: 0.8633\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.8201\n",
      "Epoch 00098: val_loss improved from 0.51173 to 0.50502, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/098-0.5050.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6115 - acc: 0.8201 - val_loss: 0.5050 - val_acc: 0.8654\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8207\n",
      "Epoch 00099: val_loss did not improve from 0.50502\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6156 - acc: 0.8207 - val_loss: 0.5149 - val_acc: 0.8593\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6152 - acc: 0.8222\n",
      "Epoch 00100: val_loss did not improve from 0.50502\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6152 - acc: 0.8222 - val_loss: 0.5058 - val_acc: 0.8658\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6142 - acc: 0.8205\n",
      "Epoch 00101: val_loss improved from 0.50502 to 0.50165, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/101-0.5016.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6141 - acc: 0.8206 - val_loss: 0.5016 - val_acc: 0.8682\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.8245\n",
      "Epoch 00102: val_loss improved from 0.50165 to 0.50122, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/102-0.5012.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6047 - acc: 0.8245 - val_loss: 0.5012 - val_acc: 0.8647\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.8226\n",
      "Epoch 00103: val_loss improved from 0.50122 to 0.49745, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/103-0.4974.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6055 - acc: 0.8226 - val_loss: 0.4974 - val_acc: 0.8696\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.8239\n",
      "Epoch 00104: val_loss did not improve from 0.49745\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6046 - acc: 0.8239 - val_loss: 0.5010 - val_acc: 0.8670\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8257\n",
      "Epoch 00105: val_loss improved from 0.49745 to 0.49407, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/105-0.4941.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.6010 - acc: 0.8257 - val_loss: 0.4941 - val_acc: 0.8693\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8259\n",
      "Epoch 00106: val_loss did not improve from 0.49407\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5973 - acc: 0.8259 - val_loss: 0.5020 - val_acc: 0.8651\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.8260\n",
      "Epoch 00107: val_loss improved from 0.49407 to 0.48924, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/107-0.4892.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5994 - acc: 0.8260 - val_loss: 0.4892 - val_acc: 0.8714\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8264\n",
      "Epoch 00108: val_loss did not improve from 0.48924\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5929 - acc: 0.8264 - val_loss: 0.4998 - val_acc: 0.8628\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5921 - acc: 0.8284\n",
      "Epoch 00109: val_loss did not improve from 0.48924\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5920 - acc: 0.8284 - val_loss: 0.5007 - val_acc: 0.8656\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8287\n",
      "Epoch 00110: val_loss improved from 0.48924 to 0.48652, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/110-0.4865.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5886 - acc: 0.8286 - val_loss: 0.4865 - val_acc: 0.8714\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.8285\n",
      "Epoch 00111: val_loss improved from 0.48652 to 0.48233, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/111-0.4823.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5888 - acc: 0.8285 - val_loss: 0.4823 - val_acc: 0.8726\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8288\n",
      "Epoch 00112: val_loss did not improve from 0.48233\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5842 - acc: 0.8288 - val_loss: 0.4850 - val_acc: 0.8691\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.8298\n",
      "Epoch 00113: val_loss improved from 0.48233 to 0.48113, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/113-0.4811.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5856 - acc: 0.8298 - val_loss: 0.4811 - val_acc: 0.8756\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8335\n",
      "Epoch 00114: val_loss did not improve from 0.48113\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5769 - acc: 0.8335 - val_loss: 0.5010 - val_acc: 0.8621\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.8329\n",
      "Epoch 00115: val_loss improved from 0.48113 to 0.47777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/115-0.4778.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5763 - acc: 0.8329 - val_loss: 0.4778 - val_acc: 0.8728\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8326\n",
      "Epoch 00116: val_loss did not improve from 0.47777\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5744 - acc: 0.8325 - val_loss: 0.4868 - val_acc: 0.8677\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5805 - acc: 0.8295\n",
      "Epoch 00117: val_loss improved from 0.47777 to 0.47287, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/117-0.4729.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5804 - acc: 0.8295 - val_loss: 0.4729 - val_acc: 0.8775\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8343\n",
      "Epoch 00118: val_loss did not improve from 0.47287\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5670 - acc: 0.8343 - val_loss: 0.4786 - val_acc: 0.8742\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5734 - acc: 0.8314\n",
      "Epoch 00119: val_loss did not improve from 0.47287\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5735 - acc: 0.8314 - val_loss: 0.4748 - val_acc: 0.8782\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.8338\n",
      "Epoch 00120: val_loss improved from 0.47287 to 0.46767, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/120-0.4677.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5700 - acc: 0.8338 - val_loss: 0.4677 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5687 - acc: 0.8347\n",
      "Epoch 00121: val_loss did not improve from 0.46767\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5687 - acc: 0.8346 - val_loss: 0.4764 - val_acc: 0.8754\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.8346\n",
      "Epoch 00122: val_loss did not improve from 0.46767\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5670 - acc: 0.8346 - val_loss: 0.4682 - val_acc: 0.8772\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8372\n",
      "Epoch 00123: val_loss did not improve from 0.46767\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5602 - acc: 0.8372 - val_loss: 0.4832 - val_acc: 0.8654\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.8372\n",
      "Epoch 00124: val_loss improved from 0.46767 to 0.45938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/124-0.4594.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5562 - acc: 0.8372 - val_loss: 0.4594 - val_acc: 0.8754\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8378\n",
      "Epoch 00125: val_loss did not improve from 0.45938\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5616 - acc: 0.8378 - val_loss: 0.4687 - val_acc: 0.8786\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8384\n",
      "Epoch 00126: val_loss did not improve from 0.45938\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5557 - acc: 0.8384 - val_loss: 0.4922 - val_acc: 0.8668\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8382\n",
      "Epoch 00127: val_loss improved from 0.45938 to 0.45926, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/127-0.4593.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5599 - acc: 0.8382 - val_loss: 0.4593 - val_acc: 0.8761\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8389\n",
      "Epoch 00128: val_loss did not improve from 0.45926\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5529 - acc: 0.8389 - val_loss: 0.4617 - val_acc: 0.8779\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8385\n",
      "Epoch 00129: val_loss did not improve from 0.45926\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5519 - acc: 0.8385 - val_loss: 0.4680 - val_acc: 0.8789\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8397\n",
      "Epoch 00130: val_loss improved from 0.45926 to 0.45101, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/130-0.4510.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5529 - acc: 0.8397 - val_loss: 0.4510 - val_acc: 0.8798\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8408\n",
      "Epoch 00131: val_loss did not improve from 0.45101\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5497 - acc: 0.8408 - val_loss: 0.4568 - val_acc: 0.8791\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8428\n",
      "Epoch 00132: val_loss did not improve from 0.45101\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5392 - acc: 0.8428 - val_loss: 0.4533 - val_acc: 0.8833\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8412\n",
      "Epoch 00133: val_loss did not improve from 0.45101\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5458 - acc: 0.8412 - val_loss: 0.4720 - val_acc: 0.8763\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5470 - acc: 0.8423\n",
      "Epoch 00134: val_loss did not improve from 0.45101\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5471 - acc: 0.8423 - val_loss: 0.4529 - val_acc: 0.8800\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5375 - acc: 0.8441\n",
      "Epoch 00135: val_loss improved from 0.45101 to 0.44647, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/135-0.4465.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5375 - acc: 0.8440 - val_loss: 0.4465 - val_acc: 0.8817\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.8451\n",
      "Epoch 00136: val_loss did not improve from 0.44647\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5328 - acc: 0.8451 - val_loss: 0.4526 - val_acc: 0.8819\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8432\n",
      "Epoch 00137: val_loss improved from 0.44647 to 0.43930, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/137-0.4393.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5346 - acc: 0.8432 - val_loss: 0.4393 - val_acc: 0.8840\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.8443\n",
      "Epoch 00138: val_loss did not improve from 0.43930\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5349 - acc: 0.8443 - val_loss: 0.4477 - val_acc: 0.8796\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.8451\n",
      "Epoch 00139: val_loss did not improve from 0.43930\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5263 - acc: 0.8451 - val_loss: 0.4485 - val_acc: 0.8856\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5306 - acc: 0.8446\n",
      "Epoch 00140: val_loss did not improve from 0.43930\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5305 - acc: 0.8446 - val_loss: 0.4469 - val_acc: 0.8840\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8482\n",
      "Epoch 00141: val_loss did not improve from 0.43930\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5269 - acc: 0.8482 - val_loss: 0.4429 - val_acc: 0.8791\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.8481\n",
      "Epoch 00142: val_loss improved from 0.43930 to 0.43924, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/142-0.4392.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5244 - acc: 0.8481 - val_loss: 0.4392 - val_acc: 0.8784\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8471\n",
      "Epoch 00143: val_loss did not improve from 0.43924\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5202 - acc: 0.8471 - val_loss: 0.4483 - val_acc: 0.8814\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.8489\n",
      "Epoch 00144: val_loss improved from 0.43924 to 0.43853, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/144-0.4385.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5206 - acc: 0.8490 - val_loss: 0.4385 - val_acc: 0.8849\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8494\n",
      "Epoch 00145: val_loss did not improve from 0.43853\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5262 - acc: 0.8494 - val_loss: 0.4613 - val_acc: 0.8754\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8466\n",
      "Epoch 00146: val_loss improved from 0.43853 to 0.43740, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/146-0.4374.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5276 - acc: 0.8466 - val_loss: 0.4374 - val_acc: 0.8821\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5176 - acc: 0.8483\n",
      "Epoch 00147: val_loss did not improve from 0.43740\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5176 - acc: 0.8484 - val_loss: 0.4446 - val_acc: 0.8803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5169 - acc: 0.8494\n",
      "Epoch 00148: val_loss did not improve from 0.43740\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5169 - acc: 0.8494 - val_loss: 0.4375 - val_acc: 0.8807\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.8489\n",
      "Epoch 00149: val_loss did not improve from 0.43740\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5157 - acc: 0.8489 - val_loss: 0.4432 - val_acc: 0.8810\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.8511\n",
      "Epoch 00150: val_loss improved from 0.43740 to 0.43139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/150-0.4314.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5179 - acc: 0.8512 - val_loss: 0.4314 - val_acc: 0.8842\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8510\n",
      "Epoch 00151: val_loss did not improve from 0.43139\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5120 - acc: 0.8510 - val_loss: 0.4440 - val_acc: 0.8812\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8503\n",
      "Epoch 00152: val_loss did not improve from 0.43139\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5145 - acc: 0.8503 - val_loss: 0.4431 - val_acc: 0.8798\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5065 - acc: 0.8523\n",
      "Epoch 00153: val_loss did not improve from 0.43139\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5064 - acc: 0.8524 - val_loss: 0.4434 - val_acc: 0.8819\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8511\n",
      "Epoch 00154: val_loss did not improve from 0.43139\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5074 - acc: 0.8511 - val_loss: 0.4342 - val_acc: 0.8889\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8526\n",
      "Epoch 00155: val_loss did not improve from 0.43139\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5039 - acc: 0.8527 - val_loss: 0.4421 - val_acc: 0.8835\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8550\n",
      "Epoch 00156: val_loss improved from 0.43139 to 0.42861, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/156-0.4286.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5035 - acc: 0.8550 - val_loss: 0.4286 - val_acc: 0.8845\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8534\n",
      "Epoch 00157: val_loss improved from 0.42861 to 0.42438, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/157-0.4244.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5017 - acc: 0.8534 - val_loss: 0.4244 - val_acc: 0.8905\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8551\n",
      "Epoch 00158: val_loss did not improve from 0.42438\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4956 - acc: 0.8551 - val_loss: 0.4298 - val_acc: 0.8791\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8547\n",
      "Epoch 00159: val_loss did not improve from 0.42438\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4968 - acc: 0.8547 - val_loss: 0.4422 - val_acc: 0.8824\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8544\n",
      "Epoch 00160: val_loss did not improve from 0.42438\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.5018 - acc: 0.8544 - val_loss: 0.4426 - val_acc: 0.8761\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8540\n",
      "Epoch 00161: val_loss did not improve from 0.42438\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4955 - acc: 0.8540 - val_loss: 0.4275 - val_acc: 0.8905\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4900 - acc: 0.8569\n",
      "Epoch 00162: val_loss did not improve from 0.42438\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4899 - acc: 0.8569 - val_loss: 0.4255 - val_acc: 0.8884\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8555\n",
      "Epoch 00163: val_loss improved from 0.42438 to 0.41879, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/163-0.4188.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4934 - acc: 0.8555 - val_loss: 0.4188 - val_acc: 0.8889\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4959 - acc: 0.8550\n",
      "Epoch 00164: val_loss did not improve from 0.41879\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4959 - acc: 0.8550 - val_loss: 0.4376 - val_acc: 0.8814\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8582\n",
      "Epoch 00165: val_loss improved from 0.41879 to 0.41725, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/165-0.4172.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4894 - acc: 0.8582 - val_loss: 0.4172 - val_acc: 0.8877\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8595\n",
      "Epoch 00166: val_loss did not improve from 0.41725\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4833 - acc: 0.8595 - val_loss: 0.4179 - val_acc: 0.8868\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8571\n",
      "Epoch 00167: val_loss did not improve from 0.41725\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4842 - acc: 0.8572 - val_loss: 0.4233 - val_acc: 0.8887\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.8615\n",
      "Epoch 00168: val_loss improved from 0.41725 to 0.41513, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/168-0.4151.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4798 - acc: 0.8615 - val_loss: 0.4151 - val_acc: 0.8908\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.8577\n",
      "Epoch 00169: val_loss improved from 0.41513 to 0.41046, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/169-0.4105.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4857 - acc: 0.8577 - val_loss: 0.4105 - val_acc: 0.8880\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8601\n",
      "Epoch 00170: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4788 - acc: 0.8601 - val_loss: 0.4188 - val_acc: 0.8947\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.8592\n",
      "Epoch 00171: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4780 - acc: 0.8591 - val_loss: 0.4161 - val_acc: 0.8889\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8590\n",
      "Epoch 00172: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4788 - acc: 0.8591 - val_loss: 0.4224 - val_acc: 0.8877\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.8613\n",
      "Epoch 00173: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4743 - acc: 0.8613 - val_loss: 0.4175 - val_acc: 0.8880\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4753 - acc: 0.8610\n",
      "Epoch 00174: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4753 - acc: 0.8611 - val_loss: 0.4127 - val_acc: 0.8908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4717 - acc: 0.8632\n",
      "Epoch 00175: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4717 - acc: 0.8633 - val_loss: 0.4206 - val_acc: 0.8868\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8622\n",
      "Epoch 00176: val_loss did not improve from 0.41046\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4703 - acc: 0.8622 - val_loss: 0.4181 - val_acc: 0.8849\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4753 - acc: 0.8609\n",
      "Epoch 00177: val_loss improved from 0.41046 to 0.40969, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/177-0.4097.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4753 - acc: 0.8609 - val_loss: 0.4097 - val_acc: 0.8910\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8640\n",
      "Epoch 00178: val_loss did not improve from 0.40969\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4672 - acc: 0.8640 - val_loss: 0.4171 - val_acc: 0.8882\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.8614\n",
      "Epoch 00179: val_loss did not improve from 0.40969\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4712 - acc: 0.8614 - val_loss: 0.4141 - val_acc: 0.8915\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8608\n",
      "Epoch 00180: val_loss did not improve from 0.40969\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4706 - acc: 0.8608 - val_loss: 0.4114 - val_acc: 0.8954\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4680 - acc: 0.8616\n",
      "Epoch 00181: val_loss did not improve from 0.40969\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4679 - acc: 0.8616 - val_loss: 0.4177 - val_acc: 0.8921\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4664 - acc: 0.8639\n",
      "Epoch 00182: val_loss improved from 0.40969 to 0.40454, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/182-0.4045.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4664 - acc: 0.8639 - val_loss: 0.4045 - val_acc: 0.8942\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8634\n",
      "Epoch 00183: val_loss improved from 0.40454 to 0.40307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/183-0.4031.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4631 - acc: 0.8634 - val_loss: 0.4031 - val_acc: 0.8921\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8646\n",
      "Epoch 00184: val_loss did not improve from 0.40307\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4609 - acc: 0.8645 - val_loss: 0.4050 - val_acc: 0.8973\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4605 - acc: 0.8656\n",
      "Epoch 00185: val_loss improved from 0.40307 to 0.40161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/185-0.4016.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4604 - acc: 0.8656 - val_loss: 0.4016 - val_acc: 0.8917\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8663\n",
      "Epoch 00186: val_loss did not improve from 0.40161\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4556 - acc: 0.8663 - val_loss: 0.4049 - val_acc: 0.8947\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8647\n",
      "Epoch 00187: val_loss did not improve from 0.40161\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4608 - acc: 0.8647 - val_loss: 0.4081 - val_acc: 0.8917\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8645\n",
      "Epoch 00188: val_loss did not improve from 0.40161\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4566 - acc: 0.8645 - val_loss: 0.4106 - val_acc: 0.8896\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8632\n",
      "Epoch 00189: val_loss did not improve from 0.40161\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4611 - acc: 0.8632 - val_loss: 0.4079 - val_acc: 0.8921\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8659\n",
      "Epoch 00190: val_loss did not improve from 0.40161\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4539 - acc: 0.8659 - val_loss: 0.4030 - val_acc: 0.8954\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8684\n",
      "Epoch 00191: val_loss improved from 0.40161 to 0.39448, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/191-0.3945.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4488 - acc: 0.8684 - val_loss: 0.3945 - val_acc: 0.8970\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8679\n",
      "Epoch 00192: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4491 - acc: 0.8679 - val_loss: 0.3996 - val_acc: 0.8977\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8687\n",
      "Epoch 00193: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4450 - acc: 0.8687 - val_loss: 0.4070 - val_acc: 0.8928\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8692\n",
      "Epoch 00194: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4445 - acc: 0.8692 - val_loss: 0.4153 - val_acc: 0.8898\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8667\n",
      "Epoch 00195: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4527 - acc: 0.8668 - val_loss: 0.4023 - val_acc: 0.8963\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8691\n",
      "Epoch 00196: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4428 - acc: 0.8691 - val_loss: 0.4142 - val_acc: 0.8887\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8674\n",
      "Epoch 00197: val_loss did not improve from 0.39448\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4480 - acc: 0.8674 - val_loss: 0.3953 - val_acc: 0.8940\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4445 - acc: 0.8696\n",
      "Epoch 00198: val_loss improved from 0.39448 to 0.38903, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/198-0.3890.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4446 - acc: 0.8696 - val_loss: 0.3890 - val_acc: 0.8975\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8693\n",
      "Epoch 00199: val_loss did not improve from 0.38903\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4435 - acc: 0.8693 - val_loss: 0.3999 - val_acc: 0.8977\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.8702\n",
      "Epoch 00200: val_loss did not improve from 0.38903\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4414 - acc: 0.8702 - val_loss: 0.3990 - val_acc: 0.8931\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8729\n",
      "Epoch 00201: val_loss did not improve from 0.38903\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4364 - acc: 0.8728 - val_loss: 0.3961 - val_acc: 0.8998\n",
      "Epoch 202/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8738\n",
      "Epoch 00202: val_loss improved from 0.38903 to 0.38870, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/202-0.3887.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4338 - acc: 0.8737 - val_loss: 0.3887 - val_acc: 0.8994\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8743\n",
      "Epoch 00203: val_loss improved from 0.38870 to 0.38800, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/203-0.3880.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4323 - acc: 0.8743 - val_loss: 0.3880 - val_acc: 0.8961\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8712\n",
      "Epoch 00204: val_loss did not improve from 0.38800\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4376 - acc: 0.8712 - val_loss: 0.3916 - val_acc: 0.8973\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8709\n",
      "Epoch 00205: val_loss did not improve from 0.38800\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4356 - acc: 0.8709 - val_loss: 0.3889 - val_acc: 0.9005\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8737\n",
      "Epoch 00206: val_loss improved from 0.38800 to 0.38212, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/206-0.3821.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4316 - acc: 0.8737 - val_loss: 0.3821 - val_acc: 0.8994\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8740\n",
      "Epoch 00207: val_loss did not improve from 0.38212\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4299 - acc: 0.8740 - val_loss: 0.3908 - val_acc: 0.8956\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4280 - acc: 0.8735\n",
      "Epoch 00208: val_loss did not improve from 0.38212\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4279 - acc: 0.8735 - val_loss: 0.3843 - val_acc: 0.8970\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8720\n",
      "Epoch 00209: val_loss did not improve from 0.38212\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4284 - acc: 0.8720 - val_loss: 0.3897 - val_acc: 0.9010\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8758\n",
      "Epoch 00210: val_loss did not improve from 0.38212\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4256 - acc: 0.8758 - val_loss: 0.3998 - val_acc: 0.8966\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.8732\n",
      "Epoch 00211: val_loss improved from 0.38212 to 0.37905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/211-0.3791.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4302 - acc: 0.8732 - val_loss: 0.3791 - val_acc: 0.9019\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8742\n",
      "Epoch 00212: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4313 - acc: 0.8742 - val_loss: 0.3887 - val_acc: 0.8970\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8735\n",
      "Epoch 00213: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4270 - acc: 0.8735 - val_loss: 0.3819 - val_acc: 0.9003\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8734\n",
      "Epoch 00214: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4264 - acc: 0.8734 - val_loss: 0.3986 - val_acc: 0.8956\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8762\n",
      "Epoch 00215: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4168 - acc: 0.8762 - val_loss: 0.3921 - val_acc: 0.8987\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8739\n",
      "Epoch 00216: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4276 - acc: 0.8739 - val_loss: 0.3866 - val_acc: 0.8968\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4239 - acc: 0.8741\n",
      "Epoch 00217: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4238 - acc: 0.8741 - val_loss: 0.3884 - val_acc: 0.9012\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.8764\n",
      "Epoch 00218: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4182 - acc: 0.8764 - val_loss: 0.3918 - val_acc: 0.8980\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8762\n",
      "Epoch 00219: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.4155 - acc: 0.8762 - val_loss: 0.3889 - val_acc: 0.8989\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8764\n",
      "Epoch 00220: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4201 - acc: 0.8764 - val_loss: 0.3843 - val_acc: 0.9029\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8765\n",
      "Epoch 00221: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4163 - acc: 0.8765 - val_loss: 0.3920 - val_acc: 0.9012\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8769\n",
      "Epoch 00222: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4173 - acc: 0.8769 - val_loss: 0.3869 - val_acc: 0.9019\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8787\n",
      "Epoch 00223: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4127 - acc: 0.8787 - val_loss: 0.3922 - val_acc: 0.8963\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8799\n",
      "Epoch 00224: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4116 - acc: 0.8798 - val_loss: 0.3910 - val_acc: 0.9017\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8793\n",
      "Epoch 00225: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4097 - acc: 0.8792 - val_loss: 0.3863 - val_acc: 0.9012\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8785\n",
      "Epoch 00226: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4091 - acc: 0.8785 - val_loss: 0.3929 - val_acc: 0.9003\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8811\n",
      "Epoch 00227: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4070 - acc: 0.8811 - val_loss: 0.3841 - val_acc: 0.9031\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8786\n",
      "Epoch 00228: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4094 - acc: 0.8786 - val_loss: 0.3791 - val_acc: 0.9017\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8770\n",
      "Epoch 00229: val_loss did not improve from 0.37905\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4127 - acc: 0.8769 - val_loss: 0.3888 - val_acc: 0.8996\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8788\n",
      "Epoch 00230: val_loss improved from 0.37905 to 0.37794, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/230-0.3779.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4058 - acc: 0.8788 - val_loss: 0.3779 - val_acc: 0.9005\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8788\n",
      "Epoch 00231: val_loss did not improve from 0.37794\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4075 - acc: 0.8788 - val_loss: 0.3873 - val_acc: 0.8991\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8797\n",
      "Epoch 00232: val_loss did not improve from 0.37794\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4060 - acc: 0.8797 - val_loss: 0.3951 - val_acc: 0.8977\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8784\n",
      "Epoch 00233: val_loss did not improve from 0.37794\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4080 - acc: 0.8784 - val_loss: 0.3892 - val_acc: 0.8954\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8817\n",
      "Epoch 00234: val_loss did not improve from 0.37794\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3987 - acc: 0.8817 - val_loss: 0.3865 - val_acc: 0.9031\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8794\n",
      "Epoch 00235: val_loss improved from 0.37794 to 0.37703, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/235-0.3770.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4030 - acc: 0.8794 - val_loss: 0.3770 - val_acc: 0.9010\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8816\n",
      "Epoch 00236: val_loss did not improve from 0.37703\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3983 - acc: 0.8816 - val_loss: 0.3857 - val_acc: 0.9008\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8816\n",
      "Epoch 00237: val_loss did not improve from 0.37703\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3974 - acc: 0.8816 - val_loss: 0.3861 - val_acc: 0.9001\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8814\n",
      "Epoch 00238: val_loss did not improve from 0.37703\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.4042 - acc: 0.8814 - val_loss: 0.3860 - val_acc: 0.9026\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8823\n",
      "Epoch 00239: val_loss did not improve from 0.37703\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3970 - acc: 0.8822 - val_loss: 0.3779 - val_acc: 0.9017\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8824\n",
      "Epoch 00240: val_loss improved from 0.37703 to 0.37059, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/240-0.3706.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3956 - acc: 0.8824 - val_loss: 0.3706 - val_acc: 0.9043\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8828\n",
      "Epoch 00241: val_loss improved from 0.37059 to 0.36920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/241-0.3692.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3958 - acc: 0.8828 - val_loss: 0.3692 - val_acc: 0.9036\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.8832\n",
      "Epoch 00242: val_loss did not improve from 0.36920\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3943 - acc: 0.8832 - val_loss: 0.3800 - val_acc: 0.9019\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8821\n",
      "Epoch 00243: val_loss improved from 0.36920 to 0.36661, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/243-0.3666.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3954 - acc: 0.8821 - val_loss: 0.3666 - val_acc: 0.9017\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8825\n",
      "Epoch 00244: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3963 - acc: 0.8825 - val_loss: 0.3697 - val_acc: 0.9054\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8841\n",
      "Epoch 00245: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3913 - acc: 0.8841 - val_loss: 0.3734 - val_acc: 0.9036\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8833\n",
      "Epoch 00246: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3952 - acc: 0.8833 - val_loss: 0.3748 - val_acc: 0.9010\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8836\n",
      "Epoch 00247: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3934 - acc: 0.8836 - val_loss: 0.3792 - val_acc: 0.9017\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8832\n",
      "Epoch 00248: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3921 - acc: 0.8832 - val_loss: 0.3863 - val_acc: 0.8959\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8839\n",
      "Epoch 00249: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3929 - acc: 0.8839 - val_loss: 0.3740 - val_acc: 0.9010\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8841\n",
      "Epoch 00250: val_loss did not improve from 0.36661\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3875 - acc: 0.8841 - val_loss: 0.3726 - val_acc: 0.9024\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8838\n",
      "Epoch 00251: val_loss improved from 0.36661 to 0.36627, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/251-0.3663.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3879 - acc: 0.8838 - val_loss: 0.3663 - val_acc: 0.9045\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3898 - acc: 0.8853\n",
      "Epoch 00252: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3900 - acc: 0.8853 - val_loss: 0.3747 - val_acc: 0.9024\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8844\n",
      "Epoch 00253: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3847 - acc: 0.8844 - val_loss: 0.3697 - val_acc: 0.9015\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8841\n",
      "Epoch 00254: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3833 - acc: 0.8841 - val_loss: 0.3733 - val_acc: 0.9019\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8870\n",
      "Epoch 00255: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3789 - acc: 0.8870 - val_loss: 0.3706 - val_acc: 0.9026\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8858\n",
      "Epoch 00256: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3808 - acc: 0.8858 - val_loss: 0.3789 - val_acc: 0.9033\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8864\n",
      "Epoch 00257: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3867 - acc: 0.8864 - val_loss: 0.3699 - val_acc: 0.9033\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8867\n",
      "Epoch 00258: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3803 - acc: 0.8868 - val_loss: 0.3762 - val_acc: 0.9061\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8883\n",
      "Epoch 00259: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3770 - acc: 0.8883 - val_loss: 0.3734 - val_acc: 0.9036\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8895\n",
      "Epoch 00260: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3758 - acc: 0.8895 - val_loss: 0.3853 - val_acc: 0.8982\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8878\n",
      "Epoch 00261: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3762 - acc: 0.8878 - val_loss: 0.3771 - val_acc: 0.9012\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8886\n",
      "Epoch 00262: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3759 - acc: 0.8886 - val_loss: 0.3739 - val_acc: 0.9047\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8890\n",
      "Epoch 00263: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3733 - acc: 0.8890 - val_loss: 0.3890 - val_acc: 0.9001\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8888\n",
      "Epoch 00264: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3747 - acc: 0.8887 - val_loss: 0.3744 - val_acc: 0.9031\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8860\n",
      "Epoch 00265: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3806 - acc: 0.8860 - val_loss: 0.3663 - val_acc: 0.9052\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8898\n",
      "Epoch 00266: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3691 - acc: 0.8898 - val_loss: 0.3767 - val_acc: 0.9043\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8909\n",
      "Epoch 00267: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3674 - acc: 0.8909 - val_loss: 0.3711 - val_acc: 0.9036\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3698 - acc: 0.8880\n",
      "Epoch 00268: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3698 - acc: 0.8880 - val_loss: 0.3716 - val_acc: 0.9057\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8905\n",
      "Epoch 00269: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3668 - acc: 0.8905 - val_loss: 0.3752 - val_acc: 0.9024\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8901\n",
      "Epoch 00270: val_loss did not improve from 0.36627\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3656 - acc: 0.8900 - val_loss: 0.3838 - val_acc: 0.9008\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8895\n",
      "Epoch 00271: val_loss improved from 0.36627 to 0.35811, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/271-0.3581.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3689 - acc: 0.8895 - val_loss: 0.3581 - val_acc: 0.9089\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8884\n",
      "Epoch 00272: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3696 - acc: 0.8884 - val_loss: 0.3755 - val_acc: 0.9024\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8901\n",
      "Epoch 00273: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3685 - acc: 0.8901 - val_loss: 0.3660 - val_acc: 0.9017\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8895\n",
      "Epoch 00274: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3693 - acc: 0.8895 - val_loss: 0.3608 - val_acc: 0.9031\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8912\n",
      "Epoch 00275: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3619 - acc: 0.8912 - val_loss: 0.3616 - val_acc: 0.9043\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8919\n",
      "Epoch 00276: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3633 - acc: 0.8919 - val_loss: 0.3630 - val_acc: 0.9054\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8908\n",
      "Epoch 00277: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3670 - acc: 0.8908 - val_loss: 0.3732 - val_acc: 0.9054\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3644 - acc: 0.8910\n",
      "Epoch 00278: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3644 - acc: 0.8910 - val_loss: 0.3711 - val_acc: 0.9024\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8905\n",
      "Epoch 00279: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3637 - acc: 0.8905 - val_loss: 0.3652 - val_acc: 0.9054\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8907\n",
      "Epoch 00280: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3630 - acc: 0.8907 - val_loss: 0.3632 - val_acc: 0.9064\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8924\n",
      "Epoch 00281: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3614 - acc: 0.8924 - val_loss: 0.3679 - val_acc: 0.9052\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8907\n",
      "Epoch 00282: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3586 - acc: 0.8907 - val_loss: 0.3779 - val_acc: 0.9054\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8943\n",
      "Epoch 00283: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3569 - acc: 0.8943 - val_loss: 0.3658 - val_acc: 0.9061\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8915\n",
      "Epoch 00284: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3629 - acc: 0.8915 - val_loss: 0.3738 - val_acc: 0.9029\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8917\n",
      "Epoch 00285: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3629 - acc: 0.8917 - val_loss: 0.3786 - val_acc: 0.9047\n",
      "Epoch 286/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8940\n",
      "Epoch 00286: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3560 - acc: 0.8940 - val_loss: 0.3621 - val_acc: 0.9082\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.8932\n",
      "Epoch 00287: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3601 - acc: 0.8932 - val_loss: 0.3618 - val_acc: 0.9038\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8941\n",
      "Epoch 00288: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3520 - acc: 0.8941 - val_loss: 0.3664 - val_acc: 0.9092\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8939\n",
      "Epoch 00289: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3550 - acc: 0.8938 - val_loss: 0.3651 - val_acc: 0.9082\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8960\n",
      "Epoch 00290: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3503 - acc: 0.8959 - val_loss: 0.3613 - val_acc: 0.9085\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8939\n",
      "Epoch 00291: val_loss did not improve from 0.35811\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3540 - acc: 0.8940 - val_loss: 0.3630 - val_acc: 0.9087\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8942\n",
      "Epoch 00292: val_loss improved from 0.35811 to 0.35676, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/292-0.3568.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3497 - acc: 0.8943 - val_loss: 0.3568 - val_acc: 0.9085\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8953\n",
      "Epoch 00293: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3471 - acc: 0.8953 - val_loss: 0.3645 - val_acc: 0.9057\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8932\n",
      "Epoch 00294: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3502 - acc: 0.8932 - val_loss: 0.3606 - val_acc: 0.9085\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8950\n",
      "Epoch 00295: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3474 - acc: 0.8950 - val_loss: 0.3691 - val_acc: 0.9040\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8960\n",
      "Epoch 00296: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3494 - acc: 0.8960 - val_loss: 0.3668 - val_acc: 0.9101\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8948\n",
      "Epoch 00297: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3455 - acc: 0.8948 - val_loss: 0.3628 - val_acc: 0.9075\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8953\n",
      "Epoch 00298: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3510 - acc: 0.8953 - val_loss: 0.3654 - val_acc: 0.9052\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8923\n",
      "Epoch 00299: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3542 - acc: 0.8923 - val_loss: 0.3599 - val_acc: 0.9059\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8970\n",
      "Epoch 00300: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3455 - acc: 0.8971 - val_loss: 0.3598 - val_acc: 0.9066\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8977\n",
      "Epoch 00301: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3451 - acc: 0.8977 - val_loss: 0.3662 - val_acc: 0.9071\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8959\n",
      "Epoch 00302: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3438 - acc: 0.8959 - val_loss: 0.3576 - val_acc: 0.9061\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8978\n",
      "Epoch 00303: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3447 - acc: 0.8978 - val_loss: 0.3624 - val_acc: 0.9045\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8958\n",
      "Epoch 00304: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3427 - acc: 0.8958 - val_loss: 0.3703 - val_acc: 0.9057\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8992\n",
      "Epoch 00305: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3420 - acc: 0.8993 - val_loss: 0.3582 - val_acc: 0.9066\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8973\n",
      "Epoch 00306: val_loss did not improve from 0.35676\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3385 - acc: 0.8973 - val_loss: 0.3701 - val_acc: 0.9066\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8986\n",
      "Epoch 00307: val_loss improved from 0.35676 to 0.35662, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/307-0.3566.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3384 - acc: 0.8986 - val_loss: 0.3566 - val_acc: 0.9101\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8974\n",
      "Epoch 00308: val_loss improved from 0.35662 to 0.35421, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/308-0.3542.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3399 - acc: 0.8975 - val_loss: 0.3542 - val_acc: 0.9106\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8977\n",
      "Epoch 00309: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3396 - acc: 0.8978 - val_loss: 0.3611 - val_acc: 0.9087\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8994\n",
      "Epoch 00310: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3403 - acc: 0.8994 - val_loss: 0.3626 - val_acc: 0.9082\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8983\n",
      "Epoch 00311: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3395 - acc: 0.8983 - val_loss: 0.3638 - val_acc: 0.9085\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8998\n",
      "Epoch 00312: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3339 - acc: 0.8998 - val_loss: 0.3589 - val_acc: 0.9078\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.9004\n",
      "Epoch 00313: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3322 - acc: 0.9004 - val_loss: 0.3596 - val_acc: 0.9115\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.9005\n",
      "Epoch 00314: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3347 - acc: 0.9005 - val_loss: 0.3638 - val_acc: 0.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8979\n",
      "Epoch 00315: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3341 - acc: 0.8979 - val_loss: 0.3687 - val_acc: 0.9066\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8993\n",
      "Epoch 00316: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3347 - acc: 0.8993 - val_loss: 0.3639 - val_acc: 0.9094\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9007\n",
      "Epoch 00317: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3297 - acc: 0.9007 - val_loss: 0.3643 - val_acc: 0.9078\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9008\n",
      "Epoch 00318: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3282 - acc: 0.9008 - val_loss: 0.3557 - val_acc: 0.9106\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.9000\n",
      "Epoch 00319: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3348 - acc: 0.9000 - val_loss: 0.3617 - val_acc: 0.9080\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.9006\n",
      "Epoch 00320: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3300 - acc: 0.9006 - val_loss: 0.3726 - val_acc: 0.9096\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.9007\n",
      "Epoch 00321: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3271 - acc: 0.9006 - val_loss: 0.3635 - val_acc: 0.9038\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8996\n",
      "Epoch 00322: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3370 - acc: 0.8996 - val_loss: 0.3629 - val_acc: 0.9117\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9005\n",
      "Epoch 00323: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3297 - acc: 0.9005 - val_loss: 0.3802 - val_acc: 0.9038\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8992\n",
      "Epoch 00324: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3312 - acc: 0.8992 - val_loss: 0.3580 - val_acc: 0.9108\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9022\n",
      "Epoch 00325: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3284 - acc: 0.9022 - val_loss: 0.3629 - val_acc: 0.9103\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9028\n",
      "Epoch 00326: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3260 - acc: 0.9028 - val_loss: 0.3562 - val_acc: 0.9075\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.9004\n",
      "Epoch 00327: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3274 - acc: 0.9004 - val_loss: 0.3707 - val_acc: 0.9061\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9029\n",
      "Epoch 00328: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3197 - acc: 0.9028 - val_loss: 0.3588 - val_acc: 0.9066\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.9001\n",
      "Epoch 00329: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3281 - acc: 0.9001 - val_loss: 0.3658 - val_acc: 0.9068\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9032\n",
      "Epoch 00330: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3248 - acc: 0.9032 - val_loss: 0.3639 - val_acc: 0.9068\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9007\n",
      "Epoch 00331: val_loss did not improve from 0.35421\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3227 - acc: 0.9007 - val_loss: 0.3598 - val_acc: 0.9078\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9030\n",
      "Epoch 00332: val_loss improved from 0.35421 to 0.35019, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/332-0.3502.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3204 - acc: 0.9030 - val_loss: 0.3502 - val_acc: 0.9117\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9027\n",
      "Epoch 00333: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3198 - acc: 0.9027 - val_loss: 0.3635 - val_acc: 0.9078\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9008\n",
      "Epoch 00334: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3244 - acc: 0.9008 - val_loss: 0.3793 - val_acc: 0.9068\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9020\n",
      "Epoch 00335: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3243 - acc: 0.9021 - val_loss: 0.3574 - val_acc: 0.9094\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.9019\n",
      "Epoch 00336: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3204 - acc: 0.9019 - val_loss: 0.3695 - val_acc: 0.9033\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.9008\n",
      "Epoch 00337: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3251 - acc: 0.9008 - val_loss: 0.3608 - val_acc: 0.9089\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9043\n",
      "Epoch 00338: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3152 - acc: 0.9043 - val_loss: 0.3570 - val_acc: 0.9101\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.9024\n",
      "Epoch 00339: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3208 - acc: 0.9024 - val_loss: 0.3578 - val_acc: 0.9082\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.9045\n",
      "Epoch 00340: val_loss did not improve from 0.35019\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3170 - acc: 0.9045 - val_loss: 0.3575 - val_acc: 0.9073\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9052\n",
      "Epoch 00341: val_loss improved from 0.35019 to 0.34950, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/341-0.3495.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3178 - acc: 0.9052 - val_loss: 0.3495 - val_acc: 0.9126\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9051\n",
      "Epoch 00342: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3164 - acc: 0.9050 - val_loss: 0.3609 - val_acc: 0.9092\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9041\n",
      "Epoch 00343: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3156 - acc: 0.9041 - val_loss: 0.3554 - val_acc: 0.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9056\n",
      "Epoch 00344: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3093 - acc: 0.9056 - val_loss: 0.3575 - val_acc: 0.9099\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9048\n",
      "Epoch 00345: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3136 - acc: 0.9048 - val_loss: 0.3710 - val_acc: 0.9036\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.9047\n",
      "Epoch 00346: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3156 - acc: 0.9047 - val_loss: 0.3549 - val_acc: 0.9089\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9054\n",
      "Epoch 00347: val_loss did not improve from 0.34950\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3145 - acc: 0.9054 - val_loss: 0.3583 - val_acc: 0.9085\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9034\n",
      "Epoch 00348: val_loss improved from 0.34950 to 0.34897, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/348-0.3490.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3162 - acc: 0.9034 - val_loss: 0.3490 - val_acc: 0.9094\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9058\n",
      "Epoch 00349: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3101 - acc: 0.9058 - val_loss: 0.3587 - val_acc: 0.9092\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9066\n",
      "Epoch 00350: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3061 - acc: 0.9065 - val_loss: 0.3597 - val_acc: 0.9106\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9028\n",
      "Epoch 00351: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3147 - acc: 0.9028 - val_loss: 0.3624 - val_acc: 0.9073\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9064\n",
      "Epoch 00352: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3133 - acc: 0.9064 - val_loss: 0.3652 - val_acc: 0.9101\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9049\n",
      "Epoch 00353: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3079 - acc: 0.9049 - val_loss: 0.3575 - val_acc: 0.9124\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.9066\n",
      "Epoch 00354: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3095 - acc: 0.9066 - val_loss: 0.3721 - val_acc: 0.9087\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9069\n",
      "Epoch 00355: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3075 - acc: 0.9069 - val_loss: 0.3623 - val_acc: 0.9057\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9072\n",
      "Epoch 00356: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3077 - acc: 0.9072 - val_loss: 0.3527 - val_acc: 0.9117\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9064\n",
      "Epoch 00357: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3061 - acc: 0.9064 - val_loss: 0.3590 - val_acc: 0.9080\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3032 - acc: 0.9086\n",
      "Epoch 00358: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3031 - acc: 0.9087 - val_loss: 0.3574 - val_acc: 0.9106\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.9068\n",
      "Epoch 00359: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3085 - acc: 0.9068 - val_loss: 0.3713 - val_acc: 0.9059\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3077 - acc: 0.9077\n",
      "Epoch 00360: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3077 - acc: 0.9077 - val_loss: 0.3690 - val_acc: 0.9082\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9080\n",
      "Epoch 00361: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3019 - acc: 0.9080 - val_loss: 0.3611 - val_acc: 0.9115\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9071\n",
      "Epoch 00362: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3033 - acc: 0.9071 - val_loss: 0.3612 - val_acc: 0.9122\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9076\n",
      "Epoch 00363: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3034 - acc: 0.9076 - val_loss: 0.3756 - val_acc: 0.9092\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9074\n",
      "Epoch 00364: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3052 - acc: 0.9074 - val_loss: 0.3665 - val_acc: 0.9078\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.9090\n",
      "Epoch 00365: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2985 - acc: 0.9090 - val_loss: 0.3712 - val_acc: 0.9031\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3042 - acc: 0.9068\n",
      "Epoch 00366: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3042 - acc: 0.9068 - val_loss: 0.3588 - val_acc: 0.9096\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9077\n",
      "Epoch 00367: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2989 - acc: 0.9077 - val_loss: 0.3686 - val_acc: 0.9054\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9101\n",
      "Epoch 00368: val_loss did not improve from 0.34897\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2978 - acc: 0.9101 - val_loss: 0.3701 - val_acc: 0.9057\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3003 - acc: 0.9093\n",
      "Epoch 00369: val_loss improved from 0.34897 to 0.34832, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv_checkpoint/369-0.3483.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.3004 - acc: 0.9093 - val_loss: 0.3483 - val_acc: 0.9103\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9098\n",
      "Epoch 00370: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2998 - acc: 0.9098 - val_loss: 0.3620 - val_acc: 0.9101\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9075\n",
      "Epoch 00371: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2981 - acc: 0.9075 - val_loss: 0.3566 - val_acc: 0.9108\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.9098\n",
      "Epoch 00372: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.3000 - acc: 0.9097 - val_loss: 0.3582 - val_acc: 0.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9095\n",
      "Epoch 00373: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2997 - acc: 0.9094 - val_loss: 0.3568 - val_acc: 0.9092\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.9102\n",
      "Epoch 00374: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2948 - acc: 0.9103 - val_loss: 0.3635 - val_acc: 0.9096\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9105\n",
      "Epoch 00375: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2950 - acc: 0.9106 - val_loss: 0.3554 - val_acc: 0.9075\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9094\n",
      "Epoch 00376: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2951 - acc: 0.9094 - val_loss: 0.3613 - val_acc: 0.9085\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9089\n",
      "Epoch 00377: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2970 - acc: 0.9089 - val_loss: 0.3706 - val_acc: 0.9096\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9092\n",
      "Epoch 00378: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2953 - acc: 0.9093 - val_loss: 0.3663 - val_acc: 0.9122\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9120\n",
      "Epoch 00379: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2894 - acc: 0.9120 - val_loss: 0.3625 - val_acc: 0.9133\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9092\n",
      "Epoch 00380: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2968 - acc: 0.9092 - val_loss: 0.3536 - val_acc: 0.9092\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9107\n",
      "Epoch 00381: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2905 - acc: 0.9107 - val_loss: 0.3592 - val_acc: 0.9110\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9129\n",
      "Epoch 00382: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2855 - acc: 0.9129 - val_loss: 0.3602 - val_acc: 0.9101\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.9115\n",
      "Epoch 00383: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2897 - acc: 0.9115 - val_loss: 0.3614 - val_acc: 0.9106\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9131\n",
      "Epoch 00384: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2868 - acc: 0.9131 - val_loss: 0.3554 - val_acc: 0.9092\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2931 - acc: 0.9094\n",
      "Epoch 00385: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2931 - acc: 0.9094 - val_loss: 0.3632 - val_acc: 0.9092\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9111\n",
      "Epoch 00386: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2891 - acc: 0.9110 - val_loss: 0.3647 - val_acc: 0.9071\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.9093\n",
      "Epoch 00387: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2934 - acc: 0.9093 - val_loss: 0.3575 - val_acc: 0.9092\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9114\n",
      "Epoch 00388: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2903 - acc: 0.9114 - val_loss: 0.3571 - val_acc: 0.9066\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9108\n",
      "Epoch 00389: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2893 - acc: 0.9107 - val_loss: 0.3664 - val_acc: 0.9050\n",
      "Epoch 390/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9120\n",
      "Epoch 00390: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2859 - acc: 0.9120 - val_loss: 0.3740 - val_acc: 0.9031\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9111\n",
      "Epoch 00391: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2903 - acc: 0.9110 - val_loss: 0.3767 - val_acc: 0.9066\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9115\n",
      "Epoch 00392: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2867 - acc: 0.9115 - val_loss: 0.3715 - val_acc: 0.9066\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2823 - acc: 0.9137\n",
      "Epoch 00393: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2822 - acc: 0.9137 - val_loss: 0.3639 - val_acc: 0.9052\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9124\n",
      "Epoch 00394: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2850 - acc: 0.9124 - val_loss: 0.3558 - val_acc: 0.9113\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9111\n",
      "Epoch 00395: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2908 - acc: 0.9111 - val_loss: 0.3638 - val_acc: 0.9078\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9107\n",
      "Epoch 00396: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2886 - acc: 0.9106 - val_loss: 0.3608 - val_acc: 0.9106\n",
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9131\n",
      "Epoch 00397: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2821 - acc: 0.9131 - val_loss: 0.3582 - val_acc: 0.9108\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.9122\n",
      "Epoch 00398: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2874 - acc: 0.9122 - val_loss: 0.3592 - val_acc: 0.9126\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9143\n",
      "Epoch 00399: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2783 - acc: 0.9143 - val_loss: 0.3600 - val_acc: 0.9094\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9122\n",
      "Epoch 00400: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2821 - acc: 0.9122 - val_loss: 0.3724 - val_acc: 0.9113\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9154\n",
      "Epoch 00401: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2773 - acc: 0.9154 - val_loss: 0.3660 - val_acc: 0.9119\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9157\n",
      "Epoch 00402: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2787 - acc: 0.9157 - val_loss: 0.3655 - val_acc: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9136\n",
      "Epoch 00403: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2811 - acc: 0.9136 - val_loss: 0.3530 - val_acc: 0.9129\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9148\n",
      "Epoch 00404: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2790 - acc: 0.9148 - val_loss: 0.3576 - val_acc: 0.9110\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9162\n",
      "Epoch 00405: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2739 - acc: 0.9163 - val_loss: 0.3552 - val_acc: 0.9138\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9151\n",
      "Epoch 00406: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2738 - acc: 0.9151 - val_loss: 0.3778 - val_acc: 0.9087\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9150\n",
      "Epoch 00407: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2776 - acc: 0.9150 - val_loss: 0.3589 - val_acc: 0.9092\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9167\n",
      "Epoch 00408: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2696 - acc: 0.9167 - val_loss: 0.3630 - val_acc: 0.9087\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9152\n",
      "Epoch 00409: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2747 - acc: 0.9152 - val_loss: 0.3596 - val_acc: 0.9129\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9136\n",
      "Epoch 00410: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2751 - acc: 0.9136 - val_loss: 0.3619 - val_acc: 0.9110\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9136\n",
      "Epoch 00411: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2756 - acc: 0.9137 - val_loss: 0.3616 - val_acc: 0.9106\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9161\n",
      "Epoch 00412: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2708 - acc: 0.9161 - val_loss: 0.3589 - val_acc: 0.9087\n",
      "Epoch 413/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9163\n",
      "Epoch 00413: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2747 - acc: 0.9163 - val_loss: 0.3685 - val_acc: 0.9057\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9177\n",
      "Epoch 00414: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2695 - acc: 0.9177 - val_loss: 0.3739 - val_acc: 0.9071\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9182\n",
      "Epoch 00415: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2697 - acc: 0.9182 - val_loss: 0.3720 - val_acc: 0.9068\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9169\n",
      "Epoch 00416: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2768 - acc: 0.9169 - val_loss: 0.3747 - val_acc: 0.9117\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9167\n",
      "Epoch 00417: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2703 - acc: 0.9166 - val_loss: 0.3726 - val_acc: 0.9066\n",
      "Epoch 418/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2740 - acc: 0.9146\n",
      "Epoch 00418: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 0.2740 - acc: 0.9146 - val_loss: 0.3619 - val_acc: 0.9124\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9182\n",
      "Epoch 00419: val_loss did not improve from 0.34832\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 0.2680 - acc: 0.9182 - val_loss: 0.3577 - val_acc: 0.9131\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 671us/sample - loss: 0.4066 - acc: 0.8883\n",
      "Loss: 0.4066322600111164 Accuracy: 0.88826585\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3608 - acc: 0.2212\n",
      "Epoch 00001: val_loss improved from inf to 1.89358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/001-1.8936.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 2.3608 - acc: 0.2212 - val_loss: 1.8936 - val_acc: 0.4039\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8700 - acc: 0.3845\n",
      "Epoch 00002: val_loss improved from 1.89358 to 1.58830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/002-1.5883.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.8698 - acc: 0.3845 - val_loss: 1.5883 - val_acc: 0.5195\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6602 - acc: 0.4555\n",
      "Epoch 00003: val_loss improved from 1.58830 to 1.41856, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/003-1.4186.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.6604 - acc: 0.4555 - val_loss: 1.4186 - val_acc: 0.5714\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5055 - acc: 0.5125\n",
      "Epoch 00004: val_loss improved from 1.41856 to 1.29050, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/004-1.2905.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.5055 - acc: 0.5124 - val_loss: 1.2905 - val_acc: 0.6033\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3823 - acc: 0.5598\n",
      "Epoch 00005: val_loss improved from 1.29050 to 1.18382, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/005-1.1838.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.3822 - acc: 0.5599 - val_loss: 1.1838 - val_acc: 0.6410\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2832 - acc: 0.5958\n",
      "Epoch 00006: val_loss improved from 1.18382 to 1.06147, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/006-1.0615.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.2832 - acc: 0.5958 - val_loss: 1.0615 - val_acc: 0.6851\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1925 - acc: 0.6271\n",
      "Epoch 00007: val_loss improved from 1.06147 to 0.97126, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/007-0.9713.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1924 - acc: 0.6271 - val_loss: 0.9713 - val_acc: 0.7298\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1162 - acc: 0.6558\n",
      "Epoch 00008: val_loss improved from 0.97126 to 0.89808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/008-0.8981.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1162 - acc: 0.6558 - val_loss: 0.8981 - val_acc: 0.7407\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0495 - acc: 0.6805\n",
      "Epoch 00009: val_loss improved from 0.89808 to 0.86118, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/009-0.8612.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0494 - acc: 0.6805 - val_loss: 0.8612 - val_acc: 0.7591\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9904 - acc: 0.6977\n",
      "Epoch 00010: val_loss improved from 0.86118 to 0.79627, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/010-0.7963.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9904 - acc: 0.6977 - val_loss: 0.7963 - val_acc: 0.7824\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9401 - acc: 0.7192\n",
      "Epoch 00011: val_loss improved from 0.79627 to 0.76171, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/011-0.7617.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.9400 - acc: 0.7192 - val_loss: 0.7617 - val_acc: 0.7794\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8915 - acc: 0.7357\n",
      "Epoch 00012: val_loss improved from 0.76171 to 0.69774, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/012-0.6977.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.8915 - acc: 0.7357 - val_loss: 0.6977 - val_acc: 0.8006\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8479 - acc: 0.7469\n",
      "Epoch 00013: val_loss improved from 0.69774 to 0.65123, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/013-0.6512.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8481 - acc: 0.7469 - val_loss: 0.6512 - val_acc: 0.8178\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8126 - acc: 0.7592\n",
      "Epoch 00014: val_loss did not improve from 0.65123\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.8127 - acc: 0.7592 - val_loss: 0.6875 - val_acc: 0.8078\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7803 - acc: 0.7707\n",
      "Epoch 00015: val_loss improved from 0.65123 to 0.62194, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/015-0.6219.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7805 - acc: 0.7706 - val_loss: 0.6219 - val_acc: 0.8272\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7579 - acc: 0.7774\n",
      "Epoch 00016: val_loss improved from 0.62194 to 0.58728, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/016-0.5873.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7579 - acc: 0.7774 - val_loss: 0.5873 - val_acc: 0.8358\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7879\n",
      "Epoch 00017: val_loss improved from 0.58728 to 0.56993, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/017-0.5699.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7274 - acc: 0.7879 - val_loss: 0.5699 - val_acc: 0.8400\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6973 - acc: 0.7950\n",
      "Epoch 00018: val_loss improved from 0.56993 to 0.55745, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/018-0.5575.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6973 - acc: 0.7950 - val_loss: 0.5575 - val_acc: 0.8444\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6773 - acc: 0.8014\n",
      "Epoch 00019: val_loss improved from 0.55745 to 0.51911, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/019-0.5191.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6773 - acc: 0.8014 - val_loss: 0.5191 - val_acc: 0.8514\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6590 - acc: 0.8088\n",
      "Epoch 00020: val_loss improved from 0.51911 to 0.50636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/020-0.5064.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6590 - acc: 0.8089 - val_loss: 0.5064 - val_acc: 0.8612\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6384 - acc: 0.8132\n",
      "Epoch 00021: val_loss improved from 0.50636 to 0.48386, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/021-0.4839.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6384 - acc: 0.8132 - val_loss: 0.4839 - val_acc: 0.8691\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6293 - acc: 0.8176\n",
      "Epoch 00022: val_loss did not improve from 0.48386\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6293 - acc: 0.8176 - val_loss: 0.5129 - val_acc: 0.8481\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8256\n",
      "Epoch 00023: val_loss improved from 0.48386 to 0.48133, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/023-0.4813.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5992 - acc: 0.8256 - val_loss: 0.4813 - val_acc: 0.8719\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.8277\n",
      "Epoch 00024: val_loss improved from 0.48133 to 0.43750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/024-0.4375.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5859 - acc: 0.8277 - val_loss: 0.4375 - val_acc: 0.8763\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5699 - acc: 0.8354\n",
      "Epoch 00025: val_loss did not improve from 0.43750\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5698 - acc: 0.8353 - val_loss: 0.4407 - val_acc: 0.8765\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8381\n",
      "Epoch 00026: val_loss improved from 0.43750 to 0.42621, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/026-0.4262.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5597 - acc: 0.8381 - val_loss: 0.4262 - val_acc: 0.8814\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8425\n",
      "Epoch 00027: val_loss improved from 0.42621 to 0.40908, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/027-0.4091.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5436 - acc: 0.8425 - val_loss: 0.4091 - val_acc: 0.8924\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8460\n",
      "Epoch 00028: val_loss improved from 0.40908 to 0.40257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/028-0.4026.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5314 - acc: 0.8459 - val_loss: 0.4026 - val_acc: 0.8903\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5201 - acc: 0.8476\n",
      "Epoch 00029: val_loss improved from 0.40257 to 0.39553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/029-0.3955.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5201 - acc: 0.8476 - val_loss: 0.3955 - val_acc: 0.8928\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8532\n",
      "Epoch 00030: val_loss improved from 0.39553 to 0.37577, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/030-0.3758.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5044 - acc: 0.8533 - val_loss: 0.3758 - val_acc: 0.8980\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.8559\n",
      "Epoch 00031: val_loss did not improve from 0.37577\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4931 - acc: 0.8559 - val_loss: 0.3850 - val_acc: 0.8977\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8581\n",
      "Epoch 00032: val_loss improved from 0.37577 to 0.37305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/032-0.3730.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4835 - acc: 0.8581 - val_loss: 0.3730 - val_acc: 0.9040\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8611\n",
      "Epoch 00033: val_loss improved from 0.37305 to 0.35577, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/033-0.3558.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4757 - acc: 0.8611 - val_loss: 0.3558 - val_acc: 0.9017\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4637 - acc: 0.8657\n",
      "Epoch 00034: val_loss did not improve from 0.35577\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4637 - acc: 0.8657 - val_loss: 0.3596 - val_acc: 0.9061\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8657\n",
      "Epoch 00035: val_loss did not improve from 0.35577\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4595 - acc: 0.8656 - val_loss: 0.3724 - val_acc: 0.9026\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8686\n",
      "Epoch 00036: val_loss did not improve from 0.35577\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4504 - acc: 0.8687 - val_loss: 0.4090 - val_acc: 0.8856\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8731\n",
      "Epoch 00037: val_loss improved from 0.35577 to 0.34895, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/037-0.3490.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4371 - acc: 0.8731 - val_loss: 0.3490 - val_acc: 0.9024\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8736\n",
      "Epoch 00038: val_loss improved from 0.34895 to 0.33610, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/038-0.3361.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4311 - acc: 0.8736 - val_loss: 0.3361 - val_acc: 0.9089\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8757\n",
      "Epoch 00039: val_loss improved from 0.33610 to 0.31905, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/039-0.3191.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4268 - acc: 0.8757 - val_loss: 0.3191 - val_acc: 0.9147\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8787\n",
      "Epoch 00040: val_loss did not improve from 0.31905\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4149 - acc: 0.8787 - val_loss: 0.3279 - val_acc: 0.9101\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8801\n",
      "Epoch 00041: val_loss improved from 0.31905 to 0.31835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/041-0.3183.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4095 - acc: 0.8800 - val_loss: 0.3183 - val_acc: 0.9094\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8814\n",
      "Epoch 00042: val_loss improved from 0.31835 to 0.30910, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/042-0.3091.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4032 - acc: 0.8814 - val_loss: 0.3091 - val_acc: 0.9147\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8866\n",
      "Epoch 00043: val_loss improved from 0.30910 to 0.29790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/043-0.2979.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3877 - acc: 0.8866 - val_loss: 0.2979 - val_acc: 0.9175\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8863\n",
      "Epoch 00044: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3853 - acc: 0.8863 - val_loss: 0.3032 - val_acc: 0.9159\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8875\n",
      "Epoch 00045: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3818 - acc: 0.8875 - val_loss: 0.3243 - val_acc: 0.9103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8893\n",
      "Epoch 00046: val_loss did not improve from 0.29790\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3751 - acc: 0.8893 - val_loss: 0.3249 - val_acc: 0.9152\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8909\n",
      "Epoch 00047: val_loss improved from 0.29790 to 0.29079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/047-0.2908.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3701 - acc: 0.8909 - val_loss: 0.2908 - val_acc: 0.9201\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8922\n",
      "Epoch 00048: val_loss improved from 0.29079 to 0.28586, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/048-0.2859.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3703 - acc: 0.8922 - val_loss: 0.2859 - val_acc: 0.9210\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8945\n",
      "Epoch 00049: val_loss did not improve from 0.28586\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3576 - acc: 0.8945 - val_loss: 0.3123 - val_acc: 0.9126\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.8951\n",
      "Epoch 00050: val_loss did not improve from 0.28586\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3521 - acc: 0.8951 - val_loss: 0.3043 - val_acc: 0.9168\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8964\n",
      "Epoch 00051: val_loss did not improve from 0.28586\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3556 - acc: 0.8964 - val_loss: 0.3017 - val_acc: 0.9203\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8953\n",
      "Epoch 00052: val_loss improved from 0.28586 to 0.27742, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/052-0.2774.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3499 - acc: 0.8953 - val_loss: 0.2774 - val_acc: 0.9206\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8990\n",
      "Epoch 00053: val_loss did not improve from 0.27742\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3404 - acc: 0.8990 - val_loss: 0.2872 - val_acc: 0.9220\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.9004\n",
      "Epoch 00054: val_loss improved from 0.27742 to 0.27252, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/054-0.2725.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3340 - acc: 0.9004 - val_loss: 0.2725 - val_acc: 0.9269\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.9004\n",
      "Epoch 00055: val_loss did not improve from 0.27252\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3336 - acc: 0.9004 - val_loss: 0.2822 - val_acc: 0.9248\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9034\n",
      "Epoch 00056: val_loss improved from 0.27252 to 0.26365, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/056-0.2636.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3228 - acc: 0.9034 - val_loss: 0.2636 - val_acc: 0.9311\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.9027\n",
      "Epoch 00057: val_loss did not improve from 0.26365\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3250 - acc: 0.9027 - val_loss: 0.2740 - val_acc: 0.9245\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.9042\n",
      "Epoch 00058: val_loss improved from 0.26365 to 0.25890, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/058-0.2589.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3218 - acc: 0.9041 - val_loss: 0.2589 - val_acc: 0.9317\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.9054\n",
      "Epoch 00059: val_loss did not improve from 0.25890\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3134 - acc: 0.9054 - val_loss: 0.2594 - val_acc: 0.9292\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9054\n",
      "Epoch 00060: val_loss did not improve from 0.25890\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3094 - acc: 0.9054 - val_loss: 0.2601 - val_acc: 0.9257\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9078\n",
      "Epoch 00061: val_loss did not improve from 0.25890\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3071 - acc: 0.9078 - val_loss: 0.3024 - val_acc: 0.9187\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3079 - acc: 0.9076\n",
      "Epoch 00062: val_loss did not improve from 0.25890\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3079 - acc: 0.9076 - val_loss: 0.2745 - val_acc: 0.9257\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9097\n",
      "Epoch 00063: val_loss improved from 0.25890 to 0.25877, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/063-0.2588.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3021 - acc: 0.9097 - val_loss: 0.2588 - val_acc: 0.9301\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9124\n",
      "Epoch 00064: val_loss improved from 0.25877 to 0.25492, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/064-0.2549.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2930 - acc: 0.9124 - val_loss: 0.2549 - val_acc: 0.9322\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2968 - acc: 0.9108\n",
      "Epoch 00065: val_loss did not improve from 0.25492\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2967 - acc: 0.9108 - val_loss: 0.2744 - val_acc: 0.9294\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9136\n",
      "Epoch 00066: val_loss did not improve from 0.25492\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2861 - acc: 0.9136 - val_loss: 0.2552 - val_acc: 0.9308\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.9144\n",
      "Epoch 00067: val_loss did not improve from 0.25492\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2867 - acc: 0.9144 - val_loss: 0.2551 - val_acc: 0.9317\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9128\n",
      "Epoch 00068: val_loss did not improve from 0.25492\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2840 - acc: 0.9128 - val_loss: 0.2620 - val_acc: 0.9290\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9147\n",
      "Epoch 00069: val_loss improved from 0.25492 to 0.25335, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/069-0.2533.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2801 - acc: 0.9147 - val_loss: 0.2533 - val_acc: 0.9359\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9161\n",
      "Epoch 00070: val_loss improved from 0.25335 to 0.24810, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/070-0.2481.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2780 - acc: 0.9162 - val_loss: 0.2481 - val_acc: 0.9343\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9170\n",
      "Epoch 00071: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2748 - acc: 0.9170 - val_loss: 0.2561 - val_acc: 0.9283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9180\n",
      "Epoch 00072: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2699 - acc: 0.9180 - val_loss: 0.2548 - val_acc: 0.9331\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9183\n",
      "Epoch 00073: val_loss improved from 0.24810 to 0.23770, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/073-0.2377.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2675 - acc: 0.9184 - val_loss: 0.2377 - val_acc: 0.9380\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9196\n",
      "Epoch 00074: val_loss did not improve from 0.23770\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2615 - acc: 0.9196 - val_loss: 0.2407 - val_acc: 0.9373\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9197\n",
      "Epoch 00075: val_loss did not improve from 0.23770\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2639 - acc: 0.9197 - val_loss: 0.2789 - val_acc: 0.9301\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9200\n",
      "Epoch 00076: val_loss improved from 0.23770 to 0.23406, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/076-0.2341.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2626 - acc: 0.9200 - val_loss: 0.2341 - val_acc: 0.9371\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9219\n",
      "Epoch 00077: val_loss did not improve from 0.23406\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2526 - acc: 0.9219 - val_loss: 0.2378 - val_acc: 0.9385\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9231\n",
      "Epoch 00078: val_loss improved from 0.23406 to 0.23101, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/078-0.2310.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2530 - acc: 0.9231 - val_loss: 0.2310 - val_acc: 0.9373\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9241\n",
      "Epoch 00079: val_loss did not improve from 0.23101\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2477 - acc: 0.9241 - val_loss: 0.2594 - val_acc: 0.9341\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9230\n",
      "Epoch 00080: val_loss did not improve from 0.23101\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2528 - acc: 0.9230 - val_loss: 0.2536 - val_acc: 0.9306\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9243\n",
      "Epoch 00081: val_loss did not improve from 0.23101\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2470 - acc: 0.9243 - val_loss: 0.2389 - val_acc: 0.9369\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9258\n",
      "Epoch 00082: val_loss did not improve from 0.23101\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2469 - acc: 0.9257 - val_loss: 0.2352 - val_acc: 0.9376\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9234\n",
      "Epoch 00083: val_loss did not improve from 0.23101\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2449 - acc: 0.9234 - val_loss: 0.2457 - val_acc: 0.9394\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9280\n",
      "Epoch 00084: val_loss improved from 0.23101 to 0.23005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/084-0.2301.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2346 - acc: 0.9280 - val_loss: 0.2301 - val_acc: 0.9399\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9282\n",
      "Epoch 00085: val_loss did not improve from 0.23005\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2358 - acc: 0.9282 - val_loss: 0.2394 - val_acc: 0.9345\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9265\n",
      "Epoch 00086: val_loss improved from 0.23005 to 0.22633, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/086-0.2263.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2367 - acc: 0.9266 - val_loss: 0.2263 - val_acc: 0.9406\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9287\n",
      "Epoch 00087: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2345 - acc: 0.9288 - val_loss: 0.2342 - val_acc: 0.9355\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9305\n",
      "Epoch 00088: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2293 - acc: 0.9306 - val_loss: 0.2452 - val_acc: 0.9378\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9276\n",
      "Epoch 00089: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2292 - acc: 0.9276 - val_loss: 0.2405 - val_acc: 0.9397\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9291\n",
      "Epoch 00090: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2281 - acc: 0.9291 - val_loss: 0.2354 - val_acc: 0.9406\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9308\n",
      "Epoch 00091: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2262 - acc: 0.9308 - val_loss: 0.2301 - val_acc: 0.9357\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9307\n",
      "Epoch 00092: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2238 - acc: 0.9307 - val_loss: 0.2371 - val_acc: 0.9392\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9290\n",
      "Epoch 00093: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2243 - acc: 0.9290 - val_loss: 0.2308 - val_acc: 0.9387\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9329\n",
      "Epoch 00094: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2165 - acc: 0.9329 - val_loss: 0.2287 - val_acc: 0.9413\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9320\n",
      "Epoch 00095: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2214 - acc: 0.9319 - val_loss: 0.2428 - val_acc: 0.9355\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9335\n",
      "Epoch 00096: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2189 - acc: 0.9335 - val_loss: 0.2319 - val_acc: 0.9425\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9330\n",
      "Epoch 00097: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2173 - acc: 0.9330 - val_loss: 0.2288 - val_acc: 0.9392\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9339\n",
      "Epoch 00098: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2145 - acc: 0.9339 - val_loss: 0.2376 - val_acc: 0.9413\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9355\n",
      "Epoch 00099: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2066 - acc: 0.9356 - val_loss: 0.2267 - val_acc: 0.9427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9351\n",
      "Epoch 00100: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2080 - acc: 0.9351 - val_loss: 0.2324 - val_acc: 0.9415\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9353\n",
      "Epoch 00101: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2067 - acc: 0.9353 - val_loss: 0.2296 - val_acc: 0.9429\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9373\n",
      "Epoch 00102: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2007 - acc: 0.9373 - val_loss: 0.2270 - val_acc: 0.9404\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9372\n",
      "Epoch 00103: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1991 - acc: 0.9372 - val_loss: 0.2407 - val_acc: 0.9387\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9400\n",
      "Epoch 00104: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1977 - acc: 0.9400 - val_loss: 0.2316 - val_acc: 0.9404\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9396\n",
      "Epoch 00105: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1946 - acc: 0.9396 - val_loss: 0.2300 - val_acc: 0.9404\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9395\n",
      "Epoch 00106: val_loss did not improve from 0.22633\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1937 - acc: 0.9395 - val_loss: 0.2369 - val_acc: 0.9434\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9395\n",
      "Epoch 00107: val_loss improved from 0.22633 to 0.21809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv_checkpoint/107-0.2181.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1902 - acc: 0.9395 - val_loss: 0.2181 - val_acc: 0.9443\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9401\n",
      "Epoch 00108: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1923 - acc: 0.9401 - val_loss: 0.2425 - val_acc: 0.9359\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9404\n",
      "Epoch 00109: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1928 - acc: 0.9404 - val_loss: 0.2532 - val_acc: 0.9378\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9407\n",
      "Epoch 00110: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1880 - acc: 0.9407 - val_loss: 0.2358 - val_acc: 0.9413\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9412\n",
      "Epoch 00111: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1880 - acc: 0.9412 - val_loss: 0.2391 - val_acc: 0.9420\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9409\n",
      "Epoch 00112: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1838 - acc: 0.9409 - val_loss: 0.2502 - val_acc: 0.9397\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9422\n",
      "Epoch 00113: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1854 - acc: 0.9422 - val_loss: 0.2193 - val_acc: 0.9457\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9430\n",
      "Epoch 00114: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1833 - acc: 0.9429 - val_loss: 0.2293 - val_acc: 0.9420\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1809 - acc: 0.9437\n",
      "Epoch 00115: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1809 - acc: 0.9437 - val_loss: 0.2287 - val_acc: 0.9415\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9440\n",
      "Epoch 00116: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1794 - acc: 0.9440 - val_loss: 0.2344 - val_acc: 0.9420\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9433\n",
      "Epoch 00117: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1812 - acc: 0.9433 - val_loss: 0.2274 - val_acc: 0.9450\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9442\n",
      "Epoch 00118: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1752 - acc: 0.9442 - val_loss: 0.2271 - val_acc: 0.9467\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9449\n",
      "Epoch 00119: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1744 - acc: 0.9449 - val_loss: 0.2246 - val_acc: 0.9460\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9465\n",
      "Epoch 00120: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1738 - acc: 0.9465 - val_loss: 0.2265 - val_acc: 0.9422\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9445\n",
      "Epoch 00121: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1714 - acc: 0.9445 - val_loss: 0.2204 - val_acc: 0.9462\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9466\n",
      "Epoch 00122: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1727 - acc: 0.9466 - val_loss: 0.2441 - val_acc: 0.9404\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9455\n",
      "Epoch 00123: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1726 - acc: 0.9455 - val_loss: 0.2307 - val_acc: 0.9432\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9461\n",
      "Epoch 00124: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1700 - acc: 0.9461 - val_loss: 0.2236 - val_acc: 0.9420\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9477\n",
      "Epoch 00125: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1644 - acc: 0.9477 - val_loss: 0.2224 - val_acc: 0.9488\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9467\n",
      "Epoch 00126: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1658 - acc: 0.9467 - val_loss: 0.2325 - val_acc: 0.9413\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9488\n",
      "Epoch 00127: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1628 - acc: 0.9488 - val_loss: 0.2249 - val_acc: 0.9441\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9485\n",
      "Epoch 00128: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1629 - acc: 0.9485 - val_loss: 0.2301 - val_acc: 0.9467\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9483\n",
      "Epoch 00129: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1630 - acc: 0.9483 - val_loss: 0.2397 - val_acc: 0.9441\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9496\n",
      "Epoch 00130: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1576 - acc: 0.9496 - val_loss: 0.2200 - val_acc: 0.9460\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9498\n",
      "Epoch 00131: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1561 - acc: 0.9498 - val_loss: 0.2255 - val_acc: 0.9420\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9505\n",
      "Epoch 00132: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1564 - acc: 0.9505 - val_loss: 0.2351 - val_acc: 0.9383\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9485\n",
      "Epoch 00133: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1590 - acc: 0.9485 - val_loss: 0.2324 - val_acc: 0.9422\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9516\n",
      "Epoch 00134: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1527 - acc: 0.9516 - val_loss: 0.2357 - val_acc: 0.9450\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9507\n",
      "Epoch 00135: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1541 - acc: 0.9507 - val_loss: 0.2206 - val_acc: 0.9476\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9528\n",
      "Epoch 00136: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1483 - acc: 0.9528 - val_loss: 0.2389 - val_acc: 0.9457\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9515\n",
      "Epoch 00137: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1497 - acc: 0.9515 - val_loss: 0.2223 - val_acc: 0.9450\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9523\n",
      "Epoch 00138: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1479 - acc: 0.9523 - val_loss: 0.2208 - val_acc: 0.9453\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9522\n",
      "Epoch 00139: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1496 - acc: 0.9522 - val_loss: 0.2240 - val_acc: 0.9429\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9522\n",
      "Epoch 00140: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1468 - acc: 0.9522 - val_loss: 0.2272 - val_acc: 0.9469\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9546\n",
      "Epoch 00141: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1424 - acc: 0.9546 - val_loss: 0.2320 - val_acc: 0.9439\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9533\n",
      "Epoch 00142: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1440 - acc: 0.9533 - val_loss: 0.2240 - val_acc: 0.9443\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9537\n",
      "Epoch 00143: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1406 - acc: 0.9537 - val_loss: 0.2386 - val_acc: 0.9439\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9546\n",
      "Epoch 00144: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1422 - acc: 0.9546 - val_loss: 0.2309 - val_acc: 0.9455\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9555\n",
      "Epoch 00145: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1379 - acc: 0.9555 - val_loss: 0.2234 - val_acc: 0.9467\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9539\n",
      "Epoch 00146: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1425 - acc: 0.9539 - val_loss: 0.2421 - val_acc: 0.9450\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9548\n",
      "Epoch 00147: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1392 - acc: 0.9548 - val_loss: 0.2396 - val_acc: 0.9432\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9549\n",
      "Epoch 00148: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1372 - acc: 0.9549 - val_loss: 0.2297 - val_acc: 0.9467\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9552\n",
      "Epoch 00149: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1372 - acc: 0.9552 - val_loss: 0.2251 - val_acc: 0.9436\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9560\n",
      "Epoch 00150: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1336 - acc: 0.9560 - val_loss: 0.2498 - val_acc: 0.9443\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9574\n",
      "Epoch 00151: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1327 - acc: 0.9573 - val_loss: 0.2405 - val_acc: 0.9471\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9572\n",
      "Epoch 00152: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1329 - acc: 0.9572 - val_loss: 0.2289 - val_acc: 0.9471\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9584\n",
      "Epoch 00153: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1312 - acc: 0.9584 - val_loss: 0.2312 - val_acc: 0.9474\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9583\n",
      "Epoch 00154: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1298 - acc: 0.9583 - val_loss: 0.2351 - val_acc: 0.9469\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9586\n",
      "Epoch 00155: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1268 - acc: 0.9586 - val_loss: 0.2382 - val_acc: 0.9436\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9588\n",
      "Epoch 00156: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1284 - acc: 0.9588 - val_loss: 0.2263 - val_acc: 0.9455\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9595\n",
      "Epoch 00157: val_loss did not improve from 0.21809\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1264 - acc: 0.9595 - val_loss: 0.2488 - val_acc: 0.9397\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 679us/sample - loss: 0.2629 - acc: 0.9225\n",
      "Loss: 0.26294008484758197 Accuracy: 0.92253375\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1933 - acc: 0.2710\n",
      "Epoch 00001: val_loss improved from inf to 1.58756, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/001-1.5876.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 2.1932 - acc: 0.2710 - val_loss: 1.5876 - val_acc: 0.5171\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5373 - acc: 0.5049\n",
      "Epoch 00002: val_loss improved from 1.58756 to 1.23462, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/002-1.2346.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.5374 - acc: 0.5049 - val_loss: 1.2346 - val_acc: 0.6427\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2288 - acc: 0.6154\n",
      "Epoch 00003: val_loss improved from 1.23462 to 0.89640, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/003-0.8964.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.2287 - acc: 0.6154 - val_loss: 0.8964 - val_acc: 0.7396\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0221 - acc: 0.6845\n",
      "Epoch 00004: val_loss improved from 0.89640 to 0.73467, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/004-0.7347.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.0221 - acc: 0.6845 - val_loss: 0.7347 - val_acc: 0.7920\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8789 - acc: 0.7319\n",
      "Epoch 00005: val_loss improved from 0.73467 to 0.65528, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/005-0.6553.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.8789 - acc: 0.7319 - val_loss: 0.6553 - val_acc: 0.8109\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7658\n",
      "Epoch 00006: val_loss improved from 0.65528 to 0.56694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/006-0.5669.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7760 - acc: 0.7658 - val_loss: 0.5669 - val_acc: 0.8334\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6911 - acc: 0.7921\n",
      "Epoch 00007: val_loss improved from 0.56694 to 0.48240, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/007-0.4824.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.6911 - acc: 0.7921 - val_loss: 0.4824 - val_acc: 0.8689\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6248 - acc: 0.8145\n",
      "Epoch 00008: val_loss improved from 0.48240 to 0.46256, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/008-0.4626.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.6248 - acc: 0.8145 - val_loss: 0.4626 - val_acc: 0.8733\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.8268\n",
      "Epoch 00009: val_loss improved from 0.46256 to 0.42862, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/009-0.4286.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5761 - acc: 0.8268 - val_loss: 0.4286 - val_acc: 0.8810\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8411\n",
      "Epoch 00010: val_loss improved from 0.42862 to 0.38210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/010-0.3821.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5363 - acc: 0.8411 - val_loss: 0.3821 - val_acc: 0.8926\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8513\n",
      "Epoch 00011: val_loss improved from 0.38210 to 0.32862, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/011-0.3286.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5040 - acc: 0.8513 - val_loss: 0.3286 - val_acc: 0.9143\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8606\n",
      "Epoch 00012: val_loss improved from 0.32862 to 0.31601, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/012-0.3160.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4655 - acc: 0.8606 - val_loss: 0.3160 - val_acc: 0.9126\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8683\n",
      "Epoch 00013: val_loss improved from 0.31601 to 0.30422, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/013-0.3042.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4400 - acc: 0.8683 - val_loss: 0.3042 - val_acc: 0.9178\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8739\n",
      "Epoch 00014: val_loss improved from 0.30422 to 0.29432, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/014-0.2943.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4188 - acc: 0.8739 - val_loss: 0.2943 - val_acc: 0.9213\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8828\n",
      "Epoch 00015: val_loss improved from 0.29432 to 0.27312, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/015-0.2731.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3948 - acc: 0.8828 - val_loss: 0.2731 - val_acc: 0.9241\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8873\n",
      "Epoch 00016: val_loss improved from 0.27312 to 0.25396, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/016-0.2540.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3802 - acc: 0.8873 - val_loss: 0.2540 - val_acc: 0.9322\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8907\n",
      "Epoch 00017: val_loss improved from 0.25396 to 0.24560, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/017-0.2456.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3650 - acc: 0.8907 - val_loss: 0.2456 - val_acc: 0.9327\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8964\n",
      "Epoch 00018: val_loss did not improve from 0.24560\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3460 - acc: 0.8964 - val_loss: 0.2476 - val_acc: 0.9285\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8994\n",
      "Epoch 00019: val_loss improved from 0.24560 to 0.22530, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/019-0.2253.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3294 - acc: 0.8993 - val_loss: 0.2253 - val_acc: 0.9385\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.9027\n",
      "Epoch 00020: val_loss improved from 0.22530 to 0.21293, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/020-0.2129.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3234 - acc: 0.9027 - val_loss: 0.2129 - val_acc: 0.9413\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.9073\n",
      "Epoch 00021: val_loss did not improve from 0.21293\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3072 - acc: 0.9073 - val_loss: 0.2207 - val_acc: 0.9387\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9112\n",
      "Epoch 00022: val_loss improved from 0.21293 to 0.19961, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/022-0.1996.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2969 - acc: 0.9112 - val_loss: 0.1996 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2902 - acc: 0.9112\n",
      "Epoch 00023: val_loss improved from 0.19961 to 0.19643, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/023-0.1964.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2902 - acc: 0.9112 - val_loss: 0.1964 - val_acc: 0.9485\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9144\n",
      "Epoch 00024: val_loss did not improve from 0.19643\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2842 - acc: 0.9144 - val_loss: 0.1988 - val_acc: 0.9464\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9171\n",
      "Epoch 00025: val_loss did not improve from 0.19643\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2717 - acc: 0.9171 - val_loss: 0.2012 - val_acc: 0.9427\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9209\n",
      "Epoch 00026: val_loss improved from 0.19643 to 0.19444, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/026-0.1944.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2617 - acc: 0.9209 - val_loss: 0.1944 - val_acc: 0.9455\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9236\n",
      "Epoch 00027: val_loss improved from 0.19444 to 0.18518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/027-0.1852.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2510 - acc: 0.9236 - val_loss: 0.1852 - val_acc: 0.9471\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9251\n",
      "Epoch 00028: val_loss did not improve from 0.18518\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2506 - acc: 0.9251 - val_loss: 0.1965 - val_acc: 0.9432\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9264\n",
      "Epoch 00029: val_loss improved from 0.18518 to 0.17715, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/029-0.1772.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2408 - acc: 0.9264 - val_loss: 0.1772 - val_acc: 0.9509\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9274\n",
      "Epoch 00030: val_loss improved from 0.17715 to 0.16479, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/030-0.1648.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2343 - acc: 0.9274 - val_loss: 0.1648 - val_acc: 0.9536\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9315\n",
      "Epoch 00031: val_loss did not improve from 0.16479\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2251 - acc: 0.9315 - val_loss: 0.1737 - val_acc: 0.9534\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9329\n",
      "Epoch 00032: val_loss improved from 0.16479 to 0.16295, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/032-0.1629.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2197 - acc: 0.9329 - val_loss: 0.1629 - val_acc: 0.9548\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9330\n",
      "Epoch 00033: val_loss did not improve from 0.16295\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2170 - acc: 0.9330 - val_loss: 0.1633 - val_acc: 0.9555\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9353\n",
      "Epoch 00034: val_loss did not improve from 0.16295\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2061 - acc: 0.9353 - val_loss: 0.1689 - val_acc: 0.9541\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9372\n",
      "Epoch 00035: val_loss did not improve from 0.16295\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2038 - acc: 0.9372 - val_loss: 0.1711 - val_acc: 0.9525\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9376\n",
      "Epoch 00036: val_loss did not improve from 0.16295\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2010 - acc: 0.9376 - val_loss: 0.1704 - val_acc: 0.9511\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9402\n",
      "Epoch 00037: val_loss improved from 0.16295 to 0.15705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/037-0.1570.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1921 - acc: 0.9402 - val_loss: 0.1570 - val_acc: 0.9562\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9399\n",
      "Epoch 00038: val_loss did not improve from 0.15705\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1937 - acc: 0.9399 - val_loss: 0.1571 - val_acc: 0.9588\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9429\n",
      "Epoch 00039: val_loss improved from 0.15705 to 0.15054, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/039-0.1505.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1828 - acc: 0.9429 - val_loss: 0.1505 - val_acc: 0.9604\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9429\n",
      "Epoch 00040: val_loss did not improve from 0.15054\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1831 - acc: 0.9429 - val_loss: 0.1591 - val_acc: 0.9569\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9439\n",
      "Epoch 00041: val_loss improved from 0.15054 to 0.14365, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/041-0.1436.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1781 - acc: 0.9439 - val_loss: 0.1436 - val_acc: 0.9590\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9447\n",
      "Epoch 00042: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1750 - acc: 0.9447 - val_loss: 0.1483 - val_acc: 0.9581\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9466\n",
      "Epoch 00043: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1698 - acc: 0.9466 - val_loss: 0.1548 - val_acc: 0.9581\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9483\n",
      "Epoch 00044: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1627 - acc: 0.9483 - val_loss: 0.1482 - val_acc: 0.9588\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9495\n",
      "Epoch 00045: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1614 - acc: 0.9495 - val_loss: 0.1438 - val_acc: 0.9590\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9497\n",
      "Epoch 00046: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1600 - acc: 0.9497 - val_loss: 0.1684 - val_acc: 0.9522\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9507\n",
      "Epoch 00047: val_loss did not improve from 0.14365\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1565 - acc: 0.9507 - val_loss: 0.1589 - val_acc: 0.9548\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9517\n",
      "Epoch 00048: val_loss improved from 0.14365 to 0.14323, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/048-0.1432.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1546 - acc: 0.9517 - val_loss: 0.1432 - val_acc: 0.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9516\n",
      "Epoch 00049: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1534 - acc: 0.9516 - val_loss: 0.1536 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9540\n",
      "Epoch 00050: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1442 - acc: 0.9540 - val_loss: 0.1567 - val_acc: 0.9562\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9544\n",
      "Epoch 00051: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1442 - acc: 0.9544 - val_loss: 0.1456 - val_acc: 0.9609\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9540\n",
      "Epoch 00052: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1406 - acc: 0.9541 - val_loss: 0.1559 - val_acc: 0.9578\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9559\n",
      "Epoch 00053: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1384 - acc: 0.9559 - val_loss: 0.1499 - val_acc: 0.9592\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9584\n",
      "Epoch 00054: val_loss did not improve from 0.14323\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1314 - acc: 0.9584 - val_loss: 0.1670 - val_acc: 0.9557\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9561\n",
      "Epoch 00055: val_loss improved from 0.14323 to 0.14083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/055-0.1408.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1348 - acc: 0.9561 - val_loss: 0.1408 - val_acc: 0.9634\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9576\n",
      "Epoch 00056: val_loss improved from 0.14083 to 0.13954, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/056-0.1395.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1296 - acc: 0.9576 - val_loss: 0.1395 - val_acc: 0.9588\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9585\n",
      "Epoch 00057: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1271 - acc: 0.9585 - val_loss: 0.1510 - val_acc: 0.9569\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9602\n",
      "Epoch 00058: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1228 - acc: 0.9602 - val_loss: 0.1553 - val_acc: 0.9581\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9612\n",
      "Epoch 00059: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1211 - acc: 0.9612 - val_loss: 0.1455 - val_acc: 0.9590\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9619\n",
      "Epoch 00060: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1187 - acc: 0.9619 - val_loss: 0.1736 - val_acc: 0.9509\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9639\n",
      "Epoch 00061: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1137 - acc: 0.9639 - val_loss: 0.1506 - val_acc: 0.9604\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9636\n",
      "Epoch 00062: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1103 - acc: 0.9636 - val_loss: 0.1544 - val_acc: 0.9625\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9633\n",
      "Epoch 00063: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1133 - acc: 0.9633 - val_loss: 0.1702 - val_acc: 0.9515\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9652\n",
      "Epoch 00064: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1074 - acc: 0.9652 - val_loss: 0.1498 - val_acc: 0.9618\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9651\n",
      "Epoch 00065: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1083 - acc: 0.9651 - val_loss: 0.1445 - val_acc: 0.9625\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9665\n",
      "Epoch 00066: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1021 - acc: 0.9665 - val_loss: 0.1460 - val_acc: 0.9630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9661\n",
      "Epoch 00067: val_loss did not improve from 0.13954\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1031 - acc: 0.9661 - val_loss: 0.1522 - val_acc: 0.9606\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9665\n",
      "Epoch 00068: val_loss improved from 0.13954 to 0.13866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/068-0.1387.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1037 - acc: 0.9665 - val_loss: 0.1387 - val_acc: 0.9620\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9683\n",
      "Epoch 00069: val_loss did not improve from 0.13866\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0962 - acc: 0.9683 - val_loss: 0.1390 - val_acc: 0.9644\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9695\n",
      "Epoch 00070: val_loss did not improve from 0.13866\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0958 - acc: 0.9695 - val_loss: 0.1685 - val_acc: 0.9564\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9680\n",
      "Epoch 00071: val_loss did not improve from 0.13866\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0969 - acc: 0.9679 - val_loss: 0.1412 - val_acc: 0.9599\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9694\n",
      "Epoch 00072: val_loss did not improve from 0.13866\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0918 - acc: 0.9694 - val_loss: 0.1459 - val_acc: 0.9618\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9696\n",
      "Epoch 00073: val_loss improved from 0.13866 to 0.13469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv_checkpoint/073-0.1347.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0906 - acc: 0.9696 - val_loss: 0.1347 - val_acc: 0.9627\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9712\n",
      "Epoch 00074: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0883 - acc: 0.9712 - val_loss: 0.1479 - val_acc: 0.9646\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9718\n",
      "Epoch 00075: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0868 - acc: 0.9719 - val_loss: 0.1498 - val_acc: 0.9644\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9727\n",
      "Epoch 00076: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0845 - acc: 0.9727 - val_loss: 0.1583 - val_acc: 0.9599\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9713\n",
      "Epoch 00077: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0876 - acc: 0.9713 - val_loss: 0.1399 - val_acc: 0.9648\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9725\n",
      "Epoch 00078: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0827 - acc: 0.9725 - val_loss: 0.1608 - val_acc: 0.9599\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9730\n",
      "Epoch 00079: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0809 - acc: 0.9729 - val_loss: 0.1534 - val_acc: 0.9606\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9740\n",
      "Epoch 00080: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0824 - acc: 0.9740 - val_loss: 0.1465 - val_acc: 0.9634\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9751\n",
      "Epoch 00081: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0771 - acc: 0.9751 - val_loss: 0.1417 - val_acc: 0.9625\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9735\n",
      "Epoch 00082: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0797 - acc: 0.9735 - val_loss: 0.1533 - val_acc: 0.9613\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9759\n",
      "Epoch 00083: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0744 - acc: 0.9759 - val_loss: 0.1521 - val_acc: 0.9632\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9758\n",
      "Epoch 00084: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0736 - acc: 0.9758 - val_loss: 0.1395 - val_acc: 0.9634\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9764\n",
      "Epoch 00085: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0723 - acc: 0.9764 - val_loss: 0.1362 - val_acc: 0.9646\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9751\n",
      "Epoch 00086: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0742 - acc: 0.9751 - val_loss: 0.1538 - val_acc: 0.9613\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9764\n",
      "Epoch 00087: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0715 - acc: 0.9764 - val_loss: 0.1509 - val_acc: 0.9618\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9766\n",
      "Epoch 00088: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0713 - acc: 0.9766 - val_loss: 0.1566 - val_acc: 0.9623\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9782\n",
      "Epoch 00089: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0668 - acc: 0.9782 - val_loss: 0.1637 - val_acc: 0.9602\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9774\n",
      "Epoch 00090: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0663 - acc: 0.9774 - val_loss: 0.1645 - val_acc: 0.9595\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9777\n",
      "Epoch 00091: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0654 - acc: 0.9777 - val_loss: 0.1848 - val_acc: 0.9557\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9776\n",
      "Epoch 00092: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0663 - acc: 0.9775 - val_loss: 0.1595 - val_acc: 0.9613\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9790\n",
      "Epoch 00093: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0623 - acc: 0.9790 - val_loss: 0.1636 - val_acc: 0.9616\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9772\n",
      "Epoch 00094: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0679 - acc: 0.9771 - val_loss: 0.1688 - val_acc: 0.9578\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9792\n",
      "Epoch 00095: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0610 - acc: 0.9792 - val_loss: 0.1668 - val_acc: 0.9613\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9807\n",
      "Epoch 00096: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0576 - acc: 0.9807 - val_loss: 0.1531 - val_acc: 0.9637\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9800\n",
      "Epoch 00097: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0593 - acc: 0.9800 - val_loss: 0.1654 - val_acc: 0.9616\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9812\n",
      "Epoch 00098: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0574 - acc: 0.9812 - val_loss: 0.1484 - val_acc: 0.9651\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9801\n",
      "Epoch 00099: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0591 - acc: 0.9801 - val_loss: 0.1668 - val_acc: 0.9639\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9816\n",
      "Epoch 00100: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0550 - acc: 0.9816 - val_loss: 0.1447 - val_acc: 0.9639\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9816\n",
      "Epoch 00101: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0538 - acc: 0.9816 - val_loss: 0.1633 - val_acc: 0.9602\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9823\n",
      "Epoch 00102: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0535 - acc: 0.9823 - val_loss: 0.1503 - val_acc: 0.9627\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9820\n",
      "Epoch 00103: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0532 - acc: 0.9820 - val_loss: 0.1414 - val_acc: 0.9660\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9811\n",
      "Epoch 00104: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0540 - acc: 0.9811 - val_loss: 0.1754 - val_acc: 0.9625\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9835\n",
      "Epoch 00105: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0487 - acc: 0.9835 - val_loss: 0.1828 - val_acc: 0.9602\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9820\n",
      "Epoch 00106: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0547 - acc: 0.9820 - val_loss: 0.1786 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9832\n",
      "Epoch 00107: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0503 - acc: 0.9832 - val_loss: 0.1910 - val_acc: 0.9592\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9835\n",
      "Epoch 00108: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0488 - acc: 0.9835 - val_loss: 0.1742 - val_acc: 0.9630\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9845\n",
      "Epoch 00109: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0463 - acc: 0.9845 - val_loss: 0.1746 - val_acc: 0.9616\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9830\n",
      "Epoch 00110: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0505 - acc: 0.9830 - val_loss: 0.1626 - val_acc: 0.9660\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9849\n",
      "Epoch 00111: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0465 - acc: 0.9849 - val_loss: 0.1653 - val_acc: 0.9648\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9841\n",
      "Epoch 00112: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0491 - acc: 0.9841 - val_loss: 0.1784 - val_acc: 0.9606\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9848\n",
      "Epoch 00113: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0443 - acc: 0.9848 - val_loss: 0.1766 - val_acc: 0.9632\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9845\n",
      "Epoch 00114: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0457 - acc: 0.9845 - val_loss: 0.1644 - val_acc: 0.9648\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9869\n",
      "Epoch 00115: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0393 - acc: 0.9869 - val_loss: 0.1828 - val_acc: 0.9590\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9849\n",
      "Epoch 00116: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0464 - acc: 0.9849 - val_loss: 0.2609 - val_acc: 0.9469\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9851\n",
      "Epoch 00117: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0443 - acc: 0.9851 - val_loss: 0.1733 - val_acc: 0.9599\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9864\n",
      "Epoch 00118: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0409 - acc: 0.9864 - val_loss: 0.1736 - val_acc: 0.9644\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9861\n",
      "Epoch 00119: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0430 - acc: 0.9861 - val_loss: 0.1703 - val_acc: 0.9618\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9863\n",
      "Epoch 00120: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0419 - acc: 0.9863 - val_loss: 0.1843 - val_acc: 0.9606\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9867\n",
      "Epoch 00121: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0408 - acc: 0.9866 - val_loss: 0.1821 - val_acc: 0.9609\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9867\n",
      "Epoch 00122: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0398 - acc: 0.9866 - val_loss: 0.2028 - val_acc: 0.9592\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9871\n",
      "Epoch 00123: val_loss did not improve from 0.13469\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0390 - acc: 0.9871 - val_loss: 0.1970 - val_acc: 0.9590\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 746us/sample - loss: 0.1704 - acc: 0.9516\n",
      "Loss: 0.17036764390557727 Accuracy: 0.95160955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9480 - acc: 0.3663\n",
      "Epoch 00001: val_loss improved from inf to 1.11162, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/001-1.1116.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.9480 - acc: 0.3663 - val_loss: 1.1116 - val_acc: 0.6781\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0536 - acc: 0.6708\n",
      "Epoch 00002: val_loss improved from 1.11162 to 0.71153, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/002-0.7115.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.0536 - acc: 0.6708 - val_loss: 0.7115 - val_acc: 0.7987\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7623 - acc: 0.7653\n",
      "Epoch 00003: val_loss improved from 0.71153 to 0.51172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/003-0.5117.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7623 - acc: 0.7653 - val_loss: 0.5117 - val_acc: 0.8623\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6027 - acc: 0.8139\n",
      "Epoch 00004: val_loss improved from 0.51172 to 0.39375, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/004-0.3937.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.6027 - acc: 0.8139 - val_loss: 0.3937 - val_acc: 0.8896\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.8433\n",
      "Epoch 00005: val_loss improved from 0.39375 to 0.32253, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/005-0.3225.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5078 - acc: 0.8433 - val_loss: 0.3225 - val_acc: 0.9138\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8636\n",
      "Epoch 00006: val_loss improved from 0.32253 to 0.31014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/006-0.3101.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4437 - acc: 0.8637 - val_loss: 0.3101 - val_acc: 0.9061\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8774\n",
      "Epoch 00007: val_loss improved from 0.31014 to 0.25924, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/007-0.2592.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4009 - acc: 0.8774 - val_loss: 0.2592 - val_acc: 0.9206\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8863\n",
      "Epoch 00008: val_loss improved from 0.25924 to 0.24441, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/008-0.2444.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3700 - acc: 0.8863 - val_loss: 0.2444 - val_acc: 0.9245\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8970\n",
      "Epoch 00009: val_loss improved from 0.24441 to 0.23195, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/009-0.2320.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3344 - acc: 0.8970 - val_loss: 0.2320 - val_acc: 0.9301\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3174 - acc: 0.9024\n",
      "Epoch 00010: val_loss improved from 0.23195 to 0.20522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/010-0.2052.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3174 - acc: 0.9025 - val_loss: 0.2052 - val_acc: 0.9390\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9095\n",
      "Epoch 00011: val_loss did not improve from 0.20522\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2926 - acc: 0.9095 - val_loss: 0.2152 - val_acc: 0.9311\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9141\n",
      "Epoch 00012: val_loss did not improve from 0.20522\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2725 - acc: 0.9141 - val_loss: 0.2083 - val_acc: 0.9329\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9189\n",
      "Epoch 00013: val_loss improved from 0.20522 to 0.16757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/013-0.1676.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2568 - acc: 0.9189 - val_loss: 0.1676 - val_acc: 0.9511\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9258\n",
      "Epoch 00014: val_loss did not improve from 0.16757\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2377 - acc: 0.9258 - val_loss: 0.1716 - val_acc: 0.9476\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9289\n",
      "Epoch 00015: val_loss improved from 0.16757 to 0.15706, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/015-0.1571.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2242 - acc: 0.9289 - val_loss: 0.1571 - val_acc: 0.9532\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9322\n",
      "Epoch 00016: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2119 - acc: 0.9322 - val_loss: 0.1701 - val_acc: 0.9497\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9353\n",
      "Epoch 00017: val_loss did not improve from 0.15706\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2032 - acc: 0.9353 - val_loss: 0.1824 - val_acc: 0.9411\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9389\n",
      "Epoch 00018: val_loss improved from 0.15706 to 0.14394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/018-0.1439.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1897 - acc: 0.9389 - val_loss: 0.1439 - val_acc: 0.9560\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9429\n",
      "Epoch 00019: val_loss did not improve from 0.14394\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1831 - acc: 0.9429 - val_loss: 0.1579 - val_acc: 0.9527\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9461\n",
      "Epoch 00020: val_loss improved from 0.14394 to 0.14281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/020-0.1428.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1721 - acc: 0.9461 - val_loss: 0.1428 - val_acc: 0.9571\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9464\n",
      "Epoch 00021: val_loss improved from 0.14281 to 0.13654, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/021-0.1365.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1675 - acc: 0.9464 - val_loss: 0.1365 - val_acc: 0.9602\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9485\n",
      "Epoch 00022: val_loss did not improve from 0.13654\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1598 - acc: 0.9485 - val_loss: 0.1410 - val_acc: 0.9541\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9505\n",
      "Epoch 00023: val_loss did not improve from 0.13654\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1548 - acc: 0.9505 - val_loss: 0.1540 - val_acc: 0.9532\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9541\n",
      "Epoch 00024: val_loss improved from 0.13654 to 0.13034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/024-0.1303.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1453 - acc: 0.9541 - val_loss: 0.1303 - val_acc: 0.9630\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9560\n",
      "Epoch 00025: val_loss improved from 0.13034 to 0.12850, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/025-0.1285.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1397 - acc: 0.9560 - val_loss: 0.1285 - val_acc: 0.9609\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9576\n",
      "Epoch 00026: val_loss did not improve from 0.12850\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1295 - acc: 0.9576 - val_loss: 0.1390 - val_acc: 0.9620\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9598\n",
      "Epoch 00027: val_loss did not improve from 0.12850\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1253 - acc: 0.9598 - val_loss: 0.1332 - val_acc: 0.9588\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9596\n",
      "Epoch 00028: val_loss did not improve from 0.12850\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1249 - acc: 0.9596 - val_loss: 0.1396 - val_acc: 0.9618\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9615\n",
      "Epoch 00029: val_loss did not improve from 0.12850\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1188 - acc: 0.9615 - val_loss: 0.1568 - val_acc: 0.9560\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9601\n",
      "Epoch 00030: val_loss did not improve from 0.12850\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1180 - acc: 0.9601 - val_loss: 0.1353 - val_acc: 0.9604\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9634\n",
      "Epoch 00031: val_loss improved from 0.12850 to 0.12833, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/031-0.1283.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1089 - acc: 0.9634 - val_loss: 0.1283 - val_acc: 0.9613\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9649\n",
      "Epoch 00032: val_loss did not improve from 0.12833\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1035 - acc: 0.9649 - val_loss: 0.1353 - val_acc: 0.9627\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9671\n",
      "Epoch 00033: val_loss improved from 0.12833 to 0.12789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/033-0.1279.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1008 - acc: 0.9671 - val_loss: 0.1279 - val_acc: 0.9597\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9670\n",
      "Epoch 00034: val_loss did not improve from 0.12789\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0993 - acc: 0.9670 - val_loss: 0.1525 - val_acc: 0.9560\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9678\n",
      "Epoch 00035: val_loss did not improve from 0.12789\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0950 - acc: 0.9678 - val_loss: 0.1320 - val_acc: 0.9625\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9708\n",
      "Epoch 00036: val_loss did not improve from 0.12789\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0897 - acc: 0.9708 - val_loss: 0.1447 - val_acc: 0.9641\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9705\n",
      "Epoch 00037: val_loss improved from 0.12789 to 0.12594, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/037-0.1259.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0868 - acc: 0.9705 - val_loss: 0.1259 - val_acc: 0.9667\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9715\n",
      "Epoch 00038: val_loss did not improve from 0.12594\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0865 - acc: 0.9715 - val_loss: 0.1295 - val_acc: 0.9627\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9743\n",
      "Epoch 00039: val_loss did not improve from 0.12594\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0796 - acc: 0.9743 - val_loss: 0.1276 - val_acc: 0.9655\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9747\n",
      "Epoch 00040: val_loss improved from 0.12594 to 0.12431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/040-0.1243.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0749 - acc: 0.9747 - val_loss: 0.1243 - val_acc: 0.9644\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9737\n",
      "Epoch 00041: val_loss did not improve from 0.12431\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0796 - acc: 0.9737 - val_loss: 0.1251 - val_acc: 0.9662\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9759\n",
      "Epoch 00042: val_loss improved from 0.12431 to 0.12230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv_checkpoint/042-0.1223.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0714 - acc: 0.9759 - val_loss: 0.1223 - val_acc: 0.9679\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9763\n",
      "Epoch 00043: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0723 - acc: 0.9763 - val_loss: 0.1354 - val_acc: 0.9641\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9768\n",
      "Epoch 00044: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0680 - acc: 0.9768 - val_loss: 0.1383 - val_acc: 0.9620\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9775\n",
      "Epoch 00045: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0662 - acc: 0.9775 - val_loss: 0.1302 - val_acc: 0.9658\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9797\n",
      "Epoch 00046: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0607 - acc: 0.9797 - val_loss: 0.1467 - val_acc: 0.9632\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9806\n",
      "Epoch 00047: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0601 - acc: 0.9806 - val_loss: 0.1429 - val_acc: 0.9613\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9803\n",
      "Epoch 00048: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0567 - acc: 0.9803 - val_loss: 0.1298 - val_acc: 0.9669\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9805\n",
      "Epoch 00049: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0573 - acc: 0.9805 - val_loss: 0.1306 - val_acc: 0.9623\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9815\n",
      "Epoch 00050: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0555 - acc: 0.9815 - val_loss: 0.1265 - val_acc: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9815\n",
      "Epoch 00051: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0544 - acc: 0.9815 - val_loss: 0.1343 - val_acc: 0.9665\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9833\n",
      "Epoch 00052: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0485 - acc: 0.9833 - val_loss: 0.1382 - val_acc: 0.9646\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9827\n",
      "Epoch 00053: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0510 - acc: 0.9827 - val_loss: 0.1304 - val_acc: 0.9655\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9826\n",
      "Epoch 00054: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0519 - acc: 0.9826 - val_loss: 0.1520 - val_acc: 0.9627\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9829\n",
      "Epoch 00055: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0540 - acc: 0.9829 - val_loss: 0.1366 - val_acc: 0.9674\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9844\n",
      "Epoch 00056: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0471 - acc: 0.9843 - val_loss: 0.1541 - val_acc: 0.9637\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9853\n",
      "Epoch 00057: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0429 - acc: 0.9853 - val_loss: 0.1451 - val_acc: 0.9658\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9855\n",
      "Epoch 00058: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0428 - acc: 0.9855 - val_loss: 0.1547 - val_acc: 0.9681\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9863\n",
      "Epoch 00059: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0407 - acc: 0.9863 - val_loss: 0.1431 - val_acc: 0.9646\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9875\n",
      "Epoch 00060: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0393 - acc: 0.9875 - val_loss: 0.1582 - val_acc: 0.9672\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9867\n",
      "Epoch 00061: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0400 - acc: 0.9867 - val_loss: 0.1666 - val_acc: 0.9599\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9857\n",
      "Epoch 00062: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0430 - acc: 0.9857 - val_loss: 0.1361 - val_acc: 0.9660\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9867\n",
      "Epoch 00063: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0403 - acc: 0.9867 - val_loss: 0.1250 - val_acc: 0.9679\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9887\n",
      "Epoch 00064: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0341 - acc: 0.9887 - val_loss: 0.1513 - val_acc: 0.9646\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9877\n",
      "Epoch 00065: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0360 - acc: 0.9877 - val_loss: 0.1865 - val_acc: 0.9613\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9875\n",
      "Epoch 00066: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0373 - acc: 0.9875 - val_loss: 0.1627 - val_acc: 0.9648\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9898\n",
      "Epoch 00067: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0316 - acc: 0.9898 - val_loss: 0.1557 - val_acc: 0.9641\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9880\n",
      "Epoch 00068: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0356 - acc: 0.9880 - val_loss: 0.1509 - val_acc: 0.9630\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9894\n",
      "Epoch 00069: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0321 - acc: 0.9894 - val_loss: 0.1590 - val_acc: 0.9655\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9891\n",
      "Epoch 00070: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0327 - acc: 0.9891 - val_loss: 0.1654 - val_acc: 0.9609\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9897\n",
      "Epoch 00071: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0311 - acc: 0.9897 - val_loss: 0.1672 - val_acc: 0.9639\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9904\n",
      "Epoch 00072: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0302 - acc: 0.9904 - val_loss: 0.1765 - val_acc: 0.9611\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9876\n",
      "Epoch 00073: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0353 - acc: 0.9876 - val_loss: 0.1464 - val_acc: 0.9683\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9904\n",
      "Epoch 00074: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0288 - acc: 0.9904 - val_loss: 0.1693 - val_acc: 0.9646\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9894\n",
      "Epoch 00075: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0318 - acc: 0.9894 - val_loss: 0.1542 - val_acc: 0.9651\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9906\n",
      "Epoch 00076: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0278 - acc: 0.9906 - val_loss: 0.1467 - val_acc: 0.9662\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9911\n",
      "Epoch 00077: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0281 - acc: 0.9911 - val_loss: 0.1667 - val_acc: 0.9672\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9905\n",
      "Epoch 00078: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0273 - acc: 0.9905 - val_loss: 0.1695 - val_acc: 0.9641\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 00079: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0255 - acc: 0.9916 - val_loss: 0.1613 - val_acc: 0.9653\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9914\n",
      "Epoch 00080: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0253 - acc: 0.9914 - val_loss: 0.1632 - val_acc: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9916\n",
      "Epoch 00081: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0267 - acc: 0.9916 - val_loss: 0.1732 - val_acc: 0.9637\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9911\n",
      "Epoch 00082: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0271 - acc: 0.9911 - val_loss: 0.1584 - val_acc: 0.9646\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9929\n",
      "Epoch 00083: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 0.1617 - val_acc: 0.9681\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9927\n",
      "Epoch 00084: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0230 - acc: 0.9927 - val_loss: 0.1572 - val_acc: 0.9648\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9920\n",
      "Epoch 00085: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.1652 - val_acc: 0.9653\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9917\n",
      "Epoch 00086: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0247 - acc: 0.9917 - val_loss: 0.1741 - val_acc: 0.9651\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9927\n",
      "Epoch 00087: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0220 - acc: 0.9927 - val_loss: 0.1855 - val_acc: 0.9662\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9920\n",
      "Epoch 00088: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0237 - acc: 0.9920 - val_loss: 0.1647 - val_acc: 0.9662\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9928\n",
      "Epoch 00089: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0211 - acc: 0.9928 - val_loss: 0.1760 - val_acc: 0.9634\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9922\n",
      "Epoch 00090: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0250 - acc: 0.9922 - val_loss: 0.1812 - val_acc: 0.9630\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9933\n",
      "Epoch 00091: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0211 - acc: 0.9933 - val_loss: 0.1811 - val_acc: 0.9646\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9932\n",
      "Epoch 00092: val_loss did not improve from 0.12230\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0213 - acc: 0.9932 - val_loss: 0.1767 - val_acc: 0.9672\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 731us/sample - loss: 0.1606 - acc: 0.9558\n",
      "Loss: 0.16059520496819496 Accuracy: 0.9557632\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6743 - acc: 0.4588\n",
      "Epoch 00001: val_loss improved from inf to 0.97354, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/001-0.9735.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 1.6742 - acc: 0.4588 - val_loss: 0.9735 - val_acc: 0.6965\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7549\n",
      "Epoch 00002: val_loss improved from 0.97354 to 0.52610, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/002-0.5261.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.7736 - acc: 0.7549 - val_loss: 0.5261 - val_acc: 0.8314\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.8248\n",
      "Epoch 00003: val_loss improved from 0.52610 to 0.39407, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/003-0.3941.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.5520 - acc: 0.8247 - val_loss: 0.3941 - val_acc: 0.8782\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8615\n",
      "Epoch 00004: val_loss improved from 0.39407 to 0.28768, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/004-0.2877.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.4376 - acc: 0.8615 - val_loss: 0.2877 - val_acc: 0.9108\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3633 - acc: 0.8852\n",
      "Epoch 00005: val_loss improved from 0.28768 to 0.21387, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/005-0.2139.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3633 - acc: 0.8852 - val_loss: 0.2139 - val_acc: 0.9359\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.8987\n",
      "Epoch 00006: val_loss did not improve from 0.21387\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3170 - acc: 0.8987 - val_loss: 0.2251 - val_acc: 0.9292\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9104\n",
      "Epoch 00007: val_loss improved from 0.21387 to 0.18920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/007-0.1892.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2814 - acc: 0.9104 - val_loss: 0.1892 - val_acc: 0.9422\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9210\n",
      "Epoch 00008: val_loss improved from 0.18920 to 0.17299, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/008-0.1730.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2484 - acc: 0.9210 - val_loss: 0.1730 - val_acc: 0.9467\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9276\n",
      "Epoch 00009: val_loss did not improve from 0.17299\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2282 - acc: 0.9276 - val_loss: 0.1787 - val_acc: 0.9439\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9329\n",
      "Epoch 00010: val_loss improved from 0.17299 to 0.16920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/010-0.1692.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2105 - acc: 0.9329 - val_loss: 0.1692 - val_acc: 0.9450\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9365\n",
      "Epoch 00011: val_loss improved from 0.16920 to 0.15846, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/011-0.1585.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1932 - acc: 0.9365 - val_loss: 0.1585 - val_acc: 0.9478\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9423\n",
      "Epoch 00012: val_loss did not improve from 0.15846\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1773 - acc: 0.9423 - val_loss: 0.1713 - val_acc: 0.9481\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9469\n",
      "Epoch 00013: val_loss improved from 0.15846 to 0.15208, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/013-0.1521.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1620 - acc: 0.9469 - val_loss: 0.1521 - val_acc: 0.9522\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9510\n",
      "Epoch 00014: val_loss did not improve from 0.15208\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1481 - acc: 0.9510 - val_loss: 0.1605 - val_acc: 0.9548\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9559\n",
      "Epoch 00015: val_loss did not improve from 0.15208\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1355 - acc: 0.9559 - val_loss: 0.1863 - val_acc: 0.9418\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9564\n",
      "Epoch 00016: val_loss improved from 0.15208 to 0.13636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/016-0.1364.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1319 - acc: 0.9564 - val_loss: 0.1364 - val_acc: 0.9588\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9619\n",
      "Epoch 00017: val_loss did not improve from 0.13636\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1179 - acc: 0.9619 - val_loss: 0.1543 - val_acc: 0.9536\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9610\n",
      "Epoch 00018: val_loss did not improve from 0.13636\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1130 - acc: 0.9610 - val_loss: 0.1365 - val_acc: 0.9609\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9636\n",
      "Epoch 00019: val_loss improved from 0.13636 to 0.12695, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/019-0.1269.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1075 - acc: 0.9636 - val_loss: 0.1269 - val_acc: 0.9627\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9663\n",
      "Epoch 00020: val_loss improved from 0.12695 to 0.11677, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv_checkpoint/020-0.1168.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0999 - acc: 0.9663 - val_loss: 0.1168 - val_acc: 0.9646\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9680\n",
      "Epoch 00021: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0923 - acc: 0.9680 - val_loss: 0.1345 - val_acc: 0.9625\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9708\n",
      "Epoch 00022: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0873 - acc: 0.9708 - val_loss: 0.1558 - val_acc: 0.9560\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9735\n",
      "Epoch 00023: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0778 - acc: 0.9735 - val_loss: 0.1248 - val_acc: 0.9660\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9744\n",
      "Epoch 00024: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0757 - acc: 0.9744 - val_loss: 0.1238 - val_acc: 0.9651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9766\n",
      "Epoch 00025: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0698 - acc: 0.9766 - val_loss: 0.1216 - val_acc: 0.9658\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9771\n",
      "Epoch 00026: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0673 - acc: 0.9771 - val_loss: 0.1355 - val_acc: 0.9630\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9777\n",
      "Epoch 00027: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0651 - acc: 0.9777 - val_loss: 0.1618 - val_acc: 0.9597\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9798\n",
      "Epoch 00028: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0589 - acc: 0.9798 - val_loss: 0.1320 - val_acc: 0.9672\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9795\n",
      "Epoch 00029: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0586 - acc: 0.9795 - val_loss: 0.1306 - val_acc: 0.9651\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9822\n",
      "Epoch 00030: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0529 - acc: 0.9822 - val_loss: 0.1484 - val_acc: 0.9609\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9828\n",
      "Epoch 00031: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0511 - acc: 0.9828 - val_loss: 0.1592 - val_acc: 0.9581\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9825\n",
      "Epoch 00032: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0509 - acc: 0.9825 - val_loss: 0.1588 - val_acc: 0.9585\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9843\n",
      "Epoch 00033: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0451 - acc: 0.9843 - val_loss: 0.1553 - val_acc: 0.9627\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9851\n",
      "Epoch 00034: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0456 - acc: 0.9851 - val_loss: 0.1378 - val_acc: 0.9672\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9862\n",
      "Epoch 00035: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.0413 - acc: 0.9862 - val_loss: 0.1541 - val_acc: 0.9655\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9858\n",
      "Epoch 00036: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.0414 - acc: 0.9858 - val_loss: 0.1403 - val_acc: 0.9653\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9869\n",
      "Epoch 00037: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.0406 - acc: 0.9869 - val_loss: 0.1519 - val_acc: 0.9651\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9861\n",
      "Epoch 00038: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.0412 - acc: 0.9861 - val_loss: 0.1509 - val_acc: 0.9639\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9880\n",
      "Epoch 00039: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 0.0360 - acc: 0.9880 - val_loss: 0.1394 - val_acc: 0.9679\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9902\n",
      "Epoch 00040: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 0.0302 - acc: 0.9902 - val_loss: 0.1653 - val_acc: 0.9623\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9886\n",
      "Epoch 00041: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 0.0331 - acc: 0.9886 - val_loss: 0.1748 - val_acc: 0.9611\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9893\n",
      "Epoch 00042: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0316 - acc: 0.9893 - val_loss: 0.1679 - val_acc: 0.9646\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9892\n",
      "Epoch 00043: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0318 - acc: 0.9892 - val_loss: 0.1422 - val_acc: 0.9711\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9892\n",
      "Epoch 00044: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0309 - acc: 0.9892 - val_loss: 0.1617 - val_acc: 0.9641\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9904\n",
      "Epoch 00045: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0289 - acc: 0.9904 - val_loss: 0.1508 - val_acc: 0.9648\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9902\n",
      "Epoch 00046: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0302 - acc: 0.9902 - val_loss: 0.1688 - val_acc: 0.9648\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9923\n",
      "Epoch 00047: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0234 - acc: 0.9923 - val_loss: 0.1648 - val_acc: 0.9665\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9897\n",
      "Epoch 00048: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0300 - acc: 0.9897 - val_loss: 0.1423 - val_acc: 0.9700\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9924\n",
      "Epoch 00049: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0236 - acc: 0.9924 - val_loss: 0.1719 - val_acc: 0.9639\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9917\n",
      "Epoch 00050: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0256 - acc: 0.9917 - val_loss: 0.1737 - val_acc: 0.9648\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 00051: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0284 - acc: 0.9908 - val_loss: 0.1529 - val_acc: 0.9690\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9916\n",
      "Epoch 00052: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0261 - acc: 0.9916 - val_loss: 0.1772 - val_acc: 0.9662\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 00053: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0174 - acc: 0.9943 - val_loss: 0.1677 - val_acc: 0.9676\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9925\n",
      "Epoch 00054: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0239 - acc: 0.9925 - val_loss: 0.1707 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9921\n",
      "Epoch 00055: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0244 - acc: 0.9921 - val_loss: 0.1706 - val_acc: 0.9688\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9940\n",
      "Epoch 00056: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0186 - acc: 0.9940 - val_loss: 0.1871 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9933\n",
      "Epoch 00057: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0217 - acc: 0.9933 - val_loss: 0.1695 - val_acc: 0.9679\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9935\n",
      "Epoch 00058: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0228 - acc: 0.9935 - val_loss: 0.1464 - val_acc: 0.9697\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9952\n",
      "Epoch 00059: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0150 - acc: 0.9952 - val_loss: 0.1891 - val_acc: 0.9660\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9918\n",
      "Epoch 00060: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0243 - acc: 0.9918 - val_loss: 0.1955 - val_acc: 0.9644\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9942\n",
      "Epoch 00061: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0166 - acc: 0.9942 - val_loss: 0.1800 - val_acc: 0.9674\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 00062: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.1883 - val_acc: 0.9679\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9923\n",
      "Epoch 00063: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0243 - acc: 0.9923 - val_loss: 0.1776 - val_acc: 0.9639\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 00064: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0147 - acc: 0.9954 - val_loss: 0.1899 - val_acc: 0.9653\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 00065: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.1730 - val_acc: 0.9704\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9944\n",
      "Epoch 00066: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0178 - acc: 0.9944 - val_loss: 0.1880 - val_acc: 0.9660\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9944\n",
      "Epoch 00067: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0173 - acc: 0.9944 - val_loss: 0.1772 - val_acc: 0.9690\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9942\n",
      "Epoch 00068: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0181 - acc: 0.9942 - val_loss: 0.1908 - val_acc: 0.9662\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9954\n",
      "Epoch 00069: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0154 - acc: 0.9954 - val_loss: 0.1776 - val_acc: 0.9648\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9958\n",
      "Epoch 00070: val_loss did not improve from 0.11677\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0136 - acc: 0.9958 - val_loss: 0.1984 - val_acc: 0.9639\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 809us/sample - loss: 0.1962 - acc: 0.9439\n",
      "Loss: 0.19617292084155735 Accuracy: 0.94392526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 746us/sample - loss: 0.6031 - acc: 0.8297\n",
      "Loss: 0.6031385346736492 Accuracy: 0.82969886\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 757us/sample - loss: 0.4066 - acc: 0.8883\n",
      "Loss: 0.4066322600111164 Accuracy: 0.88826585\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 782us/sample - loss: 0.2629 - acc: 0.9225\n",
      "Loss: 0.26294008484758197 Accuracy: 0.92253375\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 815us/sample - loss: 0.1704 - acc: 0.9516\n",
      "Loss: 0.17036764390557727 Accuracy: 0.95160955\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 829us/sample - loss: 0.1606 - acc: 0.9558\n",
      "Loss: 0.16059520496819496 Accuracy: 0.9557632\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 840us/sample - loss: 0.1962 - acc: 0.9439\n",
      "Loss: 0.19617292084155735 Accuracy: 0.94392526\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_ch_128_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           4112        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 168,976\n",
      "Trainable params: 168,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 782us/sample - loss: 0.6060 - acc: 0.8264\n",
      "Loss: 0.6060064157592916 Accuracy: 0.8263759\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 128)   0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 128)    0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 128)    0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 128)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 128)     0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           4112        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 251,024\n",
      "Trainable params: 251,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 806us/sample - loss: 0.4098 - acc: 0.8893\n",
      "Loss: 0.40976378576035066 Accuracy: 0.8893043\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 128)   0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 128)    0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 128)    0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 128)    0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 128)    0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 128)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 128)     0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 128)     0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 256)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 256)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 256)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 384)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 384)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           6160        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 417,168\n",
      "Trainable params: 417,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 834us/sample - loss: 0.2957 - acc: 0.9223\n",
      "Loss: 0.2957171440248301 Accuracy: 0.9223261\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 128)   0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 128)    0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 128)    0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 128)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 128)    0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 128)     0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 128)     0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 128)     0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 256)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 256)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 256)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 256)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 256)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 256)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           8208        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 747,152\n",
      "Trainable params: 747,152\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 861us/sample - loss: 0.2245 - acc: 0.9472\n",
      "Loss: 0.22450598624388873 Accuracy: 0.94724816\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 128)   0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 128)    0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 128)    0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 128)    0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 128)    0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 128)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 128)     0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 128)     0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 256)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 256)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 256)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 256)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 256)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 256)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 256)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 256)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           8208        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,075,088\n",
      "Trainable params: 1,075,088\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 887us/sample - loss: 0.2028 - acc: 0.9585\n",
      "Loss: 0.2028033662915253 Accuracy: 0.95846313\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 128)   768         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 128)   0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 128)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 128)    0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 128)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 128)    0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 128)     0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 128)     0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 128)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 256)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 256)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 256)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 256)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 256)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 256)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 256)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 256)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 256)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           8208        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,403,024\n",
      "Trainable params: 1,403,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 894us/sample - loss: 0.2786 - acc: 0.9472\n",
      "Loss: 0.27856179629388494 Accuracy: 0.94724816\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
