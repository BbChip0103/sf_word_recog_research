{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,195,504\n",
      "Trainable params: 8,195,376\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 4,105,968\n",
      "Trainable params: 4,105,712\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,064,432\n",
      "Trainable params: 2,064,048\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,046,896\n",
      "Trainable params: 1,046,384\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,065,968\n",
      "Trainable params: 1,065,200\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 579,184\n",
      "Trainable params: 578,160\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 348,400\n",
      "Trainable params: 347,120\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 245,616\n",
      "Trainable params: 244,080\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 319,600\n",
      "Trainable params: 317,552\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 355,696\n",
      "Trainable params: 353,136\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 422,512\n",
      "Trainable params: 419,440\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 505,712\n",
      "Trainable params: 502,128\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 801,136\n",
      "Trainable params: 796,528\n",
      "Non-trainable params: 4,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4862 - acc: 0.3072\n",
      "Epoch 00001: val_loss improved from inf to 2.04571, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_3_conv_checkpoint/001-2.0457.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 2.4863 - acc: 0.3072 - val_loss: 2.0457 - val_acc: 0.3538\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6876 - acc: 0.4980\n",
      "Epoch 00002: val_loss improved from 2.04571 to 1.69938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_3_conv_checkpoint/002-1.6994.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.6877 - acc: 0.4980 - val_loss: 1.6994 - val_acc: 0.5108\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3139 - acc: 0.6055\n",
      "Epoch 00003: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.3141 - acc: 0.6054 - val_loss: 1.7734 - val_acc: 0.4875\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.7006\n",
      "Epoch 00004: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9719 - acc: 0.7006 - val_loss: 1.7422 - val_acc: 0.5160\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7680\n",
      "Epoch 00005: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7408 - acc: 0.7680 - val_loss: 1.7548 - val_acc: 0.5174\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.8188\n",
      "Epoch 00006: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5772 - acc: 0.8188 - val_loss: 1.9646 - val_acc: 0.5010\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8522\n",
      "Epoch 00007: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4701 - acc: 0.8522 - val_loss: 1.9261 - val_acc: 0.5267\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8813\n",
      "Epoch 00008: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3788 - acc: 0.8813 - val_loss: 2.0821 - val_acc: 0.5206\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8965\n",
      "Epoch 00009: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3227 - acc: 0.8965 - val_loss: 2.0707 - val_acc: 0.5355\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9098\n",
      "Epoch 00010: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2875 - acc: 0.9098 - val_loss: 2.4023 - val_acc: 0.5006\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9174\n",
      "Epoch 00011: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2649 - acc: 0.9173 - val_loss: 2.1902 - val_acc: 0.5418\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9242\n",
      "Epoch 00012: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2418 - acc: 0.9242 - val_loss: 2.0249 - val_acc: 0.5565\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9305\n",
      "Epoch 00013: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2185 - acc: 0.9305 - val_loss: 2.4552 - val_acc: 0.5115\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9406\n",
      "Epoch 00014: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1937 - acc: 0.9406 - val_loss: 2.2315 - val_acc: 0.5481\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9439\n",
      "Epoch 00015: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1816 - acc: 0.9438 - val_loss: 2.2004 - val_acc: 0.5448\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9488\n",
      "Epoch 00016: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1678 - acc: 0.9488 - val_loss: 2.3779 - val_acc: 0.5353\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9487\n",
      "Epoch 00017: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1628 - acc: 0.9486 - val_loss: 2.4637 - val_acc: 0.5372\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1658 - acc: 0.9488\n",
      "Epoch 00018: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1660 - acc: 0.9488 - val_loss: 2.4008 - val_acc: 0.5444\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9583\n",
      "Epoch 00019: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1365 - acc: 0.9583 - val_loss: 2.5550 - val_acc: 0.5469\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9597\n",
      "Epoch 00020: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1316 - acc: 0.9597 - val_loss: 2.3673 - val_acc: 0.5677\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9602\n",
      "Epoch 00021: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1335 - acc: 0.9602 - val_loss: 2.3394 - val_acc: 0.5637\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9625\n",
      "Epoch 00022: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1211 - acc: 0.9625 - val_loss: 2.4087 - val_acc: 0.5567\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9616\n",
      "Epoch 00023: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1251 - acc: 0.9616 - val_loss: 2.5872 - val_acc: 0.5339\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9653\n",
      "Epoch 00024: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1145 - acc: 0.9653 - val_loss: 2.3206 - val_acc: 0.5721\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9651\n",
      "Epoch 00025: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1183 - acc: 0.9651 - val_loss: 2.5945 - val_acc: 0.5395\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9660\n",
      "Epoch 00026: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1129 - acc: 0.9660 - val_loss: 2.5307 - val_acc: 0.5497\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9682\n",
      "Epoch 00027: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1060 - acc: 0.9682 - val_loss: 2.7151 - val_acc: 0.5563\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9684\n",
      "Epoch 00028: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1039 - acc: 0.9684 - val_loss: 2.6295 - val_acc: 0.5609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9702\n",
      "Epoch 00029: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1012 - acc: 0.9701 - val_loss: 3.0232 - val_acc: 0.5181\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9746\n",
      "Epoch 00030: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0873 - acc: 0.9746 - val_loss: 2.5253 - val_acc: 0.5649\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9713\n",
      "Epoch 00031: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0958 - acc: 0.9713 - val_loss: 2.7900 - val_acc: 0.5320\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9724\n",
      "Epoch 00032: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0949 - acc: 0.9723 - val_loss: 2.8698 - val_acc: 0.5306\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9730\n",
      "Epoch 00033: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0910 - acc: 0.9730 - val_loss: 3.1634 - val_acc: 0.4997\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9743\n",
      "Epoch 00034: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0880 - acc: 0.9743 - val_loss: 2.7306 - val_acc: 0.5593\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9760\n",
      "Epoch 00035: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0819 - acc: 0.9760 - val_loss: 2.5531 - val_acc: 0.5565\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9738\n",
      "Epoch 00036: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0889 - acc: 0.9738 - val_loss: 2.6699 - val_acc: 0.5721\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9752\n",
      "Epoch 00037: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0825 - acc: 0.9752 - val_loss: 2.7492 - val_acc: 0.5497\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9773\n",
      "Epoch 00038: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0777 - acc: 0.9773 - val_loss: 2.6671 - val_acc: 0.5730\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9783\n",
      "Epoch 00039: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0742 - acc: 0.9783 - val_loss: 2.7338 - val_acc: 0.5702\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9767\n",
      "Epoch 00040: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0790 - acc: 0.9767 - val_loss: 2.6878 - val_acc: 0.5653\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9799\n",
      "Epoch 00041: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0691 - acc: 0.9799 - val_loss: 2.5916 - val_acc: 0.5754\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9795\n",
      "Epoch 00042: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0764 - acc: 0.9794 - val_loss: 2.6801 - val_acc: 0.5677\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9786\n",
      "Epoch 00043: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0732 - acc: 0.9786 - val_loss: 2.7817 - val_acc: 0.5479\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9800\n",
      "Epoch 00044: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0707 - acc: 0.9800 - val_loss: 2.6383 - val_acc: 0.5742\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9795\n",
      "Epoch 00045: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0713 - acc: 0.9795 - val_loss: 2.8803 - val_acc: 0.5362\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9810\n",
      "Epoch 00046: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0665 - acc: 0.9810 - val_loss: 2.7124 - val_acc: 0.5728\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9819\n",
      "Epoch 00047: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0635 - acc: 0.9819 - val_loss: 2.9531 - val_acc: 0.5535\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9792\n",
      "Epoch 00048: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0734 - acc: 0.9792 - val_loss: 2.7774 - val_acc: 0.5642\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9814\n",
      "Epoch 00049: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0677 - acc: 0.9814 - val_loss: 2.6009 - val_acc: 0.5791\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9844\n",
      "Epoch 00050: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0582 - acc: 0.9844 - val_loss: 2.6713 - val_acc: 0.5819\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9814\n",
      "Epoch 00051: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0641 - acc: 0.9814 - val_loss: 2.6175 - val_acc: 0.5791\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9832\n",
      "Epoch 00052: val_loss did not improve from 1.69938\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.0598 - acc: 0.9832 - val_loss: 3.1302 - val_acc: 0.5469\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 921us/sample - loss: 1.7901 - acc: 0.4741\n",
      "Loss: 1.7900645954710424 Accuracy: 0.4741433\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4832 - acc: 0.3126\n",
      "Epoch 00001: val_loss improved from inf to 1.76034, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv_checkpoint/001-1.7603.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 2.4831 - acc: 0.3127 - val_loss: 1.7603 - val_acc: 0.4547\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6443 - acc: 0.5017\n",
      "Epoch 00002: val_loss improved from 1.76034 to 1.51407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv_checkpoint/002-1.5141.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.6441 - acc: 0.5017 - val_loss: 1.5141 - val_acc: 0.5332\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3547 - acc: 0.5844\n",
      "Epoch 00003: val_loss improved from 1.51407 to 1.49689, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv_checkpoint/003-1.4969.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.3547 - acc: 0.5844 - val_loss: 1.4969 - val_acc: 0.5483\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1350 - acc: 0.6489\n",
      "Epoch 00004: val_loss did not improve from 1.49689\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1352 - acc: 0.6489 - val_loss: 1.5509 - val_acc: 0.5339\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9590 - acc: 0.7018\n",
      "Epoch 00005: val_loss improved from 1.49689 to 1.33957, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv_checkpoint/005-1.3396.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9592 - acc: 0.7017 - val_loss: 1.3396 - val_acc: 0.5993\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8328 - acc: 0.7404\n",
      "Epoch 00006: val_loss improved from 1.33957 to 1.23576, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv_checkpoint/006-1.2358.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8330 - acc: 0.7404 - val_loss: 1.2358 - val_acc: 0.6306\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7059 - acc: 0.7785\n",
      "Epoch 00007: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7059 - acc: 0.7785 - val_loss: 1.3370 - val_acc: 0.6245\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8108\n",
      "Epoch 00008: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6038 - acc: 0.8107 - val_loss: 1.2686 - val_acc: 0.6457\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.8316\n",
      "Epoch 00009: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5286 - acc: 0.8316 - val_loss: 1.2993 - val_acc: 0.6431\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8511\n",
      "Epoch 00010: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4663 - acc: 0.8510 - val_loss: 1.3322 - val_acc: 0.6473\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8641\n",
      "Epoch 00011: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4240 - acc: 0.8641 - val_loss: 1.4225 - val_acc: 0.6334\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8852\n",
      "Epoch 00012: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3625 - acc: 0.8853 - val_loss: 1.4624 - val_acc: 0.6320\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8933\n",
      "Epoch 00013: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3323 - acc: 0.8933 - val_loss: 1.2867 - val_acc: 0.6622\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9032\n",
      "Epoch 00014: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3036 - acc: 0.9032 - val_loss: 1.3429 - val_acc: 0.6578\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9100\n",
      "Epoch 00015: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2792 - acc: 0.9100 - val_loss: 1.4235 - val_acc: 0.6608\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9200\n",
      "Epoch 00016: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2574 - acc: 0.9199 - val_loss: 1.4867 - val_acc: 0.6431\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9219\n",
      "Epoch 00017: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2442 - acc: 0.9219 - val_loss: 1.4645 - val_acc: 0.6525\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9283\n",
      "Epoch 00018: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2229 - acc: 0.9283 - val_loss: 1.6856 - val_acc: 0.6154\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9319\n",
      "Epoch 00019: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2139 - acc: 0.9319 - val_loss: 1.4097 - val_acc: 0.6671\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9394\n",
      "Epoch 00020: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1930 - acc: 0.9394 - val_loss: 1.4272 - val_acc: 0.6713\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9415\n",
      "Epoch 00021: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1857 - acc: 0.9415 - val_loss: 1.4639 - val_acc: 0.6720\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9428\n",
      "Epoch 00022: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1842 - acc: 0.9428 - val_loss: 1.5844 - val_acc: 0.6569\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9468\n",
      "Epoch 00023: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1706 - acc: 0.9468 - val_loss: 1.5004 - val_acc: 0.6695\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9483\n",
      "Epoch 00024: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1652 - acc: 0.9483 - val_loss: 1.6624 - val_acc: 0.6569\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9489\n",
      "Epoch 00025: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1633 - acc: 0.9488 - val_loss: 1.9824 - val_acc: 0.5875\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9524\n",
      "Epoch 00026: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1487 - acc: 0.9525 - val_loss: 1.5407 - val_acc: 0.6720\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9516\n",
      "Epoch 00027: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1523 - acc: 0.9516 - val_loss: 1.6565 - val_acc: 0.6580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9558\n",
      "Epoch 00028: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1436 - acc: 0.9558 - val_loss: 2.0255 - val_acc: 0.6201\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9545\n",
      "Epoch 00029: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1430 - acc: 0.9545 - val_loss: 1.6299 - val_acc: 0.6636\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9593\n",
      "Epoch 00030: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1303 - acc: 0.9594 - val_loss: 1.6175 - val_acc: 0.6639\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9615\n",
      "Epoch 00031: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1248 - acc: 0.9615 - val_loss: 1.9013 - val_acc: 0.6334\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9610\n",
      "Epoch 00032: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1264 - acc: 0.9609 - val_loss: 1.7671 - val_acc: 0.6345\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9614\n",
      "Epoch 00033: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1257 - acc: 0.9614 - val_loss: 1.8644 - val_acc: 0.6343\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9632\n",
      "Epoch 00034: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1199 - acc: 0.9631 - val_loss: 1.6020 - val_acc: 0.6776\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9614\n",
      "Epoch 00035: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1264 - acc: 0.9613 - val_loss: 1.6855 - val_acc: 0.6632\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9665\n",
      "Epoch 00036: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1087 - acc: 0.9665 - val_loss: 1.7415 - val_acc: 0.6569\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9679\n",
      "Epoch 00037: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1054 - acc: 0.9679 - val_loss: 1.6860 - val_acc: 0.6778\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9680\n",
      "Epoch 00038: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1064 - acc: 0.9680 - val_loss: 1.6825 - val_acc: 0.6704\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9671\n",
      "Epoch 00039: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1072 - acc: 0.9671 - val_loss: 1.5780 - val_acc: 0.6867\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9679\n",
      "Epoch 00040: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1046 - acc: 0.9679 - val_loss: 1.6291 - val_acc: 0.6760\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9676\n",
      "Epoch 00041: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1049 - acc: 0.9676 - val_loss: 1.7419 - val_acc: 0.6706\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9706\n",
      "Epoch 00042: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0981 - acc: 0.9706 - val_loss: 1.6917 - val_acc: 0.6746\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9682\n",
      "Epoch 00043: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1050 - acc: 0.9682 - val_loss: 1.5732 - val_acc: 0.6946\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9709\n",
      "Epoch 00044: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0944 - acc: 0.9709 - val_loss: 1.7533 - val_acc: 0.6650\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9717\n",
      "Epoch 00045: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0936 - acc: 0.9717 - val_loss: 1.6706 - val_acc: 0.6830\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9706\n",
      "Epoch 00046: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0964 - acc: 0.9705 - val_loss: 1.7502 - val_acc: 0.6704\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9712\n",
      "Epoch 00047: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0952 - acc: 0.9711 - val_loss: 1.6970 - val_acc: 0.6795\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9714\n",
      "Epoch 00048: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0943 - acc: 0.9714 - val_loss: 1.6128 - val_acc: 0.7011\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9754\n",
      "Epoch 00049: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0817 - acc: 0.9754 - val_loss: 1.7428 - val_acc: 0.6890\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9749\n",
      "Epoch 00050: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0832 - acc: 0.9749 - val_loss: 1.7410 - val_acc: 0.6839\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9720\n",
      "Epoch 00051: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0958 - acc: 0.9720 - val_loss: 1.7361 - val_acc: 0.6832\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9739\n",
      "Epoch 00052: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0870 - acc: 0.9738 - val_loss: 1.7960 - val_acc: 0.6671\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9735\n",
      "Epoch 00053: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0862 - acc: 0.9735 - val_loss: 1.9346 - val_acc: 0.6646\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9753\n",
      "Epoch 00054: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0795 - acc: 0.9752 - val_loss: 1.7688 - val_acc: 0.6785\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9717\n",
      "Epoch 00055: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0936 - acc: 0.9717 - val_loss: 1.7663 - val_acc: 0.6653\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9776\n",
      "Epoch 00056: val_loss did not improve from 1.23576\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0779 - acc: 0.9776 - val_loss: 2.0048 - val_acc: 0.6534\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.3347 - acc: 0.5981\n",
      "Loss: 1.3346540165096057 Accuracy: 0.5981308\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3475 - acc: 0.3321\n",
      "Epoch 00001: val_loss improved from inf to 2.00329, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/001-2.0033.hdf5\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 2.3476 - acc: 0.3321 - val_loss: 2.0033 - val_acc: 0.3811\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5479 - acc: 0.5287\n",
      "Epoch 00002: val_loss improved from 2.00329 to 1.33594, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/002-1.3359.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 1.5479 - acc: 0.5287 - val_loss: 1.3359 - val_acc: 0.6059\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2724 - acc: 0.6067\n",
      "Epoch 00003: val_loss did not improve from 1.33594\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 1.2724 - acc: 0.6067 - val_loss: 1.4732 - val_acc: 0.5609\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0575 - acc: 0.6735\n",
      "Epoch 00004: val_loss did not improve from 1.33594\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 1.0577 - acc: 0.6734 - val_loss: 1.4694 - val_acc: 0.5735\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8979 - acc: 0.7215\n",
      "Epoch 00005: val_loss improved from 1.33594 to 1.28940, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/005-1.2894.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.8978 - acc: 0.7216 - val_loss: 1.2894 - val_acc: 0.6189\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7652\n",
      "Epoch 00006: val_loss improved from 1.28940 to 1.17607, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/006-1.1761.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.7468 - acc: 0.7652 - val_loss: 1.1761 - val_acc: 0.6667\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6265 - acc: 0.8004\n",
      "Epoch 00007: val_loss did not improve from 1.17607\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.6268 - acc: 0.8003 - val_loss: 1.1851 - val_acc: 0.6711\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5387 - acc: 0.8302\n",
      "Epoch 00008: val_loss did not improve from 1.17607\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.5389 - acc: 0.8302 - val_loss: 1.2286 - val_acc: 0.6599\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4630 - acc: 0.8525\n",
      "Epoch 00009: val_loss did not improve from 1.17607\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.4630 - acc: 0.8525 - val_loss: 1.1964 - val_acc: 0.6634\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8741\n",
      "Epoch 00010: val_loss improved from 1.17607 to 1.16589, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/010-1.1659.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.3987 - acc: 0.8740 - val_loss: 1.1659 - val_acc: 0.6890\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8892\n",
      "Epoch 00011: val_loss improved from 1.16589 to 1.13811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv_checkpoint/011-1.1381.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.3489 - acc: 0.8892 - val_loss: 1.1381 - val_acc: 0.7037\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8990\n",
      "Epoch 00012: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.3140 - acc: 0.8990 - val_loss: 1.1413 - val_acc: 0.7079\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9084\n",
      "Epoch 00013: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2900 - acc: 0.9084 - val_loss: 1.2795 - val_acc: 0.6883\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9151\n",
      "Epoch 00014: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2635 - acc: 0.9150 - val_loss: 1.2710 - val_acc: 0.6904\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9246\n",
      "Epoch 00015: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2343 - acc: 0.9245 - val_loss: 1.2415 - val_acc: 0.7086\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9295\n",
      "Epoch 00016: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2236 - acc: 0.9295 - val_loss: 1.2694 - val_acc: 0.7058\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9341\n",
      "Epoch 00017: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2052 - acc: 0.9341 - val_loss: 1.2500 - val_acc: 0.7037\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9390\n",
      "Epoch 00018: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1953 - acc: 0.9389 - val_loss: 1.3261 - val_acc: 0.6914\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9441\n",
      "Epoch 00019: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1797 - acc: 0.9441 - val_loss: 1.3468 - val_acc: 0.6962\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9434\n",
      "Epoch 00020: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1766 - acc: 0.9434 - val_loss: 1.3434 - val_acc: 0.6953\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9480\n",
      "Epoch 00021: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1624 - acc: 0.9480 - val_loss: 1.2420 - val_acc: 0.7128\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9506\n",
      "Epoch 00022: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1567 - acc: 0.9506 - val_loss: 1.3245 - val_acc: 0.7095\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9530\n",
      "Epoch 00023: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1498 - acc: 0.9530 - val_loss: 1.3168 - val_acc: 0.7181\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9525\n",
      "Epoch 00024: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1516 - acc: 0.9525 - val_loss: 1.3574 - val_acc: 0.7158\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9562\n",
      "Epoch 00025: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1379 - acc: 0.9562 - val_loss: 1.3189 - val_acc: 0.7065\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9570\n",
      "Epoch 00026: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1370 - acc: 0.9570 - val_loss: 1.3457 - val_acc: 0.7123\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9592\n",
      "Epoch 00027: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1297 - acc: 0.9592 - val_loss: 1.4364 - val_acc: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9598\n",
      "Epoch 00028: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1291 - acc: 0.9598 - val_loss: 1.3824 - val_acc: 0.7130\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9609\n",
      "Epoch 00029: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1251 - acc: 0.9609 - val_loss: 1.5275 - val_acc: 0.7081\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9640\n",
      "Epoch 00030: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1146 - acc: 0.9639 - val_loss: 1.4412 - val_acc: 0.7186\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9672\n",
      "Epoch 00031: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1085 - acc: 0.9672 - val_loss: 1.3500 - val_acc: 0.7251\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9678\n",
      "Epoch 00032: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1039 - acc: 0.9678 - val_loss: 1.4625 - val_acc: 0.7121\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9645\n",
      "Epoch 00033: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1132 - acc: 0.9644 - val_loss: 1.5336 - val_acc: 0.7058\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9667\n",
      "Epoch 00034: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1082 - acc: 0.9667 - val_loss: 1.4340 - val_acc: 0.7193\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9680\n",
      "Epoch 00035: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1029 - acc: 0.9680 - val_loss: 1.4478 - val_acc: 0.7254\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9688\n",
      "Epoch 00036: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1008 - acc: 0.9688 - val_loss: 1.3904 - val_acc: 0.7216\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9665\n",
      "Epoch 00037: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1034 - acc: 0.9665 - val_loss: 1.5312 - val_acc: 0.7163\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9693\n",
      "Epoch 00038: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1013 - acc: 0.9693 - val_loss: 1.4116 - val_acc: 0.7286\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9711\n",
      "Epoch 00039: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0939 - acc: 0.9711 - val_loss: 1.6605 - val_acc: 0.6904\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9732\n",
      "Epoch 00040: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0900 - acc: 0.9732 - val_loss: 1.5339 - val_acc: 0.7121\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9707\n",
      "Epoch 00041: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0958 - acc: 0.9707 - val_loss: 1.3776 - val_acc: 0.7310\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9724\n",
      "Epoch 00042: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0885 - acc: 0.9724 - val_loss: 1.5335 - val_acc: 0.7221\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9716\n",
      "Epoch 00043: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0920 - acc: 0.9716 - val_loss: 1.4682 - val_acc: 0.7195\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9739\n",
      "Epoch 00044: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0873 - acc: 0.9739 - val_loss: 1.4506 - val_acc: 0.7286\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9741\n",
      "Epoch 00045: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0852 - acc: 0.9741 - val_loss: 1.4618 - val_acc: 0.7282\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9731\n",
      "Epoch 00046: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0878 - acc: 0.9731 - val_loss: 1.4798 - val_acc: 0.7307\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9761\n",
      "Epoch 00047: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0796 - acc: 0.9761 - val_loss: 1.7655 - val_acc: 0.6932\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9759\n",
      "Epoch 00048: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0808 - acc: 0.9758 - val_loss: 1.4996 - val_acc: 0.7154\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9746\n",
      "Epoch 00049: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0847 - acc: 0.9746 - val_loss: 1.4009 - val_acc: 0.7414\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9751\n",
      "Epoch 00050: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0820 - acc: 0.9751 - val_loss: 1.4055 - val_acc: 0.7426\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9768\n",
      "Epoch 00051: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0775 - acc: 0.9768 - val_loss: 1.4508 - val_acc: 0.7356\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9784\n",
      "Epoch 00052: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0739 - acc: 0.9784 - val_loss: 1.4450 - val_acc: 0.7333\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9758\n",
      "Epoch 00053: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0803 - acc: 0.9758 - val_loss: 1.3875 - val_acc: 0.7405\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9751\n",
      "Epoch 00054: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0816 - acc: 0.9751 - val_loss: 1.4198 - val_acc: 0.7345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9802\n",
      "Epoch 00055: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0692 - acc: 0.9802 - val_loss: 1.5276 - val_acc: 0.7340\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9785\n",
      "Epoch 00056: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0720 - acc: 0.9785 - val_loss: 1.4978 - val_acc: 0.7319\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9806\n",
      "Epoch 00057: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0668 - acc: 0.9805 - val_loss: 1.5240 - val_acc: 0.7214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9780\n",
      "Epoch 00058: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0740 - acc: 0.9780 - val_loss: 1.4291 - val_acc: 0.7447\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9792\n",
      "Epoch 00059: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0708 - acc: 0.9792 - val_loss: 1.5077 - val_acc: 0.7386\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9793\n",
      "Epoch 00060: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0692 - acc: 0.9793 - val_loss: 1.5901 - val_acc: 0.7167\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9775\n",
      "Epoch 00061: val_loss did not improve from 1.13811\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0770 - acc: 0.9775 - val_loss: 1.4303 - val_acc: 0.7396\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.2683 - acc: 0.6567\n",
      "Loss: 1.2683235645541768 Accuracy: 0.6566978\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2820 - acc: 0.3200\n",
      "Epoch 00001: val_loss improved from inf to 1.70820, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/001-1.7082.hdf5\n",
      "36805/36805 [==============================] - 135s 4ms/sample - loss: 2.2819 - acc: 0.3200 - val_loss: 1.7082 - val_acc: 0.4608\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5300 - acc: 0.5208\n",
      "Epoch 00002: val_loss improved from 1.70820 to 1.22487, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/002-1.2249.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 1.5303 - acc: 0.5208 - val_loss: 1.2249 - val_acc: 0.6143\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2390 - acc: 0.6182\n",
      "Epoch 00003: val_loss improved from 1.22487 to 1.06997, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/003-1.0700.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 1.2393 - acc: 0.6181 - val_loss: 1.0700 - val_acc: 0.6716\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0444 - acc: 0.6815\n",
      "Epoch 00004: val_loss improved from 1.06997 to 1.00211, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/004-1.0021.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.0444 - acc: 0.6815 - val_loss: 1.0021 - val_acc: 0.6972\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9145 - acc: 0.7202\n",
      "Epoch 00005: val_loss improved from 1.00211 to 0.96006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/005-0.9601.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9146 - acc: 0.7202 - val_loss: 0.9601 - val_acc: 0.7193\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8143 - acc: 0.7560\n",
      "Epoch 00006: val_loss improved from 0.96006 to 0.88720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/006-0.8872.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8145 - acc: 0.7559 - val_loss: 0.8872 - val_acc: 0.7391\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7386 - acc: 0.7727\n",
      "Epoch 00007: val_loss improved from 0.88720 to 0.79180, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/007-0.7918.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7385 - acc: 0.7727 - val_loss: 0.7918 - val_acc: 0.7743\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.7971\n",
      "Epoch 00008: val_loss did not improve from 0.79180\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.6678 - acc: 0.7971 - val_loss: 0.8848 - val_acc: 0.7433\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6104 - acc: 0.8135\n",
      "Epoch 00009: val_loss did not improve from 0.79180\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.6105 - acc: 0.8134 - val_loss: 0.8742 - val_acc: 0.7529\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5662 - acc: 0.8264\n",
      "Epoch 00010: val_loss did not improve from 0.79180\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5663 - acc: 0.8263 - val_loss: 0.8112 - val_acc: 0.7747\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5221 - acc: 0.8397\n",
      "Epoch 00011: val_loss did not improve from 0.79180\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.5222 - acc: 0.8396 - val_loss: 0.8622 - val_acc: 0.7736\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.8546\n",
      "Epoch 00012: val_loss improved from 0.79180 to 0.78550, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/012-0.7855.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4725 - acc: 0.8546 - val_loss: 0.7855 - val_acc: 0.7855\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4316 - acc: 0.8662\n",
      "Epoch 00013: val_loss improved from 0.78550 to 0.77488, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/013-0.7749.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4317 - acc: 0.8662 - val_loss: 0.7749 - val_acc: 0.7939\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8738\n",
      "Epoch 00014: val_loss improved from 0.77488 to 0.77432, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/014-0.7743.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4018 - acc: 0.8738 - val_loss: 0.7743 - val_acc: 0.7857\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8835\n",
      "Epoch 00015: val_loss did not improve from 0.77432\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3748 - acc: 0.8835 - val_loss: 0.7855 - val_acc: 0.7948\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8927\n",
      "Epoch 00016: val_loss did not improve from 0.77432\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3389 - acc: 0.8927 - val_loss: 0.8580 - val_acc: 0.7747\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8971\n",
      "Epoch 00017: val_loss did not improve from 0.77432\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3282 - acc: 0.8971 - val_loss: 0.7790 - val_acc: 0.7932\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9072\n",
      "Epoch 00018: val_loss did not improve from 0.77432\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2937 - acc: 0.9072 - val_loss: 0.8378 - val_acc: 0.7876\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9143\n",
      "Epoch 00019: val_loss did not improve from 0.77432\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2728 - acc: 0.9142 - val_loss: 0.9566 - val_acc: 0.7596\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9143\n",
      "Epoch 00020: val_loss improved from 0.77432 to 0.76965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv_checkpoint/020-0.7697.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2680 - acc: 0.9143 - val_loss: 0.7697 - val_acc: 0.7941\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9186\n",
      "Epoch 00021: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2533 - acc: 0.9185 - val_loss: 0.8056 - val_acc: 0.8013\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9248\n",
      "Epoch 00022: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2394 - acc: 0.9247 - val_loss: 0.8400 - val_acc: 0.7918\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9286\n",
      "Epoch 00023: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2218 - acc: 0.9285 - val_loss: 0.8292 - val_acc: 0.7962\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9329\n",
      "Epoch 00024: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2134 - acc: 0.9329 - val_loss: 0.7964 - val_acc: 0.8074\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9353\n",
      "Epoch 00025: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2004 - acc: 0.9353 - val_loss: 0.8864 - val_acc: 0.7929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9350\n",
      "Epoch 00026: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2030 - acc: 0.9350 - val_loss: 0.8620 - val_acc: 0.7883\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9402\n",
      "Epoch 00027: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1865 - acc: 0.9402 - val_loss: 0.8819 - val_acc: 0.7906\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9422\n",
      "Epoch 00028: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1793 - acc: 0.9422 - val_loss: 0.8517 - val_acc: 0.8083\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9430\n",
      "Epoch 00029: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1767 - acc: 0.9430 - val_loss: 0.8689 - val_acc: 0.7992\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9473\n",
      "Epoch 00030: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1685 - acc: 0.9472 - val_loss: 0.8461 - val_acc: 0.7976\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9473\n",
      "Epoch 00031: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1661 - acc: 0.9473 - val_loss: 0.8635 - val_acc: 0.7994\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9507\n",
      "Epoch 00032: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1573 - acc: 0.9507 - val_loss: 0.8516 - val_acc: 0.7983\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9537\n",
      "Epoch 00033: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1464 - acc: 0.9537 - val_loss: 0.9240 - val_acc: 0.7955\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9546\n",
      "Epoch 00034: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1469 - acc: 0.9545 - val_loss: 0.9488 - val_acc: 0.7796\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9525\n",
      "Epoch 00035: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1481 - acc: 0.9524 - val_loss: 0.8424 - val_acc: 0.8067\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9561\n",
      "Epoch 00036: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1361 - acc: 0.9560 - val_loss: 0.8608 - val_acc: 0.8076\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9573\n",
      "Epoch 00037: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1353 - acc: 0.9573 - val_loss: 0.9228 - val_acc: 0.7999\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9567\n",
      "Epoch 00038: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1317 - acc: 0.9567 - val_loss: 0.8702 - val_acc: 0.8118\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9594\n",
      "Epoch 00039: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1301 - acc: 0.9594 - val_loss: 0.9531 - val_acc: 0.7906\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9593\n",
      "Epoch 00040: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1300 - acc: 0.9594 - val_loss: 0.9645 - val_acc: 0.7941\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9597\n",
      "Epoch 00041: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1264 - acc: 0.9597 - val_loss: 0.8913 - val_acc: 0.8001\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9632\n",
      "Epoch 00042: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1166 - acc: 0.9632 - val_loss: 0.9538 - val_acc: 0.7873\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9611\n",
      "Epoch 00043: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1225 - acc: 0.9611 - val_loss: 0.8934 - val_acc: 0.8057\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9642\n",
      "Epoch 00044: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1160 - acc: 0.9642 - val_loss: 0.9006 - val_acc: 0.8090\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9636\n",
      "Epoch 00045: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1147 - acc: 0.9636 - val_loss: 0.9368 - val_acc: 0.8001\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9648\n",
      "Epoch 00046: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1097 - acc: 0.9647 - val_loss: 0.9129 - val_acc: 0.8090\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1176 - acc: 0.9629\n",
      "Epoch 00047: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1176 - acc: 0.9629 - val_loss: 0.8847 - val_acc: 0.8106\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9673\n",
      "Epoch 00048: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1059 - acc: 0.9673 - val_loss: 1.0682 - val_acc: 0.7866\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9671\n",
      "Epoch 00049: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1079 - acc: 0.9670 - val_loss: 0.9381 - val_acc: 0.8050\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9636\n",
      "Epoch 00050: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1159 - acc: 0.9636 - val_loss: 1.2421 - val_acc: 0.7636\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9707\n",
      "Epoch 00051: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0949 - acc: 0.9706 - val_loss: 0.9148 - val_acc: 0.8083\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9681\n",
      "Epoch 00052: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1024 - acc: 0.9681 - val_loss: 0.9805 - val_acc: 0.8116\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9722\n",
      "Epoch 00053: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0947 - acc: 0.9722 - val_loss: 0.9650 - val_acc: 0.8011\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9727\n",
      "Epoch 00054: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0920 - acc: 0.9727 - val_loss: 0.9363 - val_acc: 0.8192\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9714\n",
      "Epoch 00055: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0950 - acc: 0.9714 - val_loss: 0.9651 - val_acc: 0.7980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9694\n",
      "Epoch 00056: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0995 - acc: 0.9694 - val_loss: 1.0005 - val_acc: 0.7997\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9707\n",
      "Epoch 00057: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0960 - acc: 0.9707 - val_loss: 1.0396 - val_acc: 0.7843\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9719\n",
      "Epoch 00058: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0925 - acc: 0.9719 - val_loss: 1.0704 - val_acc: 0.7983\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9696\n",
      "Epoch 00059: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0938 - acc: 0.9696 - val_loss: 0.9312 - val_acc: 0.8137\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9718\n",
      "Epoch 00060: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0906 - acc: 0.9718 - val_loss: 1.0130 - val_acc: 0.7950\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9749\n",
      "Epoch 00061: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0825 - acc: 0.9749 - val_loss: 0.9880 - val_acc: 0.8013\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9744\n",
      "Epoch 00062: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0835 - acc: 0.9744 - val_loss: 1.0300 - val_acc: 0.7894\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9746\n",
      "Epoch 00063: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0849 - acc: 0.9746 - val_loss: 0.9688 - val_acc: 0.8078\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9740\n",
      "Epoch 00064: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0834 - acc: 0.9740 - val_loss: 0.9671 - val_acc: 0.8102\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9717\n",
      "Epoch 00065: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0913 - acc: 0.9717 - val_loss: 0.9452 - val_acc: 0.8137\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9758\n",
      "Epoch 00066: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0785 - acc: 0.9758 - val_loss: 0.9783 - val_acc: 0.8099\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9743\n",
      "Epoch 00067: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0820 - acc: 0.9742 - val_loss: 1.0880 - val_acc: 0.7943\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9752\n",
      "Epoch 00068: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0806 - acc: 0.9752 - val_loss: 0.9833 - val_acc: 0.8095\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9718\n",
      "Epoch 00069: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0877 - acc: 0.9718 - val_loss: 0.9834 - val_acc: 0.8102\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9753\n",
      "Epoch 00070: val_loss did not improve from 0.76965\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0796 - acc: 0.9752 - val_loss: 0.9659 - val_acc: 0.8109\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8936 - acc: 0.7570\n",
      "Loss: 0.8935574294374492 Accuracy: 0.7570093\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5153 - acc: 0.2480\n",
      "Epoch 00001: val_loss improved from inf to 2.04073, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/001-2.0407.hdf5\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 2.5153 - acc: 0.2480 - val_loss: 2.0407 - val_acc: 0.3543\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6430 - acc: 0.4767\n",
      "Epoch 00002: val_loss improved from 2.04073 to 1.27347, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/002-1.2735.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.6430 - acc: 0.4768 - val_loss: 1.2735 - val_acc: 0.6007\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3126 - acc: 0.5889\n",
      "Epoch 00003: val_loss improved from 1.27347 to 1.09083, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/003-1.0908.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.3126 - acc: 0.5889 - val_loss: 1.0908 - val_acc: 0.6643\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1012 - acc: 0.6621\n",
      "Epoch 00004: val_loss improved from 1.09083 to 0.97044, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/004-0.9704.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.1011 - acc: 0.6621 - val_loss: 0.9704 - val_acc: 0.7098\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9633 - acc: 0.7076\n",
      "Epoch 00005: val_loss improved from 0.97044 to 0.90902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/005-0.9090.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9639 - acc: 0.7076 - val_loss: 0.9090 - val_acc: 0.7310\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8609 - acc: 0.7394\n",
      "Epoch 00006: val_loss did not improve from 0.90902\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8610 - acc: 0.7394 - val_loss: 0.9120 - val_acc: 0.7296\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7866 - acc: 0.7622\n",
      "Epoch 00007: val_loss improved from 0.90902 to 0.77679, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/007-0.7768.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.7867 - acc: 0.7622 - val_loss: 0.7768 - val_acc: 0.7696\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7822\n",
      "Epoch 00008: val_loss improved from 0.77679 to 0.72837, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/008-0.7284.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.7296 - acc: 0.7822 - val_loss: 0.7284 - val_acc: 0.7845\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.7981\n",
      "Epoch 00009: val_loss improved from 0.72837 to 0.70934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/009-0.7093.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.6762 - acc: 0.7981 - val_loss: 0.7093 - val_acc: 0.7997\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6408 - acc: 0.8097\n",
      "Epoch 00010: val_loss improved from 0.70934 to 0.66383, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/010-0.6638.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6407 - acc: 0.8097 - val_loss: 0.6638 - val_acc: 0.8183\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5939 - acc: 0.8225\n",
      "Epoch 00011: val_loss improved from 0.66383 to 0.64938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/011-0.6494.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5939 - acc: 0.8225 - val_loss: 0.6494 - val_acc: 0.8143\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5644 - acc: 0.8330\n",
      "Epoch 00012: val_loss did not improve from 0.64938\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5643 - acc: 0.8330 - val_loss: 0.6660 - val_acc: 0.8106\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5361 - acc: 0.8408\n",
      "Epoch 00013: val_loss improved from 0.64938 to 0.64856, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/013-0.6486.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5362 - acc: 0.8407 - val_loss: 0.6486 - val_acc: 0.8260\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.8504\n",
      "Epoch 00014: val_loss did not improve from 0.64856\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4987 - acc: 0.8503 - val_loss: 0.6608 - val_acc: 0.8102\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.8547\n",
      "Epoch 00015: val_loss improved from 0.64856 to 0.59969, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/015-0.5997.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4806 - acc: 0.8547 - val_loss: 0.5997 - val_acc: 0.8234\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8662\n",
      "Epoch 00016: val_loss did not improve from 0.59969\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4477 - acc: 0.8662 - val_loss: 0.7473 - val_acc: 0.7932\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8683\n",
      "Epoch 00017: val_loss did not improve from 0.59969\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4316 - acc: 0.8682 - val_loss: 0.6270 - val_acc: 0.8216\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8729\n",
      "Epoch 00018: val_loss did not improve from 0.59969\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4188 - acc: 0.8729 - val_loss: 0.8876 - val_acc: 0.7778\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8814\n",
      "Epoch 00019: val_loss did not improve from 0.59969\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3866 - acc: 0.8814 - val_loss: 0.6302 - val_acc: 0.8155\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8835\n",
      "Epoch 00020: val_loss improved from 0.59969 to 0.58737, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv_checkpoint/020-0.5874.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3818 - acc: 0.8835 - val_loss: 0.5874 - val_acc: 0.8409\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8919\n",
      "Epoch 00021: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3557 - acc: 0.8919 - val_loss: 0.6593 - val_acc: 0.8237\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3462 - acc: 0.8937\n",
      "Epoch 00022: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3462 - acc: 0.8937 - val_loss: 0.6470 - val_acc: 0.8330\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8977\n",
      "Epoch 00023: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3293 - acc: 0.8976 - val_loss: 0.6104 - val_acc: 0.8397\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9008\n",
      "Epoch 00024: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3176 - acc: 0.9008 - val_loss: 0.6992 - val_acc: 0.8171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9067\n",
      "Epoch 00025: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3020 - acc: 0.9066 - val_loss: 0.6448 - val_acc: 0.8258\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9083\n",
      "Epoch 00026: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2957 - acc: 0.9083 - val_loss: 0.6334 - val_acc: 0.8300\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9152\n",
      "Epoch 00027: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2715 - acc: 0.9152 - val_loss: 0.5895 - val_acc: 0.8442\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2638 - acc: 0.9171\n",
      "Epoch 00028: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2643 - acc: 0.9171 - val_loss: 0.6329 - val_acc: 0.8323\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9180\n",
      "Epoch 00029: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2611 - acc: 0.9181 - val_loss: 0.6400 - val_acc: 0.8400\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9235\n",
      "Epoch 00030: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2459 - acc: 0.9235 - val_loss: 0.6508 - val_acc: 0.8383\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9236\n",
      "Epoch 00031: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2409 - acc: 0.9236 - val_loss: 0.6607 - val_acc: 0.8307\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9296\n",
      "Epoch 00032: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2284 - acc: 0.9296 - val_loss: 0.7095 - val_acc: 0.8267\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9266\n",
      "Epoch 00033: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2331 - acc: 0.9266 - val_loss: 0.6995 - val_acc: 0.8167\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9271\n",
      "Epoch 00034: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2292 - acc: 0.9271 - val_loss: 0.6209 - val_acc: 0.8444\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9347\n",
      "Epoch 00035: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2094 - acc: 0.9347 - val_loss: 1.0172 - val_acc: 0.7685\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9330\n",
      "Epoch 00036: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2099 - acc: 0.9330 - val_loss: 0.6088 - val_acc: 0.8474\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9363\n",
      "Epoch 00037: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2013 - acc: 0.9363 - val_loss: 0.6357 - val_acc: 0.8397\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9391\n",
      "Epoch 00038: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1936 - acc: 0.9390 - val_loss: 0.6919 - val_acc: 0.8348\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9392\n",
      "Epoch 00039: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1939 - acc: 0.9391 - val_loss: 0.6866 - val_acc: 0.8269\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9416\n",
      "Epoch 00040: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1854 - acc: 0.9416 - val_loss: 0.6128 - val_acc: 0.8523\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9471\n",
      "Epoch 00041: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1719 - acc: 0.9470 - val_loss: 0.6565 - val_acc: 0.8402\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9452\n",
      "Epoch 00042: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1730 - acc: 0.9451 - val_loss: 0.7170 - val_acc: 0.8239\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9466\n",
      "Epoch 00043: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1696 - acc: 0.9466 - val_loss: 0.6482 - val_acc: 0.8519\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9479\n",
      "Epoch 00044: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1663 - acc: 0.9479 - val_loss: 0.6367 - val_acc: 0.8528\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9496\n",
      "Epoch 00045: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1635 - acc: 0.9497 - val_loss: 0.6597 - val_acc: 0.8407\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9502\n",
      "Epoch 00046: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1552 - acc: 0.9502 - val_loss: 0.6630 - val_acc: 0.8418\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9479\n",
      "Epoch 00047: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1664 - acc: 0.9479 - val_loss: 0.6375 - val_acc: 0.8512\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9534\n",
      "Epoch 00048: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1522 - acc: 0.9533 - val_loss: 0.6431 - val_acc: 0.8491\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9529\n",
      "Epoch 00049: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1510 - acc: 0.9529 - val_loss: 0.6445 - val_acc: 0.8509\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9554\n",
      "Epoch 00050: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1427 - acc: 0.9553 - val_loss: 0.6660 - val_acc: 0.8512\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.9554\n",
      "Epoch 00051: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1454 - acc: 0.9554 - val_loss: 0.6412 - val_acc: 0.8553\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9572\n",
      "Epoch 00052: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1378 - acc: 0.9572 - val_loss: 0.6886 - val_acc: 0.8428\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9573\n",
      "Epoch 00053: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1368 - acc: 0.9573 - val_loss: 0.6620 - val_acc: 0.8544\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9562\n",
      "Epoch 00054: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1372 - acc: 0.9562 - val_loss: 0.6600 - val_acc: 0.8479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9595\n",
      "Epoch 00055: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1299 - acc: 0.9595 - val_loss: 0.6652 - val_acc: 0.8507\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.9609\n",
      "Epoch 00056: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1274 - acc: 0.9608 - val_loss: 0.7181 - val_acc: 0.8470\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9589\n",
      "Epoch 00057: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1314 - acc: 0.9589 - val_loss: 0.7146 - val_acc: 0.8477\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9627\n",
      "Epoch 00058: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1179 - acc: 0.9627 - val_loss: 0.7059 - val_acc: 0.8414\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9616\n",
      "Epoch 00059: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1254 - acc: 0.9616 - val_loss: 0.6488 - val_acc: 0.8537\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9641\n",
      "Epoch 00060: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1135 - acc: 0.9641 - val_loss: 0.7079 - val_acc: 0.8446\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9629\n",
      "Epoch 00061: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1186 - acc: 0.9629 - val_loss: 0.7230 - val_acc: 0.8400\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9664\n",
      "Epoch 00062: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1150 - acc: 0.9664 - val_loss: 0.7297 - val_acc: 0.8453\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9646\n",
      "Epoch 00063: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1140 - acc: 0.9646 - val_loss: 0.6761 - val_acc: 0.8500\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9644\n",
      "Epoch 00064: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1132 - acc: 0.9644 - val_loss: 0.7726 - val_acc: 0.8346\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9636\n",
      "Epoch 00065: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1164 - acc: 0.9635 - val_loss: 0.7155 - val_acc: 0.8458\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9652\n",
      "Epoch 00066: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1127 - acc: 0.9652 - val_loss: 0.7058 - val_acc: 0.8456\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9672\n",
      "Epoch 00067: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1066 - acc: 0.9672 - val_loss: 0.6886 - val_acc: 0.8465\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9684\n",
      "Epoch 00068: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1029 - acc: 0.9684 - val_loss: 0.6457 - val_acc: 0.8544\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9681\n",
      "Epoch 00069: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1052 - acc: 0.9680 - val_loss: 0.6686 - val_acc: 0.8574\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9658\n",
      "Epoch 00070: val_loss did not improve from 0.58737\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1089 - acc: 0.9658 - val_loss: 0.6684 - val_acc: 0.8521\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.6763 - acc: 0.8073\n",
      "Loss: 0.6762765360398456 Accuracy: 0.807269\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5579 - acc: 0.2335\n",
      "Epoch 00001: val_loss improved from inf to 2.20675, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/001-2.2068.hdf5\n",
      "36805/36805 [==============================] - 158s 4ms/sample - loss: 2.5578 - acc: 0.2335 - val_loss: 2.2068 - val_acc: 0.2970\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6794 - acc: 0.4641\n",
      "Epoch 00002: val_loss improved from 2.20675 to 1.25153, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/002-1.2515.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.6795 - acc: 0.4641 - val_loss: 1.2515 - val_acc: 0.6150\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3302 - acc: 0.5797\n",
      "Epoch 00003: val_loss improved from 1.25153 to 1.13749, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/003-1.1375.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3302 - acc: 0.5797 - val_loss: 1.1375 - val_acc: 0.6520\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1260 - acc: 0.6505\n",
      "Epoch 00004: val_loss improved from 1.13749 to 0.90848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/004-0.9085.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1264 - acc: 0.6505 - val_loss: 0.9085 - val_acc: 0.7270\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9843 - acc: 0.6965\n",
      "Epoch 00005: val_loss improved from 0.90848 to 0.86729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/005-0.8673.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.9842 - acc: 0.6965 - val_loss: 0.8673 - val_acc: 0.7417\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8698 - acc: 0.7377\n",
      "Epoch 00006: val_loss did not improve from 0.86729\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.8700 - acc: 0.7376 - val_loss: 0.8997 - val_acc: 0.7291\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7901 - acc: 0.7614\n",
      "Epoch 00007: val_loss did not improve from 0.86729\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7901 - acc: 0.7613 - val_loss: 0.9715 - val_acc: 0.7100\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7247 - acc: 0.7836\n",
      "Epoch 00008: val_loss improved from 0.86729 to 0.66231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/008-0.6623.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.7250 - acc: 0.7835 - val_loss: 0.6623 - val_acc: 0.8067\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8031\n",
      "Epoch 00009: val_loss did not improve from 0.66231\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6659 - acc: 0.8031 - val_loss: 0.6680 - val_acc: 0.8078\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8147\n",
      "Epoch 00010: val_loss improved from 0.66231 to 0.56581, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/010-0.5658.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.6205 - acc: 0.8147 - val_loss: 0.5658 - val_acc: 0.8355\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8260\n",
      "Epoch 00011: val_loss did not improve from 0.56581\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5814 - acc: 0.8259 - val_loss: 0.6214 - val_acc: 0.8358\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8407\n",
      "Epoch 00012: val_loss did not improve from 0.56581\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5357 - acc: 0.8407 - val_loss: 0.5682 - val_acc: 0.8414\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8511\n",
      "Epoch 00013: val_loss did not improve from 0.56581\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5051 - acc: 0.8511 - val_loss: 0.5717 - val_acc: 0.8484\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4739 - acc: 0.8600\n",
      "Epoch 00014: val_loss did not improve from 0.56581\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4739 - acc: 0.8600 - val_loss: 0.6407 - val_acc: 0.8272\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8670\n",
      "Epoch 00015: val_loss improved from 0.56581 to 0.54068, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/015-0.5407.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4457 - acc: 0.8669 - val_loss: 0.5407 - val_acc: 0.8535\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8732\n",
      "Epoch 00016: val_loss did not improve from 0.54068\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4218 - acc: 0.8732 - val_loss: 0.5566 - val_acc: 0.8465\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8814\n",
      "Epoch 00017: val_loss improved from 0.54068 to 0.47231, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/017-0.4723.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4006 - acc: 0.8814 - val_loss: 0.4723 - val_acc: 0.8733\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3799 - acc: 0.8865\n",
      "Epoch 00018: val_loss did not improve from 0.47231\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3800 - acc: 0.8865 - val_loss: 0.5586 - val_acc: 0.8553\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8915\n",
      "Epoch 00019: val_loss did not improve from 0.47231\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3611 - acc: 0.8915 - val_loss: 0.4863 - val_acc: 0.8661\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8972\n",
      "Epoch 00020: val_loss improved from 0.47231 to 0.46650, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/020-0.4665.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3437 - acc: 0.8972 - val_loss: 0.4665 - val_acc: 0.8775\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.9036\n",
      "Epoch 00021: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3242 - acc: 0.9036 - val_loss: 0.4865 - val_acc: 0.8742\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.9044\n",
      "Epoch 00022: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3149 - acc: 0.9044 - val_loss: 0.4742 - val_acc: 0.8661\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9079\n",
      "Epoch 00023: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2979 - acc: 0.9078 - val_loss: 0.4683 - val_acc: 0.8710\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9107\n",
      "Epoch 00024: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2888 - acc: 0.9107 - val_loss: 0.5091 - val_acc: 0.8658\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9146\n",
      "Epoch 00025: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2732 - acc: 0.9146 - val_loss: 0.4710 - val_acc: 0.8679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9226\n",
      "Epoch 00026: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2535 - acc: 0.9225 - val_loss: 0.4755 - val_acc: 0.8779\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9210\n",
      "Epoch 00027: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2569 - acc: 0.9210 - val_loss: 0.5631 - val_acc: 0.8556\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9229\n",
      "Epoch 00028: val_loss did not improve from 0.46650\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2455 - acc: 0.9228 - val_loss: 0.5284 - val_acc: 0.8682\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9272\n",
      "Epoch 00029: val_loss improved from 0.46650 to 0.46284, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/029-0.4628.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2340 - acc: 0.9272 - val_loss: 0.4628 - val_acc: 0.8910\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2175 - acc: 0.9324\n",
      "Epoch 00030: val_loss improved from 0.46284 to 0.43514, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/030-0.4351.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2175 - acc: 0.9324 - val_loss: 0.4351 - val_acc: 0.8866\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9340\n",
      "Epoch 00031: val_loss did not improve from 0.43514\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2118 - acc: 0.9340 - val_loss: 0.5259 - val_acc: 0.8719\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9345\n",
      "Epoch 00032: val_loss did not improve from 0.43514\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2087 - acc: 0.9345 - val_loss: 0.4773 - val_acc: 0.8740\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9371\n",
      "Epoch 00033: val_loss did not improve from 0.43514\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2027 - acc: 0.9370 - val_loss: 0.4552 - val_acc: 0.8758\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9373\n",
      "Epoch 00034: val_loss improved from 0.43514 to 0.43161, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/034-0.4316.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1969 - acc: 0.9372 - val_loss: 0.4316 - val_acc: 0.8889\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9405\n",
      "Epoch 00035: val_loss did not improve from 0.43161\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1877 - acc: 0.9405 - val_loss: 0.5369 - val_acc: 0.8747\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9429\n",
      "Epoch 00036: val_loss improved from 0.43161 to 0.41647, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv_checkpoint/036-0.4165.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1801 - acc: 0.9428 - val_loss: 0.4165 - val_acc: 0.8917\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9428\n",
      "Epoch 00037: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1806 - acc: 0.9428 - val_loss: 0.5540 - val_acc: 0.8696\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9468\n",
      "Epoch 00038: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1699 - acc: 0.9468 - val_loss: 0.4232 - val_acc: 0.8910\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1661 - acc: 0.9468 - val_loss: 0.5174 - val_acc: 0.8691\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9444\n",
      "Epoch 00040: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1711 - acc: 0.9444 - val_loss: 0.4282 - val_acc: 0.8921\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9501\n",
      "Epoch 00041: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1552 - acc: 0.9501 - val_loss: 0.4293 - val_acc: 0.8954\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9520\n",
      "Epoch 00042: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1515 - acc: 0.9520 - val_loss: 0.4820 - val_acc: 0.8817\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9533\n",
      "Epoch 00043: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1491 - acc: 0.9533 - val_loss: 0.4758 - val_acc: 0.8859\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9540\n",
      "Epoch 00044: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1409 - acc: 0.9539 - val_loss: 0.6796 - val_acc: 0.8463\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9543\n",
      "Epoch 00045: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1446 - acc: 0.9543 - val_loss: 0.5552 - val_acc: 0.8635\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9546\n",
      "Epoch 00046: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1420 - acc: 0.9547 - val_loss: 0.4493 - val_acc: 0.8912\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9614\n",
      "Epoch 00047: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1229 - acc: 0.9613 - val_loss: 0.4358 - val_acc: 0.8966\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9585\n",
      "Epoch 00048: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1301 - acc: 0.9585 - val_loss: 0.4814 - val_acc: 0.8905\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9605\n",
      "Epoch 00049: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1243 - acc: 0.9605 - val_loss: 0.5353 - val_acc: 0.8838\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9617\n",
      "Epoch 00050: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1212 - acc: 0.9617 - val_loss: 0.4758 - val_acc: 0.8894\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9579\n",
      "Epoch 00051: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1299 - acc: 0.9579 - val_loss: 0.6228 - val_acc: 0.8572\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9623\n",
      "Epoch 00052: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1178 - acc: 0.9623 - val_loss: 0.4296 - val_acc: 0.8977\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9661\n",
      "Epoch 00053: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1090 - acc: 0.9661 - val_loss: 0.5541 - val_acc: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9654\n",
      "Epoch 00054: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1094 - acc: 0.9654 - val_loss: 0.4814 - val_acc: 0.8924\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9635\n",
      "Epoch 00055: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1112 - acc: 0.9635 - val_loss: 0.4703 - val_acc: 0.8915\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9655\n",
      "Epoch 00056: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1070 - acc: 0.9655 - val_loss: 0.5224 - val_acc: 0.8884\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9649\n",
      "Epoch 00057: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1066 - acc: 0.9650 - val_loss: 0.5021 - val_acc: 0.8875\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9681\n",
      "Epoch 00058: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1008 - acc: 0.9681 - val_loss: 0.4212 - val_acc: 0.8982\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9674\n",
      "Epoch 00059: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1011 - acc: 0.9674 - val_loss: 0.5238 - val_acc: 0.8947\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9698\n",
      "Epoch 00060: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0993 - acc: 0.9698 - val_loss: 0.4616 - val_acc: 0.8998\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0949 - acc: 0.9713\n",
      "Epoch 00061: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0949 - acc: 0.9713 - val_loss: 0.4781 - val_acc: 0.8982\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9700\n",
      "Epoch 00062: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0938 - acc: 0.9700 - val_loss: 0.5089 - val_acc: 0.8833\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9713\n",
      "Epoch 00063: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0912 - acc: 0.9713 - val_loss: 0.5049 - val_acc: 0.8928\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9717\n",
      "Epoch 00064: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0872 - acc: 0.9716 - val_loss: 0.5032 - val_acc: 0.8963\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9690\n",
      "Epoch 00065: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0992 - acc: 0.9690 - val_loss: 0.6150 - val_acc: 0.8654\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9710\n",
      "Epoch 00066: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0873 - acc: 0.9710 - val_loss: 0.4995 - val_acc: 0.8982\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9723\n",
      "Epoch 00067: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0880 - acc: 0.9723 - val_loss: 0.5225 - val_acc: 0.8898\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9730\n",
      "Epoch 00068: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0831 - acc: 0.9730 - val_loss: 0.5014 - val_acc: 0.8940\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9735\n",
      "Epoch 00069: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0828 - acc: 0.9734 - val_loss: 0.5765 - val_acc: 0.8756\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9733\n",
      "Epoch 00070: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0840 - acc: 0.9733 - val_loss: 0.5417 - val_acc: 0.8765\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9728\n",
      "Epoch 00071: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0868 - acc: 0.9728 - val_loss: 0.4815 - val_acc: 0.8982\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9763\n",
      "Epoch 00072: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0752 - acc: 0.9763 - val_loss: 0.4596 - val_acc: 0.9012\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9754\n",
      "Epoch 00073: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0778 - acc: 0.9754 - val_loss: 0.5129 - val_acc: 0.8901\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9723\n",
      "Epoch 00074: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0833 - acc: 0.9723 - val_loss: 0.5856 - val_acc: 0.8730\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9725\n",
      "Epoch 00075: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0870 - acc: 0.9725 - val_loss: 0.4955 - val_acc: 0.8921\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9762\n",
      "Epoch 00076: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0764 - acc: 0.9762 - val_loss: 0.5186 - val_acc: 0.8947\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9742\n",
      "Epoch 00077: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0815 - acc: 0.9742 - val_loss: 0.5793 - val_acc: 0.8828\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9786\n",
      "Epoch 00078: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0703 - acc: 0.9786 - val_loss: 0.5605 - val_acc: 0.8928\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9779\n",
      "Epoch 00079: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0704 - acc: 0.9779 - val_loss: 0.4749 - val_acc: 0.8970\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9792\n",
      "Epoch 00080: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0699 - acc: 0.9792 - val_loss: 0.5355 - val_acc: 0.8901\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9786\n",
      "Epoch 00081: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0692 - acc: 0.9785 - val_loss: 0.4949 - val_acc: 0.8919\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9758\n",
      "Epoch 00082: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0775 - acc: 0.9758 - val_loss: 0.5161 - val_acc: 0.8889\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9781\n",
      "Epoch 00083: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0703 - acc: 0.9781 - val_loss: 0.5333 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9751\n",
      "Epoch 00084: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0788 - acc: 0.9751 - val_loss: 0.4591 - val_acc: 0.9036\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9813\n",
      "Epoch 00085: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0619 - acc: 0.9813 - val_loss: 0.5263 - val_acc: 0.8973\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9763\n",
      "Epoch 00086: val_loss did not improve from 0.41647\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0742 - acc: 0.9763 - val_loss: 0.5098 - val_acc: 0.8938\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5044 - acc: 0.8658\n",
      "Loss: 0.5043950633715494 Accuracy: 0.8658359\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6045 - acc: 0.2252\n",
      "Epoch 00001: val_loss improved from inf to 2.15517, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/001-2.1552.hdf5\n",
      "36805/36805 [==============================] - 172s 5ms/sample - loss: 2.6044 - acc: 0.2253 - val_loss: 2.1552 - val_acc: 0.2842\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6273 - acc: 0.4785\n",
      "Epoch 00002: val_loss improved from 2.15517 to 1.30786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/002-1.3079.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.6272 - acc: 0.4785 - val_loss: 1.3079 - val_acc: 0.5989\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2551 - acc: 0.6052\n",
      "Epoch 00003: val_loss improved from 1.30786 to 0.98561, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/003-0.9856.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.2551 - acc: 0.6052 - val_loss: 0.9856 - val_acc: 0.7137\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0339 - acc: 0.6845\n",
      "Epoch 00004: val_loss improved from 0.98561 to 0.88324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/004-0.8832.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.0340 - acc: 0.6844 - val_loss: 0.8832 - val_acc: 0.7452\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7364\n",
      "Epoch 00005: val_loss improved from 0.88324 to 0.68982, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/005-0.6898.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8744 - acc: 0.7363 - val_loss: 0.6898 - val_acc: 0.8060\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7427 - acc: 0.7804\n",
      "Epoch 00006: val_loss improved from 0.68982 to 0.68662, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/006-0.6866.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7430 - acc: 0.7804 - val_loss: 0.6866 - val_acc: 0.8123\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.8068\n",
      "Epoch 00007: val_loss did not improve from 0.68662\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6508 - acc: 0.8068 - val_loss: 0.8271 - val_acc: 0.7713\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8309\n",
      "Epoch 00008: val_loss improved from 0.68662 to 0.53449, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/008-0.5345.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5769 - acc: 0.8309 - val_loss: 0.5345 - val_acc: 0.8453\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8480\n",
      "Epoch 00009: val_loss improved from 0.53449 to 0.50217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/009-0.5022.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5143 - acc: 0.8480 - val_loss: 0.5022 - val_acc: 0.8551\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.8608\n",
      "Epoch 00010: val_loss improved from 0.50217 to 0.44632, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/010-0.4463.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4689 - acc: 0.8607 - val_loss: 0.4463 - val_acc: 0.8784\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8730\n",
      "Epoch 00011: val_loss did not improve from 0.44632\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4246 - acc: 0.8730 - val_loss: 0.4610 - val_acc: 0.8749\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8848\n",
      "Epoch 00012: val_loss did not improve from 0.44632\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3908 - acc: 0.8848 - val_loss: 0.4723 - val_acc: 0.8663\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8906\n",
      "Epoch 00013: val_loss improved from 0.44632 to 0.39550, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/013-0.3955.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3640 - acc: 0.8906 - val_loss: 0.3955 - val_acc: 0.8942\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8987\n",
      "Epoch 00014: val_loss did not improve from 0.39550\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3368 - acc: 0.8987 - val_loss: 0.4497 - val_acc: 0.8768\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.9035\n",
      "Epoch 00015: val_loss improved from 0.39550 to 0.37242, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/015-0.3724.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3127 - acc: 0.9035 - val_loss: 0.3724 - val_acc: 0.9029\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9119\n",
      "Epoch 00016: val_loss improved from 0.37242 to 0.33641, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/016-0.3364.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2895 - acc: 0.9119 - val_loss: 0.3364 - val_acc: 0.9045\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9149\n",
      "Epoch 00017: val_loss did not improve from 0.33641\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2762 - acc: 0.9149 - val_loss: 0.3390 - val_acc: 0.9057\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9221\n",
      "Epoch 00018: val_loss did not improve from 0.33641\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2509 - acc: 0.9220 - val_loss: 0.3436 - val_acc: 0.9133\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9274\n",
      "Epoch 00019: val_loss did not improve from 0.33641\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2348 - acc: 0.9274 - val_loss: 0.3474 - val_acc: 0.9043\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9288\n",
      "Epoch 00020: val_loss improved from 0.33641 to 0.31246, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/020-0.3125.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2291 - acc: 0.9288 - val_loss: 0.3125 - val_acc: 0.9140\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9335\n",
      "Epoch 00021: val_loss improved from 0.31246 to 0.30538, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/021-0.3054.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2133 - acc: 0.9335 - val_loss: 0.3054 - val_acc: 0.9164\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9383\n",
      "Epoch 00022: val_loss did not improve from 0.30538\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1958 - acc: 0.9382 - val_loss: 0.3608 - val_acc: 0.9066\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9381\n",
      "Epoch 00023: val_loss did not improve from 0.30538\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1991 - acc: 0.9381 - val_loss: 0.3386 - val_acc: 0.9140\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9442\n",
      "Epoch 00024: val_loss improved from 0.30538 to 0.30137, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/024-0.3014.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1793 - acc: 0.9441 - val_loss: 0.3014 - val_acc: 0.9147\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9443\n",
      "Epoch 00025: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1715 - acc: 0.9442 - val_loss: 0.3524 - val_acc: 0.9085\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9491\n",
      "Epoch 00026: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1634 - acc: 0.9491 - val_loss: 0.3443 - val_acc: 0.9092\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9533\n",
      "Epoch 00027: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1514 - acc: 0.9533 - val_loss: 0.3260 - val_acc: 0.9229\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9530\n",
      "Epoch 00028: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1459 - acc: 0.9530 - val_loss: 0.3896 - val_acc: 0.9096\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9567\n",
      "Epoch 00029: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1348 - acc: 0.9567 - val_loss: 0.3371 - val_acc: 0.9206\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9555\n",
      "Epoch 00030: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1351 - acc: 0.9555 - val_loss: 0.3265 - val_acc: 0.9171\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9585\n",
      "Epoch 00031: val_loss did not improve from 0.30137\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1289 - acc: 0.9584 - val_loss: 0.3091 - val_acc: 0.9264\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9590\n",
      "Epoch 00032: val_loss improved from 0.30137 to 0.29217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/032-0.2922.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1280 - acc: 0.9590 - val_loss: 0.2922 - val_acc: 0.9297\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9589\n",
      "Epoch 00033: val_loss did not improve from 0.29217\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1242 - acc: 0.9589 - val_loss: 0.3255 - val_acc: 0.9201\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9635\n",
      "Epoch 00034: val_loss did not improve from 0.29217\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1128 - acc: 0.9635 - val_loss: 0.3416 - val_acc: 0.9194\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9660\n",
      "Epoch 00035: val_loss did not improve from 0.29217\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1079 - acc: 0.9660 - val_loss: 0.3331 - val_acc: 0.9208\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9674\n",
      "Epoch 00036: val_loss did not improve from 0.29217\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1021 - acc: 0.9674 - val_loss: 0.3325 - val_acc: 0.9259\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9691\n",
      "Epoch 00037: val_loss did not improve from 0.29217\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0990 - acc: 0.9690 - val_loss: 0.3381 - val_acc: 0.9185\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9642\n",
      "Epoch 00038: val_loss improved from 0.29217 to 0.28978, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/038-0.2898.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1101 - acc: 0.9641 - val_loss: 0.2898 - val_acc: 0.9308\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9706\n",
      "Epoch 00039: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0942 - acc: 0.9706 - val_loss: 0.3112 - val_acc: 0.9290\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9698\n",
      "Epoch 00040: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0912 - acc: 0.9698 - val_loss: 0.2975 - val_acc: 0.9262\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9724\n",
      "Epoch 00041: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0858 - acc: 0.9724 - val_loss: 0.3044 - val_acc: 0.9283\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9707\n",
      "Epoch 00042: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0878 - acc: 0.9707 - val_loss: 0.3002 - val_acc: 0.9283\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9746\n",
      "Epoch 00043: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0816 - acc: 0.9745 - val_loss: 0.3531 - val_acc: 0.9227\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9701\n",
      "Epoch 00044: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0930 - acc: 0.9701 - val_loss: 0.3071 - val_acc: 0.9306\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9766\n",
      "Epoch 00045: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0748 - acc: 0.9766 - val_loss: 0.3484 - val_acc: 0.9285\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9773\n",
      "Epoch 00046: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0722 - acc: 0.9773 - val_loss: 0.3658 - val_acc: 0.9168\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9769\n",
      "Epoch 00047: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0698 - acc: 0.9769 - val_loss: 0.3366 - val_acc: 0.9229\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9729\n",
      "Epoch 00048: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0833 - acc: 0.9729 - val_loss: 0.3767 - val_acc: 0.9145\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9767\n",
      "Epoch 00049: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0721 - acc: 0.9767 - val_loss: 0.3122 - val_acc: 0.9292\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9799\n",
      "Epoch 00050: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0626 - acc: 0.9799 - val_loss: 0.3050 - val_acc: 0.9336\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9790\n",
      "Epoch 00051: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0650 - acc: 0.9790 - val_loss: 0.3487 - val_acc: 0.9243\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9751\n",
      "Epoch 00052: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0771 - acc: 0.9751 - val_loss: 0.3452 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9754\n",
      "Epoch 00053: val_loss did not improve from 0.28978\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0769 - acc: 0.9754 - val_loss: 0.3434 - val_acc: 0.9189\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9820\n",
      "Epoch 00054: val_loss improved from 0.28978 to 0.28776, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv_checkpoint/054-0.2878.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0571 - acc: 0.9820 - val_loss: 0.2878 - val_acc: 0.9299\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9777\n",
      "Epoch 00055: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0687 - acc: 0.9776 - val_loss: 0.3318 - val_acc: 0.9259\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9805\n",
      "Epoch 00056: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0613 - acc: 0.9805 - val_loss: 0.3870 - val_acc: 0.9129\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9817\n",
      "Epoch 00057: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0558 - acc: 0.9817 - val_loss: 0.3176 - val_acc: 0.9311\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9827\n",
      "Epoch 00058: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0537 - acc: 0.9827 - val_loss: 0.3129 - val_acc: 0.9299\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9832\n",
      "Epoch 00059: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0522 - acc: 0.9832 - val_loss: 0.3534 - val_acc: 0.9259\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9812\n",
      "Epoch 00060: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0577 - acc: 0.9813 - val_loss: 0.4060 - val_acc: 0.9082\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9830\n",
      "Epoch 00061: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0527 - acc: 0.9830 - val_loss: 0.3140 - val_acc: 0.9264\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9814\n",
      "Epoch 00062: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0577 - acc: 0.9814 - val_loss: 0.3276 - val_acc: 0.9355\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9850\n",
      "Epoch 00063: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0461 - acc: 0.9849 - val_loss: 0.3374 - val_acc: 0.9280\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9796\n",
      "Epoch 00064: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0656 - acc: 0.9796 - val_loss: 0.3443 - val_acc: 0.9273\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9839\n",
      "Epoch 00065: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0485 - acc: 0.9839 - val_loss: 0.3318 - val_acc: 0.9341\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9858\n",
      "Epoch 00066: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0452 - acc: 0.9858 - val_loss: 0.3638 - val_acc: 0.9259\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9803\n",
      "Epoch 00067: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0595 - acc: 0.9803 - val_loss: 0.3127 - val_acc: 0.9348\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9842\n",
      "Epoch 00068: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0480 - acc: 0.9842 - val_loss: 0.3186 - val_acc: 0.9338\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9864\n",
      "Epoch 00069: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0438 - acc: 0.9863 - val_loss: 0.3093 - val_acc: 0.9348\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9852\n",
      "Epoch 00070: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0458 - acc: 0.9852 - val_loss: 0.3421 - val_acc: 0.9341\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9861\n",
      "Epoch 00071: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0437 - acc: 0.9861 - val_loss: 0.3914 - val_acc: 0.9159\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9849\n",
      "Epoch 00072: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0463 - acc: 0.9849 - val_loss: 0.3339 - val_acc: 0.9357\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9851\n",
      "Epoch 00073: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0486 - acc: 0.9851 - val_loss: 0.3184 - val_acc: 0.9371\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9865\n",
      "Epoch 00074: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0422 - acc: 0.9864 - val_loss: 0.3303 - val_acc: 0.9297\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9839\n",
      "Epoch 00075: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0505 - acc: 0.9838 - val_loss: 0.3759 - val_acc: 0.9199\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9874\n",
      "Epoch 00076: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0402 - acc: 0.9874 - val_loss: 0.3265 - val_acc: 0.9350\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9879\n",
      "Epoch 00077: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0375 - acc: 0.9879 - val_loss: 0.3392 - val_acc: 0.9278\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9860\n",
      "Epoch 00078: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0443 - acc: 0.9860 - val_loss: 0.3595 - val_acc: 0.9266\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9856\n",
      "Epoch 00079: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0447 - acc: 0.9856 - val_loss: 0.3220 - val_acc: 0.9373\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9882\n",
      "Epoch 00080: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0367 - acc: 0.9882 - val_loss: 0.3705 - val_acc: 0.9294\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9882\n",
      "Epoch 00081: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0368 - acc: 0.9882 - val_loss: 0.3499 - val_acc: 0.9299\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9873\n",
      "Epoch 00082: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0400 - acc: 0.9873 - val_loss: 0.3492 - val_acc: 0.9292\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9869\n",
      "Epoch 00083: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0412 - acc: 0.9868 - val_loss: 0.3573 - val_acc: 0.9292\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9849\n",
      "Epoch 00084: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0459 - acc: 0.9849 - val_loss: 0.3643 - val_acc: 0.9255\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9886\n",
      "Epoch 00085: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0374 - acc: 0.9886 - val_loss: 0.3560 - val_acc: 0.9311\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9889\n",
      "Epoch 00086: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0359 - acc: 0.9889 - val_loss: 0.3087 - val_acc: 0.9364\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9904\n",
      "Epoch 00087: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0310 - acc: 0.9904 - val_loss: 0.3020 - val_acc: 0.9371\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9873\n",
      "Epoch 00088: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0387 - acc: 0.9873 - val_loss: 0.3456 - val_acc: 0.9266\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9887\n",
      "Epoch 00089: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0359 - acc: 0.9887 - val_loss: 0.3297 - val_acc: 0.9383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9897\n",
      "Epoch 00090: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.3757 - val_acc: 0.9306\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9875\n",
      "Epoch 00091: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0420 - acc: 0.9875 - val_loss: 0.2999 - val_acc: 0.9390\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9900\n",
      "Epoch 00092: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0306 - acc: 0.9899 - val_loss: 0.5338 - val_acc: 0.9061\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9855\n",
      "Epoch 00093: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0451 - acc: 0.9855 - val_loss: 0.3487 - val_acc: 0.9273\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9901\n",
      "Epoch 00094: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0322 - acc: 0.9900 - val_loss: 0.3475 - val_acc: 0.9341\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9862\n",
      "Epoch 00095: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0438 - acc: 0.9862 - val_loss: 0.3359 - val_acc: 0.9304\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9902\n",
      "Epoch 00096: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.3378 - val_acc: 0.9357\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9890\n",
      "Epoch 00097: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0342 - acc: 0.9890 - val_loss: 0.3575 - val_acc: 0.9338\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9912\n",
      "Epoch 00098: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0275 - acc: 0.9912 - val_loss: 0.3270 - val_acc: 0.9315\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9851\n",
      "Epoch 00099: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0466 - acc: 0.9851 - val_loss: 0.3116 - val_acc: 0.9415\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9926\n",
      "Epoch 00100: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0252 - acc: 0.9925 - val_loss: 0.3694 - val_acc: 0.9345\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9835\n",
      "Epoch 00101: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0517 - acc: 0.9835 - val_loss: 0.3147 - val_acc: 0.9355\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9910\n",
      "Epoch 00102: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0274 - acc: 0.9910 - val_loss: 0.4025 - val_acc: 0.9271\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9922\n",
      "Epoch 00103: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0260 - acc: 0.9921 - val_loss: 0.3760 - val_acc: 0.9269\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9884\n",
      "Epoch 00104: val_loss did not improve from 0.28776\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0369 - acc: 0.9884 - val_loss: 0.3792 - val_acc: 0.9294\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 0.3428 - acc: 0.9111\n",
      "Loss: 0.34281196426039295 Accuracy: 0.9111111\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8133 - acc: 0.1626\n",
      "Epoch 00001: val_loss improved from inf to 2.29843, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/001-2.2984.hdf5\n",
      "36805/36805 [==============================] - 198s 5ms/sample - loss: 2.8132 - acc: 0.1626 - val_loss: 2.2984 - val_acc: 0.2211\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0138 - acc: 0.3630\n",
      "Epoch 00002: val_loss improved from 2.29843 to 1.50403, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/002-1.5040.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 2.0140 - acc: 0.3630 - val_loss: 1.5040 - val_acc: 0.5451\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4802 - acc: 0.5286\n",
      "Epoch 00003: val_loss improved from 1.50403 to 1.06577, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/003-1.0658.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.4803 - acc: 0.5285 - val_loss: 1.0658 - val_acc: 0.6830\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1354 - acc: 0.6452\n",
      "Epoch 00004: val_loss improved from 1.06577 to 0.98497, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/004-0.9850.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1354 - acc: 0.6452 - val_loss: 0.9850 - val_acc: 0.6995\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8970 - acc: 0.7246\n",
      "Epoch 00005: val_loss improved from 0.98497 to 0.66605, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/005-0.6661.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.8971 - acc: 0.7246 - val_loss: 0.6661 - val_acc: 0.8067\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7820\n",
      "Epoch 00006: val_loss improved from 0.66605 to 0.61543, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/006-0.6154.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7163 - acc: 0.7820 - val_loss: 0.6154 - val_acc: 0.8279\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.8176\n",
      "Epoch 00007: val_loss improved from 0.61543 to 0.47848, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/007-0.4785.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5961 - acc: 0.8176 - val_loss: 0.4785 - val_acc: 0.8642\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8484\n",
      "Epoch 00008: val_loss improved from 0.47848 to 0.42397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/008-0.4240.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4968 - acc: 0.8484 - val_loss: 0.4240 - val_acc: 0.8775\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4278 - acc: 0.8696\n",
      "Epoch 00009: val_loss improved from 0.42397 to 0.34790, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/009-0.3479.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4279 - acc: 0.8696 - val_loss: 0.3479 - val_acc: 0.9024\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8825\n",
      "Epoch 00010: val_loss did not improve from 0.34790\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3797 - acc: 0.8825 - val_loss: 0.4010 - val_acc: 0.8840\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8924\n",
      "Epoch 00011: val_loss improved from 0.34790 to 0.30165, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/011-0.3016.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3449 - acc: 0.8924 - val_loss: 0.3016 - val_acc: 0.9129\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.9057\n",
      "Epoch 00012: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3083 - acc: 0.9057 - val_loss: 0.3679 - val_acc: 0.8924\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.9098\n",
      "Epoch 00013: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2890 - acc: 0.9098 - val_loss: 0.3090 - val_acc: 0.9094\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9184\n",
      "Epoch 00014: val_loss did not improve from 0.30165\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2590 - acc: 0.9184 - val_loss: 0.3109 - val_acc: 0.9157\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9241\n",
      "Epoch 00015: val_loss improved from 0.30165 to 0.26205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/015-0.2621.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2404 - acc: 0.9241 - val_loss: 0.2621 - val_acc: 0.9248\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9292\n",
      "Epoch 00016: val_loss improved from 0.26205 to 0.24498, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/016-0.2450.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2268 - acc: 0.9292 - val_loss: 0.2450 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9356\n",
      "Epoch 00017: val_loss did not improve from 0.24498\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2051 - acc: 0.9356 - val_loss: 0.2574 - val_acc: 0.9280\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9386\n",
      "Epoch 00018: val_loss did not improve from 0.24498\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1928 - acc: 0.9385 - val_loss: 0.3201 - val_acc: 0.9157\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9415\n",
      "Epoch 00019: val_loss improved from 0.24498 to 0.21467, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/019-0.2147.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1845 - acc: 0.9415 - val_loss: 0.2147 - val_acc: 0.9366\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9446\n",
      "Epoch 00020: val_loss did not improve from 0.21467\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1741 - acc: 0.9446 - val_loss: 0.2319 - val_acc: 0.9317\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9477\n",
      "Epoch 00021: val_loss did not improve from 0.21467\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1580 - acc: 0.9477 - val_loss: 0.2426 - val_acc: 0.9311\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9519\n",
      "Epoch 00022: val_loss did not improve from 0.21467\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1488 - acc: 0.9519 - val_loss: 0.2447 - val_acc: 0.9329\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9531\n",
      "Epoch 00023: val_loss improved from 0.21467 to 0.20813, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/023-0.2081.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1456 - acc: 0.9531 - val_loss: 0.2081 - val_acc: 0.9392\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9557\n",
      "Epoch 00024: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1345 - acc: 0.9557 - val_loss: 0.3236 - val_acc: 0.9080\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9585\n",
      "Epoch 00025: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1281 - acc: 0.9584 - val_loss: 0.2713 - val_acc: 0.9250\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9580\n",
      "Epoch 00026: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1276 - acc: 0.9579 - val_loss: 0.7711 - val_acc: 0.8139\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9613\n",
      "Epoch 00027: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1160 - acc: 0.9613 - val_loss: 0.2470 - val_acc: 0.9287\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9658\n",
      "Epoch 00028: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1065 - acc: 0.9658 - val_loss: 0.2154 - val_acc: 0.9385\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9659\n",
      "Epoch 00029: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1055 - acc: 0.9659 - val_loss: 0.2111 - val_acc: 0.9390\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9670\n",
      "Epoch 00030: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1004 - acc: 0.9670 - val_loss: 0.2496 - val_acc: 0.9317\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9697\n",
      "Epoch 00031: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0939 - acc: 0.9697 - val_loss: 0.2256 - val_acc: 0.9399\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9711\n",
      "Epoch 00032: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0917 - acc: 0.9710 - val_loss: 0.2383 - val_acc: 0.9406\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9682\n",
      "Epoch 00033: val_loss did not improve from 0.20813\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0973 - acc: 0.9681 - val_loss: 0.2211 - val_acc: 0.9439\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9702\n",
      "Epoch 00034: val_loss improved from 0.20813 to 0.18733, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv_checkpoint/034-0.1873.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0896 - acc: 0.9702 - val_loss: 0.1873 - val_acc: 0.9464\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9742\n",
      "Epoch 00035: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0785 - acc: 0.9741 - val_loss: 0.2416 - val_acc: 0.9331\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9734\n",
      "Epoch 00036: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0825 - acc: 0.9734 - val_loss: 0.2019 - val_acc: 0.9483\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9767\n",
      "Epoch 00037: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0719 - acc: 0.9766 - val_loss: 0.2234 - val_acc: 0.9387\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9730\n",
      "Epoch 00038: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0832 - acc: 0.9730 - val_loss: 0.1937 - val_acc: 0.9485\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9769\n",
      "Epoch 00039: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0697 - acc: 0.9769 - val_loss: 0.2172 - val_acc: 0.9399\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9787\n",
      "Epoch 00040: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0649 - acc: 0.9787 - val_loss: 0.2490 - val_acc: 0.9394\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9790\n",
      "Epoch 00041: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0641 - acc: 0.9790 - val_loss: 0.2075 - val_acc: 0.9439\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9801\n",
      "Epoch 00042: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0606 - acc: 0.9801 - val_loss: 0.3113 - val_acc: 0.9315\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9807\n",
      "Epoch 00043: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0587 - acc: 0.9807 - val_loss: 0.2326 - val_acc: 0.9411\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9813\n",
      "Epoch 00044: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0564 - acc: 0.9813 - val_loss: 0.2531 - val_acc: 0.9341\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9831\n",
      "Epoch 00045: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0533 - acc: 0.9830 - val_loss: 0.2030 - val_acc: 0.9474\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9785\n",
      "Epoch 00046: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0636 - acc: 0.9785 - val_loss: 0.2434 - val_acc: 0.9411\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9780\n",
      "Epoch 00047: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0675 - acc: 0.9780 - val_loss: 0.2116 - val_acc: 0.9478\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9821\n",
      "Epoch 00048: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0523 - acc: 0.9821 - val_loss: 0.1983 - val_acc: 0.9453\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9845\n",
      "Epoch 00049: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0486 - acc: 0.9845 - val_loss: 0.2126 - val_acc: 0.9509\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9859\n",
      "Epoch 00050: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 0.2223 - val_acc: 0.9464\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9819\n",
      "Epoch 00051: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0565 - acc: 0.9819 - val_loss: 0.2660 - val_acc: 0.9380\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9861\n",
      "Epoch 00052: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0428 - acc: 0.9861 - val_loss: 0.2185 - val_acc: 0.9504\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9826\n",
      "Epoch 00053: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0536 - acc: 0.9825 - val_loss: 0.2555 - val_acc: 0.9364\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9838\n",
      "Epoch 00054: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0488 - acc: 0.9838 - val_loss: 0.1912 - val_acc: 0.9534\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9880\n",
      "Epoch 00055: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0371 - acc: 0.9880 - val_loss: 0.2586 - val_acc: 0.9394\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9878\n",
      "Epoch 00056: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0383 - acc: 0.9878 - val_loss: 0.2158 - val_acc: 0.9515\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9817\n",
      "Epoch 00057: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0526 - acc: 0.9817 - val_loss: 0.2558 - val_acc: 0.9399\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9887\n",
      "Epoch 00058: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0355 - acc: 0.9888 - val_loss: 0.2352 - val_acc: 0.9492\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9883\n",
      "Epoch 00059: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0363 - acc: 0.9883 - val_loss: 0.2845 - val_acc: 0.9364\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9878\n",
      "Epoch 00060: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0380 - acc: 0.9878 - val_loss: 0.2414 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9882\n",
      "Epoch 00061: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0367 - acc: 0.9882 - val_loss: 0.2658 - val_acc: 0.9406\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9861\n",
      "Epoch 00062: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0433 - acc: 0.9860 - val_loss: 0.2914 - val_acc: 0.9366\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9870\n",
      "Epoch 00063: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0417 - acc: 0.9870 - val_loss: 0.2432 - val_acc: 0.9441\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9904\n",
      "Epoch 00064: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0300 - acc: 0.9904 - val_loss: 0.2247 - val_acc: 0.9492\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9891\n",
      "Epoch 00065: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0316 - acc: 0.9891 - val_loss: 0.2126 - val_acc: 0.9509\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9882\n",
      "Epoch 00066: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0360 - acc: 0.9882 - val_loss: 0.2921 - val_acc: 0.9362\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9809\n",
      "Epoch 00067: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0616 - acc: 0.9809 - val_loss: 0.2342 - val_acc: 0.9474\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9893\n",
      "Epoch 00068: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0331 - acc: 0.9893 - val_loss: 0.2262 - val_acc: 0.9474\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9912\n",
      "Epoch 00069: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0280 - acc: 0.9912 - val_loss: 0.2798 - val_acc: 0.9394\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9910\n",
      "Epoch 00070: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0292 - acc: 0.9910 - val_loss: 0.2460 - val_acc: 0.9460\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9875\n",
      "Epoch 00071: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0381 - acc: 0.9874 - val_loss: 0.2188 - val_acc: 0.9467\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9868\n",
      "Epoch 00072: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0425 - acc: 0.9868 - val_loss: 0.2276 - val_acc: 0.9483\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9918\n",
      "Epoch 00073: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0258 - acc: 0.9918 - val_loss: 0.2120 - val_acc: 0.9474\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9904\n",
      "Epoch 00074: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0294 - acc: 0.9904 - val_loss: 0.2478 - val_acc: 0.9446\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9896\n",
      "Epoch 00075: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0316 - acc: 0.9896 - val_loss: 0.2633 - val_acc: 0.9462\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9895\n",
      "Epoch 00076: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0319 - acc: 0.9895 - val_loss: 0.2236 - val_acc: 0.9506\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9921\n",
      "Epoch 00077: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.2705 - val_acc: 0.9415\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9921\n",
      "Epoch 00078: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0264 - acc: 0.9921 - val_loss: 0.2501 - val_acc: 0.9469\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9876\n",
      "Epoch 00079: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0382 - acc: 0.9875 - val_loss: 0.2203 - val_acc: 0.9499\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9901\n",
      "Epoch 00080: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0300 - acc: 0.9900 - val_loss: 0.2604 - val_acc: 0.9422\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9877\n",
      "Epoch 00081: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0366 - acc: 0.9877 - val_loss: 0.2224 - val_acc: 0.9483\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9927\n",
      "Epoch 00082: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0224 - acc: 0.9927 - val_loss: 0.2359 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9942\n",
      "Epoch 00083: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0205 - acc: 0.9942 - val_loss: 0.2686 - val_acc: 0.9476\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9895\n",
      "Epoch 00084: val_loss did not improve from 0.18733\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0329 - acc: 0.9895 - val_loss: 0.2366 - val_acc: 0.9525\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd81dX9+PHXuTt7Q0IYYUPYQ4xFhorUUamKiNbdVlvrqLW1Urtoq7+6vrWitooT1DqKW6m4QFBBBQTZMkMGkJ3czLvO74+TmwFJCJBLCPf9fDzuIzf3fj7n877rvD/n8znnfJTWGiGEEALA0tkBCCGEOHFIUhBCCNFAkoIQQogGkhSEEEI0kKQghBCigSQFIYQQDSQpCCGEaCBJQQghRANJCkIIIRrYOjuAI5WcnKwzMjI6OwwhhOhS1qxZU6S1Tjnccl0uKWRkZLB69erODkMIIboUpVR2e5aTw0dCCCEaSFIQQgjRQJKCEEKIBl3unEJLvF4vubm51NbWdnYoXZbL5aJnz57Y7fbODkUI0YlOiqSQm5tLTEwMGRkZKKU6O5wuR2tNcXExubm59O3bt7PDEUJ0opPi8FFtbS1JSUmSEI6SUoqkpCRpaQkhTo6kAEhCOEby/gkh4CRKCofj91dTV5dHIODt7FCEEOKEFTZJIRCow+PZh9YdnxTKysr417/+dVTrnnfeeZSVlbV7+blz5/Lggw8e1baEEOJwwiYpKGUFQGt/h5fdVlLw+Xxtrrt48WLi4+M7PCYhhDgaYZQUTEcrrduupI/GnDlz2LlzJ6NHj+aOO+5g2bJlTJo0iRkzZpCZmQnAhRdeyLhx4xg2bBjz589vWDcjI4OioiL27NnD0KFDuf766xk2bBjTp0+npqamze2uW7eOrKwsRo4cyUUXXURpaSkA8+bNIzMzk5EjR3LZZZcB8OmnnzJ69GhGjx7NmDFjcLvdHf4+CCG6vpOiS2pT27ffRmXluhaeCeD3V2GxuFDqyPriR0ePZuDAf7b6/L333svGjRtZt85sd9myZaxdu5aNGzc2dPF85plnSExMpKamhlNOOYWZM2eSlJR0UOzbeemll3jyySe59NJLee2117jyyitb3e7VV1/NI488wpQpU/jTn/7EX/7yF/75z39y7733snv3bpxOZ8OhqQcffJDHHnuMiRMnUllZicvlOqL3QAgRHsKmpQDB3jX6uGxtwoQJzfr8z5s3j1GjRpGVlUVOTg7bt28/ZJ2+ffsyevRoAMaNG8eePXtaLb+8vJyysjKmTJkCwDXXXMPy5csBGDlyJFdccQUvvPACNpvJ+xMnTuT2229n3rx5lJWVNTwuhBBNnXQ1Q2t79FprKivX4HCk4XSmhzyOqKiohvvLli3jo48+YuXKlURGRjJ16tQWxwQ4nc6G+1ar9bCHj1rz3nvvsXz5ct555x3uueceNmzYwJw5czj//PNZvHgxEydOZMmSJQwZMuSoyhdCnLzCpqVg+uFbQ3KiOSYmps1j9OXl5SQkJBAZGcnWrVtZtWrVMW8zLi6OhIQEVqxYAcDzzz/PlClTCAQC5OTkcMYZZ3DfffdRXl5OZWUlO3fuZMSIEdx5552ccsopbN269ZhjEEKcfE66lkJblLKF5ERzUlISEydOZPjw4Zx77rmcf/75zZ4/55xzePzxxxk6dCiDBw8mKyurQ7a7YMECfv7zn1NdXU2/fv149tln8fv9XHnllZSXl6O15tZbbyU+Pp4//vGPLF26FIvFwrBhwzj33HM7JAYhxMlFaX18jrF3lPHjx+uDL7KzZcsWhg4deth1q6o2o5SdyMiBoQqvS2vv+yiE6HqUUmu01uMPt1zYHD4CM1YhFIePhBDiZBFmScEGdPzhIyGEOFmEWVKQloIQQrQlrJJCqHofCSHEySKskoI5fBRA60BnhyKEECekMEsKoZsUTwghTgZhmhQ6/2RzdHT0ET0uhBDHQ5glheBMqdJSEEKIloRVUgBr/d+OTQpz5szhsccea/g/eCGcyspKzjrrLMaOHcuIESN466232l2m1po77riD4cOHM2LECF555RUA9u3bx+TJkxk9ejTDhw9nxYoV+P1+rr322oZlH3rooQ59fUKI8HHyTXNx222wrqWps8GqA0QEzPTZHMn02aNHwz9bnzp79uzZ3Hbbbdx0000AvPrqqyxZsgSXy8Ubb7xBbGwsRUVFZGVlMWPGjHZdD/n1119n3bp1rF+/nqKiIk455RQmT57Mf/7zH77//e/z+9//Hr/fT3V1NevWrSMvL4+NGzcCHNGV3IQQoqmQtRSUUr2UUkuVUpuVUpuUUr9sYZmpSqlypdS6+tufQhVP/QZDUuyYMWMoKCggPz+f9evXk5CQQK9evdBac9dddzFy5EimTZtGXl4eBw4caFeZn332GZdffjlWq5Xu3bszZcoUvv76a0455RSeffZZ5s6dy4YNG4iJiaFfv37s2rWLW265hffff5/Y2NiQvE4hxMkvlC0FH/BrrfVapVQMsEYp9aHWevNBy63QWv+gw7baxh49OkBN5VocjnSczrQO2yTArFmzWLRoEfv372f27NkAvPjiixQWFrJmzRrsdjsZGRktTpl9JCZPnszy5ct57733uPbaa7n99tu5+uqrWb9+PUuWLOHxxx/n1Vdf5ZlnnumIlyWECDMhaylorfdprdfW33cDW4DQX8igDUpZAEtIeh/Nnj2bl19+mUWLFjFr1izATJndrVs37HY7S5cuJTs7u93lTZo0iVdeeQW/309hYSHLly9nwoQJZGdn0717d66//np++tOfsnbtWoqKiggEAsycOZO7776btWvXdvjrE0KEh+NyTkEplQGMAb5s4enTlFLrgXzgN1rrTS2sfwNwA0Dv3r2PMZbQjGoeNmwYbreb9PR00tJMK+SKK67gggsuYMSIEYwfP/6ILmpz0UUXsXLlSkaNGoVSivvvv5/U1FQWLFjAAw88gN1uJzo6moULF5KXl8d1111HIGAG5f3973/v8NcnhAgPIZ86WykVDXwK3KO1fv2g52KBgNa6Uil1HvCw1rrNea2PZepsgKqqTVgsTiIiBhzJywgLMnW2ECevE2LqbKWUHXgNePHghACgta7QWlfW318M2JVSyaGMSeY/EkKI1oWy95ECnga2aK3/0coyqfXLoZSaUB9PcahiMtuRpCCEEK0J5TmFicBVwAalVHDgwF1AbwCt9ePAJcCNSikfUANcpkN8PEspK4FATSg3IYQQXVbIkoLW+jOgzYEBWutHgUdDFUNLzHWapaUghBAtCbNpLoKT4vnpatemFkKI4yEMk4JMiieEEK0Ju6QQiknxysrK+Ne//nVU65533nkyV5EQ4oQRdkkhFNdUaCsp+Hxtb2fx4sXEx8d3WCxCCHEswjApdPzhozlz5rBz505Gjx7NHXfcwbJly5g0aRIzZswgMzMTgAsvvJBx48YxbNgw5s+f37BuRkYGRUVF7Nmzh6FDh3L99dczbNgwpk+fTk3Nob2k3nnnHU499VTGjBnDtGnTGibYq6ys5LrrrmPEiBGMHDmS1157DYD333+fsWPHMmrUKM4666wOe81CiJPTSTd1dhszZwOgdSSBwGAsFle7J009zMzZ3HvvvWzcuJF19RtetmwZa9euZePGjfTt2xeAZ555hsTERGpqajjllFOYOXMmSUlJzcrZvn07L730Ek8++SSXXnopr732GldeeWWzZU4//XRWrVqFUoqnnnqK+++/n//7v//jb3/7G3FxcWzYsAGA0tJSCgsLuf7661m+fDl9+/alpKSkfS9YCBG2TrqkcDiN1zIIbe+jCRMmNCQEgHnz5vHGG28AkJOTw/bt2w9JCn379mX06NEAjBs3jj179hxSbm5uLrNnz2bfvn14PJ6GbXz00Ue8/PLLDcslJCTwzjvvMHny5IZlEhMTO/Q1CiFOPiddUmh1j15r8PnQVkVl1TYcjp44nakhiyMqKqrh/rJly/joo49YuXIlkZGRTJ06tcUptJ1OZ8N9q9Xa4uGjW265hdtvv50ZM2awbNky5s6dG5L4hRDhKXzOKZSUwPr1UOetf6DjzinExMTgdrtbfb68vJyEhAQiIyPZunUrq1atOuptlZeXk55uZiBfsGBBw+Nnn312s0uClpaWkpWVxfLly9m9ezeAHD4SQhxW+CQFm2kUKb8fsHVo76OkpCQmTpzI8OHDueOOOw55/pxzzsHn8zF06FDmzJlDVlbWUW9r7ty5zJo1i3HjxpGc3Dh34B/+8AdKS0sZPnw4o0aNYunSpaSkpDB//nwuvvhiRo0a1XDxHyGEaE3Ip87uaEc9dXZlJWzdCgMHUmndi9UaRUREvxBG2vXI1NlCnLxOiKmzTyj1LQV8PpkpVQghWhE+ScFaP5LZ76+fFK/jL8kphBBdXfglBWkpCCFEq8InKVgs5lbfUujI3kdCCHGyCJ+kAKa14PdjLsnpk+mzhRDiIOGVFGy2hsNHZkSzJAUhhGgqvJJCfUuhcVK8zjvZHB0d3WnbFkKI1oRpUghOny3nFYQQoqnwSgrNDh91XFKYM2dOsykm5s6dy4MPPkhlZSVnnXUWY8eOZcSIEbz11luHLau1KbZbmgK7temyhRDiaJ10E+Ld9v5trNvfytzZdXXg9aLXRBAIVGOxRDQcSmrL6NTR/POc1ufOnj17Nrfddhs33XQTAK+++ipLlizB5XLxxhtvEBsbS1FREVlZWcyYMaPJTK2HammK7UAg0OIU2C1Nly2EEMfipEsKbVLKzJZKx06fPWbMGAoKCsjPz6ewsJCEhAR69eqF1+vlrrvuYvny5VgsFvLy8jhw4ACpqa3PztrSFNuFhYUtToHd0nTZQghxLE66pNDWHj0HDkBODoFRw6mq3YjT2QuHo3uHbHfWrFksWrSI/fv3N0w89+KLL1JYWMiaNWuw2+1kZGS0OGV2UHun2BZCiFAJr3MK9aOald+0EDryRPPs2bN5+eWXWbRoEbNmzQLMNNfdunXDbrezdOlSsrOz2yyjtSm2W5sCu6XpsoUQ4liEV1JomD47AFg6NCkMGzYMt9tNeno6aWlpAFxxxRWsXr2aESNGsHDhQoYMGdJmGa1Nsd3aFNgtTZcthBDHInymzgZwu2HbNhg0iErLHqzWGCIi+h5+vTAhU2cLcfLq9KmzlVK9lFJLlVKblVKblFK/bGEZpZSap5TaoZT6Vik1NlTxAIdMiifzHwkhRHOhPNHsA36ttV6rlIoB1iilPtRab26yzLnAwPrbqcC/6/+GRvCaCvUD2GTwmhBCNBeyloLWep/Wem39fTewBUg/aLEfAgu1sQqIV0qlHeX2Dr9Qk2sqdPQlObu6rnYYUQgRGsflRLNSKgMYA3x50FPpQE6T/3M5NHEclsvlori4+PAVm8Vixir4fFgskhSCtNYUFxfjcrk6OxQhRCcL+TgFpVQ08Bpwm9a64ijLuAG4AaB3796HPN+zZ09yc3MpLCw8fGHFxVBTg6/Uis9Xjstlp3EwW/hyuVz07Nmzs8MQQnSykCYFpZQdkxBe1Fq/3sIieUCvJv/3rH+sGa31fGA+mN5HBz9vt9sbRvse1oUXwpgx5P3fZLZvv4nMzHyczqM6YiWEECedUPY+UsDTwBat9T9aWext4Or6XkhZQLnWel+oYgIgIQFKS3E4TCLweEK7OSGE6EpC2VKYCFwFbFBKBWeouwvoDaC1fhxYDJwH7ACqgetCGI8RHw8lJZIUhBCiBSFLClrrzzjMwXptzgzfFKoYWpSQALt2NRwyqquTpCCEEEHhNc0FNDl8ZGYqlZaCEEI0CtukYFEObLZEPJ79nR2REEKcMMIvKcTHm8FrVVU4HGnSUhBCiCbCLykEL0RTfwhJkoIQQjQK66TgdKbJiWYhhGgi/JJCfLz5W1bWcPhI5v0RQggj/JJCs8NHaWjtweeTK5YJIQRIUgCkW6oQQgSFX1IIHj6qP6cASLdUIYSoF35JIS7OTJ9df04BZFSzEEIEhV9SsFhMYpBRzUIIcYjwSwrQMKrZao3BYomUpCCEEPXCMynEx0NZGUopGdUshBBNhGdSqG8pADKATQghmgj7pCAtBSGEaBSeSaH+8BEEk4J0SRVCCAjXpHBQS8Hvr8Dvr+7koIQQovOFb1KoqYG6OumWKoQQTYRnUmhhVLOcbBZCiHBNCsH5j5qMapaWghBChHtSkEnxhBCimbBPCnZ7EkrZpAeSEEIQrkmhyYV2lLLIZTmFEKJeeCaFJi0FMN1S5USzEEKEa1Jo0vsIZFSzEEIEhWdScDggMrLJqGY5fCSEEBCuSQEOGdXs9RYSCHg7OSghhOhcIUsKSqlnlFIFSqmNrTw/VSlVrpRaV3/7U6hiadFBM6UCeDwHjmsIQghxogllS+E54JzDLLNCaz26/vbXEMZyqIMmxQO5VrMQQrQrKSilfqmUilXG00qptUqp6W2to7VeDpR0SJShkJAAJSY8GcAmhBBGe1sKP9ZaVwDTgQTgKuDeDtj+aUqp9Uqp/ymlhrW2kFLqBqXUaqXU6sLCwg7YLJCaCvtMEpCkIIQQRnuTgqr/ex7wvNZ6U5PHjtZaoI/WehTwCPBmawtqredrrcdrrcenpKQc42br9e4NBQVQU4PD0R1Q1NXldUzZQgjRRbU3KaxRSn2ASQpLlFIxQOBYNqy1rtBaV9bfXwzYlVLJx1LmEenTx/zNzcViseNy9aGmZsdx27wQQpyI2psUfgLMAU7RWlcDduC6Y9mwUipVKaXq70+oj6X4WMo8Ir17m7979wIQETGI6uptx23zQghxIrK1c7nTgHVa6yql1JXAWODhtlZQSr0ETAWSlVK5wJ8xyQSt9ePAJcCNSikfUANcprXWR/UqjkYwKWRnAxAZOZj9+59Da019rhJCiLDT3qTwb2CUUmoU8GvgKWAhMKW1FbTWl7dVoNb6UeDRdm6/46Wng1LNWgp+vxuPZ3/DuAUhhAg37T185Kvfi/8h8KjW+jEgJnRhHQcOB/To0ZAUIiMHA1BT811nRiWEEJ2qvUnBrZT6HaYr6ntKKQv1h4K6tN69mySFQQByXkEIEdbamxRmA3WY8Qr7gZ7AAyGL6njp3bvhnILT2QuLxUV1tbQUhBDhq11JoT4RvAjEKaV+ANRqrReGNLLjoXdvyMmBQAClLEREDJTDR0KIsNbeaS4uBb4CZgGXAl8qpS4JZWDHRZ8+UFcH9aOkpVuqECLctbf30e8xYxQKAJRSKcBHwKJQBXZcNB2r0L07kZGDKS5+i0DAi8XS9U+ZCCHEkWrvOQVLMCHUKz6CdU9ch4xVGITWPmprd3diUEII0Xna21J4Xym1BHip/v/ZwOLQhHQcHTKq2XRLra7+rqE3khBChJN2JQWt9R1KqZnAxPqH5mut3whdWMdJfDzExBzSLbWmZhvwg04MTAghOkd7WwporV8DXgthLMefUs26pdrtidhsSdItVQgRttpMCkopN9DSfEQK0Frr2JBEdTw1GcAGZmSz9EASQoSrNpOC1rprT2XRHr17w9dfN/wbGTmIkpIlnRiQEEJ0nq7fg+hY9ekDRUVQXQ2Yk80ezz58PncnByaEEMefJIWDeiA1nmyW8wpCiPAjSeGQpNDYLVUIIcKNJIWDkoLL1R9QcrJZCBGWJCmkp4PF0pAUrFYXLleGHD4SQoQlSQo2m0kM9WMVQCbGE0KEL0kK0MJYhUHU1HzH8bxktBBCnAgkKUCLA9j8/ko8nn2dGJQQQhx/khTAjFWov9gOQFTUCAAqK7/pzKiEEOK4k6QApqXg9cL+/QDExIwDrFRUrOrcuIQQ4jiTpACHdEu1WqOIjh5BRcWXnRiUEEIcf5IU4JCkABAbm0VFxZdoHeikoIQQ4viTpADmnALAd41jE2JiTsXvr5CuqUKIsCJJASA2FiZNgocegoKC+oeyAOS8ghAirIQsKSilnlFKFSilNrbyvFJKzVNK7VBKfauUGhuqWNrl8cehshJuuw0wYxVstnhJCkKIsBLKlsJzwDltPH8uMLD+dgPw7xDGcniZmfD738NLL8F776GUhZiYCXKyWQgRVkKWFLTWy4GSNhb5IbBQG6uAeKVUWqjiaZc5c2DYMLjxRnC7iY3NoqpqAz5fZaeGJYQQx0u7r9EcAulATpP/c+sf67xhxA4HPPkkTJwIv/89sX8+Fwjgdq8mIWFqp4UlwpvW4PFAVRXU1Zn7dXVgtUJSEsTFmcuNg3m8sBAqKiAmBhITITKy8XkAnw9KS821pQoLzX2tzTJKQVSU6ZDXqxe4XGZMZ14e7NwJubmmDIvFbN9iaX5fa3OrHwdKRIQpLzLSTDPm9TbeKiqgrMxs3+0Gux2cTvMzdDjM/zab+ZuWBgMGQPfuJkavF3bsgM2bTWw1NeZWW2u2HXwtNpt5fxISzC0mxsRqs5l4S0vN8KR9+8z7YbOZGIJx2GyNN0uTXehAwBxtdrvNra6u+WcWH29i7dbNfAY1NY3L+/0QHd34vlRUmO3v22c+j5oaU15dndlut26mrO7d4fTT4bTTQvddg85NCu2mlLoBc4iJ3sHuo6Fy2mlw883w6KPE3nIdAG73l5IUOpHX2/gjqaszP87YWFPhNK3svF7zwysuNreiIvMDC1YCVqupWMvLTWVUVWUqjMREc7NYGtcrLjZlBSubujpTEQQrPb/fVM5er/nr85nnAwHznM/X+NfrbaywamtNxRSsMJKTTaUXrFyDlXpRkblVVJg4fb7W3x+bzVR4wYrnYHa7ea88HnMLHEEv6+RkE4PH0/51Qikqyrx3e/e2/J44HM2TU/BzaY/4ePOZBRNvezgc5vN0Ohu/i4GA+X7V1LSvjKCYGPOdiIgwydjpNElk2zY4cMB8d+666+ROCnlAryb/96x/7BBa6/nAfIDx48eHfpa6q66CRx7BvnEPET0GysnmVng8jXtKbre5oqlSjRWcx2P2xIK3oiLz5S4ogJKS5j/WYCUbvFVVmTIrK1v/gdps5ofk95ttt1VxHimLxVRAERGNP1KLpXEP1GptvkcbEWEeC+4xB5OQ1dr4fESE+aFXVDS+D3v2mPiDCcVuh5QUM3HvyJEmaUVHm1tkpInD4TDleL2NCbC42JSfnGzWj401719JiXnva2sb937tdlMBpqSYW0JCY0UKJmnm5JiKNyfHxNC/v7n16mVeUzDmpn/9/sbPPlheTY35bKqrTbx2e+MtJsbEEdyD9/kaW0HBRBt8LDfXtAx27jR71LNnm9OAmZmmVRN8bywHHRDX2nyHgt/BysrGWH0+s/20NJNo7Pbm63m9zRN70/kxlTKficPR8vdHa/MdPnDAfAaRkWb5mBgTY3W1iaWqynxWqanm+9Yarc3neTzm6OzMpPA2cLNS6mXgVKBca31izEA3ZIj5u3kzsUOyKC39EK01quluaReltakgyssbb2Vlze8XF5u91eChheCP1OMxX+ZgEjiavcekpMYmta3Jt89uNz+KYKUVFWV+QDEx5n5wz8npBL9fU1jhptBdRnFVGX5bBRZXJcrpxu4MMCrpVIamZZCUZH6MPp/mu7LNrCr4iITIGMb1HMWEjGEkxrrILaxg1e5vWZ27jmpvFSPTMjklI5ORvftiszavYbTWVNRVUFBVgMvmoldcL9pLa83O0p2syF6B3WqnT1wf+sT3ISUyhTx3HrtKd7G7dDduj5toRzQxjhiiHFGU1ZaRW5HL3oo8imqKSHAl0C2qG92iutEzticTkocyIHEAdqsdr9/L2n1rWbF3BV8XbWPCwAnM7n82GfEZAGwu3MyizYt4b/t7dIvqxtn9zmZ0v7MZkjyE7SXb+WzvZ3y29zOqvFVkDcxi2pmnMSZ1DLW+WjYUbODbA9/ycU4OY9PGMiVjCt2iujW8tr3le9lYsBG71U56TDrpsenEOeNa/M1Ue6vZWbKTnIocvnXvY9++fRRUFVDjraHWX0utr5ZEVyLT+09nWr9pxLniGD4c3HVuVuevZmPBRvzaTxGw3AMxe2MYlDSIIclDSI5MRqPZX7mfPWV7yKvIw+P34Av48Gs/XqeXWp/ZRq2qRVUrbLtt2LPtOKwOoh3RxDpjiXXGYrfaG5f11VJSU0JBVQEFVQVUeioZmDiQ4d2GM6L7CJIiksh35zfcimuKKa8tp6KuglpfLRnxGQxSgxjkGkSUI4ocTw45tTnsrdxL4YFCir8tpqSmBLfHTWJEIt0izWdss9jYX7mf/VX72V+5nytGXMHNE24+4t/dkVChmh5aKfUSMBVIBg4AfwbsAFrrx5X5tjyK6aFUDVyntV59uHLHjx+vV68+7GLHrk8fmDSJvPtOY/v2m8nKysblCvGhq6OgNRQV+9mZU0X23gB792r25mhK9kdTVeFoqOwrKqCssoay9P/iz1gCtfFQmWputfEQsIG2QsCK1ekhJqGGqPgaXLFufNF78biyqXVlExlI4/Sa++gTmdlQacfGmr+RkSam4F5v8LBGQgJYIsvYUfMVG4vW8c3+b9hWtA1vwEtABwjoAFH2KAYkDmBA4gD6xveluKaYrUVb2VK0hd2lu/H4Pfi1H1/AR62vlsBhRppnxGdwZsaZOG1OFm9fTHZ5drPnrcpKanQqee4WG6dE2CKIdcZiURYsykJAByiuKcbjb8yEvWJ7ManPJCb2mkhFXQXrD6xn/f717C7bTXpMOv0T+9Mvvh/VvmqW7l5KTkVOi9tqj+TIZJIjkymtKaWwurDZ67dZbPRL6EduRS7V3moA4pxxlNeVAzAgcQA2i42tRVtRKLJ6ZlFYXciOkh0AuGwuan21ACRGJBLtiGZvuRndb7fY8Qa8DdsKvhcAQ5KH0C2qG98e+Jay2rJDYnbZXKREppAUmURyZDK+gI/txdtbfM/jnHFEOaJw2Vy4bC5yK3KpqKvAZrFxavqplNeVs6lgE5q266t4Vzw13hrq/HVtLne0FIrkyGQi7ZHsLd/bZjwKRawzFofVQWF1YavLxThiSIpMIjEikSh7FKW1pRRUFVBUXURAB0iOTCYtOo3U6FSuGHEF14y+5uhiV2qN1nr8YZfratcMOG5J4ZxzoKAA96dPsmbNeDIzX6Fbt0tDv92D+P2wbYcq/O/qAAAgAElEQVSHz77NZfWe79hasoG9tRsosmyi1lqA31YOTvehK/qcRJVNINE9iW6+CVQmL2NP3ALqLKXEkEpAeajSbXUOaxRpj6RPXB96x/Xmq7yvcHvc/CrrV/xpyp9w2Vws27OMRZsXsalwE/+d9V9So1ObrV9eW86Qx4awv9JMONgnrg+ZKZlE2CMaKt2y2jJ2luxkT9ke/NoPQFp0GkOSh9AvoR8RtgisFis2iw2XzUWCK4F4VzxxrjhinbHEOGKIdkTjC/j4bO9nLN2zlGV7llHnr2Nav2mcP/B8zhlwDnW+uobKe0/5HgYnDWZM6hhGp44m2hHN5sLNbC7czJaiLVR6KgnoAP6AH6VMZRDcSy+tKWXF3hWs2Lui2esalTqK/gn9yXfns7N0JztLdmKz2JiaMZUzMs5gSsYULMpCdlk22eXZFFYVkh6bTr+EfvSN70ucK44qTxVuj5tKTyXxrnh6xPTAZXM1vJ8BHaC0ppTdZbvZUriFLUVb2Fa8jZ4xPZnUZxKn9z6d7lHd2Vq0lQ93fcgHOz+gzl/HhYMv5OKhF5MWYzr57S7dzYe7PmTDgQ2MSh3FxF4TGZw8GIuysM+9j5W5K/kq7yviXfGM7D6Skd1H0j2qO2v3reXT7E9ZtmcZZbVljOw+ktGpoxnRbQQBHSDPnUdeRR77KvdRVF1EcU0xxdXFKKUYkDiAgYkDGZA4gN5xvekR04PU6NRmrw/A6/eyKncV7+94n493f0xCRAKnpp9KVs8sRqeObrZ8SU0J24q2sa14G9uLtxPtiCYjPoOM+Ax6xvbEZXM1fHfsFntD4nHanA3b8ga8ePwe3HVu3B43FXUVePweImwRDcsmRiSSFJGE1WIFTItnc+FmNhzYQHldOekx6fSI6UGPmB4kRSYR7YjGokxr013nZkfJDr4r/o4qbxW943rTO643vWJ7EWGPaPm3H/AT0AHsVnuLzx8pSQrH6vbb4fHHCVSU8tkX8fTocSMDBvzjiIv5rvg7UiJTSIhIaPb4FzlfcOdHd5LvzufKEVeR5fwJ277qxd69sKN8M1vUIvZFfEiVYzfE5INq/Jxs1enE1g4nwdaD+Ig4kqLiSImLJinRSlKiIjZGkVOxlxV7V7B231r82o/dYufioRfzs3E/Y2rGVJRS1PnqKKgqoKy2DL/24w/48Ws/DquDCFsEEfYIoh3RJLgSGg4DFFYVMuejOTyz7hlSo1Px+r0U1xQTZY+ixlfDb077DfedfV+z1/rA5w/w249+y8szX+bs/meTGJHY6vvl9XvJrcglIcJU+sci2AqxWUJ3lFRrTXZ5NnHOuEM+YyFOJJIUjtWTT8INN8CuXawtvQrwM3bsynatqrVm6Z6l3L38bpbuWYrD6uC8gedxxYgryEzJ5Hcf/Jm3dywiVqXhrMikMPoT0Ap2TUPF56CTt4BWxFdNoIdjCP2T+zCydx+yBg3gtP7DSYpsvVI9WKWnkm/2fcOgpEF0j+5+lG/GoVbmrGTup3NJjkzmkqGX8P0B3+fHb/2Y/+34H3tv20ucKw4Aj99Dv4f7MShpEJ9c80mHbV8IcWTamxS6RJfUTpGZaf5u2UL8kKns3XsvXm8Zdnvre6855Tl8svsTHl/zOKtyV5Eancrfz7qXbbkHeOO7l3hz65tmQU8kfD6Xii9+Q0Z6FFPP3o1n+NOsi3uJvkm9mZV5MxcNuaihmX8soh3RTOoz6ZjLOdhpvU5jyZVLmj1258Q7eWXTK/x79b+Zc/ocAF7d9Cp57jzmXzC/w2MQQnQ8aSm0pqTEdJV54AHKrz+Nb745nczMV+nWbVazxXaX7ua+z+/j490fN5y46xPXhxtH3EnNyutY8LSLPXsA5Sd+zFJ6TVjLjIwrmTK2B6NHmy6BJ5Pvv/B9c7z+tj04rU7Gzh+Lx+9hw40bGo6vCiGOP2kpHKvERNN5ecsWYmJuw2aLp6Tkfw1JQWvNgvULuOV/txDQAc7qexY3nXITKZVn8sYTw/nDHRZ8PjjrLLjzTpg61crgwdNQalonv7DQunPinZy18CwWrl9I/4T+rNu/jqcueEoSghBdhCSFtmRmwubNWCw2EhK+T0nJ/9A6QElNKTe8ewOvb3mdKX2msODCBUR4+jBnDjz7rMknv/ylOSUxaFBnv4jj64yMMxjfYzz3f34//RP70z2qO1eMvKKzwxJCtJPsvrVl6FAzuYrWJCWdS3b5fn79v58w+NHBvLPtHe6fdj8fXPExbz/fh0GD4IUX4Le/hexsePDB8EsIAEop5kycw87SnXyw8wNunnDzId0NhRAnLmkptCUzEyoqWLfxQ+asW8gHu0CpBcwY/EP+POXPRFeO5oyp8MUXcPbZMG9e42DocHbhkAsZmDiQ3Ipcbhx/Y2eHI4Q4ApIU2pKZiV/BpYt/TKmljp8OTOOiPumc8703eOIJ+PWvzbQML74Il1/efHK2cGa1WHn5kpcprCokKTKps8MRQhwBSQptGTqU/w6D7bV5LJq1iLER6/juu3/wgx94WLzYwfTp8MwzZvIy0dzYtM69kJ4Q4uhIUmhDoFsK90y1MNQXx0VDL6K0JI277x7L55/bmTfPzLAtrYNW3HefOR+zYEFnRyKEOAKSFNrw9nfvsDE5wAubu6OwMGdOFp99ZuGuu17glluu7OzwTmwffADfftvZUQghjpD0PmqF1pq7l99Nf18ss5cVMXcuPPWUhRtueIPzzvsN+jCzdIa9/HxzAYUT5eosQoh2kaTQig92fsCafWv4XfQ5LCiYwV//Cj/+Mcyd68brPUBl5TedHeKJLa9+euQDBzo3DiHEEZGk0AKtNX9b/jd6xfZiYsr13Mo8zh5fyhNPQFLSOYCiqOjNzg7zxBW8Cg+YFoMQosuQpNBElaeK17e8zo9e/xGf53zOHd/7LTc9fTpW/Dx98XvYbOBwdCMx8Vzy858kEJBDIy1qmgj2nRgX0xNCtI8kBUzL4Ofv/pyUB1KY+epMPtz5Ib8Y/wus63/KJ1+4eMD5B3odaJyEr2fPX+L1HqCg4NVOjPoEJklBiC5Leh9hLnjzxJonmD1sNj8b9zMm9ZnE/nwbw34EU6fC9e5VsCGmYfmEhLOJjBxCXt7DdO9+xUlx7eYOldfkcoty+EiILkVaCsBz654jyh7FUzOe4oy+Z2BVNm68EbxeeOopsJw5FT75BP5hrrymlCI9/Rbc7tVUVHzZmaGfmIJJITZWWgpCdDFhnxSqvdW8sukVLsm8hGhHNADvvAPvvgt33w39+wP33AOzZpl5Le69F4Du3a/Gao0jL+/hToz+BJWfDzExMGCAJAUhupiwP3z0xpY3cHvcXDf6uobHHngAMjLg1lvrH7Db4T//MX9/9zvweLD96U+kpf2EvLx51NU9iNMpc100yMuDHj3MremhJCHECS/sWwrPrX+OvvF9Gy5Z+fXX8Nln5noItqYp02aDhQvh6qvhz3+Gp58mPf0mtPaTn/945wR/osrPNxNCpaXJOQUhupiwTgp7y/fy8a6PuWbUNQ1XBnvoIXPk48c/bmEFq9VcRWfsWJg3jwhXX5KSLiA//wn8/trjG/yJLC+vMSkUFIDP19kRCSHaKayTwsL1C9Forh51NQA5OfDqq3D99eYcaYssFvjJT8y8PuvW0bPnr/B6C9m374njF/iJLBAw5xF69DBJQWuTGIQQXULYJgWtNc+te44zMs6gb0JfAB591NRht9xymJUvvxycTnj2WRISphIffybZ2f8Pn68y9IGf6IqKTLet9HSTGEAOIQnRhYRtUvg853N2lu7k2tHXAlBZCU88ATNnmpPMbUpIgAsvNFfXqaujb9978HoLyMubF+qwT3zBBBBsKYD0QBKiCwlpUlBKnaOU2qaU2qGUmtPC89cqpQqVUuvqbz8NZTxNvfDtC0Q7opk5dCZgThWUl8Ptt7ezgOuug5ISeOcd4uKySEq6gL1778frLQ1d0F1BsLdR8JwCSFIQogsJWVJQSlmBx4BzgUzgcqVUZguLvqK1Hl1/eypU8Rxszb41ZPXMIsoRRSAADz8MWVnm1i7TpkHPniabAH373o3fX05OzoOhC7oraNpS6N7dXIVIkoIQXUYoWwoTgB1a611aaw/wMvDDEG6v3bTWbC3aytDkoQCsWQM7d8LPfnYEhVitpnvq++9Dfj7R0SPp1u0ycnP/iccTxtNF5+WZRJCWZsZ1pKTIOQUhupBQJoV0IKfJ/7n1jx1splLqW6XUIqVUrxDG0xhIRS6VnsqGpPDmm6aOv+CCIyzo2mtNb5vnnwcgI+MvBAJ1ZGff07EBdyX5+dCtm0kIYJKDtBSE6DI6+0TzO0CG1nok8CHQ4gV9lVI3KKVWK6VWFxYWHvNGtxRtAWBoSmNSmDwZkpKOsKCBA2HiRHjuOdCayMhB9OhxPXl5j1Ja+skxx9klBUczB0lSEKJLCWVSyAOa7vn3rH+sgda6WGtdV//vU8C4lgrSWs/XWo/XWo9PSUk55sC2FNYnheShfPedub78hRceZWE/+Qls3QoPmnMJ/fs/SGTkEDZv/hF1dfuPOdYuJzhwLahHDzl8JEQXEsqk8DUwUCnVVynlAC4D3m66gFIqrcm/M4AtIYynwZaiLSS4EugW1Y233jKP/fBoz3ZcfTXMng2//S38+99YrVEMG/Zf/P4Ktmz5EVr7OyzuLiE//9CWwoED4A+z90GILipkSUFr7QNuBpZgKvtXtdablFJ/VUrNqF/sVqXUJqXUeuBW4NpQxdPUlqItDE0ZilKKN980s1b06XOUhVmt5pzCD34Av/gFLFxIVNQwBg78F2VlS9mz528dGvsJra4OCgubtxTS0kxCKCrqvLiEEO0W0llStdaLgcUHPfanJvd/B/wulDG0ZEvhFmYMnsH+/bByJfzlL8dYoN0O//0vnH++Gb8QFUXazGspL/+U7Oy/Eht7CklJ53dI7Ce0/fWHy5q2FJqOau7e/fjHJIQ4Ip19ovm4K64uprC6kKHJQ3n7bTOtxVGfT2jK5YK33oIJE+Caa2DnTgYOfJTo6DFs3HgRBQX/7YCNnOCaDlwLkgFsQnQpYZcUthZtBUzPozffhH79YPjwDio8OtrMqGezwbXXYsXFqFEfExt7Kps3X0bhe3fBVVeZkdAno6YD14IkKQjRpYRdUgh2R+3pGsrHH8NFF5mxVh2mVy+YN89clOGhh7Db4xk5cgnp+7NImP13eOEFdP3V2046LbUUUlPNX0kKQnQJ4ZcUCrcQYYtg8xd98Hg66NDRwa66yhT8+9/Dpk1YN25nwC+2EIiLpPhU0I/8g7o96w9d78MPTU+drio/HxyO5gM+nE7zv3RLFaJLCL+kULSFwcmDefcdCykpcNppIdiIUvD44+aiDJddBtOmoaKisK9YT90Dd4LXT/GvT+XAgZfQWpt1XnkFpk+HqVO77uGl4MC1g5teMoBNiC4jLJPCkKQhfPSRqYOt1hBtqHt3kxg2bjS9kz75BNVvAD0m3Yv/mtmkvu1h99IfsWnTLLzrPjeD4IYPh127TCujtgteyS14Gc6DSVIQossIq6RQ7a0muyybJIZy4ACceWaINzhzpumq+vnnZkqMeva/PIiyOhj++jjK9r6Fb8YUAlEOWLIEFiyAFStMD6ZAoGPiCLZGOtquXVBd3fj/wVNcBMmoZiG6jLBKCtuKtqHR1GSbOY9CnhQALrkE+vZt/ljPnqibbiL6tW/I+vsoXPl+1t9Vyo7qB/HPmgH33296Md1yC7z7rrm/YAGsXn3k29+923SxmtfBFwDKzYXMTJgyBcrKzGNttRT27w9dcupsX3wBxcWdHYUQHSKskkKw51H2mqH07duOK6yF0pw5EBmJ7bM16PvuJercm8jNfYgvvkhj2wXbqLv+YvjXv8zUrbNnmxlZs7LgP/9p/zbcbpgxA/bsgblzoaKi4+J/5BFz2c31681xuJwcc/m6lloKaWlm2ZOx4ty0CU4/3bzPMpVHaHRUi1m0S3glhcItWJSF1R8MPD6thLakpJhzDn/4A5bbf8ugQY8yZsxnJCdfxIGCl1l5+etseK4XJe//P/SGDWbSvUmT4MorYf78w5cfCJhlt2yBe++F0lKTZDpC8NqlF18MixbBunVwxhnmuZZaCsFEcTKeV/jb38yJ9S++MBf5DierVpnDo23RGt5+2wzqHDXKtDCPRGGhaen+4hfta2kWFoLHc2TbEM1prbvUbdy4cfpozXxlpu79wEANWr/44lEXE3Jer1vv2/ec/uqrkXrpUvTatZN1RcU3WldXa33++VqD1g880HYhv/udWW7ePPP/eedpnZystdt97AE+/LApe+VK8/8bb2hts5nHli49dPnPPjPPvf/+sW/7RLJxo9ZKaT1njtbnnqt1RITWO3Z0Tiz5+Vp7PMdve2+/rbXDYT7XG288dNt+v9avv6716NFmmb59tY6N1bp3b623bWv/di6/3KwPWv/xj20vu3Sp+QyGDNH6iy+O+CUdtc8+0/qxx7QuKurYcrds0frTTzusOGC1bkcd2+mV/JHejiUpZD6WqTP/NkOD+Q2d6AIBn87Le1yvWJGkly616M2br9Ql+9/XgdmXmo/urLO0fuIJrQsKzAo1NeZL9NvfmudvuEHrQMA8t3Jl+5LJ4fh85gf+ve81f/z117UeObLlH8bOnWbbzz57bNvuTMH3sanZs7WOjta6sFDrnBxT6Z1xhqkQg3bv1vq770Ib27PPam23m8+kuPjQ51ev1vqjj1ped9cuU6nv2XPoc36/1q++qvUnnzR//S+/bHYCxo/X+le/Mp/tmWeabXs8Wi9cqPWwYebxAQO0fu458/iaNVqnpJjb2rWHf11vv23K+MtftP7xj839xx9vednPP9c6KkrrwYNN4lFK61/+UuvKysNvpzUHDpj39uKLtU5M1PpHP9K6oqL5Mi+/bN570NrpNEns4PcryOvV+g9/MOUEf7MtqakxywXLveQSrXNzj/511JOkcBCv36vtf7XrvtffqTMzj6qITuPxlOjt23+lly+P0UuXoj9f3kMX3T5J+/r1NB+hxaL18OGNe26g9YwZWtfVNS/o7LO17tZN66qq5o8HAqZSe/1182X8+c+1vuoqrWfONF/yzZsbl/3vf035r73W/hdQXW3WmTnz8C2Vt97SOj1d6wkTtF60yCShpnHu3q319u3t2+7atVq/807742xJebnWt96qdVKSqeyCNm0yFc/vftf42JNPmtf58MOmMj37bPN/RITWH3xwaNnbtpkEvmvX0cXm92t9551mG6ecYj7/4cMbKxCvV+s//9l8P6zWQxNDdbXWo0aZ9VNStF6xovG54uLGVmmwcr/vPtPyVErrSZPMe6O1qfQdDq379dO6Tx+z/PDhWr/wgomhqa1bte7VyyTQJ57QurS05ddWWqp1jx5ajxhhvscej2ntWiymZdrU11+b8gYONHt7FRVa/+IXuqGF8tJLzRP14RQWmoSvlCkjPd18dy0Wk3Q2bDDLBd+LyZNNa+Hmm7WOjzfrnHpqY0taa6337TPLgUmoaWkmeRzsk0/M6wCtr75a67/+VWuXy+x8/POfh76fR0CSwkG2Fm7VzEU7Jjynb775qIrodD5ftT5w4BX97bcX6GXLbHrpJ+jVz0bo/dcP0DWTBmvfr35hKtXWmrErVpiP/KGHzN7IG29ofcUVWqemNv74rVZTQfTpo3VmptZxcaZSe+wxUylnZWndv3/zyro9fvMbU37PnqayP3hPqq5O69tuM8uMHGm2AeYH8re/aX3llaYyCcY5fLjZg2yasJpaudLsOYLWP/2peb1N+f1a5+W1Hm8gYPYC09LMD3/wYFPWL39pKqjLLjPlFxY2X2fatMYYe/UylfLIkWYvsmmCev11rWNizHIul9b33HNoEm9LebnWF15o1v/Zz0xMH39sKo8+fUwS+t73zPNXXmk+y6Sk5gnouuvMa3v0Ua0HDTJ7pk89ZVoWGRnm/3nztH7+eZMEgq9r+vRDdyw+/9y8VxMnav3uuy3vKQdlZzceVnI6zZ7wa69pXVLSuMxPf2oq4a+/bnysstLsLNhsZv3LLjM7MQkJJt69e5tv59NPzfcEzPLvvWfiys429//xD63ffLN5a+K998zvwW43hwXXrm18LcuWad29u/k9XFrfWr/ooubfrepqs3OQlmaev+IK89pSU816zz+v9bp15vuklIn/lVdMqz74ne/fX+sPP2wsc+dOrc85xzx3001tfi3aIknhIG9ueVMzF036l/r114+qiBOKx1OsCwoW6W3bbtSrVg3SS5eily1z6s2br9Ll5at0oLUf5dSppjKKjjYff2Ki+eI+/LA5Dltd3Xz5/Hytv/99s+xpp5m/jzxydEF/8UXjnunUqVr/+tdm7/Ppp82eLmh9yy1a19aapPPqq1qPG2ce79ZN61mzTAX28MOmkgruyV1wQfPm9TffmD22/v21vv32xkphxw5Tid93n9mDDK7b9Bh3IGAq1zPPNM+PG6f1V1+ZSveXv9QNe4HBcwkHy8kxe4yLFzcmzuJic6jFbjevqene/RdfmL1Q0HroUFNJHdya8vtNDPffb1puQ4eaCtNiMXuPTT/r1atNUgez9/yf/5jHt28378moUaYSfPpp3ew4fUlJY8vGZjMJbdWq5nFs3qz1/Pnm82lJW4mgpWW//NK0woLxgjns9KMfmfu//e2h6xUWan3HHeYcTt++5nPIyGi9teXzmYo4+HlHRDRuK3hzOk15we2OGGEq7pbk52s9ZYpuODzb2s6R2631XXeZsoM7N99+2/h8ZaVJysEYYmNN6/6RRw79DQbfr1dfNS2toyRJ4SCbCzbrM+b+ReOsaPGwa1dXWblJb9t2k16+PFovXYr+8ssheuvWn+n9+1/QNTXZjUli5UrzBf3pT7VesqR9Jyf9flMRO51mr+xYTlZ7vaasfv3MHnLwRxEX1/IhqUBA6/37W65w8vLMHnZEhPlRzZ9vDuskJ5tKLXic/N13TdzR0Y0/0smTTeUcE2Mq69tvN4eHxowxz3fvbn6gB//oFywwZRzcSjicsrLGPffg3n3TyvW99xorLpvNLHvnnWZvOCmpcb1evUwi++MfWz+Zum2bSWC7dzd//H//M5XoWWeZ937atOavz+s127z00iN7bcfK6zV74XffbfaIY2NNy6alyvFg1dXtO6RSV2cOV916q9b//rdpNR84YHYAbrvN7EAoZRJOa0mvabyrV7cvCe7aZVokZWUtP79ihUmOx3BYqL3amxSUWbbrGD9+vF59NIO4MNMKud2wZk3HxnQi8fkqOHDgBYqL36G8/Av8fjM2wWZLJCoqk8jIoURFDSM6eizR0WOw2aLbX/iOHWb6jY6aa1xrqKoy3QgTEiA+/ujK2bEDrr8eli0zU4okJcHy5c1GkbNnj7lkakqK6d44bJh5/MAB+MMf4OmnTTyDB8NvfmO687pcLW9v82bTLXfChCOLs7ISfvUrmDzZTJp4sNpaM5r9k09g6VIzWDElxYwDmT4dpk079gsV3XefGSOTng7ffGPKP9EEx3uEbA6aFmhtrhzY2md+ElBKrdFajz/scuGSFKqrTb1z663wwAMhCOwEpLWfysoNlJd/RlXVBqqrt1BVtRmfLziITBEZOZjY2IkkJ88gIeFsrNaITo35qAUC8NRTsHChGf9xpIlr40aTIM44AywnyPCd2lozy2xHzu2utRmvMnkyjBjRceWKE54khYN89BGcfTYsXgznnhuCwLqQurr9VFauxe1ejdv9NWVly/H7K7BYIkhImE58/CSiokYQFTUChyMV1aEXnBBCdIb2JoWQXqP5RBIXB5dfbgYFhzunMxWn8zySks4DIBDwUFb2KUVFb1Fc/C7FxW81LGuzxWO3J2OzxWOzxeN09iImZgKxsROIihqBxWLvrJchhAiBsGkpiPbzeIqoqtpQf8hpGz5fCT5fGV5vKbW1O/F6iwCwWCKIiBiAy9UHlysDhyMdi8UBWFDKit2eQlzcaTidvaW1IUQnk5aCOGoORzIOxxkkJJxxyHNaa2pr91BR8SVu99fU1OygtnZPwyGolstLIzb2e8TEjCM6eiRRUaNwOtMlUQhxApKkII6IUoqIiL5ERPSle/fLmj3n91ehtR+t/UCA2tpsKipWUl6+koqKlRQVvdawrNUag8USicXiQCk7Vms0DkcaTmcaDkcPXK7euFwZuFwZOJ29sVhczZKI319DXV0eHk8+NlsiEREDsFpP3p4jQhwvkhREh7Fao5r9b7cnERMzlvT0mwDw+cqprPyWysr11NR8RyBQi9ZeAgEvfn8FHs8+qqo24vHsBw6ehtqK1RqF1RpJIODB5zv4kqUKp7M3kZFDiIkZT2zsqcTGnordnoLfX4nXW4zPV4LT2ROHo1uzNbUOUF29Da09REWNQKkTpPeREJ1AkoI4bmy2OOLjJxEf3/bZfq391NXto7Z2D7W1u6mry8XvryQQqMbvr0IpG05nz/oKPg2vt5iamu+orv6O6upN7N17L8GkopQdrb3Nyjcny8fjcvWjqupbKiq+wu8vr48xkfj4qSQknEVk5FAcjlQcju7YbAkhO9yltcbvr8LnK8ZqjcZuTwrJdoRoD0kK4oSjlBWXqycuV0/g9CNe3++vxu1ei9v9JR5PIXZ7UkMPqtra3fVdcVdTXPwOkZHD6NbtMmJjs1DKSlnZUkpLP6ao6PWDSrXWH+qyoZQNiyUCmy0Buz0Bmy0BqzUaiyUCiyUCqzUSqzUKiyUKqzUamy0GqzUOmy0eqzWa2tpduN1rcLvXUFW1Ea+3EK2D1wBQxMScQlLSD0hKOp/IyKGHHTvi8RRQXv4FTmca0dHjsFia/6y93mK83hIcju5YrTFHlNz8/ioslogTovUUCPgoKHiRkpIlpKZeS2Li9M4O6aQkvY9E2NI60GJlFzyZXlu7B8QRCC0AAAqYSURBVI9nPx7PgfqK24vWPrT21+/Zl+LzleL1lhAIVOH31xAIVBMI1BAI1La5baVsREYOIzp6FA5Han3iSqKuLp/i4vdwu78CzG/TYnFhsyVityfhcPSobyWl4/OVU1b2CVVVGxrKtVpjiY8/g5iYMVRVbcHt/pra2l0Nz1ssETgcqURFjSA29jTi4r5HdPRYlLKitYdAoI7q6u8oLV1CSckHuN1f43Ckkpx8Md26zSIu7nSUaj7SWGtNIFBHIFCLUhaUcmCxONA6QF1dLrW1u6mt3Y3PV4HF4mq4RUYOISpq+CFJ7GCBgJcDB54nO/seamt3YbFEEAjUkJAwnf79HyA6euThPur6cnx4PPl4PAV4vYV4vUX1o/vHhLgV6D7iZBwKJ8TgNaXUOcDDgBV4Smt970HPO4GFwDigGJittd7TVpmSFERXEEwc5laBz1eOz1eO31+B09mLqKiRbZ4Y93gKKC39kNraHHy+ErzeErzeQjyefOrqcvF4DmCxOImLO534+DOJj59MXV0epaUfU1r6EbW1u3A6exMTcwqxsRNwOFLxeArqk1webvdaamq+a+MVWIiNPZX4+DOort5KScliAoFarNZYLBZnfXL0EQh40LruqN8niyWK2NhTiYkZCygCgVoCgVp8vgq83oL6mPPw+cqIjh5HRsafSEiYTn7+v8jOvhufr4yEhOn1HRLScDjSUMpWH5cHn6+ifiT/RqqrtzZpkTVyOvuQknIxiYnnoLWvPtGX4veXN/kMq9Dah0nUgfpDmL2JiBhARER/7Pbk+thr8PurqKz8loqKzykv/6K+M0Q8kZFDiYwcUt9a3FN/20tERF8SEqaTmDid2NiJzb4XZlaCbykvX0F5+QqSkn5Aauo1R/Ved3pSUGZ34jvgbCAX+Bq4XGu9uckyvwBGaq1/rpS6DLhIaz27rXIlKQhh9p5B///27j82r6qO4/j7U5726Q9mu9/B/aIIonMR0AXBOUM2ExEX5Q9QFIwxGv6BCEaizPiTxBATI/oHUQhoJhJF+REXg04cZBH5DUNxm8aJuA03KOm6bg1b2/XrH+f02bOuax9LuufO5/P6p733Ob33PKfn3u+959x7Tn4v5FiHDw8c0/E/1uDga/T3P8HAwAtHXeG3tJxGV9cqmpuPjEU1PHyA3t7f0tf3CEClGU1qrrr6LwOjdw2DQFAuL6KtrZvW1m5KpZmVO4qRkdET52Ps2/dYzkOpsq30NNp8mpvn0dIyj9mz1zBr1iVHXW0PDe1lx46b6e3dwODgboaGesb9nuXyYjo6ltHRsYy2trNoaZlHc/M8SqUu+vsfp6fnPvbufWjcgJHylB5wkJrznaUYGRlkcPA/jN7Njb/fJXR2rqCjYxmHDu3MwWkbIyMD+cm6bsrlRQwMbKG//0856JxCqTQjN0W2MzTUU3nUu1xewqJFN7Bw4bUT/l+PpwhB4ULgmxHxwby8FiAibq5KsyGneVxSCdgDzI0JMuWgYGbjSSfqV0hX8inANTW11zSe1/BwP/v3P01TU0fuJ5pFqdR53KCb9neIgwdf4vXX/8nw8N58Im+lqamN9va3Ui6PM1/5cfe/n76+TfT3P171UMXrlEoz6OxcSWfnSlpbF9W8vfEU4eW1BcDOquVdwHuOlyYihiXtA2YDr1UnknQ1cDXA4sWLpyu/ZnYSa2pqmfKJs1R6EzNnrv4f91emvf1s2tvPntI+j97/DObMWcOcOWve8LbeqPo/UlCDiLg9IpZHxPK5RRzq18zs/8R0BoWXgeqwvTCvGzdNbj7qJHU4m5lZHUxnUHgaOEtSt6QW4Apg/Zg064HRrvTLgIcn6k8wM7PpNW19CrmP4FpgA+mR1B9HxBZJN5GmhVsP3AncJWk70EsKHGZmVifT+kZzRDwIPDhm3derfj8IXD6deTAzs9qdFB3NZmZ2YjgomJlZhYOCmZlVnHQD4knqAf49xT+fw5gX42xcLqfJuYwm5zKqzYkqpyURMemLXiddUHgjJD1Ty2vejc7lNDmX0eRcRrUpWjm5+cjMzCocFMzMrKLRgsLt9c7AScLlNDmX0eRcRrUpVDk1VJ+CmZlNrNHuFMzMbAINExQkXSzp75K2S7qx3vkpAkmLJD0iaaukLZKuy+tnSXpI0j/yz5n1zmu9STpF0mZJv8nL3ZKezPXpnjzoY0OT1CXpXkl/k7RN0oWuS0eT9IV8rP1V0s8ltRatLjVEUMhTg94KfAhYCnxC0tL65qoQhoEvRsRS4ALgmlwuNwIbI+IsYGNebnTXAduqlr8D3BIRZwJ7gc/WJVfF8gPgdxHxNuAcUnm5LmWSFgCfB5ZHxDLSQKFXULC61BBBATgf2B4RL0aaiPUXwEfrnKe6i4jdEfFc/n0/6SBeQCqbdTnZOuDS+uSwGCQtBD4M3JGXBawC7s1JXEZSJ/B+0sjHRMRgRPThujRWCWjL88e0A7spWF1qlKAw3tSgtU+g2gAknQ6cBzwJzI+I3fmjPcD8OmWrKL4PfAkYycuzgb5IM62D6xNAN9AD/CQ3s90hqQPXpYqIeBn4LrCDFAz2Ac9SsLrUKEHBJiDpVOA+4PqI6K/+LE961LCPqElaA7waEc/WOy8FVwLeBfwwIs4DBhjTVOS6pJmkO6du4M1AB3BxXTM1jkYJCrVMDdqQJDWTAsLdEXF/Xv2KpNPy56cBr9YrfwWwAviIpJdIzY6rSG3nXbkJAFyfIF3h7oqIJ/PyvaQg4bp0xAeAf0VET0QMAfeT6leh6lKjBIVapgZtOLlt/E5gW0R8r+qj6mlSPw38+kTnrSgiYm1ELIyI00n15uGIuBJ4hDSFLDR4GQFExB5gp6Sz86rVwFZcl6rtAC6Q1J6PvdEyKlRdapiX1yRdQmobHp0a9Nt1zlLdSXof8EfgBY60l3+F1K/wS2AxaUTaj0VEb10yWSCSLgJuiIg1ks4g3TnMAjYDV0XEoXrmr94knUvqjG8BXgQ+Q7rwdF3KJH0L+Djpyb/NwOdIfQiFqUsNExTMzGxyjdJ8ZGZmNXBQMDOzCgcFMzOrcFAwM7MKBwUzM6twUDA7gSRdNDrSqlkROSiYmVmFg4LZOCRdJekpSc9Lui3Pp3BA0i15PPyNkubmtOdKekLSXyQ9MDpngKQzJf1B0p8lPSfpLXnzp1bNO3B3frvVrBAcFMzGkPR20lunKyLiXOAwcCVpALNnIuIdwCbgG/lPfgp8OSLeSXo7fHT93cCtEXEO8F7SyJiQRqO9njS3xxmk8W/MCqE0eRKzhrMaeDfwdL6IbyMN5DYC3JPT/Ay4P88j0BURm/L6dcCvJM0AFkTEAwARcRAgb++piNiVl58HTgcenf6vZTY5BwWzYwlYFxFrj1opfW1MuqmOEVM9rs1hfBxagbj5yOxYG4HLJM2DypzVS0jHy+holp8EHo2IfcBeSSvz+k8Bm/JMdrskXZq3UZbUfkK/hdkU+ArFbIyI2Crpq8DvJTUBQ8A1pIljzs+fvUrqd4A03PGP8kl/dHRQSAHiNkk35W1cfgK/htmUeJRUsxpJOhARp9Y7H2bTyc1HZmZW4TsFMzOr8J2CmZlVOCiYmVmFg4KZmVU4KJiZWYWDgpmZVTgomJlZxX8BCAHksdWKyqYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 0.2554 - acc: 0.9288\n",
      "Loss: 0.2554334831262551 Accuracy: 0.9287643\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7973 - acc: 0.1733\n",
      "Epoch 00001: val_loss improved from inf to 2.20146, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/001-2.2015.hdf5\n",
      "36805/36805 [==============================] - 220s 6ms/sample - loss: 2.7972 - acc: 0.1733 - val_loss: 2.2015 - val_acc: 0.2905\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9529 - acc: 0.3781\n",
      "Epoch 00002: val_loss improved from 2.20146 to 1.34449, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/002-1.3445.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.9529 - acc: 0.3782 - val_loss: 1.3445 - val_acc: 0.5786\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3167 - acc: 0.5774\n",
      "Epoch 00003: val_loss improved from 1.34449 to 0.87959, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/003-0.8796.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.3167 - acc: 0.5774 - val_loss: 0.8796 - val_acc: 0.7468\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.7077\n",
      "Epoch 00004: val_loss improved from 0.87959 to 0.63311, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/004-0.6331.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.9251 - acc: 0.7077 - val_loss: 0.6331 - val_acc: 0.8202\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7831\n",
      "Epoch 00005: val_loss improved from 0.63311 to 0.48442, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/005-0.4844.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.6949 - acc: 0.7831 - val_loss: 0.4844 - val_acc: 0.8642\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8294\n",
      "Epoch 00006: val_loss improved from 0.48442 to 0.38800, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/006-0.3880.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.5548 - acc: 0.8294 - val_loss: 0.3880 - val_acc: 0.8877\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4556 - acc: 0.8596\n",
      "Epoch 00007: val_loss improved from 0.38800 to 0.36830, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/007-0.3683.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4556 - acc: 0.8596 - val_loss: 0.3683 - val_acc: 0.8961\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8792\n",
      "Epoch 00008: val_loss improved from 0.36830 to 0.32798, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/008-0.3280.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3850 - acc: 0.8792 - val_loss: 0.3280 - val_acc: 0.9038\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8927\n",
      "Epoch 00009: val_loss improved from 0.32798 to 0.28919, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/009-0.2892.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3409 - acc: 0.8926 - val_loss: 0.2892 - val_acc: 0.9122\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9080\n",
      "Epoch 00010: val_loss improved from 0.28919 to 0.25531, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/010-0.2553.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2962 - acc: 0.9080 - val_loss: 0.2553 - val_acc: 0.9245\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9185\n",
      "Epoch 00011: val_loss did not improve from 0.25531\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2632 - acc: 0.9185 - val_loss: 0.2862 - val_acc: 0.9199\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9241\n",
      "Epoch 00012: val_loss improved from 0.25531 to 0.22047, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/012-0.2205.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2406 - acc: 0.9241 - val_loss: 0.2205 - val_acc: 0.9334\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9300\n",
      "Epoch 00013: val_loss did not improve from 0.22047\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2215 - acc: 0.9300 - val_loss: 0.2305 - val_acc: 0.9299\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9358\n",
      "Epoch 00014: val_loss did not improve from 0.22047\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2033 - acc: 0.9357 - val_loss: 0.2492 - val_acc: 0.9250\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9414\n",
      "Epoch 00015: val_loss improved from 0.22047 to 0.19786, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/015-0.1979.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1863 - acc: 0.9414 - val_loss: 0.1979 - val_acc: 0.9394\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9448\n",
      "Epoch 00016: val_loss did not improve from 0.19786\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1733 - acc: 0.9448 - val_loss: 0.2001 - val_acc: 0.9418\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9486\n",
      "Epoch 00017: val_loss did not improve from 0.19786\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1613 - acc: 0.9486 - val_loss: 0.2003 - val_acc: 0.9394\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9519\n",
      "Epoch 00018: val_loss did not improve from 0.19786\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1523 - acc: 0.9519 - val_loss: 0.2119 - val_acc: 0.9366\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9561\n",
      "Epoch 00019: val_loss improved from 0.19786 to 0.19410, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/019-0.1941.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1392 - acc: 0.9561 - val_loss: 0.1941 - val_acc: 0.9425\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9594\n",
      "Epoch 00020: val_loss did not improve from 0.19410\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1274 - acc: 0.9594 - val_loss: 0.2208 - val_acc: 0.9397\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9618\n",
      "Epoch 00021: val_loss improved from 0.19410 to 0.18472, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/021-0.1847.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1209 - acc: 0.9618 - val_loss: 0.1847 - val_acc: 0.9457\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9622\n",
      "Epoch 00022: val_loss did not improve from 0.18472\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1185 - acc: 0.9622 - val_loss: 0.1870 - val_acc: 0.9432\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9658\n",
      "Epoch 00023: val_loss did not improve from 0.18472\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1060 - acc: 0.9658 - val_loss: 0.2044 - val_acc: 0.9376\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9663\n",
      "Epoch 00024: val_loss improved from 0.18472 to 0.17705, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/024-0.1771.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1039 - acc: 0.9663 - val_loss: 0.1771 - val_acc: 0.9527\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9684\n",
      "Epoch 00025: val_loss did not improve from 0.17705\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0975 - acc: 0.9684 - val_loss: 0.1838 - val_acc: 0.9532\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9669\n",
      "Epoch 00026: val_loss improved from 0.17705 to 0.16812, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/026-0.1681.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1019 - acc: 0.9669 - val_loss: 0.1681 - val_acc: 0.9513\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9724\n",
      "Epoch 00027: val_loss did not improve from 0.16812\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0861 - acc: 0.9724 - val_loss: 0.2246 - val_acc: 0.9362\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9716\n",
      "Epoch 00028: val_loss improved from 0.16812 to 0.16644, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/028-0.1664.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0882 - acc: 0.9716 - val_loss: 0.1664 - val_acc: 0.9502\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9745\n",
      "Epoch 00029: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0788 - acc: 0.9745 - val_loss: 0.2026 - val_acc: 0.9441\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9758\n",
      "Epoch 00030: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0749 - acc: 0.9758 - val_loss: 0.1737 - val_acc: 0.9527\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9778\n",
      "Epoch 00031: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0711 - acc: 0.9778 - val_loss: 0.1995 - val_acc: 0.9495\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9776\n",
      "Epoch 00032: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0699 - acc: 0.9776 - val_loss: 0.1740 - val_acc: 0.9525\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9775\n",
      "Epoch 00033: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0716 - acc: 0.9774 - val_loss: 0.1845 - val_acc: 0.9467\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9770\n",
      "Epoch 00034: val_loss did not improve from 0.16644\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0725 - acc: 0.9770 - val_loss: 0.1913 - val_acc: 0.9499\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9797\n",
      "Epoch 00035: val_loss improved from 0.16644 to 0.16499, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/035-0.1650.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0641 - acc: 0.9797 - val_loss: 0.1650 - val_acc: 0.9509\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9816\n",
      "Epoch 00036: val_loss did not improve from 0.16499\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0572 - acc: 0.9816 - val_loss: 0.1836 - val_acc: 0.9504\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9820\n",
      "Epoch 00037: val_loss did not improve from 0.16499\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0559 - acc: 0.9820 - val_loss: 0.1962 - val_acc: 0.9518\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9809\n",
      "Epoch 00038: val_loss improved from 0.16499 to 0.15643, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv_checkpoint/038-0.1564.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0567 - acc: 0.9808 - val_loss: 0.1564 - val_acc: 0.9583\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9804\n",
      "Epoch 00039: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0591 - acc: 0.9804 - val_loss: 0.1703 - val_acc: 0.9555\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9818\n",
      "Epoch 00040: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0561 - acc: 0.9817 - val_loss: 0.1865 - val_acc: 0.9527\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9842\n",
      "Epoch 00041: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0509 - acc: 0.9842 - val_loss: 0.1732 - val_acc: 0.9536\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9862\n",
      "Epoch 00042: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0435 - acc: 0.9862 - val_loss: 0.1955 - val_acc: 0.9474\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9818\n",
      "Epoch 00043: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0562 - acc: 0.9818 - val_loss: 0.1941 - val_acc: 0.9502\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0428 - acc: 0.9864 - val_loss: 0.2100 - val_acc: 0.9446\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9825\n",
      "Epoch 00045: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0541 - acc: 0.9825 - val_loss: 0.2100 - val_acc: 0.9483\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9855\n",
      "Epoch 00046: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0462 - acc: 0.9855 - val_loss: 0.1636 - val_acc: 0.9564\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9842\n",
      "Epoch 00047: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0478 - acc: 0.9842 - val_loss: 0.1831 - val_acc: 0.9536\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9849\n",
      "Epoch 00048: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0479 - acc: 0.9849 - val_loss: 0.1905 - val_acc: 0.9546\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9884\n",
      "Epoch 00049: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0368 - acc: 0.9884 - val_loss: 0.1907 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9884\n",
      "Epoch 00050: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0353 - acc: 0.9884 - val_loss: 0.1822 - val_acc: 0.9557\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9882\n",
      "Epoch 00051: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0359 - acc: 0.9882 - val_loss: 0.2028 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9882\n",
      "Epoch 00052: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0366 - acc: 0.9882 - val_loss: 0.2142 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9892\n",
      "Epoch 00053: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0342 - acc: 0.9892 - val_loss: 0.2998 - val_acc: 0.9392\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9880\n",
      "Epoch 00054: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0366 - acc: 0.9880 - val_loss: 0.1856 - val_acc: 0.9541\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9903\n",
      "Epoch 00055: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0311 - acc: 0.9903 - val_loss: 0.1693 - val_acc: 0.9590\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9899\n",
      "Epoch 00056: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0323 - acc: 0.9899 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9887\n",
      "Epoch 00057: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0335 - acc: 0.9887 - val_loss: 0.2963 - val_acc: 0.9450\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9860\n",
      "Epoch 00058: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0449 - acc: 0.9860 - val_loss: 0.1904 - val_acc: 0.9541\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9919\n",
      "Epoch 00059: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0258 - acc: 0.9919 - val_loss: 0.1843 - val_acc: 0.9553\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9903\n",
      "Epoch 00060: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0320 - acc: 0.9903 - val_loss: 0.1809 - val_acc: 0.9567\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9908\n",
      "Epoch 00061: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0282 - acc: 0.9908 - val_loss: 0.1999 - val_acc: 0.9548\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9912\n",
      "Epoch 00062: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0272 - acc: 0.9912 - val_loss: 0.1738 - val_acc: 0.9595\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9918\n",
      "Epoch 00063: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0268 - acc: 0.9918 - val_loss: 0.2852 - val_acc: 0.9343\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9923\n",
      "Epoch 00064: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0254 - acc: 0.9923 - val_loss: 0.2216 - val_acc: 0.9478\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9905\n",
      "Epoch 00065: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0298 - acc: 0.9905 - val_loss: 0.1674 - val_acc: 0.9604\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9916\n",
      "Epoch 00066: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0274 - acc: 0.9916 - val_loss: 0.2656 - val_acc: 0.9371\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9881\n",
      "Epoch 00067: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0355 - acc: 0.9881 - val_loss: 0.2253 - val_acc: 0.9497\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9903\n",
      "Epoch 00068: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0308 - acc: 0.9903 - val_loss: 0.1906 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00069: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0201 - acc: 0.9941 - val_loss: 0.1604 - val_acc: 0.9613\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9928\n",
      "Epoch 00070: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0229 - acc: 0.9928 - val_loss: 0.1864 - val_acc: 0.9590\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9929\n",
      "Epoch 00071: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 0.2153 - val_acc: 0.9550\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9909\n",
      "Epoch 00072: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0273 - acc: 0.9909 - val_loss: 0.2139 - val_acc: 0.9490\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9929\n",
      "Epoch 00073: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0243 - acc: 0.9929 - val_loss: 0.2144 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 00074: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0225 - acc: 0.9929 - val_loss: 0.2183 - val_acc: 0.9532\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9885\n",
      "Epoch 00075: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0359 - acc: 0.9885 - val_loss: 0.2079 - val_acc: 0.9546\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9936\n",
      "Epoch 00076: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0202 - acc: 0.9936 - val_loss: 0.2132 - val_acc: 0.9536\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9891\n",
      "Epoch 00077: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0348 - acc: 0.9891 - val_loss: 0.1660 - val_acc: 0.9611\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9939\n",
      "Epoch 00078: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0181 - acc: 0.9939 - val_loss: 0.1687 - val_acc: 0.9616\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9945\n",
      "Epoch 00079: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0171 - acc: 0.9945 - val_loss: 0.2057 - val_acc: 0.9562\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9941\n",
      "Epoch 00080: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0200 - acc: 0.9940 - val_loss: 0.1905 - val_acc: 0.9578\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9883\n",
      "Epoch 00081: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0352 - acc: 0.9883 - val_loss: 0.1800 - val_acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9901\n",
      "Epoch 00082: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0300 - acc: 0.9901 - val_loss: 0.2567 - val_acc: 0.9495\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9945\n",
      "Epoch 00083: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0173 - acc: 0.9945 - val_loss: 0.1802 - val_acc: 0.9620\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9956\n",
      "Epoch 00084: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0156 - acc: 0.9956 - val_loss: 0.2158 - val_acc: 0.9499\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9899\n",
      "Epoch 00085: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0306 - acc: 0.9899 - val_loss: 0.1737 - val_acc: 0.9613\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9953\n",
      "Epoch 00086: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0151 - acc: 0.9953 - val_loss: 0.2056 - val_acc: 0.9511\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9918\n",
      "Epoch 00087: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0270 - acc: 0.9918 - val_loss: 0.2319 - val_acc: 0.9509\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 00088: val_loss did not improve from 0.15643\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0171 - acc: 0.9947 - val_loss: 0.1770 - val_acc: 0.9611\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.1748 - acc: 0.9497\n",
      "Loss: 0.17475470805295035 Accuracy: 0.9497404\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8864 - acc: 0.1546\n",
      "Epoch 00001: val_loss improved from inf to 2.22280, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/001-2.2228.hdf5\n",
      "36805/36805 [==============================] - 250s 7ms/sample - loss: 2.8864 - acc: 0.1546 - val_loss: 2.2228 - val_acc: 0.2844\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8706 - acc: 0.4039\n",
      "Epoch 00002: val_loss improved from 2.22280 to 1.28113, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/002-1.2811.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 1.8709 - acc: 0.4039 - val_loss: 1.2811 - val_acc: 0.5975\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2888 - acc: 0.5834\n",
      "Epoch 00003: val_loss improved from 1.28113 to 0.83583, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/003-0.8358.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 1.2888 - acc: 0.5834 - val_loss: 0.8358 - val_acc: 0.7372\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9354 - acc: 0.7019\n",
      "Epoch 00004: val_loss improved from 0.83583 to 0.60576, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/004-0.6058.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.9354 - acc: 0.7019 - val_loss: 0.6058 - val_acc: 0.8127\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7158 - acc: 0.7726\n",
      "Epoch 00005: val_loss improved from 0.60576 to 0.49765, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/005-0.4976.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.7159 - acc: 0.7726 - val_loss: 0.4976 - val_acc: 0.8446\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5727 - acc: 0.8192\n",
      "Epoch 00006: val_loss improved from 0.49765 to 0.42756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/006-0.4276.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.5732 - acc: 0.8192 - val_loss: 0.4276 - val_acc: 0.8654\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8491\n",
      "Epoch 00007: val_loss improved from 0.42756 to 0.33775, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/007-0.3378.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.4762 - acc: 0.8491 - val_loss: 0.3378 - val_acc: 0.9033\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8756\n",
      "Epoch 00008: val_loss improved from 0.33775 to 0.28372, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/008-0.2837.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.3936 - acc: 0.8756 - val_loss: 0.2837 - val_acc: 0.9131\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8934\n",
      "Epoch 00009: val_loss did not improve from 0.28372\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3366 - acc: 0.8934 - val_loss: 0.3233 - val_acc: 0.9015\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2938 - acc: 0.9081\n",
      "Epoch 00010: val_loss improved from 0.28372 to 0.23144, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/010-0.2314.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2938 - acc: 0.9081 - val_loss: 0.2314 - val_acc: 0.9327\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9179\n",
      "Epoch 00011: val_loss did not improve from 0.23144\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2598 - acc: 0.9178 - val_loss: 0.3276 - val_acc: 0.9015\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9230\n",
      "Epoch 00012: val_loss did not improve from 0.23144\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2432 - acc: 0.9229 - val_loss: 0.2579 - val_acc: 0.9283\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9293\n",
      "Epoch 00013: val_loss did not improve from 0.23144\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2216 - acc: 0.9293 - val_loss: 0.2348 - val_acc: 0.9304\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9399\n",
      "Epoch 00014: val_loss improved from 0.23144 to 0.18762, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/014-0.1876.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1915 - acc: 0.9399 - val_loss: 0.1876 - val_acc: 0.9425\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9443\n",
      "Epoch 00015: val_loss did not improve from 0.18762\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1746 - acc: 0.9442 - val_loss: 0.2081 - val_acc: 0.9313\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9447\n",
      "Epoch 00016: val_loss improved from 0.18762 to 0.18697, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/016-0.1870.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1759 - acc: 0.9447 - val_loss: 0.1870 - val_acc: 0.9432\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9508\n",
      "Epoch 00017: val_loss did not improve from 0.18697\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1561 - acc: 0.9508 - val_loss: 0.1910 - val_acc: 0.9443\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9546\n",
      "Epoch 00018: val_loss improved from 0.18697 to 0.16007, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/018-0.1601.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1410 - acc: 0.9546 - val_loss: 0.1601 - val_acc: 0.9515\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9588\n",
      "Epoch 00019: val_loss did not improve from 0.16007\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1314 - acc: 0.9588 - val_loss: 0.2455 - val_acc: 0.9290\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9606\n",
      "Epoch 00020: val_loss did not improve from 0.16007\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1253 - acc: 0.9606 - val_loss: 0.2050 - val_acc: 0.9378\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9611\n",
      "Epoch 00021: val_loss did not improve from 0.16007\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1218 - acc: 0.9611 - val_loss: 0.1659 - val_acc: 0.9483\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9637\n",
      "Epoch 00022: val_loss improved from 0.16007 to 0.15542, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/022-0.1554.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1127 - acc: 0.9637 - val_loss: 0.1554 - val_acc: 0.9504\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9670\n",
      "Epoch 00023: val_loss improved from 0.15542 to 0.14998, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/023-0.1500.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1034 - acc: 0.9670 - val_loss: 0.1500 - val_acc: 0.9557\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9690\n",
      "Epoch 00024: val_loss did not improve from 0.14998\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0967 - acc: 0.9690 - val_loss: 0.1592 - val_acc: 0.9511\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9712\n",
      "Epoch 00025: val_loss did not improve from 0.14998\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0896 - acc: 0.9712 - val_loss: 0.1626 - val_acc: 0.9515\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9715\n",
      "Epoch 00026: val_loss did not improve from 0.14998\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0884 - acc: 0.9715 - val_loss: 0.1664 - val_acc: 0.9502\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9716\n",
      "Epoch 00027: val_loss did not improve from 0.14998\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0898 - acc: 0.9716 - val_loss: 0.1710 - val_acc: 0.9478\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9752\n",
      "Epoch 00028: val_loss did not improve from 0.14998\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0780 - acc: 0.9752 - val_loss: 0.1747 - val_acc: 0.9499\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9767\n",
      "Epoch 00029: val_loss improved from 0.14998 to 0.14720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/029-0.1472.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0692 - acc: 0.9767 - val_loss: 0.1472 - val_acc: 0.9585\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9764\n",
      "Epoch 00030: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0699 - acc: 0.9764 - val_loss: 0.1627 - val_acc: 0.9548\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9790\n",
      "Epoch 00031: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0674 - acc: 0.9790 - val_loss: 0.1693 - val_acc: 0.9509\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9749\n",
      "Epoch 00032: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0767 - acc: 0.9749 - val_loss: 0.1707 - val_acc: 0.9534\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9781\n",
      "Epoch 00033: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0702 - acc: 0.9780 - val_loss: 0.1541 - val_acc: 0.9546\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9806\n",
      "Epoch 00034: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0620 - acc: 0.9806 - val_loss: 0.1974 - val_acc: 0.9483\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9798\n",
      "Epoch 00035: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0633 - acc: 0.9798 - val_loss: 0.1633 - val_acc: 0.9557\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9830\n",
      "Epoch 00036: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0533 - acc: 0.9830 - val_loss: 0.1649 - val_acc: 0.9513\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9790\n",
      "Epoch 00037: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0646 - acc: 0.9791 - val_loss: 0.1650 - val_acc: 0.9539\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9853\n",
      "Epoch 00038: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0450 - acc: 0.9853 - val_loss: 0.1476 - val_acc: 0.9590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9852\n",
      "Epoch 00039: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1839 - val_acc: 0.9527\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9853\n",
      "Epoch 00040: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0460 - acc: 0.9853 - val_loss: 0.1551 - val_acc: 0.9585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9823\n",
      "Epoch 00041: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0532 - acc: 0.9822 - val_loss: 0.1499 - val_acc: 0.9599\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9846\n",
      "Epoch 00042: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0486 - acc: 0.9846 - val_loss: 0.1583 - val_acc: 0.9571\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9889\n",
      "Epoch 00043: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0373 - acc: 0.9888 - val_loss: 0.1549 - val_acc: 0.9585\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9841\n",
      "Epoch 00044: val_loss did not improve from 0.14720\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0510 - acc: 0.9840 - val_loss: 0.1912 - val_acc: 0.9506\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9812\n",
      "Epoch 00045: val_loss improved from 0.14720 to 0.13133, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv_checkpoint/045-0.1313.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0600 - acc: 0.9811 - val_loss: 0.1313 - val_acc: 0.9627\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9890\n",
      "Epoch 00046: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0365 - acc: 0.9890 - val_loss: 0.1365 - val_acc: 0.9641\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0334 - acc: 0.9898 - val_loss: 0.1424 - val_acc: 0.9644\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9898\n",
      "Epoch 00048: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0327 - acc: 0.9898 - val_loss: 0.1598 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9882\n",
      "Epoch 00049: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0375 - acc: 0.9882 - val_loss: 0.1795 - val_acc: 0.9536\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9861\n",
      "Epoch 00050: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0425 - acc: 0.9861 - val_loss: 0.1685 - val_acc: 0.9548\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9906\n",
      "Epoch 00051: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0299 - acc: 0.9906 - val_loss: 0.1529 - val_acc: 0.9585\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9883\n",
      "Epoch 00052: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0363 - acc: 0.9883 - val_loss: 0.1710 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9893\n",
      "Epoch 00053: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0343 - acc: 0.9893 - val_loss: 0.1993 - val_acc: 0.9497\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9860\n",
      "Epoch 00054: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0437 - acc: 0.9860 - val_loss: 0.1455 - val_acc: 0.9641\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 00055: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0337 - acc: 0.9894 - val_loss: 0.1868 - val_acc: 0.9515\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9892\n",
      "Epoch 00056: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0335 - acc: 0.9892 - val_loss: 0.1634 - val_acc: 0.9609\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9910\n",
      "Epoch 00057: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0291 - acc: 0.9910 - val_loss: 0.1835 - val_acc: 0.9543\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9918\n",
      "Epoch 00058: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0269 - acc: 0.9918 - val_loss: 0.1638 - val_acc: 0.9609\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9915\n",
      "Epoch 00059: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0276 - acc: 0.9915 - val_loss: 0.1966 - val_acc: 0.9557\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9863\n",
      "Epoch 00060: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0437 - acc: 0.9863 - val_loss: 0.1711 - val_acc: 0.9553\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 00061: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0223 - acc: 0.9930 - val_loss: 0.1693 - val_acc: 0.9583\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9920\n",
      "Epoch 00062: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0269 - acc: 0.9920 - val_loss: 0.1471 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9902\n",
      "Epoch 00063: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0316 - acc: 0.9902 - val_loss: 0.1642 - val_acc: 0.9574\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9924\n",
      "Epoch 00064: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.1618 - val_acc: 0.9574\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9885\n",
      "Epoch 00065: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0354 - acc: 0.9885 - val_loss: 0.1723 - val_acc: 0.9583\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9897\n",
      "Epoch 00066: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0339 - acc: 0.9897 - val_loss: 0.1482 - val_acc: 0.9627\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 00067: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0301 - acc: 0.9906 - val_loss: 0.1377 - val_acc: 0.9660\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 00068: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0266 - acc: 0.9918 - val_loss: 0.1374 - val_acc: 0.9667\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9941\n",
      "Epoch 00069: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0203 - acc: 0.9941 - val_loss: 0.1904 - val_acc: 0.9611\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9894\n",
      "Epoch 00070: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0354 - acc: 0.9894 - val_loss: 0.1350 - val_acc: 0.9672\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9937\n",
      "Epoch 00071: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0201 - acc: 0.9937 - val_loss: 0.1680 - val_acc: 0.9606\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9947\n",
      "Epoch 00072: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0179 - acc: 0.9947 - val_loss: 0.1965 - val_acc: 0.9555\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9879\n",
      "Epoch 00073: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0381 - acc: 0.9878 - val_loss: 0.1629 - val_acc: 0.9609\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9906\n",
      "Epoch 00074: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0320 - acc: 0.9906 - val_loss: 0.1547 - val_acc: 0.9632\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9951\n",
      "Epoch 00075: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0164 - acc: 0.9951 - val_loss: 0.1815 - val_acc: 0.9576\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9912\n",
      "Epoch 00076: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0264 - acc: 0.9912 - val_loss: 0.1537 - val_acc: 0.9613\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9916\n",
      "Epoch 00077: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0260 - acc: 0.9916 - val_loss: 0.1718 - val_acc: 0.9625\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9929\n",
      "Epoch 00078: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.1665 - val_acc: 0.9660\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9957\n",
      "Epoch 00079: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0157 - acc: 0.9957 - val_loss: 0.1787 - val_acc: 0.9597\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9902\n",
      "Epoch 00080: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0295 - acc: 0.9902 - val_loss: 0.1476 - val_acc: 0.9655\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9953\n",
      "Epoch 00081: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0164 - acc: 0.9953 - val_loss: 0.1487 - val_acc: 0.9669\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 00082: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0166 - acc: 0.9949 - val_loss: 0.1769 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9937\n",
      "Epoch 00083: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0191 - acc: 0.9937 - val_loss: 0.1841 - val_acc: 0.9576\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9942\n",
      "Epoch 00084: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0179 - acc: 0.9942 - val_loss: 0.1867 - val_acc: 0.9555\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9930\n",
      "Epoch 00085: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.1717 - val_acc: 0.9632\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9927\n",
      "Epoch 00086: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0235 - acc: 0.9927 - val_loss: 0.1845 - val_acc: 0.9562\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9915\n",
      "Epoch 00087: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0280 - acc: 0.9914 - val_loss: 0.1458 - val_acc: 0.9665\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9923\n",
      "Epoch 00088: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.1485 - val_acc: 0.9658\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9936\n",
      "Epoch 00089: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0193 - acc: 0.9936 - val_loss: 0.1761 - val_acc: 0.9623\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9925\n",
      "Epoch 00090: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0237 - acc: 0.9925 - val_loss: 0.1645 - val_acc: 0.9620\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9926\n",
      "Epoch 00091: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0246 - acc: 0.9926 - val_loss: 0.1458 - val_acc: 0.9669\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9963\n",
      "Epoch 00092: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0113 - acc: 0.9963 - val_loss: 0.1898 - val_acc: 0.9574\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9959\n",
      "Epoch 00093: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0139 - acc: 0.9959 - val_loss: 0.1678 - val_acc: 0.9627\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9917\n",
      "Epoch 00094: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0244 - acc: 0.9916 - val_loss: 0.2004 - val_acc: 0.9571\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9926\n",
      "Epoch 00095: val_loss did not improve from 0.13133\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0233 - acc: 0.9926 - val_loss: 0.1636 - val_acc: 0.9644\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.1884 - acc: 0.9514\n",
      "Loss: 0.1884096939607039 Accuracy: 0.9514019\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6858 - acc: 0.2153\n",
      "Epoch 00001: val_loss improved from inf to 1.94813, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/001-1.9481.hdf5\n",
      "36805/36805 [==============================] - 288s 8ms/sample - loss: 2.6858 - acc: 0.2152 - val_loss: 1.9481 - val_acc: 0.3613\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5621 - acc: 0.5067\n",
      "Epoch 00002: val_loss improved from 1.94813 to 1.02734, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/002-1.0273.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 1.5621 - acc: 0.5067 - val_loss: 1.0273 - val_acc: 0.6725\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6687\n",
      "Epoch 00003: val_loss improved from 1.02734 to 0.78254, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/003-0.7825.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 1.0299 - acc: 0.6687 - val_loss: 0.7825 - val_acc: 0.7512\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7411 - acc: 0.7633\n",
      "Epoch 00004: val_loss improved from 0.78254 to 0.50781, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/004-0.5078.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.7412 - acc: 0.7633 - val_loss: 0.5078 - val_acc: 0.8407\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8183\n",
      "Epoch 00005: val_loss improved from 0.50781 to 0.41407, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/005-0.4141.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.5655 - acc: 0.8183 - val_loss: 0.4141 - val_acc: 0.8726\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8543\n",
      "Epoch 00006: val_loss improved from 0.41407 to 0.35717, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/006-0.3572.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.4553 - acc: 0.8543 - val_loss: 0.3572 - val_acc: 0.8891\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8809\n",
      "Epoch 00007: val_loss improved from 0.35717 to 0.28275, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/007-0.2828.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.3793 - acc: 0.8809 - val_loss: 0.2828 - val_acc: 0.9159\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8942\n",
      "Epoch 00008: val_loss did not improve from 0.28275\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.3267 - acc: 0.8941 - val_loss: 0.3117 - val_acc: 0.9052\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9065\n",
      "Epoch 00009: val_loss did not improve from 0.28275\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.2954 - acc: 0.9065 - val_loss: 0.3058 - val_acc: 0.9059\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9198\n",
      "Epoch 00010: val_loss improved from 0.28275 to 0.26487, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/010-0.2649.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.2501 - acc: 0.9198 - val_loss: 0.2649 - val_acc: 0.9168\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9274\n",
      "Epoch 00011: val_loss improved from 0.26487 to 0.23431, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/011-0.2343.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.2246 - acc: 0.9274 - val_loss: 0.2343 - val_acc: 0.9273\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9336\n",
      "Epoch 00012: val_loss did not improve from 0.23431\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.2068 - acc: 0.9336 - val_loss: 0.2493 - val_acc: 0.9220\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9404\n",
      "Epoch 00013: val_loss improved from 0.23431 to 0.22533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/013-0.2253.hdf5\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.1838 - acc: 0.9404 - val_loss: 0.2253 - val_acc: 0.9317\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9476\n",
      "Epoch 00014: val_loss improved from 0.22533 to 0.20770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/014-0.2077.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1639 - acc: 0.9475 - val_loss: 0.2077 - val_acc: 0.9380\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9473\n",
      "Epoch 00015: val_loss did not improve from 0.20770\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1627 - acc: 0.9473 - val_loss: 0.2177 - val_acc: 0.9357\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9535\n",
      "Epoch 00016: val_loss improved from 0.20770 to 0.18687, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/016-0.1869.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1428 - acc: 0.9534 - val_loss: 0.1869 - val_acc: 0.9467\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9548\n",
      "Epoch 00017: val_loss did not improve from 0.18687\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1403 - acc: 0.9548 - val_loss: 0.2396 - val_acc: 0.9313\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9615\n",
      "Epoch 00018: val_loss did not improve from 0.18687\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1191 - acc: 0.9614 - val_loss: 0.2126 - val_acc: 0.9362\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9598\n",
      "Epoch 00019: val_loss did not improve from 0.18687\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1210 - acc: 0.9597 - val_loss: 0.2074 - val_acc: 0.9439\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9643\n",
      "Epoch 00020: val_loss did not improve from 0.18687\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1103 - acc: 0.9643 - val_loss: 0.2233 - val_acc: 0.9324\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9701\n",
      "Epoch 00021: val_loss improved from 0.18687 to 0.17643, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/021-0.1764.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0908 - acc: 0.9701 - val_loss: 0.1764 - val_acc: 0.9509\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9723\n",
      "Epoch 00022: val_loss did not improve from 0.17643\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0846 - acc: 0.9723 - val_loss: 0.1804 - val_acc: 0.9467\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9730\n",
      "Epoch 00023: val_loss did not improve from 0.17643\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0818 - acc: 0.9730 - val_loss: 0.2107 - val_acc: 0.9408\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9744\n",
      "Epoch 00024: val_loss did not improve from 0.17643\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0803 - acc: 0.9744 - val_loss: 0.2824 - val_acc: 0.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9720\n",
      "Epoch 00025: val_loss did not improve from 0.17643\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0855 - acc: 0.9720 - val_loss: 0.1868 - val_acc: 0.9453\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9743\n",
      "Epoch 00026: val_loss did not improve from 0.17643\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0801 - acc: 0.9744 - val_loss: 0.2360 - val_acc: 0.9350\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9789\n",
      "Epoch 00027: val_loss improved from 0.17643 to 0.17278, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/027-0.1728.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0659 - acc: 0.9788 - val_loss: 0.1728 - val_acc: 0.9502\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9767\n",
      "Epoch 00028: val_loss improved from 0.17278 to 0.15312, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv_checkpoint/028-0.1531.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0718 - acc: 0.9767 - val_loss: 0.1531 - val_acc: 0.9550\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9802\n",
      "Epoch 00029: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0600 - acc: 0.9801 - val_loss: 0.1837 - val_acc: 0.9515\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9804\n",
      "Epoch 00030: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0587 - acc: 0.9804 - val_loss: 0.1844 - val_acc: 0.9520\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9797\n",
      "Epoch 00031: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0633 - acc: 0.9797 - val_loss: 0.1740 - val_acc: 0.9476\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9833\n",
      "Epoch 00032: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0521 - acc: 0.9833 - val_loss: 0.2239 - val_acc: 0.9455\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9845\n",
      "Epoch 00033: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0479 - acc: 0.9845 - val_loss: 0.1869 - val_acc: 0.9532\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9854\n",
      "Epoch 00034: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0464 - acc: 0.9854 - val_loss: 0.1909 - val_acc: 0.9518\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9852\n",
      "Epoch 00035: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0460 - acc: 0.9852 - val_loss: 0.1792 - val_acc: 0.9569\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9868\n",
      "Epoch 00036: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0419 - acc: 0.9868 - val_loss: 0.2115 - val_acc: 0.9481\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9836\n",
      "Epoch 00037: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0519 - acc: 0.9836 - val_loss: 0.1851 - val_acc: 0.9495\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9871\n",
      "Epoch 00038: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0380 - acc: 0.9871 - val_loss: 0.2023 - val_acc: 0.9502\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9882\n",
      "Epoch 00039: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0373 - acc: 0.9882 - val_loss: 0.1872 - val_acc: 0.9518\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9844\n",
      "Epoch 00040: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0483 - acc: 0.9844 - val_loss: 0.2039 - val_acc: 0.9485\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9896\n",
      "Epoch 00041: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0332 - acc: 0.9896 - val_loss: 0.1861 - val_acc: 0.9576\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9893\n",
      "Epoch 00042: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0346 - acc: 0.9893 - val_loss: 0.2078 - val_acc: 0.9455\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9883\n",
      "Epoch 00043: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0365 - acc: 0.9882 - val_loss: 0.2409 - val_acc: 0.9485\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9868\n",
      "Epoch 00044: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0425 - acc: 0.9867 - val_loss: 0.1993 - val_acc: 0.9532\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9875\n",
      "Epoch 00045: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0397 - acc: 0.9874 - val_loss: 0.1884 - val_acc: 0.9560\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00046: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0300 - acc: 0.9905 - val_loss: 0.2024 - val_acc: 0.9529\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9919\n",
      "Epoch 00047: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0268 - acc: 0.9919 - val_loss: 0.2101 - val_acc: 0.9527\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9907\n",
      "Epoch 00048: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0284 - acc: 0.9907 - val_loss: 0.2579 - val_acc: 0.9462\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9866\n",
      "Epoch 00049: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0439 - acc: 0.9866 - val_loss: 0.2398 - val_acc: 0.9415\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9935\n",
      "Epoch 00050: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0209 - acc: 0.9935 - val_loss: 0.2053 - val_acc: 0.9525\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 00051: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0240 - acc: 0.9925 - val_loss: 0.2119 - val_acc: 0.9571\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9917\n",
      "Epoch 00052: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0262 - acc: 0.9917 - val_loss: 0.2193 - val_acc: 0.9513\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9887\n",
      "Epoch 00053: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0352 - acc: 0.9887 - val_loss: 0.1949 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9915\n",
      "Epoch 00054: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.2186 - val_acc: 0.9504\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9922\n",
      "Epoch 00055: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0236 - acc: 0.9921 - val_loss: 0.2055 - val_acc: 0.9557\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9861\n",
      "Epoch 00056: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0447 - acc: 0.9861 - val_loss: 0.1776 - val_acc: 0.9590\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9891\n",
      "Epoch 00057: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0351 - acc: 0.9891 - val_loss: 0.1771 - val_acc: 0.9620\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9911\n",
      "Epoch 00058: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0294 - acc: 0.9911 - val_loss: 0.1733 - val_acc: 0.9618\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9945\n",
      "Epoch 00059: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0177 - acc: 0.9945 - val_loss: 0.2089 - val_acc: 0.9562\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9922\n",
      "Epoch 00060: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0241 - acc: 0.9922 - val_loss: 0.1994 - val_acc: 0.9576\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9947\n",
      "Epoch 00061: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0174 - acc: 0.9947 - val_loss: 0.1925 - val_acc: 0.9590\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9942\n",
      "Epoch 00062: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0183 - acc: 0.9942 - val_loss: 0.2249 - val_acc: 0.9550\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9939\n",
      "Epoch 00063: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0188 - acc: 0.9939 - val_loss: 0.2063 - val_acc: 0.9595\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9935\n",
      "Epoch 00064: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0207 - acc: 0.9935 - val_loss: 0.2106 - val_acc: 0.9546\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9877\n",
      "Epoch 00065: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0383 - acc: 0.9877 - val_loss: 0.2141 - val_acc: 0.9541\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 00066: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0240 - acc: 0.9928 - val_loss: 0.1877 - val_acc: 0.9604\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9947\n",
      "Epoch 00067: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0165 - acc: 0.9947 - val_loss: 0.2100 - val_acc: 0.9532\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9936- ETA: 0s - loss: 0.0210 - acc: 0.9\n",
      "Epoch 00068: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.1889 - val_acc: 0.9588\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9946\n",
      "Epoch 00069: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0165 - acc: 0.9946 - val_loss: 0.2143 - val_acc: 0.9569\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9939\n",
      "Epoch 00070: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0198 - acc: 0.9939 - val_loss: 0.2103 - val_acc: 0.9553\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9923\n",
      "Epoch 00071: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0265 - acc: 0.9923 - val_loss: 0.1919 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9957\n",
      "Epoch 00072: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.2304 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 00073: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.2388 - val_acc: 0.9469\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9920\n",
      "Epoch 00074: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0239 - acc: 0.9920 - val_loss: 0.1969 - val_acc: 0.9564\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9950\n",
      "Epoch 00075: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0157 - acc: 0.9950 - val_loss: 0.2061 - val_acc: 0.9595\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9923\n",
      "Epoch 00076: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.1888 - val_acc: 0.9616\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9955\n",
      "Epoch 00077: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0152 - acc: 0.9955 - val_loss: 0.2301 - val_acc: 0.9564\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9943\n",
      "Epoch 00078: val_loss did not improve from 0.15312\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 0.1880 - val_acc: 0.9606\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2178 - acc: 0.9377\n",
      "Loss: 0.2177782718862019 Accuracy: 0.9376947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,064,432\n",
      "Trainable params: 2,064,048\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.7901 - acc: 0.4741\n",
      "Loss: 1.7900645954710424 Accuracy: 0.4741433\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,046,896\n",
      "Trainable params: 1,046,384\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.3347 - acc: 0.5981\n",
      "Loss: 1.3346540165096057 Accuracy: 0.5981308\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,065,968\n",
      "Trainable params: 1,065,200\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 1.2683 - acc: 0.6567\n",
      "Loss: 1.2683235645541768 Accuracy: 0.6566978\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 579,184\n",
      "Trainable params: 578,160\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.8936 - acc: 0.7570\n",
      "Loss: 0.8935574294374492 Accuracy: 0.7570093\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 348,400\n",
      "Trainable params: 347,120\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.6763 - acc: 0.8073\n",
      "Loss: 0.6762765360398456 Accuracy: 0.807269\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 245,616\n",
      "Trainable params: 244,080\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.5044 - acc: 0.8658\n",
      "Loss: 0.5043950633715494 Accuracy: 0.8658359\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 319,600\n",
      "Trainable params: 317,552\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3428 - acc: 0.9111\n",
      "Loss: 0.34281196426039295 Accuracy: 0.9111111\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 355,696\n",
      "Trainable params: 353,136\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2554 - acc: 0.9288\n",
      "Loss: 0.2554334831262551 Accuracy: 0.9287643\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 422,512\n",
      "Trainable params: 419,440\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.1748 - acc: 0.9497\n",
      "Loss: 0.17475470805295035 Accuracy: 0.9497404\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 505,712\n",
      "Trainable params: 502,128\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.1884 - acc: 0.9514\n",
      "Loss: 0.1884096939607039 Accuracy: 0.9514019\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 801,136\n",
      "Trainable params: 796,528\n",
      "Non-trainable params: 4,608\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.2178 - acc: 0.9377\n",
      "Loss: 0.2177782718862019 Accuracy: 0.9376947\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,064,432\n",
      "Trainable params: 2,064,048\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 3.4410 - acc: 0.5094\n",
      "Loss: 3.440951137255533 Accuracy: 0.50944966\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,046,896\n",
      "Trainable params: 1,046,384\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 2.1975 - acc: 0.6127\n",
      "Loss: 2.197538639971772 Accuracy: 0.61266875\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,065,968\n",
      "Trainable params: 1,065,200\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 1.6982 - acc: 0.6926\n",
      "Loss: 1.6982337263885687 Accuracy: 0.6926272\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 579,184\n",
      "Trainable params: 578,160\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 1.0891 - acc: 0.7751\n",
      "Loss: 1.0890547004444207 Accuracy: 0.7750779\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 348,400\n",
      "Trainable params: 347,120\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.7765 - acc: 0.8249\n",
      "Loss: 0.7764819572152626 Accuracy: 0.82492214\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 245,616\n",
      "Trainable params: 244,080\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.5453 - acc: 0.8798\n",
      "Loss: 0.5453438074294157 Accuracy: 0.8797508\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 319,600\n",
      "Trainable params: 317,552\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 0.4851 - acc: 0.9076\n",
      "Loss: 0.48512392078012784 Accuracy: 0.9075805\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 355,696\n",
      "Trainable params: 353,136\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.2750 - acc: 0.9358\n",
      "Loss: 0.2749593847362578 Accuracy: 0.9358255\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 422,512\n",
      "Trainable params: 419,440\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1909 - acc: 0.9526\n",
      "Loss: 0.19093569252214 Accuracy: 0.952648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 505,712\n",
      "Trainable params: 502,128\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.1998 - acc: 0.9572\n",
      "Loss: 0.19977801913764656 Accuracy: 0.95721704\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_DO_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 801,136\n",
      "Trainable params: 796,528\n",
      "Non-trainable params: 4,608\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.2355 - acc: 0.9493\n",
      "Loss: 0.23551768435456386 Accuracy: 0.949325\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
