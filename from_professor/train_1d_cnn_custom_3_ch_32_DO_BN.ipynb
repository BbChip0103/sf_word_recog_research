{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel*(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,192,336\n",
      "Trainable params: 8,192,272\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,736,112\n",
      "Trainable params: 2,735,984\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,664\n",
      "Trainable params: 627,344\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,984\n",
      "Trainable params: 243,536\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 129,616\n",
      "Trainable params: 129,040\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 147,664\n",
      "Trainable params: 146,832\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 201,552\n",
      "Trainable params: 200,464\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,528\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3004 - acc: 0.3347\n",
      "Epoch 00001: val_loss improved from inf to 1.61227, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/001-1.6123.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 2.3004 - acc: 0.3347 - val_loss: 1.6123 - val_acc: 0.4864\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4975 - acc: 0.5365\n",
      "Epoch 00002: val_loss improved from 1.61227 to 1.22746, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/002-1.2275.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4976 - acc: 0.5364 - val_loss: 1.2275 - val_acc: 0.6271\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2676 - acc: 0.6051\n",
      "Epoch 00003: val_loss improved from 1.22746 to 1.20525, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/003-1.2052.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.2675 - acc: 0.6051 - val_loss: 1.2052 - val_acc: 0.6329\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1061 - acc: 0.6584\n",
      "Epoch 00004: val_loss improved from 1.20525 to 1.14577, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/004-1.1458.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1063 - acc: 0.6584 - val_loss: 1.1458 - val_acc: 0.6548\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9925 - acc: 0.6918\n",
      "Epoch 00005: val_loss improved from 1.14577 to 1.04245, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/005-1.0424.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9927 - acc: 0.6917 - val_loss: 1.0424 - val_acc: 0.6881\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8961 - acc: 0.7207\n",
      "Epoch 00006: val_loss did not improve from 1.04245\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8961 - acc: 0.7207 - val_loss: 1.0505 - val_acc: 0.6753\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7460\n",
      "Epoch 00007: val_loss improved from 1.04245 to 1.02321, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/007-1.0232.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.8168 - acc: 0.7459 - val_loss: 1.0232 - val_acc: 0.6993\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7633\n",
      "Epoch 00008: val_loss did not improve from 1.02321\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7532 - acc: 0.7633 - val_loss: 1.0320 - val_acc: 0.6990\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6946 - acc: 0.7829\n",
      "Epoch 00009: val_loss did not improve from 1.02321\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6946 - acc: 0.7829 - val_loss: 1.1541 - val_acc: 0.6534\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6358 - acc: 0.8003\n",
      "Epoch 00010: val_loss improved from 1.02321 to 1.00297, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/010-1.0030.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.6358 - acc: 0.8003 - val_loss: 1.0030 - val_acc: 0.7109\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.8160\n",
      "Epoch 00011: val_loss did not improve from 1.00297\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5873 - acc: 0.8159 - val_loss: 1.0883 - val_acc: 0.6902\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8192\n",
      "Epoch 00012: val_loss did not improve from 1.00297\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5565 - acc: 0.8192 - val_loss: 1.0657 - val_acc: 0.6962\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8357\n",
      "Epoch 00013: val_loss improved from 1.00297 to 0.97910, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/013-0.9791.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5095 - acc: 0.8356 - val_loss: 0.9791 - val_acc: 0.7214\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8471\n",
      "Epoch 00014: val_loss did not improve from 0.97910\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4777 - acc: 0.8472 - val_loss: 0.9875 - val_acc: 0.7279\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8537\n",
      "Epoch 00015: val_loss did not improve from 0.97910\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4476 - acc: 0.8536 - val_loss: 1.0114 - val_acc: 0.7289\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8613\n",
      "Epoch 00016: val_loss improved from 0.97910 to 0.97906, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_4_conv_checkpoint/016-0.9791.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4315 - acc: 0.8613 - val_loss: 0.9791 - val_acc: 0.7340\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8705\n",
      "Epoch 00017: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3988 - acc: 0.8705 - val_loss: 1.1282 - val_acc: 0.6958\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8802\n",
      "Epoch 00018: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3757 - acc: 0.8802 - val_loss: 1.0335 - val_acc: 0.7193\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3670 - acc: 0.8813\n",
      "Epoch 00019: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3673 - acc: 0.8813 - val_loss: 1.0661 - val_acc: 0.7130\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8884\n",
      "Epoch 00020: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3492 - acc: 0.8884 - val_loss: 1.0600 - val_acc: 0.7286\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8936\n",
      "Epoch 00021: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3252 - acc: 0.8936 - val_loss: 1.1302 - val_acc: 0.7011\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.8990\n",
      "Epoch 00022: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3139 - acc: 0.8990 - val_loss: 1.0199 - val_acc: 0.7249\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9028\n",
      "Epoch 00023: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3001 - acc: 0.9028 - val_loss: 1.0489 - val_acc: 0.7405\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9040\n",
      "Epoch 00024: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2918 - acc: 0.9040 - val_loss: 1.1931 - val_acc: 0.7002\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.9094\n",
      "Epoch 00025: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2769 - acc: 0.9094 - val_loss: 1.0785 - val_acc: 0.7324\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9098\n",
      "Epoch 00026: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2716 - acc: 0.9097 - val_loss: 1.0652 - val_acc: 0.7379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9136\n",
      "Epoch 00027: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2599 - acc: 0.9135 - val_loss: 1.0207 - val_acc: 0.7447\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9152\n",
      "Epoch 00028: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2562 - acc: 0.9152 - val_loss: 1.2113 - val_acc: 0.7126\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9209\n",
      "Epoch 00029: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2401 - acc: 0.9209 - val_loss: 1.0447 - val_acc: 0.7459\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9212\n",
      "Epoch 00030: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2355 - acc: 0.9212 - val_loss: 1.2347 - val_acc: 0.7084\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9286\n",
      "Epoch 00031: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2220 - acc: 0.9286 - val_loss: 1.0919 - val_acc: 0.7414\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9289\n",
      "Epoch 00032: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2176 - acc: 0.9289 - val_loss: 1.4596 - val_acc: 0.6536\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9287\n",
      "Epoch 00033: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2156 - acc: 0.9287 - val_loss: 1.1036 - val_acc: 0.7468\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9299\n",
      "Epoch 00034: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2122 - acc: 0.9299 - val_loss: 1.1365 - val_acc: 0.7275\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9338\n",
      "Epoch 00035: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1997 - acc: 0.9338 - val_loss: 1.1816 - val_acc: 0.7207\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9328\n",
      "Epoch 00036: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2020 - acc: 0.9328 - val_loss: 1.1023 - val_acc: 0.7447\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9375\n",
      "Epoch 00037: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1937 - acc: 0.9375 - val_loss: 1.2370 - val_acc: 0.7147\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9381\n",
      "Epoch 00038: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1854 - acc: 0.9381 - val_loss: 1.3630 - val_acc: 0.6897\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9399\n",
      "Epoch 00039: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1808 - acc: 0.9399 - val_loss: 1.0800 - val_acc: 0.7522\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9415\n",
      "Epoch 00040: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1819 - acc: 0.9414 - val_loss: 1.3259 - val_acc: 0.6932\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9401\n",
      "Epoch 00041: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1837 - acc: 0.9400 - val_loss: 1.2305 - val_acc: 0.7240\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9432\n",
      "Epoch 00042: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1735 - acc: 0.9432 - val_loss: 1.0768 - val_acc: 0.7601\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9473\n",
      "Epoch 00043: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1606 - acc: 0.9473 - val_loss: 1.2611 - val_acc: 0.7188\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9471\n",
      "Epoch 00044: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1648 - acc: 0.9470 - val_loss: 1.1621 - val_acc: 0.7363\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9440\n",
      "Epoch 00045: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1692 - acc: 0.9440 - val_loss: 1.1742 - val_acc: 0.7491\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9462\n",
      "Epoch 00046: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1647 - acc: 0.9462 - val_loss: 1.2108 - val_acc: 0.7286\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9502\n",
      "Epoch 00047: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1553 - acc: 0.9502 - val_loss: 1.3890 - val_acc: 0.7028\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9527\n",
      "Epoch 00048: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1455 - acc: 0.9526 - val_loss: 1.1101 - val_acc: 0.7515\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9524\n",
      "Epoch 00049: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1478 - acc: 0.9524 - val_loss: 1.1768 - val_acc: 0.7410\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9522\n",
      "Epoch 00050: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1450 - acc: 0.9522 - val_loss: 1.2521 - val_acc: 0.7331\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9543\n",
      "Epoch 00051: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1434 - acc: 0.9543 - val_loss: 1.2964 - val_acc: 0.7140\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9550\n",
      "Epoch 00052: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1404 - acc: 0.9550 - val_loss: 1.3030 - val_acc: 0.7228\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9577\n",
      "Epoch 00053: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1374 - acc: 0.9577 - val_loss: 1.2363 - val_acc: 0.7414\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9585\n",
      "Epoch 00054: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1303 - acc: 0.9585 - val_loss: 1.1738 - val_acc: 0.7496\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9555\n",
      "Epoch 00055: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1351 - acc: 0.9555 - val_loss: 1.1482 - val_acc: 0.7494\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9577\n",
      "Epoch 00056: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1311 - acc: 0.9577 - val_loss: 1.1928 - val_acc: 0.7442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9579\n",
      "Epoch 00057: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1301 - acc: 0.9579 - val_loss: 1.0832 - val_acc: 0.7659\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9586\n",
      "Epoch 00058: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1287 - acc: 0.9586 - val_loss: 1.3578 - val_acc: 0.7263\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9579\n",
      "Epoch 00059: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1324 - acc: 0.9579 - val_loss: 1.4474 - val_acc: 0.6990\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9595\n",
      "Epoch 00060: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1247 - acc: 0.9595 - val_loss: 1.1610 - val_acc: 0.7591\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9627\n",
      "Epoch 00061: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1191 - acc: 0.9627 - val_loss: 1.3880 - val_acc: 0.7091\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9643\n",
      "Epoch 00062: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1163 - acc: 0.9642 - val_loss: 1.1732 - val_acc: 0.7554\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9607\n",
      "Epoch 00063: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1242 - acc: 0.9607 - val_loss: 1.2796 - val_acc: 0.7286\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9635\n",
      "Epoch 00064: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1151 - acc: 0.9635 - val_loss: 1.1980 - val_acc: 0.7482\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9649\n",
      "Epoch 00065: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1120 - acc: 0.9649 - val_loss: 1.2679 - val_acc: 0.7279\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9651\n",
      "Epoch 00066: val_loss did not improve from 0.97906\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1112 - acc: 0.9651 - val_loss: 1.5054 - val_acc: 0.7081\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 515us/sample - loss: 1.0876 - acc: 0.6982\n",
      "Loss: 1.0876147106305338 Accuracy: 0.6982347\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2904 - acc: 0.3099\n",
      "Epoch 00001: val_loss improved from inf to 1.73247, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/001-1.7325.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.2904 - acc: 0.3100 - val_loss: 1.7325 - val_acc: 0.4349\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5022 - acc: 0.5246\n",
      "Epoch 00002: val_loss improved from 1.73247 to 1.09685, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/002-1.0968.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.5021 - acc: 0.5246 - val_loss: 1.0968 - val_acc: 0.6711\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2635 - acc: 0.6074\n",
      "Epoch 00003: val_loss improved from 1.09685 to 1.00205, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/003-1.0020.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.2635 - acc: 0.6074 - val_loss: 1.0020 - val_acc: 0.7032\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.6551\n",
      "Epoch 00004: val_loss improved from 1.00205 to 0.93238, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/004-0.9324.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1127 - acc: 0.6551 - val_loss: 0.9324 - val_acc: 0.7170\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0108 - acc: 0.6883\n",
      "Epoch 00005: val_loss did not improve from 0.93238\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0108 - acc: 0.6883 - val_loss: 0.9580 - val_acc: 0.7079\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7139\n",
      "Epoch 00006: val_loss improved from 0.93238 to 0.84886, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/006-0.8489.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9361 - acc: 0.7139 - val_loss: 0.8489 - val_acc: 0.7496\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8730 - acc: 0.7321\n",
      "Epoch 00007: val_loss improved from 0.84886 to 0.79336, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/007-0.7934.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8734 - acc: 0.7320 - val_loss: 0.7934 - val_acc: 0.7689\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8215 - acc: 0.7465\n",
      "Epoch 00008: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8216 - acc: 0.7465 - val_loss: 0.9254 - val_acc: 0.7347\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7822 - acc: 0.7607\n",
      "Epoch 00009: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7822 - acc: 0.7607 - val_loss: 0.8969 - val_acc: 0.7307\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7738\n",
      "Epoch 00010: val_loss did not improve from 0.79336\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7415 - acc: 0.7738 - val_loss: 0.8162 - val_acc: 0.7543\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7074 - acc: 0.7835\n",
      "Epoch 00011: val_loss improved from 0.79336 to 0.76184, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/011-0.7618.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7075 - acc: 0.7835 - val_loss: 0.7618 - val_acc: 0.7782\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.7937\n",
      "Epoch 00012: val_loss improved from 0.76184 to 0.75764, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/012-0.7576.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6769 - acc: 0.7937 - val_loss: 0.7576 - val_acc: 0.7803\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.8029\n",
      "Epoch 00013: val_loss did not improve from 0.75764\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6483 - acc: 0.8029 - val_loss: 0.7848 - val_acc: 0.7608\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6178 - acc: 0.8102\n",
      "Epoch 00014: val_loss improved from 0.75764 to 0.69513, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/014-0.6951.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6179 - acc: 0.8102 - val_loss: 0.6951 - val_acc: 0.7990\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6006 - acc: 0.8158\n",
      "Epoch 00015: val_loss did not improve from 0.69513\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6005 - acc: 0.8158 - val_loss: 0.7392 - val_acc: 0.7822\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5818 - acc: 0.8204\n",
      "Epoch 00016: val_loss did not improve from 0.69513\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5818 - acc: 0.8204 - val_loss: 0.7031 - val_acc: 0.7978\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5592 - acc: 0.8298\n",
      "Epoch 00017: val_loss did not improve from 0.69513\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5592 - acc: 0.8298 - val_loss: 0.7485 - val_acc: 0.7831\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.8329\n",
      "Epoch 00018: val_loss did not improve from 0.69513\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5448 - acc: 0.8329 - val_loss: 0.7396 - val_acc: 0.7862\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8388\n",
      "Epoch 00019: val_loss did not improve from 0.69513\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5276 - acc: 0.8387 - val_loss: 0.7402 - val_acc: 0.8039\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5141 - acc: 0.8421\n",
      "Epoch 00020: val_loss improved from 0.69513 to 0.63387, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/020-0.6339.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5140 - acc: 0.8422 - val_loss: 0.6339 - val_acc: 0.8150\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8476\n",
      "Epoch 00021: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4954 - acc: 0.8476 - val_loss: 0.6847 - val_acc: 0.8057\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8512\n",
      "Epoch 00022: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4801 - acc: 0.8512 - val_loss: 0.6494 - val_acc: 0.8155\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8534\n",
      "Epoch 00023: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4675 - acc: 0.8534 - val_loss: 0.7232 - val_acc: 0.7952\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8605\n",
      "Epoch 00024: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4542 - acc: 0.8605 - val_loss: 0.7782 - val_acc: 0.7820\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8645\n",
      "Epoch 00025: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4355 - acc: 0.8645 - val_loss: 0.7702 - val_acc: 0.7831\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.8660\n",
      "Epoch 00026: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4294 - acc: 0.8659 - val_loss: 0.6709 - val_acc: 0.8164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8648\n",
      "Epoch 00027: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4264 - acc: 0.8647 - val_loss: 0.6557 - val_acc: 0.8171\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8732\n",
      "Epoch 00028: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4096 - acc: 0.8732 - val_loss: 0.6565 - val_acc: 0.8251\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8759\n",
      "Epoch 00029: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3991 - acc: 0.8759 - val_loss: 0.7023 - val_acc: 0.8041\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8786\n",
      "Epoch 00030: val_loss did not improve from 0.63387\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3864 - acc: 0.8786 - val_loss: 0.6685 - val_acc: 0.8137\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3822 - acc: 0.8802\n",
      "Epoch 00031: val_loss improved from 0.63387 to 0.60905, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_5_conv_checkpoint/031-0.6090.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3822 - acc: 0.8802 - val_loss: 0.6090 - val_acc: 0.8309\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8840\n",
      "Epoch 00032: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3738 - acc: 0.8840 - val_loss: 0.6490 - val_acc: 0.8167\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8858\n",
      "Epoch 00033: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3680 - acc: 0.8858 - val_loss: 0.6587 - val_acc: 0.8290\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8870\n",
      "Epoch 00034: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3561 - acc: 0.8869 - val_loss: 0.7002 - val_acc: 0.8053\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8870\n",
      "Epoch 00035: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3567 - acc: 0.8870 - val_loss: 0.6552 - val_acc: 0.8211\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8916\n",
      "Epoch 00036: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3433 - acc: 0.8916 - val_loss: 0.6616 - val_acc: 0.8216\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8940\n",
      "Epoch 00037: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3362 - acc: 0.8940 - val_loss: 0.8456 - val_acc: 0.7859\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8938\n",
      "Epoch 00038: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3355 - acc: 0.8938 - val_loss: 0.6285 - val_acc: 0.8267\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8951\n",
      "Epoch 00039: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3267 - acc: 0.8951 - val_loss: 0.6784 - val_acc: 0.8146\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.9006\n",
      "Epoch 00040: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3132 - acc: 0.9006 - val_loss: 0.7005 - val_acc: 0.8055\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.9010\n",
      "Epoch 00041: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3075 - acc: 0.9009 - val_loss: 0.6811 - val_acc: 0.8220\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9024\n",
      "Epoch 00042: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3054 - acc: 0.9024 - val_loss: 0.6415 - val_acc: 0.8262\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9055\n",
      "Epoch 00043: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2967 - acc: 0.9055 - val_loss: 0.7917 - val_acc: 0.7892\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9062\n",
      "Epoch 00044: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2917 - acc: 0.9062 - val_loss: 0.7198 - val_acc: 0.8064\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9059\n",
      "Epoch 00045: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2916 - acc: 0.9059 - val_loss: 0.7361 - val_acc: 0.8085\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.9096\n",
      "Epoch 00046: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2848 - acc: 0.9096 - val_loss: 0.6696 - val_acc: 0.8295\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9113\n",
      "Epoch 00047: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2766 - acc: 0.9113 - val_loss: 0.6704 - val_acc: 0.8316\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9126\n",
      "Epoch 00048: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2727 - acc: 0.9126 - val_loss: 0.6901 - val_acc: 0.8190\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9160\n",
      "Epoch 00049: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2664 - acc: 0.9160 - val_loss: 0.6804 - val_acc: 0.8141\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9137\n",
      "Epoch 00050: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2661 - acc: 0.9137 - val_loss: 0.6689 - val_acc: 0.8290\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2536 - acc: 0.9189\n",
      "Epoch 00051: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2535 - acc: 0.9190 - val_loss: 0.7078 - val_acc: 0.8169\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9173\n",
      "Epoch 00052: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2571 - acc: 0.9173 - val_loss: 0.6942 - val_acc: 0.8160\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9218\n",
      "Epoch 00053: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2478 - acc: 0.9218 - val_loss: 0.6847 - val_acc: 0.8248\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9217\n",
      "Epoch 00054: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2481 - acc: 0.9217 - val_loss: 0.6613 - val_acc: 0.8283\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9213\n",
      "Epoch 00055: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2479 - acc: 0.9213 - val_loss: 0.7278 - val_acc: 0.8155\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9239\n",
      "Epoch 00056: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2379 - acc: 0.9238 - val_loss: 0.7520 - val_acc: 0.8081\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9246\n",
      "Epoch 00057: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2368 - acc: 0.9246 - val_loss: 0.6760 - val_acc: 0.8164\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9251\n",
      "Epoch 00058: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2346 - acc: 0.9251 - val_loss: 0.6661 - val_acc: 0.8288\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9264\n",
      "Epoch 00059: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2261 - acc: 0.9264 - val_loss: 0.6768 - val_acc: 0.8290\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9247\n",
      "Epoch 00060: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2288 - acc: 0.9247 - val_loss: 0.6997 - val_acc: 0.8164\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9289\n",
      "Epoch 00061: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2192 - acc: 0.9289 - val_loss: 0.7680 - val_acc: 0.7980\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9285\n",
      "Epoch 00062: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2188 - acc: 0.9285 - val_loss: 0.7063 - val_acc: 0.8181\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9290\n",
      "Epoch 00063: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2177 - acc: 0.9290 - val_loss: 0.7642 - val_acc: 0.8069\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9316\n",
      "Epoch 00064: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2115 - acc: 0.9316 - val_loss: 0.7216 - val_acc: 0.8074\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9300\n",
      "Epoch 00065: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2128 - acc: 0.9300 - val_loss: 0.6307 - val_acc: 0.8428\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9320\n",
      "Epoch 00066: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2068 - acc: 0.9319 - val_loss: 0.7022 - val_acc: 0.8237\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9315\n",
      "Epoch 00067: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2099 - acc: 0.9315 - val_loss: 0.7529 - val_acc: 0.8032\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9357\n",
      "Epoch 00068: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1980 - acc: 0.9357 - val_loss: 0.6986 - val_acc: 0.8330\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9335\n",
      "Epoch 00069: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2002 - acc: 0.9335 - val_loss: 0.7123 - val_acc: 0.8183\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9352\n",
      "Epoch 00070: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1977 - acc: 0.9352 - val_loss: 0.6707 - val_acc: 0.8288\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9381\n",
      "Epoch 00071: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1946 - acc: 0.9381 - val_loss: 0.7633 - val_acc: 0.8057\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9388\n",
      "Epoch 00072: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1902 - acc: 0.9388 - val_loss: 0.7364 - val_acc: 0.8218\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9377\n",
      "Epoch 00073: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1934 - acc: 0.9376 - val_loss: 0.6852 - val_acc: 0.8248\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9381\n",
      "Epoch 00074: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1921 - acc: 0.9381 - val_loss: 0.6434 - val_acc: 0.8362\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9389\n",
      "Epoch 00075: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1866 - acc: 0.9389 - val_loss: 0.6763 - val_acc: 0.8390\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9428\n",
      "Epoch 00076: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1814 - acc: 0.9428 - val_loss: 0.8573 - val_acc: 0.7939\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9388\n",
      "Epoch 00077: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1873 - acc: 0.9387 - val_loss: 0.8967 - val_acc: 0.7866\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9424\n",
      "Epoch 00078: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1835 - acc: 0.9424 - val_loss: 0.7011 - val_acc: 0.8302\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9446\n",
      "Epoch 00079: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1735 - acc: 0.9446 - val_loss: 0.6710 - val_acc: 0.8321\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9430\n",
      "Epoch 00080: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1765 - acc: 0.9430 - val_loss: 0.6575 - val_acc: 0.8400\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9435\n",
      "Epoch 00081: val_loss did not improve from 0.60905\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1730 - acc: 0.9435 - val_loss: 0.6573 - val_acc: 0.8411\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 577us/sample - loss: 0.6712 - acc: 0.8027\n",
      "Loss: 0.6712016487666256 Accuracy: 0.8026999\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4792 - acc: 0.2585\n",
      "Epoch 00001: val_loss improved from inf to 1.84348, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/001-1.8435.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.4791 - acc: 0.2585 - val_loss: 1.8435 - val_acc: 0.4004\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6365 - acc: 0.4802\n",
      "Epoch 00002: val_loss improved from 1.84348 to 1.19238, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/002-1.1924.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.6367 - acc: 0.4802 - val_loss: 1.1924 - val_acc: 0.6492\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3142 - acc: 0.5826\n",
      "Epoch 00003: val_loss improved from 1.19238 to 1.02256, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/003-1.0226.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.3142 - acc: 0.5825 - val_loss: 1.0226 - val_acc: 0.6981\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.6453\n",
      "Epoch 00004: val_loss improved from 1.02256 to 0.86667, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/004-0.8667.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1318 - acc: 0.6453 - val_loss: 0.8667 - val_acc: 0.7454\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0031 - acc: 0.6909\n",
      "Epoch 00005: val_loss improved from 0.86667 to 0.79952, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/005-0.7995.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.0030 - acc: 0.6909 - val_loss: 0.7995 - val_acc: 0.7650\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9101 - acc: 0.7230\n",
      "Epoch 00006: val_loss improved from 0.79952 to 0.78654, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/006-0.7865.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.9102 - acc: 0.7230 - val_loss: 0.7865 - val_acc: 0.7724\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7429\n",
      "Epoch 00007: val_loss did not improve from 0.78654\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.8456 - acc: 0.7429 - val_loss: 0.8998 - val_acc: 0.7310\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7624\n",
      "Epoch 00008: val_loss improved from 0.78654 to 0.78609, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/008-0.7861.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7845 - acc: 0.7624 - val_loss: 0.7861 - val_acc: 0.7864\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7410 - acc: 0.7766\n",
      "Epoch 00009: val_loss improved from 0.78609 to 0.69975, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/009-0.6997.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7409 - acc: 0.7767 - val_loss: 0.6997 - val_acc: 0.8029\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7929\n",
      "Epoch 00010: val_loss improved from 0.69975 to 0.62386, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/010-0.6239.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6928 - acc: 0.7928 - val_loss: 0.6239 - val_acc: 0.8241\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.8042\n",
      "Epoch 00011: val_loss did not improve from 0.62386\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6577 - acc: 0.8042 - val_loss: 0.6615 - val_acc: 0.8074\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6306 - acc: 0.8126\n",
      "Epoch 00012: val_loss improved from 0.62386 to 0.58288, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/012-0.5829.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6309 - acc: 0.8125 - val_loss: 0.5829 - val_acc: 0.8372\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.8225\n",
      "Epoch 00013: val_loss did not improve from 0.58288\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5984 - acc: 0.8225 - val_loss: 0.6202 - val_acc: 0.8283\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8317\n",
      "Epoch 00014: val_loss did not improve from 0.58288\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5713 - acc: 0.8317 - val_loss: 0.6138 - val_acc: 0.8262\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8362\n",
      "Epoch 00015: val_loss did not improve from 0.58288\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5485 - acc: 0.8362 - val_loss: 0.6067 - val_acc: 0.8258\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8417\n",
      "Epoch 00016: val_loss improved from 0.58288 to 0.54270, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/016-0.5427.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5309 - acc: 0.8417 - val_loss: 0.5427 - val_acc: 0.8532\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8478\n",
      "Epoch 00017: val_loss did not improve from 0.54270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5135 - acc: 0.8477 - val_loss: 0.6121 - val_acc: 0.8295\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4926 - acc: 0.8540\n",
      "Epoch 00018: val_loss did not improve from 0.54270\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4927 - acc: 0.8540 - val_loss: 0.5735 - val_acc: 0.8430\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4861 - acc: 0.8546\n",
      "Epoch 00019: val_loss improved from 0.54270 to 0.49512, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/019-0.4951.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4861 - acc: 0.8546 - val_loss: 0.4951 - val_acc: 0.8612\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4664 - acc: 0.8590\n",
      "Epoch 00020: val_loss improved from 0.49512 to 0.46511, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/020-0.4651.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4663 - acc: 0.8590 - val_loss: 0.4651 - val_acc: 0.8737\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8659\n",
      "Epoch 00021: val_loss did not improve from 0.46511\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4539 - acc: 0.8658 - val_loss: 0.5138 - val_acc: 0.8640\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4421 - acc: 0.8685\n",
      "Epoch 00022: val_loss improved from 0.46511 to 0.46150, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/022-0.4615.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4421 - acc: 0.8684 - val_loss: 0.4615 - val_acc: 0.8700\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4300 - acc: 0.8714\n",
      "Epoch 00023: val_loss did not improve from 0.46150\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4302 - acc: 0.8714 - val_loss: 0.4763 - val_acc: 0.8730\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8750\n",
      "Epoch 00024: val_loss did not improve from 0.46150\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4192 - acc: 0.8750 - val_loss: 0.5252 - val_acc: 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8788\n",
      "Epoch 00025: val_loss improved from 0.46150 to 0.42253, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/025-0.4225.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4060 - acc: 0.8788 - val_loss: 0.4225 - val_acc: 0.8887\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8832\n",
      "Epoch 00026: val_loss did not improve from 0.42253\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3905 - acc: 0.8832 - val_loss: 0.4598 - val_acc: 0.8726\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8837\n",
      "Epoch 00027: val_loss did not improve from 0.42253\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3885 - acc: 0.8837 - val_loss: 0.4794 - val_acc: 0.8719\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8867\n",
      "Epoch 00028: val_loss did not improve from 0.42253\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3804 - acc: 0.8867 - val_loss: 0.5826 - val_acc: 0.8330\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3753 - acc: 0.8867\n",
      "Epoch 00029: val_loss did not improve from 0.42253\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3752 - acc: 0.8867 - val_loss: 0.4260 - val_acc: 0.8870\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8925\n",
      "Epoch 00030: val_loss did not improve from 0.42253\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3624 - acc: 0.8925 - val_loss: 0.4824 - val_acc: 0.8677\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8929\n",
      "Epoch 00031: val_loss improved from 0.42253 to 0.41912, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/031-0.4191.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3512 - acc: 0.8929 - val_loss: 0.4191 - val_acc: 0.8938\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3513 - acc: 0.8930\n",
      "Epoch 00032: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3513 - acc: 0.8930 - val_loss: 0.4787 - val_acc: 0.8717\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8977\n",
      "Epoch 00033: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3355 - acc: 0.8976 - val_loss: 0.4233 - val_acc: 0.8875\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8978\n",
      "Epoch 00034: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3308 - acc: 0.8977 - val_loss: 0.5802 - val_acc: 0.8386\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.9007\n",
      "Epoch 00035: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3245 - acc: 0.9007 - val_loss: 0.4971 - val_acc: 0.8600\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8996\n",
      "Epoch 00036: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3233 - acc: 0.8996 - val_loss: 0.4213 - val_acc: 0.8887\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.9031\n",
      "Epoch 00037: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3169 - acc: 0.9031 - val_loss: 0.4668 - val_acc: 0.8689\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9069\n",
      "Epoch 00038: val_loss did not improve from 0.41912\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3078 - acc: 0.9069 - val_loss: 0.5115 - val_acc: 0.8572\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9073\n",
      "Epoch 00039: val_loss improved from 0.41912 to 0.40508, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/039-0.4051.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3063 - acc: 0.9073 - val_loss: 0.4051 - val_acc: 0.8889\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.9106\n",
      "Epoch 00040: val_loss did not improve from 0.40508\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2983 - acc: 0.9106 - val_loss: 0.4079 - val_acc: 0.8903\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9089\n",
      "Epoch 00041: val_loss did not improve from 0.40508\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2929 - acc: 0.9089 - val_loss: 0.4601 - val_acc: 0.8768\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9120\n",
      "Epoch 00042: val_loss improved from 0.40508 to 0.38227, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/042-0.3823.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2894 - acc: 0.9119 - val_loss: 0.3823 - val_acc: 0.8966\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9143\n",
      "Epoch 00043: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2818 - acc: 0.9143 - val_loss: 0.4784 - val_acc: 0.8642\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9145\n",
      "Epoch 00044: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2785 - acc: 0.9144 - val_loss: 0.4613 - val_acc: 0.8744\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9166\n",
      "Epoch 00045: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2718 - acc: 0.9166 - val_loss: 0.4078 - val_acc: 0.8926\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9176\n",
      "Epoch 00046: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2695 - acc: 0.9176 - val_loss: 0.5283 - val_acc: 0.8602\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9193\n",
      "Epoch 00047: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2663 - acc: 0.9193 - val_loss: 0.4418 - val_acc: 0.8798\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9201\n",
      "Epoch 00048: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2593 - acc: 0.9201 - val_loss: 0.4221 - val_acc: 0.8812\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9214\n",
      "Epoch 00049: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2549 - acc: 0.9214 - val_loss: 0.4130 - val_acc: 0.8882\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9207\n",
      "Epoch 00050: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2504 - acc: 0.9207 - val_loss: 0.4039 - val_acc: 0.8875\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9230\n",
      "Epoch 00051: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2476 - acc: 0.9230 - val_loss: 0.4468 - val_acc: 0.8814\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9250\n",
      "Epoch 00052: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2441 - acc: 0.9250 - val_loss: 0.4754 - val_acc: 0.8647\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9244\n",
      "Epoch 00053: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2468 - acc: 0.9244 - val_loss: 0.5660 - val_acc: 0.8428\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9271\n",
      "Epoch 00054: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2347 - acc: 0.9270 - val_loss: 0.4072 - val_acc: 0.8875\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9274\n",
      "Epoch 00055: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2327 - acc: 0.9275 - val_loss: 0.3869 - val_acc: 0.8940\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9292\n",
      "Epoch 00056: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2301 - acc: 0.9292 - val_loss: 0.4336 - val_acc: 0.8849\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9294\n",
      "Epoch 00057: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2251 - acc: 0.9294 - val_loss: 0.3890 - val_acc: 0.9003\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9293\n",
      "Epoch 00058: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2255 - acc: 0.9293 - val_loss: 0.3967 - val_acc: 0.8994\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9308\n",
      "Epoch 00059: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2221 - acc: 0.9308 - val_loss: 0.3935 - val_acc: 0.8998\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9322\n",
      "Epoch 00060: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2210 - acc: 0.9322 - val_loss: 0.4813 - val_acc: 0.8635\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9326\n",
      "Epoch 00061: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2171 - acc: 0.9325 - val_loss: 0.3845 - val_acc: 0.9045\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9325\n",
      "Epoch 00062: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2175 - acc: 0.9325 - val_loss: 0.4385 - val_acc: 0.8826\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9359\n",
      "Epoch 00063: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2042 - acc: 0.9359 - val_loss: 0.4445 - val_acc: 0.8833\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9333\n",
      "Epoch 00064: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2080 - acc: 0.9334 - val_loss: 0.4651 - val_acc: 0.8747\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9365\n",
      "Epoch 00065: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2024 - acc: 0.9366 - val_loss: 0.3823 - val_acc: 0.8975\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9374\n",
      "Epoch 00066: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1991 - acc: 0.9373 - val_loss: 0.4525 - val_acc: 0.8814\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9362\n",
      "Epoch 00067: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2027 - acc: 0.9363 - val_loss: 0.4522 - val_acc: 0.8686\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9377\n",
      "Epoch 00068: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1948 - acc: 0.9377 - val_loss: 0.4487 - val_acc: 0.8838\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9382\n",
      "Epoch 00069: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1959 - acc: 0.9382 - val_loss: 0.4210 - val_acc: 0.8856\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9396\n",
      "Epoch 00070: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1897 - acc: 0.9396 - val_loss: 0.4069 - val_acc: 0.8977\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9387\n",
      "Epoch 00071: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1901 - acc: 0.9387 - val_loss: 0.3827 - val_acc: 0.8991\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9385\n",
      "Epoch 00072: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1877 - acc: 0.9385 - val_loss: 0.4442 - val_acc: 0.8849\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1882 - acc: 0.9405\n",
      "Epoch 00073: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1882 - acc: 0.9405 - val_loss: 0.3996 - val_acc: 0.8996\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9412\n",
      "Epoch 00074: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1849 - acc: 0.9411 - val_loss: 0.4030 - val_acc: 0.8924\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9429\n",
      "Epoch 00075: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1823 - acc: 0.9429 - val_loss: 0.3892 - val_acc: 0.9010\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9448\n",
      "Epoch 00076: val_loss did not improve from 0.38227\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1754 - acc: 0.9447 - val_loss: 0.4436 - val_acc: 0.8854\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9423\n",
      "Epoch 00077: val_loss improved from 0.38227 to 0.36839, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/077-0.3684.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1849 - acc: 0.9423 - val_loss: 0.3684 - val_acc: 0.9019\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9457\n",
      "Epoch 00078: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1738 - acc: 0.9457 - val_loss: 0.3770 - val_acc: 0.9036\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9450\n",
      "Epoch 00079: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1717 - acc: 0.9450 - val_loss: 0.4926 - val_acc: 0.8698\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9455\n",
      "Epoch 00080: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1721 - acc: 0.9454 - val_loss: 0.3777 - val_acc: 0.9008\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9462\n",
      "Epoch 00081: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1725 - acc: 0.9461 - val_loss: 0.3998 - val_acc: 0.8991\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9475\n",
      "Epoch 00082: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1633 - acc: 0.9474 - val_loss: 0.4437 - val_acc: 0.8863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9446\n",
      "Epoch 00083: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1724 - acc: 0.9446 - val_loss: 0.3718 - val_acc: 0.9017\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9468\n",
      "Epoch 00084: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1642 - acc: 0.9468 - val_loss: 0.5068 - val_acc: 0.8719\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9491\n",
      "Epoch 00085: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1583 - acc: 0.9491 - val_loss: 0.4499 - val_acc: 0.8884\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9504\n",
      "Epoch 00086: val_loss did not improve from 0.36839\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1587 - acc: 0.9504 - val_loss: 0.3824 - val_acc: 0.9071\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9503\n",
      "Epoch 00087: val_loss improved from 0.36839 to 0.36533, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/087-0.3653.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1588 - acc: 0.9503 - val_loss: 0.3653 - val_acc: 0.9094\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9520\n",
      "Epoch 00088: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1511 - acc: 0.9520 - val_loss: 0.4522 - val_acc: 0.8793\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9521\n",
      "Epoch 00089: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1509 - acc: 0.9521 - val_loss: 0.4255 - val_acc: 0.8942\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9523\n",
      "Epoch 00090: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1492 - acc: 0.9523 - val_loss: 0.3986 - val_acc: 0.8959\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9529\n",
      "Epoch 00091: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1515 - acc: 0.9529 - val_loss: 0.3925 - val_acc: 0.8994\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9532\n",
      "Epoch 00092: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1509 - acc: 0.9532 - val_loss: 0.4493 - val_acc: 0.8928\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9532\n",
      "Epoch 00093: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1463 - acc: 0.9532 - val_loss: 0.4464 - val_acc: 0.8863\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9529\n",
      "Epoch 00094: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1439 - acc: 0.9528 - val_loss: 0.4490 - val_acc: 0.8903\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9533\n",
      "Epoch 00095: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1451 - acc: 0.9533 - val_loss: 0.3832 - val_acc: 0.9031\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9554\n",
      "Epoch 00096: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1390 - acc: 0.9554 - val_loss: 0.4057 - val_acc: 0.8954\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9564\n",
      "Epoch 00097: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1408 - acc: 0.9564 - val_loss: 0.4297 - val_acc: 0.8924\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9561\n",
      "Epoch 00098: val_loss did not improve from 0.36533\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1390 - acc: 0.9561 - val_loss: 0.3945 - val_acc: 0.8980\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.9554\n",
      "Epoch 00099: val_loss improved from 0.36533 to 0.36154, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_6_conv_checkpoint/099-0.3615.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1364 - acc: 0.9553 - val_loss: 0.3615 - val_acc: 0.9033\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9560\n",
      "Epoch 00100: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1383 - acc: 0.9560 - val_loss: 0.3978 - val_acc: 0.9047\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9543\n",
      "Epoch 00101: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1415 - acc: 0.9542 - val_loss: 0.3922 - val_acc: 0.9010\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9589\n",
      "Epoch 00102: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1293 - acc: 0.9589 - val_loss: 0.4681 - val_acc: 0.8868\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9543\n",
      "Epoch 00103: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1395 - acc: 0.9544 - val_loss: 0.3982 - val_acc: 0.8991\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9596\n",
      "Epoch 00104: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1285 - acc: 0.9596 - val_loss: 0.4177 - val_acc: 0.8980\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9570\n",
      "Epoch 00105: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1331 - acc: 0.9570 - val_loss: 0.3858 - val_acc: 0.9096\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9570\n",
      "Epoch 00106: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1324 - acc: 0.9570 - val_loss: 0.4870 - val_acc: 0.8796\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9597\n",
      "Epoch 00107: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1258 - acc: 0.9597 - val_loss: 0.4224 - val_acc: 0.9005\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9595\n",
      "Epoch 00108: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1253 - acc: 0.9595 - val_loss: 0.4695 - val_acc: 0.8821\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9598\n",
      "Epoch 00109: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1264 - acc: 0.9597 - val_loss: 0.4218 - val_acc: 0.8926\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9599\n",
      "Epoch 00110: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1229 - acc: 0.9599 - val_loss: 0.3946 - val_acc: 0.9005\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9596\n",
      "Epoch 00111: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1254 - acc: 0.9595 - val_loss: 0.3743 - val_acc: 0.9045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9595\n",
      "Epoch 00112: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1256 - acc: 0.9595 - val_loss: 0.4095 - val_acc: 0.9036\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9600\n",
      "Epoch 00113: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1222 - acc: 0.9600 - val_loss: 0.3940 - val_acc: 0.9045\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9596\n",
      "Epoch 00114: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1226 - acc: 0.9596 - val_loss: 0.3729 - val_acc: 0.9106\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9620\n",
      "Epoch 00115: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1170 - acc: 0.9620 - val_loss: 0.3929 - val_acc: 0.9071\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9624\n",
      "Epoch 00116: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1188 - acc: 0.9623 - val_loss: 0.3653 - val_acc: 0.9075\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9604\n",
      "Epoch 00117: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1206 - acc: 0.9604 - val_loss: 0.5740 - val_acc: 0.8642\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9619\n",
      "Epoch 00118: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1198 - acc: 0.9619 - val_loss: 0.3669 - val_acc: 0.9080\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9636\n",
      "Epoch 00119: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1129 - acc: 0.9636 - val_loss: 0.3843 - val_acc: 0.9052\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9636\n",
      "Epoch 00120: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1138 - acc: 0.9636 - val_loss: 0.3890 - val_acc: 0.9066\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9645\n",
      "Epoch 00121: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1116 - acc: 0.9645 - val_loss: 0.6432 - val_acc: 0.8479\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9639\n",
      "Epoch 00122: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1118 - acc: 0.9639 - val_loss: 0.4056 - val_acc: 0.9052\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9624\n",
      "Epoch 00123: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1157 - acc: 0.9624 - val_loss: 0.3964 - val_acc: 0.9012\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9635\n",
      "Epoch 00124: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1131 - acc: 0.9635 - val_loss: 0.3911 - val_acc: 0.9087\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9658\n",
      "Epoch 00125: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1071 - acc: 0.9658 - val_loss: 0.4180 - val_acc: 0.8968\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9660\n",
      "Epoch 00126: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1073 - acc: 0.9660 - val_loss: 0.3797 - val_acc: 0.9113\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9636\n",
      "Epoch 00127: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1122 - acc: 0.9636 - val_loss: 0.4148 - val_acc: 0.8963\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9667\n",
      "Epoch 00128: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1053 - acc: 0.9667 - val_loss: 0.4195 - val_acc: 0.8989\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9661\n",
      "Epoch 00129: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1046 - acc: 0.9661 - val_loss: 0.3795 - val_acc: 0.9092\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9680\n",
      "Epoch 00130: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1027 - acc: 0.9680 - val_loss: 0.4345 - val_acc: 0.8898\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9657\n",
      "Epoch 00131: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1058 - acc: 0.9657 - val_loss: 0.4124 - val_acc: 0.9010\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9678\n",
      "Epoch 00132: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1030 - acc: 0.9678 - val_loss: 0.4076 - val_acc: 0.9061\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9674\n",
      "Epoch 00133: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1014 - acc: 0.9674 - val_loss: 0.4516 - val_acc: 0.9040\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9648\n",
      "Epoch 00134: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1092 - acc: 0.9648 - val_loss: 0.4294 - val_acc: 0.8949\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9681\n",
      "Epoch 00135: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0988 - acc: 0.9680 - val_loss: 0.3957 - val_acc: 0.9078\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9656\n",
      "Epoch 00136: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1057 - acc: 0.9656 - val_loss: 0.4241 - val_acc: 0.8989\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9693\n",
      "Epoch 00137: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0960 - acc: 0.9692 - val_loss: 0.3788 - val_acc: 0.9087\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9696\n",
      "Epoch 00138: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0960 - acc: 0.9696 - val_loss: 0.3698 - val_acc: 0.9096\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9677\n",
      "Epoch 00139: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1002 - acc: 0.9677 - val_loss: 0.4156 - val_acc: 0.9045\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9689\n",
      "Epoch 00140: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0970 - acc: 0.9689 - val_loss: 0.4724 - val_acc: 0.8910\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9698\n",
      "Epoch 00141: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0933 - acc: 0.9698 - val_loss: 0.3916 - val_acc: 0.9115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9700\n",
      "Epoch 00142: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0951 - acc: 0.9699 - val_loss: 0.4017 - val_acc: 0.8970\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9700\n",
      "Epoch 00143: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0956 - acc: 0.9700 - val_loss: 0.3930 - val_acc: 0.9110\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9707\n",
      "Epoch 00144: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0938 - acc: 0.9707 - val_loss: 0.4234 - val_acc: 0.9019\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9686\n",
      "Epoch 00145: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0973 - acc: 0.9686 - val_loss: 0.3960 - val_acc: 0.9122\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9704\n",
      "Epoch 00146: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0927 - acc: 0.9704 - val_loss: 0.4086 - val_acc: 0.9047\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9697\n",
      "Epoch 00147: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0970 - acc: 0.9697 - val_loss: 0.4457 - val_acc: 0.8994\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9709\n",
      "Epoch 00148: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0942 - acc: 0.9709 - val_loss: 0.3907 - val_acc: 0.9099\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9718\n",
      "Epoch 00149: val_loss did not improve from 0.36154\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0899 - acc: 0.9718 - val_loss: 0.4361 - val_acc: 0.8994\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 679us/sample - loss: 0.4524 - acc: 0.8739\n",
      "Loss: 0.4524365982224513 Accuracy: 0.87393564\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4229 - acc: 0.2683\n",
      "Epoch 00001: val_loss improved from inf to 1.68295, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/001-1.6830.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.4228 - acc: 0.2683 - val_loss: 1.6830 - val_acc: 0.4705\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5310 - acc: 0.5089\n",
      "Epoch 00002: val_loss improved from 1.68295 to 1.02013, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/002-1.0201.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5311 - acc: 0.5089 - val_loss: 1.0201 - val_acc: 0.7056\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1913 - acc: 0.6243\n",
      "Epoch 00003: val_loss did not improve from 1.02013\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1912 - acc: 0.6243 - val_loss: 1.0979 - val_acc: 0.6676\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9876 - acc: 0.6943\n",
      "Epoch 00004: val_loss improved from 1.02013 to 0.71903, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/004-0.7190.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9877 - acc: 0.6943 - val_loss: 0.7190 - val_acc: 0.7939\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8339 - acc: 0.7442\n",
      "Epoch 00005: val_loss improved from 0.71903 to 0.66881, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/005-0.6688.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8339 - acc: 0.7442 - val_loss: 0.6688 - val_acc: 0.8041\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7259 - acc: 0.7797\n",
      "Epoch 00006: val_loss improved from 0.66881 to 0.53979, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/006-0.5398.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7259 - acc: 0.7797 - val_loss: 0.5398 - val_acc: 0.8444\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6456 - acc: 0.8055\n",
      "Epoch 00007: val_loss did not improve from 0.53979\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6456 - acc: 0.8055 - val_loss: 0.5760 - val_acc: 0.8330\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5794 - acc: 0.8259\n",
      "Epoch 00008: val_loss improved from 0.53979 to 0.44816, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/008-0.4482.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5794 - acc: 0.8259 - val_loss: 0.4482 - val_acc: 0.8789\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8385\n",
      "Epoch 00009: val_loss improved from 0.44816 to 0.41415, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/009-0.4142.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5351 - acc: 0.8385 - val_loss: 0.4142 - val_acc: 0.8882\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8510\n",
      "Epoch 00010: val_loss improved from 0.41415 to 0.39823, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/010-0.3982.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4954 - acc: 0.8510 - val_loss: 0.3982 - val_acc: 0.8905\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8607\n",
      "Epoch 00011: val_loss did not improve from 0.39823\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4628 - acc: 0.8606 - val_loss: 0.4004 - val_acc: 0.8898\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8713\n",
      "Epoch 00012: val_loss improved from 0.39823 to 0.36082, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/012-0.3608.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4298 - acc: 0.8713 - val_loss: 0.3608 - val_acc: 0.9029\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8771\n",
      "Epoch 00013: val_loss did not improve from 0.36082\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4052 - acc: 0.8771 - val_loss: 0.3621 - val_acc: 0.9038\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8845\n",
      "Epoch 00014: val_loss improved from 0.36082 to 0.32586, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/014-0.3259.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3824 - acc: 0.8845 - val_loss: 0.3259 - val_acc: 0.9122\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8929\n",
      "Epoch 00015: val_loss improved from 0.32586 to 0.31338, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/015-0.3134.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3560 - acc: 0.8928 - val_loss: 0.3134 - val_acc: 0.9192\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8977\n",
      "Epoch 00016: val_loss did not improve from 0.31338\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3379 - acc: 0.8976 - val_loss: 0.3226 - val_acc: 0.9145\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.9015\n",
      "Epoch 00017: val_loss improved from 0.31338 to 0.29836, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/017-0.2984.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3303 - acc: 0.9015 - val_loss: 0.2984 - val_acc: 0.9189\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9045\n",
      "Epoch 00018: val_loss improved from 0.29836 to 0.29095, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/018-0.2910.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3145 - acc: 0.9044 - val_loss: 0.2910 - val_acc: 0.9187\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9083\n",
      "Epoch 00019: val_loss did not improve from 0.29095\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3000 - acc: 0.9082 - val_loss: 0.2914 - val_acc: 0.9194\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9107\n",
      "Epoch 00020: val_loss improved from 0.29095 to 0.27554, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/020-0.2755.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2884 - acc: 0.9107 - val_loss: 0.2755 - val_acc: 0.9271\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9147\n",
      "Epoch 00021: val_loss did not improve from 0.27554\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2766 - acc: 0.9146 - val_loss: 0.3668 - val_acc: 0.9036\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9198\n",
      "Epoch 00022: val_loss did not improve from 0.27554\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2631 - acc: 0.9198 - val_loss: 0.2764 - val_acc: 0.9227\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9206\n",
      "Epoch 00023: val_loss improved from 0.27554 to 0.26683, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/023-0.2668.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2558 - acc: 0.9206 - val_loss: 0.2668 - val_acc: 0.9264\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9241\n",
      "Epoch 00024: val_loss did not improve from 0.26683\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2459 - acc: 0.9241 - val_loss: 0.2758 - val_acc: 0.9203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9278\n",
      "Epoch 00025: val_loss improved from 0.26683 to 0.25378, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/025-0.2538.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2373 - acc: 0.9278 - val_loss: 0.2538 - val_acc: 0.9304\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9278\n",
      "Epoch 00026: val_loss did not improve from 0.25378\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2321 - acc: 0.9278 - val_loss: 0.2702 - val_acc: 0.9231\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9318\n",
      "Epoch 00027: val_loss improved from 0.25378 to 0.23117, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/027-0.2312.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2239 - acc: 0.9317 - val_loss: 0.2312 - val_acc: 0.9348\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9323\n",
      "Epoch 00028: val_loss did not improve from 0.23117\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2193 - acc: 0.9323 - val_loss: 0.2368 - val_acc: 0.9355\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9333\n",
      "Epoch 00029: val_loss did not improve from 0.23117\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2115 - acc: 0.9333 - val_loss: 0.2472 - val_acc: 0.9362\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9376\n",
      "Epoch 00030: val_loss did not improve from 0.23117\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2023 - acc: 0.9376 - val_loss: 0.2455 - val_acc: 0.9320\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9384\n",
      "Epoch 00031: val_loss improved from 0.23117 to 0.22053, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/031-0.2205.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1976 - acc: 0.9384 - val_loss: 0.2205 - val_acc: 0.9390\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9401\n",
      "Epoch 00032: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1936 - acc: 0.9400 - val_loss: 0.2647 - val_acc: 0.9243\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9404\n",
      "Epoch 00033: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1856 - acc: 0.9404 - val_loss: 0.2449 - val_acc: 0.9308\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9434\n",
      "Epoch 00034: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1787 - acc: 0.9433 - val_loss: 0.2762 - val_acc: 0.9245\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9426\n",
      "Epoch 00035: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1824 - acc: 0.9426 - val_loss: 0.2315 - val_acc: 0.9369\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9453\n",
      "Epoch 00036: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1713 - acc: 0.9453 - val_loss: 0.3443 - val_acc: 0.9029\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9471\n",
      "Epoch 00037: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1632 - acc: 0.9471 - val_loss: 0.2376 - val_acc: 0.9394\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9488\n",
      "Epoch 00038: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1625 - acc: 0.9487 - val_loss: 0.2692 - val_acc: 0.9336\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9495\n",
      "Epoch 00039: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1609 - acc: 0.9494 - val_loss: 0.2480 - val_acc: 0.9250\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9506\n",
      "Epoch 00040: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1571 - acc: 0.9506 - val_loss: 0.2276 - val_acc: 0.9392\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9535\n",
      "Epoch 00041: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1493 - acc: 0.9535 - val_loss: 0.2408 - val_acc: 0.9348\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9544\n",
      "Epoch 00042: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1441 - acc: 0.9544 - val_loss: 0.2309 - val_acc: 0.9420\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9530\n",
      "Epoch 00043: val_loss improved from 0.22053 to 0.21742, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/043-0.2174.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1472 - acc: 0.9529 - val_loss: 0.2174 - val_acc: 0.9408\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9528\n",
      "Epoch 00044: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1446 - acc: 0.9528 - val_loss: 0.2475 - val_acc: 0.9341\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9567\n",
      "Epoch 00045: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1375 - acc: 0.9567 - val_loss: 0.2446 - val_acc: 0.9362\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9563\n",
      "Epoch 00046: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1353 - acc: 0.9563 - val_loss: 0.2646 - val_acc: 0.9273\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9570\n",
      "Epoch 00047: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1338 - acc: 0.9570 - val_loss: 0.2435 - val_acc: 0.9341\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9596\n",
      "Epoch 00048: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1258 - acc: 0.9596 - val_loss: 0.2469 - val_acc: 0.9387\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9590\n",
      "Epoch 00049: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1286 - acc: 0.9589 - val_loss: 0.2591 - val_acc: 0.9334\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9599\n",
      "Epoch 00050: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1259 - acc: 0.9599 - val_loss: 0.2533 - val_acc: 0.9362\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9617\n",
      "Epoch 00051: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1209 - acc: 0.9617 - val_loss: 0.2366 - val_acc: 0.9378\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9622\n",
      "Epoch 00052: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1221 - acc: 0.9622 - val_loss: 0.2332 - val_acc: 0.9378\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9619\n",
      "Epoch 00053: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1183 - acc: 0.9619 - val_loss: 0.2271 - val_acc: 0.9436\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9639\n",
      "Epoch 00054: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1125 - acc: 0.9638 - val_loss: 0.2537 - val_acc: 0.9383\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9642\n",
      "Epoch 00055: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1116 - acc: 0.9642 - val_loss: 0.2261 - val_acc: 0.9450\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9640\n",
      "Epoch 00056: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1116 - acc: 0.9640 - val_loss: 0.2718 - val_acc: 0.9313\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9655\n",
      "Epoch 00057: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1092 - acc: 0.9655 - val_loss: 0.3048 - val_acc: 0.9217\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9675\n",
      "Epoch 00058: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1036 - acc: 0.9675 - val_loss: 0.2418 - val_acc: 0.9373\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9682\n",
      "Epoch 00059: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1024 - acc: 0.9682 - val_loss: 0.2185 - val_acc: 0.9404\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9673\n",
      "Epoch 00060: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1030 - acc: 0.9673 - val_loss: 0.2663 - val_acc: 0.9283\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9664\n",
      "Epoch 00061: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1060 - acc: 0.9664 - val_loss: 0.2571 - val_acc: 0.9357\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9680\n",
      "Epoch 00062: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1000 - acc: 0.9680 - val_loss: 0.2363 - val_acc: 0.9390\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9682\n",
      "Epoch 00063: val_loss did not improve from 0.21742\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0982 - acc: 0.9682 - val_loss: 0.3433 - val_acc: 0.9103\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9713\n",
      "Epoch 00064: val_loss improved from 0.21742 to 0.21213, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_7_conv_checkpoint/064-0.2121.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0911 - acc: 0.9713 - val_loss: 0.2121 - val_acc: 0.9474\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9697\n",
      "Epoch 00065: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0936 - acc: 0.9697 - val_loss: 0.2446 - val_acc: 0.9366\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9712\n",
      "Epoch 00066: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0905 - acc: 0.9713 - val_loss: 0.2231 - val_acc: 0.9457\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9722\n",
      "Epoch 00067: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0883 - acc: 0.9722 - val_loss: 0.2702 - val_acc: 0.9327\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9730\n",
      "Epoch 00068: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0840 - acc: 0.9730 - val_loss: 0.2786 - val_acc: 0.9376\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9729\n",
      "Epoch 00069: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0851 - acc: 0.9729 - val_loss: 0.2217 - val_acc: 0.9469\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9723\n",
      "Epoch 00070: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0867 - acc: 0.9723 - val_loss: 0.2371 - val_acc: 0.9425\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9732\n",
      "Epoch 00071: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0829 - acc: 0.9732 - val_loss: 0.2332 - val_acc: 0.9453\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9737\n",
      "Epoch 00072: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0841 - acc: 0.9736 - val_loss: 0.2466 - val_acc: 0.9418\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9711\n",
      "Epoch 00073: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0876 - acc: 0.9711 - val_loss: 0.2362 - val_acc: 0.9390\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9706\n",
      "Epoch 00074: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0907 - acc: 0.9706 - val_loss: 0.2292 - val_acc: 0.9434\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9765\n",
      "Epoch 00075: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0738 - acc: 0.9765 - val_loss: 0.2636 - val_acc: 0.9364\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9770\n",
      "Epoch 00076: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0750 - acc: 0.9770 - val_loss: 0.2700 - val_acc: 0.9306\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9755\n",
      "Epoch 00077: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0757 - acc: 0.9754 - val_loss: 0.2387 - val_acc: 0.9434\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9767\n",
      "Epoch 00078: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0756 - acc: 0.9767 - val_loss: 0.3982 - val_acc: 0.9052\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9767\n",
      "Epoch 00079: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0747 - acc: 0.9767 - val_loss: 0.2639 - val_acc: 0.9406\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9773\n",
      "Epoch 00080: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0694 - acc: 0.9773 - val_loss: 0.2352 - val_acc: 0.9462\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9770\n",
      "Epoch 00081: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0731 - acc: 0.9770 - val_loss: 0.2420 - val_acc: 0.9446\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9757\n",
      "Epoch 00082: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0742 - acc: 0.9757 - val_loss: 0.2383 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9762\n",
      "Epoch 00083: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0724 - acc: 0.9762 - val_loss: 0.2403 - val_acc: 0.9455\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9791\n",
      "Epoch 00084: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0671 - acc: 0.9791 - val_loss: 0.2642 - val_acc: 0.9378\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9759\n",
      "Epoch 00085: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0764 - acc: 0.9759 - val_loss: 0.2228 - val_acc: 0.9467\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9798\n",
      "Epoch 00086: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0648 - acc: 0.9798 - val_loss: 0.2448 - val_acc: 0.9394\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9782\n",
      "Epoch 00087: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0662 - acc: 0.9782 - val_loss: 0.2224 - val_acc: 0.9483\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9805\n",
      "Epoch 00088: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0616 - acc: 0.9805 - val_loss: 0.2406 - val_acc: 0.9474\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9788\n",
      "Epoch 00089: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0665 - acc: 0.9788 - val_loss: 0.2536 - val_acc: 0.9432\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9802\n",
      "Epoch 00090: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0610 - acc: 0.9802 - val_loss: 0.2476 - val_acc: 0.9436\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9790\n",
      "Epoch 00091: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0634 - acc: 0.9790 - val_loss: 0.2550 - val_acc: 0.9443\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9818\n",
      "Epoch 00092: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0586 - acc: 0.9817 - val_loss: 0.2685 - val_acc: 0.9380\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9803\n",
      "Epoch 00093: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0621 - acc: 0.9803 - val_loss: 0.2875 - val_acc: 0.9301\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9800\n",
      "Epoch 00094: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0605 - acc: 0.9800 - val_loss: 0.2393 - val_acc: 0.9441\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9813\n",
      "Epoch 00095: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0579 - acc: 0.9813 - val_loss: 0.2393 - val_acc: 0.9469\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9803\n",
      "Epoch 00096: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0605 - acc: 0.9802 - val_loss: 0.2343 - val_acc: 0.9455\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9795\n",
      "Epoch 00097: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0657 - acc: 0.9795 - val_loss: 0.2617 - val_acc: 0.9443\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9828\n",
      "Epoch 00098: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0539 - acc: 0.9827 - val_loss: 0.2428 - val_acc: 0.9448\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9805\n",
      "Epoch 00099: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0629 - acc: 0.9805 - val_loss: 0.2550 - val_acc: 0.9439\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9807\n",
      "Epoch 00100: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0585 - acc: 0.9807 - val_loss: 0.2273 - val_acc: 0.9483\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9840\n",
      "Epoch 00101: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0507 - acc: 0.9840 - val_loss: 0.2303 - val_acc: 0.9429\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9843\n",
      "Epoch 00102: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 0.2535 - val_acc: 0.9443\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9837\n",
      "Epoch 00103: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0528 - acc: 0.9837 - val_loss: 0.2288 - val_acc: 0.9436\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9842\n",
      "Epoch 00104: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0511 - acc: 0.9842 - val_loss: 0.2470 - val_acc: 0.9471\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9845\n",
      "Epoch 00105: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0492 - acc: 0.9845 - val_loss: 0.2711 - val_acc: 0.9408\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9828\n",
      "Epoch 00106: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0538 - acc: 0.9828 - val_loss: 0.2557 - val_acc: 0.9397\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9849\n",
      "Epoch 00107: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0479 - acc: 0.9849 - val_loss: 0.2759 - val_acc: 0.9359\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9844\n",
      "Epoch 00108: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0507 - acc: 0.9844 - val_loss: 0.2497 - val_acc: 0.9441\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9845\n",
      "Epoch 00109: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0490 - acc: 0.9845 - val_loss: 0.3185 - val_acc: 0.9280\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9838\n",
      "Epoch 00110: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0498 - acc: 0.9838 - val_loss: 0.2533 - val_acc: 0.9453\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9842\n",
      "Epoch 00111: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0486 - acc: 0.9842 - val_loss: 0.2607 - val_acc: 0.9469\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9851\n",
      "Epoch 00112: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0478 - acc: 0.9850 - val_loss: 0.2543 - val_acc: 0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9846\n",
      "Epoch 00113: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0484 - acc: 0.9846 - val_loss: 0.2403 - val_acc: 0.9436\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9858\n",
      "Epoch 00114: val_loss did not improve from 0.21213\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0447 - acc: 0.9858 - val_loss: 0.2807 - val_acc: 0.9399\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 635us/sample - loss: 0.2709 - acc: 0.9242\n",
      "Loss: 0.2708748114195692 Accuracy: 0.92419523\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4184 - acc: 0.2850\n",
      "Epoch 00001: val_loss improved from inf to 1.70616, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/001-1.7062.hdf5\n",
      "36805/36805 [==============================] - 62s 2ms/sample - loss: 2.4183 - acc: 0.2850 - val_loss: 1.7062 - val_acc: 0.4570\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4345 - acc: 0.5390\n",
      "Epoch 00002: val_loss improved from 1.70616 to 0.91074, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/002-0.9107.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4346 - acc: 0.5390 - val_loss: 0.9107 - val_acc: 0.7272\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0410 - acc: 0.6693\n",
      "Epoch 00003: val_loss improved from 0.91074 to 0.70458, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/003-0.7046.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0410 - acc: 0.6693 - val_loss: 0.7046 - val_acc: 0.7757\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7421\n",
      "Epoch 00004: val_loss improved from 0.70458 to 0.52126, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/004-0.5213.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8150 - acc: 0.7421 - val_loss: 0.5213 - val_acc: 0.8563\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.7960\n",
      "Epoch 00005: val_loss improved from 0.52126 to 0.40520, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/005-0.4052.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6610 - acc: 0.7960 - val_loss: 0.4052 - val_acc: 0.8898\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8302\n",
      "Epoch 00006: val_loss improved from 0.40520 to 0.37741, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/006-0.3774.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5466 - acc: 0.8303 - val_loss: 0.3774 - val_acc: 0.8945\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8534\n",
      "Epoch 00007: val_loss improved from 0.37741 to 0.35001, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/007-0.3500.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4711 - acc: 0.8534 - val_loss: 0.3500 - val_acc: 0.8949\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8705\n",
      "Epoch 00008: val_loss improved from 0.35001 to 0.31144, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/008-0.3114.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4197 - acc: 0.8705 - val_loss: 0.3114 - val_acc: 0.9136\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8847\n",
      "Epoch 00009: val_loss improved from 0.31144 to 0.28464, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/009-0.2846.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3710 - acc: 0.8847 - val_loss: 0.2846 - val_acc: 0.9178\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8945\n",
      "Epoch 00010: val_loss improved from 0.28464 to 0.24631, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/010-0.2463.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3391 - acc: 0.8945 - val_loss: 0.2463 - val_acc: 0.9299\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9028\n",
      "Epoch 00011: val_loss improved from 0.24631 to 0.23959, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/011-0.2396.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3103 - acc: 0.9028 - val_loss: 0.2396 - val_acc: 0.9331\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9102\n",
      "Epoch 00012: val_loss improved from 0.23959 to 0.22018, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/012-0.2202.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2861 - acc: 0.9102 - val_loss: 0.2202 - val_acc: 0.9408\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9158\n",
      "Epoch 00013: val_loss improved from 0.22018 to 0.21154, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/013-0.2115.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2699 - acc: 0.9158 - val_loss: 0.2115 - val_acc: 0.9411\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.9216\n",
      "Epoch 00014: val_loss did not improve from 0.21154\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2510 - acc: 0.9216 - val_loss: 0.2246 - val_acc: 0.9376\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9282\n",
      "Epoch 00015: val_loss did not improve from 0.21154\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2326 - acc: 0.9281 - val_loss: 0.2126 - val_acc: 0.9394\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9317\n",
      "Epoch 00016: val_loss improved from 0.21154 to 0.18439, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/016-0.1844.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2214 - acc: 0.9317 - val_loss: 0.1844 - val_acc: 0.9499\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9350\n",
      "Epoch 00017: val_loss did not improve from 0.18439\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2047 - acc: 0.9350 - val_loss: 0.2023 - val_acc: 0.9425\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9365\n",
      "Epoch 00018: val_loss did not improve from 0.18439\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2009 - acc: 0.9366 - val_loss: 0.1911 - val_acc: 0.9450\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9427\n",
      "Epoch 00019: val_loss did not improve from 0.18439\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1823 - acc: 0.9427 - val_loss: 0.1862 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9447\n",
      "Epoch 00020: val_loss did not improve from 0.18439\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1773 - acc: 0.9447 - val_loss: 0.1965 - val_acc: 0.9441\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9474\n",
      "Epoch 00021: val_loss improved from 0.18439 to 0.17434, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/021-0.1743.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1668 - acc: 0.9474 - val_loss: 0.1743 - val_acc: 0.9488\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9492\n",
      "Epoch 00022: val_loss did not improve from 0.17434\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1611 - acc: 0.9492 - val_loss: 0.1772 - val_acc: 0.9481\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9505\n",
      "Epoch 00023: val_loss did not improve from 0.17434\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1536 - acc: 0.9506 - val_loss: 0.1765 - val_acc: 0.9462\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9527\n",
      "Epoch 00024: val_loss improved from 0.17434 to 0.16972, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/024-0.1697.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1475 - acc: 0.9527 - val_loss: 0.1697 - val_acc: 0.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9549\n",
      "Epoch 00025: val_loss did not improve from 0.16972\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1418 - acc: 0.9548 - val_loss: 0.1893 - val_acc: 0.9492\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9562\n",
      "Epoch 00026: val_loss did not improve from 0.16972\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1377 - acc: 0.9561 - val_loss: 0.1888 - val_acc: 0.9448\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9592\n",
      "Epoch 00027: val_loss improved from 0.16972 to 0.15217, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/027-0.1522.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1319 - acc: 0.9591 - val_loss: 0.1522 - val_acc: 0.9581\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9608\n",
      "Epoch 00028: val_loss did not improve from 0.15217\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1233 - acc: 0.9608 - val_loss: 0.2018 - val_acc: 0.9415\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9604\n",
      "Epoch 00029: val_loss did not improve from 0.15217\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1222 - acc: 0.9604 - val_loss: 0.1779 - val_acc: 0.9476\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9640\n",
      "Epoch 00030: val_loss improved from 0.15217 to 0.15078, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/030-0.1508.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1161 - acc: 0.9640 - val_loss: 0.1508 - val_acc: 0.9522\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9657\n",
      "Epoch 00031: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1093 - acc: 0.9657 - val_loss: 0.1701 - val_acc: 0.9560\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9644\n",
      "Epoch 00032: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1108 - acc: 0.9644 - val_loss: 0.1612 - val_acc: 0.9506\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9670\n",
      "Epoch 00033: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1017 - acc: 0.9670 - val_loss: 0.1886 - val_acc: 0.9492\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9695\n",
      "Epoch 00034: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0989 - acc: 0.9695 - val_loss: 0.1744 - val_acc: 0.9502\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9680\n",
      "Epoch 00035: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1003 - acc: 0.9680 - val_loss: 0.1533 - val_acc: 0.9541\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9707\n",
      "Epoch 00036: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0919 - acc: 0.9707 - val_loss: 0.2194 - val_acc: 0.9357\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9705\n",
      "Epoch 00037: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0908 - acc: 0.9705 - val_loss: 0.1916 - val_acc: 0.9509\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9723\n",
      "Epoch 00038: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0883 - acc: 0.9722 - val_loss: 0.1935 - val_acc: 0.9492\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9725\n",
      "Epoch 00039: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0849 - acc: 0.9724 - val_loss: 0.1766 - val_acc: 0.9509\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9739\n",
      "Epoch 00040: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0825 - acc: 0.9739 - val_loss: 0.1558 - val_acc: 0.9518\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9749\n",
      "Epoch 00041: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0772 - acc: 0.9749 - val_loss: 0.1692 - val_acc: 0.9522\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9750\n",
      "Epoch 00042: val_loss did not improve from 0.15078\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0783 - acc: 0.9750 - val_loss: 0.1627 - val_acc: 0.9560\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9757\n",
      "Epoch 00043: val_loss improved from 0.15078 to 0.14381, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_8_conv_checkpoint/043-0.1438.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0754 - acc: 0.9757 - val_loss: 0.1438 - val_acc: 0.9620\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9786\n",
      "Epoch 00044: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0689 - acc: 0.9786 - val_loss: 0.1615 - val_acc: 0.9555\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9793\n",
      "Epoch 00045: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0675 - acc: 0.9793 - val_loss: 0.1961 - val_acc: 0.9471\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9791\n",
      "Epoch 00046: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0668 - acc: 0.9790 - val_loss: 0.1650 - val_acc: 0.9532\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9763\n",
      "Epoch 00047: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0757 - acc: 0.9763 - val_loss: 0.1590 - val_acc: 0.9588\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9803\n",
      "Epoch 00048: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0631 - acc: 0.9803 - val_loss: 0.1748 - val_acc: 0.9499\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9818\n",
      "Epoch 00049: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0595 - acc: 0.9818 - val_loss: 0.1648 - val_acc: 0.9529\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9809\n",
      "Epoch 00050: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0592 - acc: 0.9809 - val_loss: 0.1690 - val_acc: 0.9562\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9812\n",
      "Epoch 00051: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0606 - acc: 0.9812 - val_loss: 0.1676 - val_acc: 0.9529\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9790\n",
      "Epoch 00052: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0663 - acc: 0.9790 - val_loss: 0.1862 - val_acc: 0.9506\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9817\n",
      "Epoch 00053: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0567 - acc: 0.9817 - val_loss: 0.1517 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9832\n",
      "Epoch 00054: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0543 - acc: 0.9832 - val_loss: 0.1438 - val_acc: 0.9618\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9835\n",
      "Epoch 00055: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0532 - acc: 0.9835 - val_loss: 0.1670 - val_acc: 0.9578\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9845\n",
      "Epoch 00056: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0489 - acc: 0.9844 - val_loss: 0.1531 - val_acc: 0.9616\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9804\n",
      "Epoch 00057: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0602 - acc: 0.9804 - val_loss: 0.1792 - val_acc: 0.9529\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9846\n",
      "Epoch 00058: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0487 - acc: 0.9846 - val_loss: 0.1729 - val_acc: 0.9546\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9855\n",
      "Epoch 00059: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0474 - acc: 0.9855 - val_loss: 0.1487 - val_acc: 0.9592\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9858\n",
      "Epoch 00060: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0467 - acc: 0.9858 - val_loss: 0.1529 - val_acc: 0.9597\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9867\n",
      "Epoch 00061: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0423 - acc: 0.9867 - val_loss: 0.2021 - val_acc: 0.9464\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9855\n",
      "Epoch 00062: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0464 - acc: 0.9855 - val_loss: 0.2217 - val_acc: 0.9476\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9847\n",
      "Epoch 00063: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0476 - acc: 0.9847 - val_loss: 0.2325 - val_acc: 0.9441\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9881\n",
      "Epoch 00064: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0402 - acc: 0.9880 - val_loss: 0.2522 - val_acc: 0.9469\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9851\n",
      "Epoch 00065: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0482 - acc: 0.9851 - val_loss: 0.1773 - val_acc: 0.9576\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9881\n",
      "Epoch 00066: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0394 - acc: 0.9880 - val_loss: 0.1819 - val_acc: 0.9571\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9855\n",
      "Epoch 00067: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0469 - acc: 0.9855 - val_loss: 0.1627 - val_acc: 0.9585\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9880\n",
      "Epoch 00068: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0386 - acc: 0.9879 - val_loss: 0.1736 - val_acc: 0.9548\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9876\n",
      "Epoch 00069: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0398 - acc: 0.9876 - val_loss: 0.1906 - val_acc: 0.9515\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9894\n",
      "Epoch 00070: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0337 - acc: 0.9894 - val_loss: 0.1922 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9893\n",
      "Epoch 00071: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0349 - acc: 0.9892 - val_loss: 0.1796 - val_acc: 0.9592\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9868\n",
      "Epoch 00072: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0418 - acc: 0.9868 - val_loss: 0.1726 - val_acc: 0.9569\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9903\n",
      "Epoch 00073: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0321 - acc: 0.9903 - val_loss: 0.1587 - val_acc: 0.9613\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9897\n",
      "Epoch 00074: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0333 - acc: 0.9897 - val_loss: 0.1807 - val_acc: 0.9567\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9906\n",
      "Epoch 00075: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0321 - acc: 0.9906 - val_loss: 0.1835 - val_acc: 0.9560\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9916\n",
      "Epoch 00076: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0268 - acc: 0.9916 - val_loss: 0.1669 - val_acc: 0.9590\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9902\n",
      "Epoch 00077: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0312 - acc: 0.9901 - val_loss: 0.1871 - val_acc: 0.9543\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9885\n",
      "Epoch 00078: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0362 - acc: 0.9885 - val_loss: 0.2129 - val_acc: 0.9506\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9892\n",
      "Epoch 00079: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0346 - acc: 0.9892 - val_loss: 0.2076 - val_acc: 0.9506\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9920\n",
      "Epoch 00080: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0277 - acc: 0.9920 - val_loss: 0.1855 - val_acc: 0.9571\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9909\n",
      "Epoch 00081: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0291 - acc: 0.9909 - val_loss: 0.1891 - val_acc: 0.9564\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9917\n",
      "Epoch 00082: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0276 - acc: 0.9917 - val_loss: 0.2223 - val_acc: 0.9506\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9909\n",
      "Epoch 00083: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0299 - acc: 0.9909 - val_loss: 0.1607 - val_acc: 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9926\n",
      "Epoch 00084: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0249 - acc: 0.9926 - val_loss: 0.1677 - val_acc: 0.9611\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9905\n",
      "Epoch 00085: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0293 - acc: 0.9905 - val_loss: 0.1939 - val_acc: 0.9569\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9911\n",
      "Epoch 00086: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0274 - acc: 0.9911 - val_loss: 0.1918 - val_acc: 0.9562\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9927\n",
      "Epoch 00087: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0243 - acc: 0.9927 - val_loss: 0.2232 - val_acc: 0.9518\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9883\n",
      "Epoch 00088: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0367 - acc: 0.9883 - val_loss: 0.1583 - val_acc: 0.9646\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9924\n",
      "Epoch 00089: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0251 - acc: 0.9924 - val_loss: 0.1766 - val_acc: 0.9585\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9934\n",
      "Epoch 00090: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0222 - acc: 0.9934 - val_loss: 0.1773 - val_acc: 0.9564\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 00091: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0258 - acc: 0.9921 - val_loss: 0.1755 - val_acc: 0.9599\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 00092: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0244 - acc: 0.9927 - val_loss: 0.1978 - val_acc: 0.9553\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9936\n",
      "Epoch 00093: val_loss did not improve from 0.14381\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0222 - acc: 0.9936 - val_loss: 0.1621 - val_acc: 0.9625\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 681us/sample - loss: 0.1873 - acc: 0.9452\n",
      "Loss: 0.18727377580086144 Accuracy: 0.94517136\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1916 - acc: 0.3558\n",
      "Epoch 00001: val_loss improved from inf to 1.12863, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/001-1.1286.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 2.1916 - acc: 0.3558 - val_loss: 1.1286 - val_acc: 0.6587\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0659 - acc: 0.6612\n",
      "Epoch 00002: val_loss improved from 1.12863 to 0.58095, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/002-0.5810.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.0658 - acc: 0.6613 - val_loss: 0.5810 - val_acc: 0.8348\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7744\n",
      "Epoch 00003: val_loss improved from 0.58095 to 0.41425, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/003-0.4142.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.7172 - acc: 0.7744 - val_loss: 0.4142 - val_acc: 0.8735\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8295\n",
      "Epoch 00004: val_loss improved from 0.41425 to 0.32807, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/004-0.3281.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.5520 - acc: 0.8295 - val_loss: 0.3281 - val_acc: 0.9059\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.8601\n",
      "Epoch 00005: val_loss improved from 0.32807 to 0.28488, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/005-0.2849.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.4514 - acc: 0.8601 - val_loss: 0.2849 - val_acc: 0.9124\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8821\n",
      "Epoch 00006: val_loss improved from 0.28488 to 0.26054, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/006-0.2605.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3862 - acc: 0.8821 - val_loss: 0.2605 - val_acc: 0.9231\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.8980\n",
      "Epoch 00007: val_loss improved from 0.26054 to 0.21944, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/007-0.2194.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.3368 - acc: 0.8979 - val_loss: 0.2194 - val_acc: 0.9408\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.9093\n",
      "Epoch 00008: val_loss improved from 0.21944 to 0.19692, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/008-0.1969.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2977 - acc: 0.9094 - val_loss: 0.1969 - val_acc: 0.9415\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9183\n",
      "Epoch 00009: val_loss did not improve from 0.19692\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2717 - acc: 0.9182 - val_loss: 0.2421 - val_acc: 0.9306\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9246\n",
      "Epoch 00010: val_loss improved from 0.19692 to 0.18467, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/010-0.1847.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2434 - acc: 0.9246 - val_loss: 0.1847 - val_acc: 0.9476\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9300\n",
      "Epoch 00011: val_loss improved from 0.18467 to 0.16831, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/011-0.1683.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2238 - acc: 0.9300 - val_loss: 0.1683 - val_acc: 0.9527\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9368\n",
      "Epoch 00012: val_loss did not improve from 0.16831\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.2062 - acc: 0.9368 - val_loss: 0.1729 - val_acc: 0.9509\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9403\n",
      "Epoch 00013: val_loss improved from 0.16831 to 0.16440, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/013-0.1644.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1911 - acc: 0.9403 - val_loss: 0.1644 - val_acc: 0.9529\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9474\n",
      "Epoch 00014: val_loss did not improve from 0.16440\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1740 - acc: 0.9474 - val_loss: 0.1718 - val_acc: 0.9471\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9510\n",
      "Epoch 00015: val_loss improved from 0.16440 to 0.16243, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/015-0.1624.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1595 - acc: 0.9510 - val_loss: 0.1624 - val_acc: 0.9520\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9524\n",
      "Epoch 00016: val_loss improved from 0.16243 to 0.15682, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/016-0.1568.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1546 - acc: 0.9525 - val_loss: 0.1568 - val_acc: 0.9506\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9566\n",
      "Epoch 00017: val_loss improved from 0.15682 to 0.14602, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/017-0.1460.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1404 - acc: 0.9566 - val_loss: 0.1460 - val_acc: 0.9562\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9592\n",
      "Epoch 00018: val_loss improved from 0.14602 to 0.13782, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/018-0.1378.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1307 - acc: 0.9592 - val_loss: 0.1378 - val_acc: 0.9609\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9612\n",
      "Epoch 00019: val_loss did not improve from 0.13782\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1227 - acc: 0.9612 - val_loss: 0.1594 - val_acc: 0.9532\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9635\n",
      "Epoch 00020: val_loss did not improve from 0.13782\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1172 - acc: 0.9635 - val_loss: 0.1489 - val_acc: 0.9539\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9659\n",
      "Epoch 00021: val_loss did not improve from 0.13782\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1120 - acc: 0.9659 - val_loss: 0.1476 - val_acc: 0.9599\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9670\n",
      "Epoch 00022: val_loss improved from 0.13782 to 0.13567, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/022-0.1357.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.1050 - acc: 0.9670 - val_loss: 0.1357 - val_acc: 0.9590\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9700\n",
      "Epoch 00023: val_loss improved from 0.13567 to 0.12924, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_BN_9_conv_checkpoint/023-0.1292.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0969 - acc: 0.9700 - val_loss: 0.1292 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9720\n",
      "Epoch 00024: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0905 - acc: 0.9720 - val_loss: 0.1625 - val_acc: 0.9497\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9726\n",
      "Epoch 00025: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0884 - acc: 0.9726 - val_loss: 0.1313 - val_acc: 0.9611\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9759\n",
      "Epoch 00026: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0782 - acc: 0.9759 - val_loss: 0.1327 - val_acc: 0.9625\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9731\n",
      "Epoch 00027: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0854 - acc: 0.9730 - val_loss: 0.1566 - val_acc: 0.9522\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9750\n",
      "Epoch 00028: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0803 - acc: 0.9750 - val_loss: 0.1355 - val_acc: 0.9625\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9779\n",
      "Epoch 00029: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0706 - acc: 0.9779 - val_loss: 0.1318 - val_acc: 0.9641\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9799\n",
      "Epoch 00030: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0648 - acc: 0.9799 - val_loss: 0.1355 - val_acc: 0.9613\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9801\n",
      "Epoch 00031: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0640 - acc: 0.9801 - val_loss: 0.1341 - val_acc: 0.9637\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9813\n",
      "Epoch 00032: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0607 - acc: 0.9813 - val_loss: 0.1376 - val_acc: 0.9588\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9791\n",
      "Epoch 00033: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0668 - acc: 0.9790 - val_loss: 0.1402 - val_acc: 0.9602\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9803\n",
      "Epoch 00034: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0649 - acc: 0.9803 - val_loss: 0.1329 - val_acc: 0.9618\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9845\n",
      "Epoch 00035: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0525 - acc: 0.9845 - val_loss: 0.1427 - val_acc: 0.9613\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9825\n",
      "Epoch 00036: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0577 - acc: 0.9825 - val_loss: 0.1668 - val_acc: 0.9529\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9844\n",
      "Epoch 00037: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0508 - acc: 0.9844 - val_loss: 0.1428 - val_acc: 0.9627\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9860\n",
      "Epoch 00038: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0460 - acc: 0.9860 - val_loss: 0.1406 - val_acc: 0.9611\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9870\n",
      "Epoch 00039: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0437 - acc: 0.9870 - val_loss: 0.1576 - val_acc: 0.9553\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9836\n",
      "Epoch 00040: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0512 - acc: 0.9836 - val_loss: 0.1371 - val_acc: 0.9627\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9885\n",
      "Epoch 00041: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0388 - acc: 0.9885 - val_loss: 0.1739 - val_acc: 0.9532\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9871\n",
      "Epoch 00042: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0422 - acc: 0.9871 - val_loss: 0.1430 - val_acc: 0.9585\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9874\n",
      "Epoch 00043: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0419 - acc: 0.9874 - val_loss: 0.1480 - val_acc: 0.9623\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9899\n",
      "Epoch 00044: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0345 - acc: 0.9899 - val_loss: 0.1734 - val_acc: 0.9567\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9899\n",
      "Epoch 00045: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0350 - acc: 0.9899 - val_loss: 0.1593 - val_acc: 0.9599\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9879\n",
      "Epoch 00046: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0415 - acc: 0.9879 - val_loss: 0.1458 - val_acc: 0.9590\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9897\n",
      "Epoch 00047: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0334 - acc: 0.9897 - val_loss: 0.1690 - val_acc: 0.9541\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9908\n",
      "Epoch 00048: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0320 - acc: 0.9907 - val_loss: 0.1684 - val_acc: 0.9588\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9871\n",
      "Epoch 00049: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0429 - acc: 0.9871 - val_loss: 0.1429 - val_acc: 0.9611\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9888\n",
      "Epoch 00050: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0372 - acc: 0.9888 - val_loss: 0.1450 - val_acc: 0.9634\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9899\n",
      "Epoch 00051: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0320 - acc: 0.9899 - val_loss: 0.1611 - val_acc: 0.9564\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9924\n",
      "Epoch 00052: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0261 - acc: 0.9924 - val_loss: 0.1492 - val_acc: 0.9630\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9924\n",
      "Epoch 00053: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0248 - acc: 0.9924 - val_loss: 0.1873 - val_acc: 0.9515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9880\n",
      "Epoch 00054: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0379 - acc: 0.9880 - val_loss: 0.1473 - val_acc: 0.9644\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9864\n",
      "Epoch 00055: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0423 - acc: 0.9864 - val_loss: 0.1389 - val_acc: 0.9651\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9939\n",
      "Epoch 00056: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0222 - acc: 0.9939 - val_loss: 0.1360 - val_acc: 0.9648\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9941\n",
      "Epoch 00057: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0209 - acc: 0.9941 - val_loss: 0.1545 - val_acc: 0.9609\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9906\n",
      "Epoch 00058: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0308 - acc: 0.9906 - val_loss: 0.1455 - val_acc: 0.9637\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9930\n",
      "Epoch 00059: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0245 - acc: 0.9930 - val_loss: 0.1672 - val_acc: 0.9597\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9927\n",
      "Epoch 00060: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0250 - acc: 0.9927 - val_loss: 0.1518 - val_acc: 0.9618\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9927\n",
      "Epoch 00061: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0239 - acc: 0.9927 - val_loss: 0.1612 - val_acc: 0.9611\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9924\n",
      "Epoch 00062: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0258 - acc: 0.9924 - val_loss: 0.1598 - val_acc: 0.9611\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9932\n",
      "Epoch 00063: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0225 - acc: 0.9932 - val_loss: 0.1637 - val_acc: 0.9613\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9934\n",
      "Epoch 00064: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0219 - acc: 0.9934 - val_loss: 0.1531 - val_acc: 0.9606\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9918\n",
      "Epoch 00065: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0278 - acc: 0.9918 - val_loss: 0.1607 - val_acc: 0.9604\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9956\n",
      "Epoch 00066: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0172 - acc: 0.9956 - val_loss: 0.1780 - val_acc: 0.9543\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 00067: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0195 - acc: 0.9942 - val_loss: 0.1473 - val_acc: 0.9618\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9919\n",
      "Epoch 00068: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0269 - acc: 0.9919 - val_loss: 0.1478 - val_acc: 0.9646\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9953\n",
      "Epoch 00069: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0172 - acc: 0.9953 - val_loss: 0.1488 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9948\n",
      "Epoch 00070: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0183 - acc: 0.9948 - val_loss: 0.1558 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9948\n",
      "Epoch 00071: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0170 - acc: 0.9948 - val_loss: 0.1659 - val_acc: 0.9623\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9942\n",
      "Epoch 00072: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0198 - acc: 0.9942 - val_loss: 0.1874 - val_acc: 0.9585\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9943\n",
      "Epoch 00073: val_loss did not improve from 0.12924\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 0.0192 - acc: 0.9943 - val_loss: 0.1493 - val_acc: 0.9651\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 709us/sample - loss: 0.1752 - acc: 0.9458\n",
      "Loss: 0.1752357086582595 Accuracy: 0.9457944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_3_ch_32_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,664\n",
      "Trainable params: 627,344\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 681us/sample - loss: 1.0876 - acc: 0.6982\n",
      "Loss: 1.0876147106305338 Accuracy: 0.6982347\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,984\n",
      "Trainable params: 243,536\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 697us/sample - loss: 0.6712 - acc: 0.8027\n",
      "Loss: 0.6712016487666256 Accuracy: 0.8026999\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 129,616\n",
      "Trainable params: 129,040\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 743us/sample - loss: 0.4524 - acc: 0.8739\n",
      "Loss: 0.4524365982224513 Accuracy: 0.87393564\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 147,664\n",
      "Trainable params: 146,832\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 773us/sample - loss: 0.2709 - acc: 0.9242\n",
      "Loss: 0.2708748114195692 Accuracy: 0.92419523\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 201,552\n",
      "Trainable params: 200,464\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 779us/sample - loss: 0.1873 - acc: 0.9452\n",
      "Loss: 0.18727377580086144 Accuracy: 0.94517136\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,528\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 815us/sample - loss: 0.1752 - acc: 0.9458\n",
      "Loss: 0.1752357086582595 Accuracy: 0.9457944\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_3_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,664\n",
      "Trainable params: 627,344\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 774us/sample - loss: 1.8260 - acc: 0.6579\n",
      "Loss: 1.8260365321752563 Accuracy: 0.6579439\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,984\n",
      "Trainable params: 243,536\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 827us/sample - loss: 0.7472 - acc: 0.8083\n",
      "Loss: 0.7471519827471343 Accuracy: 0.80830735\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 129,616\n",
      "Trainable params: 129,040\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 837us/sample - loss: 0.5498 - acc: 0.8712\n",
      "Loss: 0.5498061100022938 Accuracy: 0.8712357\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 147,664\n",
      "Trainable params: 146,832\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 876us/sample - loss: 0.3617 - acc: 0.9178\n",
      "Loss: 0.3617114362501281 Accuracy: 0.91775703\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 201,552\n",
      "Trainable params: 200,464\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 901us/sample - loss: 0.2000 - acc: 0.9502\n",
      "Loss: 0.1999647291417805 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 273,872\n",
      "Trainable params: 272,528\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 937us/sample - loss: 0.1888 - acc: 0.9537\n",
      "Loss: 0.18880058390166715 Accuracy: 0.9536864\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
