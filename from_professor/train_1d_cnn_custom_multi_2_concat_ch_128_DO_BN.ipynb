{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 128)   512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 227456)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 75776)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303232)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 303232)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4851728     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,018,128\n",
      "Trainable params: 5,017,360\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 128)   512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 128)    512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 128)    512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 128)     512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 75776)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25216)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100992)       0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100992)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1615888     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,864,848\n",
      "Trainable params: 1,863,824\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 128)   512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 128)    512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 128)    512         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 128)     512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 256)     1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 25216)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 16640)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 41856)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 41856)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           669712      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,083,792\n",
      "Trainable params: 1,082,256\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 128)   512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 128)    512         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 128)     512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 256)     1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 256)      1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 16640)        0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 5376)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22016)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 22016)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           352272      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,095,312\n",
      "Trainable params: 1,093,264\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 128)   512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 128)    512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 128)    512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 128)     512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 256)     1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 256)      1024        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 256)      1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 5376)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1792)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7168)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 7168)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           114704      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,186,704\n",
      "Trainable params: 1,184,144\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 128)   512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 128)    512         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 128)    512         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 128)     512         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 256)     1024        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 256)      1024        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 256)      1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 256)       1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1792)         0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 512)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2304)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2304)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           36880       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,437,840\n",
      "Trainable params: 1,434,768\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1144 - acc: 0.4182\n",
      "Epoch 00001: val_loss improved from inf to 1.39507, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv_checkpoint/001-1.3951.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 2.1144 - acc: 0.4181 - val_loss: 1.3951 - val_acc: 0.5590\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3175 - acc: 0.6160\n",
      "Epoch 00002: val_loss did not improve from 1.39507\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.3175 - acc: 0.6161 - val_loss: 1.5006 - val_acc: 0.5658\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0813 - acc: 0.6844\n",
      "Epoch 00003: val_loss improved from 1.39507 to 1.19747, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv_checkpoint/003-1.1975.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.0813 - acc: 0.6844 - val_loss: 1.1975 - val_acc: 0.6774\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9263 - acc: 0.7277\n",
      "Epoch 00004: val_loss improved from 1.19747 to 1.03309, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv_checkpoint/004-1.0331.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.9263 - acc: 0.7277 - val_loss: 1.0331 - val_acc: 0.7077\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8009 - acc: 0.7618\n",
      "Epoch 00005: val_loss did not improve from 1.03309\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8009 - acc: 0.7618 - val_loss: 1.1723 - val_acc: 0.6858\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6959 - acc: 0.7910\n",
      "Epoch 00006: val_loss improved from 1.03309 to 0.95207, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv_checkpoint/006-0.9521.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.6959 - acc: 0.7910 - val_loss: 0.9521 - val_acc: 0.7365\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.8091\n",
      "Epoch 00007: val_loss did not improve from 0.95207\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.6235 - acc: 0.8090 - val_loss: 1.2346 - val_acc: 0.6778\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.8305\n",
      "Epoch 00008: val_loss did not improve from 0.95207\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.5514 - acc: 0.8305 - val_loss: 1.0818 - val_acc: 0.7249\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8489\n",
      "Epoch 00009: val_loss did not improve from 0.95207\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.4863 - acc: 0.8489 - val_loss: 1.1836 - val_acc: 0.7156\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8666\n",
      "Epoch 00010: val_loss did not improve from 0.95207\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4216 - acc: 0.8666 - val_loss: 1.1607 - val_acc: 0.7170\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8768\n",
      "Epoch 00011: val_loss improved from 0.95207 to 0.94536, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv_checkpoint/011-0.9454.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3931 - acc: 0.8768 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8886\n",
      "Epoch 00012: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3533 - acc: 0.8887 - val_loss: 1.0041 - val_acc: 0.7547\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9023\n",
      "Epoch 00013: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3061 - acc: 0.9022 - val_loss: 0.9691 - val_acc: 0.7615\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9069\n",
      "Epoch 00014: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2927 - acc: 0.9069 - val_loss: 0.9549 - val_acc: 0.7654\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9082\n",
      "Epoch 00015: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2833 - acc: 0.9082 - val_loss: 0.9797 - val_acc: 0.7741\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9223\n",
      "Epoch 00016: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2426 - acc: 0.9222 - val_loss: 1.4147 - val_acc: 0.6753\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9246\n",
      "Epoch 00017: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2365 - acc: 0.9246 - val_loss: 1.0496 - val_acc: 0.7675\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9355\n",
      "Epoch 00018: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2000 - acc: 0.9355 - val_loss: 1.2094 - val_acc: 0.7272\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9369\n",
      "Epoch 00019: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1992 - acc: 0.9369 - val_loss: 1.3428 - val_acc: 0.7154\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9372\n",
      "Epoch 00020: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1905 - acc: 0.9372 - val_loss: 1.1166 - val_acc: 0.7685\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9443\n",
      "Epoch 00021: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1774 - acc: 0.9443 - val_loss: 1.0857 - val_acc: 0.7659\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9457\n",
      "Epoch 00022: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1713 - acc: 0.9457 - val_loss: 1.0683 - val_acc: 0.7782\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9500\n",
      "Epoch 00023: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1549 - acc: 0.9500 - val_loss: 1.2570 - val_acc: 0.7391\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9524\n",
      "Epoch 00024: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1498 - acc: 0.9525 - val_loss: 1.1633 - val_acc: 0.7647\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9561\n",
      "Epoch 00025: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1429 - acc: 0.9560 - val_loss: 1.2960 - val_acc: 0.7515\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9533\n",
      "Epoch 00026: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1485 - acc: 0.9533 - val_loss: 1.1341 - val_acc: 0.7738\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9611\n",
      "Epoch 00027: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1238 - acc: 0.9611 - val_loss: 1.0568 - val_acc: 0.7918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9589\n",
      "Epoch 00028: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1293 - acc: 0.9589 - val_loss: 1.4887 - val_acc: 0.7347\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.9585\n",
      "Epoch 00029: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1312 - acc: 0.9585 - val_loss: 1.1213 - val_acc: 0.7864\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9638\n",
      "Epoch 00030: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1149 - acc: 0.9638 - val_loss: 1.2397 - val_acc: 0.7727\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9643\n",
      "Epoch 00031: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1191 - acc: 0.9644 - val_loss: 1.4406 - val_acc: 0.7412\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9651\n",
      "Epoch 00032: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1143 - acc: 0.9651 - val_loss: 1.0626 - val_acc: 0.7920\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9661\n",
      "Epoch 00033: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1060 - acc: 0.9661 - val_loss: 1.2019 - val_acc: 0.7682\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9688\n",
      "Epoch 00034: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.1009 - acc: 0.9688 - val_loss: 1.0638 - val_acc: 0.7978\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9706\n",
      "Epoch 00035: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0982 - acc: 0.9705 - val_loss: 1.3588 - val_acc: 0.7536\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9668\n",
      "Epoch 00036: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1097 - acc: 0.9668 - val_loss: 1.4187 - val_acc: 0.7524\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9723\n",
      "Epoch 00037: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0934 - acc: 0.9723 - val_loss: 1.4346 - val_acc: 0.7617\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9757\n",
      "Epoch 00038: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0858 - acc: 0.9757 - val_loss: 1.2029 - val_acc: 0.7803\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9757\n",
      "Epoch 00039: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0823 - acc: 0.9757 - val_loss: 1.1389 - val_acc: 0.7948\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9760\n",
      "Epoch 00040: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0836 - acc: 0.9760 - val_loss: 1.1982 - val_acc: 0.7941\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9756\n",
      "Epoch 00041: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0838 - acc: 0.9756 - val_loss: 1.3762 - val_acc: 0.7587\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9743\n",
      "Epoch 00042: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0852 - acc: 0.9743 - val_loss: 1.5970 - val_acc: 0.7305\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9743\n",
      "Epoch 00043: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0874 - acc: 0.9743 - val_loss: 1.2550 - val_acc: 0.7773\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9769\n",
      "Epoch 00044: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0788 - acc: 0.9769 - val_loss: 1.1885 - val_acc: 0.7943\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9772\n",
      "Epoch 00045: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0780 - acc: 0.9772 - val_loss: 1.1507 - val_acc: 0.7950\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9786\n",
      "Epoch 00046: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0751 - acc: 0.9786 - val_loss: 1.3579 - val_acc: 0.7664\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9775\n",
      "Epoch 00047: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0796 - acc: 0.9775 - val_loss: 1.2737 - val_acc: 0.7806\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9808\n",
      "Epoch 00048: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0700 - acc: 0.9808 - val_loss: 1.1493 - val_acc: 0.7973\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9776\n",
      "Epoch 00049: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0776 - acc: 0.9776 - val_loss: 1.2544 - val_acc: 0.7810\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9808\n",
      "Epoch 00050: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0694 - acc: 0.9808 - val_loss: 1.2120 - val_acc: 0.7964\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9805\n",
      "Epoch 00051: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0679 - acc: 0.9805 - val_loss: 1.1314 - val_acc: 0.8020\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9806\n",
      "Epoch 00052: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0675 - acc: 0.9806 - val_loss: 1.1321 - val_acc: 0.8071\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9820\n",
      "Epoch 00053: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0647 - acc: 0.9819 - val_loss: 1.4005 - val_acc: 0.7638\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9751\n",
      "Epoch 00054: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0878 - acc: 0.9751 - val_loss: 1.3157 - val_acc: 0.7845\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9827\n",
      "Epoch 00055: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0624 - acc: 0.9827 - val_loss: 1.0938 - val_acc: 0.8064\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9819\n",
      "Epoch 00056: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0637 - acc: 0.9819 - val_loss: 1.2051 - val_acc: 0.7880\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9846\n",
      "Epoch 00057: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0561 - acc: 0.9846 - val_loss: 1.3780 - val_acc: 0.7731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9802\n",
      "Epoch 00058: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0706 - acc: 0.9802 - val_loss: 1.1463 - val_acc: 0.8022\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9842\n",
      "Epoch 00059: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0587 - acc: 0.9842 - val_loss: 1.2574 - val_acc: 0.7908\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9828\n",
      "Epoch 00060: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0612 - acc: 0.9828 - val_loss: 1.3822 - val_acc: 0.7831\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9856\n",
      "Epoch 00061: val_loss did not improve from 0.94536\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0532 - acc: 0.9856 - val_loss: 1.1992 - val_acc: 0.8046\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.1085 - acc: 0.7113\n",
      "Loss: 1.1085146523957932 Accuracy: 0.7113188\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9517 - acc: 0.4331\n",
      "Epoch 00001: val_loss improved from inf to 1.46340, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/001-1.4634.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 1.9516 - acc: 0.4331 - val_loss: 1.4634 - val_acc: 0.5460\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1818 - acc: 0.6445\n",
      "Epoch 00002: val_loss improved from 1.46340 to 0.95416, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/002-0.9542.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.1817 - acc: 0.6445 - val_loss: 0.9542 - val_acc: 0.7065\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9502 - acc: 0.7170\n",
      "Epoch 00003: val_loss did not improve from 0.95416\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.9504 - acc: 0.7170 - val_loss: 1.0158 - val_acc: 0.7135\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7573\n",
      "Epoch 00004: val_loss improved from 0.95416 to 0.73495, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/004-0.7349.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8073 - acc: 0.7573 - val_loss: 0.7349 - val_acc: 0.7887\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7861\n",
      "Epoch 00005: val_loss did not improve from 0.73495\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7123 - acc: 0.7860 - val_loss: 0.7739 - val_acc: 0.7796\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6411 - acc: 0.8077\n",
      "Epoch 00006: val_loss improved from 0.73495 to 0.69271, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/006-0.6927.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6411 - acc: 0.8077 - val_loss: 0.6927 - val_acc: 0.8064\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.8283\n",
      "Epoch 00007: val_loss improved from 0.69271 to 0.64621, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/007-0.6462.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5773 - acc: 0.8283 - val_loss: 0.6462 - val_acc: 0.8164\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8448\n",
      "Epoch 00008: val_loss improved from 0.64621 to 0.62595, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/008-0.6260.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5181 - acc: 0.8448 - val_loss: 0.6260 - val_acc: 0.8232\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4814 - acc: 0.8527\n",
      "Epoch 00009: val_loss improved from 0.62595 to 0.61510, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/009-0.6151.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.4814 - acc: 0.8527 - val_loss: 0.6151 - val_acc: 0.8321\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.8653\n",
      "Epoch 00010: val_loss improved from 0.61510 to 0.59769, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/010-0.5977.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4364 - acc: 0.8652 - val_loss: 0.5977 - val_acc: 0.8339\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4053 - acc: 0.8766\n",
      "Epoch 00011: val_loss did not improve from 0.59769\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4052 - acc: 0.8766 - val_loss: 0.6583 - val_acc: 0.8199\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8884\n",
      "Epoch 00012: val_loss did not improve from 0.59769\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3610 - acc: 0.8884 - val_loss: 0.6876 - val_acc: 0.8167\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.8923\n",
      "Epoch 00013: val_loss did not improve from 0.59769\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3476 - acc: 0.8924 - val_loss: 0.7072 - val_acc: 0.8067\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.9008\n",
      "Epoch 00014: val_loss did not improve from 0.59769\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3167 - acc: 0.9008 - val_loss: 0.6561 - val_acc: 0.8269\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9065\n",
      "Epoch 00015: val_loss did not improve from 0.59769\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2997 - acc: 0.9065 - val_loss: 0.6777 - val_acc: 0.8279\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.9117\n",
      "Epoch 00016: val_loss improved from 0.59769 to 0.55505, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/016-0.5551.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2785 - acc: 0.9116 - val_loss: 0.5551 - val_acc: 0.8556\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9197\n",
      "Epoch 00017: val_loss did not improve from 0.55505\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2562 - acc: 0.9197 - val_loss: 0.7378 - val_acc: 0.8204\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9249\n",
      "Epoch 00018: val_loss improved from 0.55505 to 0.48821, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv_checkpoint/018-0.4882.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2349 - acc: 0.9249 - val_loss: 0.4882 - val_acc: 0.8756\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9297\n",
      "Epoch 00019: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2204 - acc: 0.9297 - val_loss: 0.5556 - val_acc: 0.8628\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9316\n",
      "Epoch 00020: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2130 - acc: 0.9316 - val_loss: 0.4980 - val_acc: 0.8696\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9383\n",
      "Epoch 00021: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1950 - acc: 0.9383 - val_loss: 0.5515 - val_acc: 0.8647\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9398\n",
      "Epoch 00022: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1889 - acc: 0.9397 - val_loss: 0.6259 - val_acc: 0.8530\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9372\n",
      "Epoch 00023: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1938 - acc: 0.9372 - val_loss: 0.5020 - val_acc: 0.8803\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9473\n",
      "Epoch 00024: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1642 - acc: 0.9473 - val_loss: 0.6344 - val_acc: 0.8456\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9497\n",
      "Epoch 00025: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1554 - acc: 0.9497 - val_loss: 0.6523 - val_acc: 0.8484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9526\n",
      "Epoch 00026: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1473 - acc: 0.9526 - val_loss: 0.5008 - val_acc: 0.8803\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9522\n",
      "Epoch 00027: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1460 - acc: 0.9522 - val_loss: 0.5862 - val_acc: 0.8640\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9549\n",
      "Epoch 00028: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1385 - acc: 0.9548 - val_loss: 0.5147 - val_acc: 0.8768\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9550\n",
      "Epoch 00029: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1415 - acc: 0.9550 - val_loss: 0.5571 - val_acc: 0.8744\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1225 - acc: 0.9602\n",
      "Epoch 00030: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1224 - acc: 0.9602 - val_loss: 0.6382 - val_acc: 0.8528\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9594\n",
      "Epoch 00031: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1263 - acc: 0.9594 - val_loss: 0.5326 - val_acc: 0.8796\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9651\n",
      "Epoch 00032: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1107 - acc: 0.9651 - val_loss: 0.5632 - val_acc: 0.8740\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9652\n",
      "Epoch 00033: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1106 - acc: 0.9651 - val_loss: 0.5212 - val_acc: 0.8852\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9601\n",
      "Epoch 00034: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1270 - acc: 0.9601 - val_loss: 0.6335 - val_acc: 0.8719\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9679\n",
      "Epoch 00035: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1031 - acc: 0.9679 - val_loss: 0.5477 - val_acc: 0.8796\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9678\n",
      "Epoch 00036: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1014 - acc: 0.9677 - val_loss: 0.6870 - val_acc: 0.8600\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9654\n",
      "Epoch 00037: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1062 - acc: 0.9654 - val_loss: 0.5235 - val_acc: 0.8912\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9712\n",
      "Epoch 00038: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0917 - acc: 0.9712 - val_loss: 0.5387 - val_acc: 0.8765\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9685\n",
      "Epoch 00039: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0953 - acc: 0.9684 - val_loss: 0.5235 - val_acc: 0.8854\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9668\n",
      "Epoch 00040: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1024 - acc: 0.9667 - val_loss: 0.5530 - val_acc: 0.8847\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9724\n",
      "Epoch 00041: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0855 - acc: 0.9724 - val_loss: 0.5361 - val_acc: 0.8845\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9739\n",
      "Epoch 00042: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0804 - acc: 0.9739 - val_loss: 0.5637 - val_acc: 0.8821\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9713\n",
      "Epoch 00043: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0875 - acc: 0.9713 - val_loss: 0.5602 - val_acc: 0.8800\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9737\n",
      "Epoch 00044: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0829 - acc: 0.9737 - val_loss: 0.6032 - val_acc: 0.8761\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9769\n",
      "Epoch 00045: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0738 - acc: 0.9769 - val_loss: 0.5717 - val_acc: 0.8796\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9783\n",
      "Epoch 00046: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0713 - acc: 0.9783 - val_loss: 0.7291 - val_acc: 0.8519\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9763\n",
      "Epoch 00047: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0749 - acc: 0.9763 - val_loss: 0.5072 - val_acc: 0.8952\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9778\n",
      "Epoch 00048: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0697 - acc: 0.9778 - val_loss: 0.5206 - val_acc: 0.8963\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9761\n",
      "Epoch 00049: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0755 - acc: 0.9761 - val_loss: 0.7102 - val_acc: 0.8602\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9751\n",
      "Epoch 00050: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0782 - acc: 0.9751 - val_loss: 0.5546 - val_acc: 0.8877\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9786\n",
      "Epoch 00051: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0681 - acc: 0.9786 - val_loss: 0.5732 - val_acc: 0.8812\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9791\n",
      "Epoch 00052: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0662 - acc: 0.9791 - val_loss: 0.5489 - val_acc: 0.8891\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9819\n",
      "Epoch 00053: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0608 - acc: 0.9819 - val_loss: 0.5329 - val_acc: 0.8863\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9802\n",
      "Epoch 00054: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0625 - acc: 0.9802 - val_loss: 0.5643 - val_acc: 0.8875\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9778\n",
      "Epoch 00055: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0678 - acc: 0.9778 - val_loss: 0.5296 - val_acc: 0.8977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9831\n",
      "Epoch 00056: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0571 - acc: 0.9831 - val_loss: 0.5649 - val_acc: 0.8852\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9789\n",
      "Epoch 00057: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0675 - acc: 0.9789 - val_loss: 0.5841 - val_acc: 0.8807\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9828\n",
      "Epoch 00058: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0562 - acc: 0.9828 - val_loss: 0.5862 - val_acc: 0.8817\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9830\n",
      "Epoch 00059: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0540 - acc: 0.9830 - val_loss: 0.5666 - val_acc: 0.8887\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9828\n",
      "Epoch 00060: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0554 - acc: 0.9828 - val_loss: 0.6022 - val_acc: 0.8880\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9833\n",
      "Epoch 00061: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0527 - acc: 0.9833 - val_loss: 0.6259 - val_acc: 0.8770\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9838\n",
      "Epoch 00062: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0539 - acc: 0.9837 - val_loss: 0.8015 - val_acc: 0.8514\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9804\n",
      "Epoch 00063: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0635 - acc: 0.9804 - val_loss: 0.5713 - val_acc: 0.8954\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9808\n",
      "Epoch 00064: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0630 - acc: 0.9808 - val_loss: 0.4999 - val_acc: 0.9022\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9875\n",
      "Epoch 00065: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0429 - acc: 0.9875 - val_loss: 0.5866 - val_acc: 0.8868\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9824\n",
      "Epoch 00066: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0573 - acc: 0.9824 - val_loss: 0.6524 - val_acc: 0.8749\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9828\n",
      "Epoch 00067: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0561 - acc: 0.9828 - val_loss: 0.5736 - val_acc: 0.8928\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9870\n",
      "Epoch 00068: val_loss did not improve from 0.48821\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0422 - acc: 0.9870 - val_loss: 0.7491 - val_acc: 0.8635\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5634 - acc: 0.8494\n",
      "Loss: 0.5634216163760034 Accuracy: 0.8494289\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8409 - acc: 0.4450\n",
      "Epoch 00001: val_loss improved from inf to 1.29598, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/001-1.2960.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 1.8410 - acc: 0.4450 - val_loss: 1.2960 - val_acc: 0.5835\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0296 - acc: 0.6819\n",
      "Epoch 00002: val_loss improved from 1.29598 to 0.82094, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/002-0.8209.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 1.0295 - acc: 0.6819 - val_loss: 0.8209 - val_acc: 0.7727\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7452 - acc: 0.7755\n",
      "Epoch 00003: val_loss improved from 0.82094 to 0.64203, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/003-0.6420.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.7453 - acc: 0.7755 - val_loss: 0.6420 - val_acc: 0.8125\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.8186\n",
      "Epoch 00004: val_loss improved from 0.64203 to 0.57907, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/004-0.5791.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.6041 - acc: 0.8186 - val_loss: 0.5791 - val_acc: 0.8341\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5036 - acc: 0.8467\n",
      "Epoch 00005: val_loss improved from 0.57907 to 0.42384, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/005-0.4238.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5035 - acc: 0.8467 - val_loss: 0.4238 - val_acc: 0.8810\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.8695\n",
      "Epoch 00006: val_loss improved from 0.42384 to 0.41571, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/006-0.4157.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.4362 - acc: 0.8695 - val_loss: 0.4157 - val_acc: 0.8826\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3783 - acc: 0.8858\n",
      "Epoch 00007: val_loss did not improve from 0.41571\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3785 - acc: 0.8857 - val_loss: 0.4503 - val_acc: 0.8693\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8953\n",
      "Epoch 00008: val_loss improved from 0.41571 to 0.38140, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/008-0.3814.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3395 - acc: 0.8953 - val_loss: 0.3814 - val_acc: 0.8928\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9089\n",
      "Epoch 00009: val_loss improved from 0.38140 to 0.37303, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/009-0.3730.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3021 - acc: 0.9088 - val_loss: 0.3730 - val_acc: 0.8984\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9124\n",
      "Epoch 00010: val_loss did not improve from 0.37303\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2793 - acc: 0.9124 - val_loss: 0.5362 - val_acc: 0.8470\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9210\n",
      "Epoch 00011: val_loss improved from 0.37303 to 0.36163, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/011-0.3616.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2568 - acc: 0.9210 - val_loss: 0.3616 - val_acc: 0.9024\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9263\n",
      "Epoch 00012: val_loss improved from 0.36163 to 0.29156, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/012-0.2916.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2336 - acc: 0.9263 - val_loss: 0.2916 - val_acc: 0.9227\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9302\n",
      "Epoch 00013: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2216 - acc: 0.9302 - val_loss: 0.3019 - val_acc: 0.9157\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9367\n",
      "Epoch 00014: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1968 - acc: 0.9367 - val_loss: 0.3485 - val_acc: 0.9040\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9388\n",
      "Epoch 00015: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1955 - acc: 0.9388 - val_loss: 0.7822 - val_acc: 0.7934\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9413\n",
      "Epoch 00016: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1839 - acc: 0.9413 - val_loss: 0.3624 - val_acc: 0.9108\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9454\n",
      "Epoch 00017: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1662 - acc: 0.9454 - val_loss: 0.3449 - val_acc: 0.9110\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9519\n",
      "Epoch 00018: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1479 - acc: 0.9518 - val_loss: 0.3069 - val_acc: 0.9250\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9522\n",
      "Epoch 00019: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1488 - acc: 0.9521 - val_loss: 0.3038 - val_acc: 0.9262\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1332 - acc: 0.9573\n",
      "Epoch 00020: val_loss did not improve from 0.29156\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1332 - acc: 0.9573 - val_loss: 0.3169 - val_acc: 0.9187\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9627\n",
      "Epoch 00021: val_loss improved from 0.29156 to 0.27479, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/021-0.2748.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1175 - acc: 0.9627 - val_loss: 0.2748 - val_acc: 0.9355\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9631\n",
      "Epoch 00022: val_loss did not improve from 0.27479\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1128 - acc: 0.9631 - val_loss: 0.3989 - val_acc: 0.9001\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9627\n",
      "Epoch 00023: val_loss did not improve from 0.27479\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1136 - acc: 0.9626 - val_loss: 0.3792 - val_acc: 0.9054\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9635\n",
      "Epoch 00024: val_loss did not improve from 0.27479\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1114 - acc: 0.9635 - val_loss: 0.3339 - val_acc: 0.9150\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9678\n",
      "Epoch 00025: val_loss did not improve from 0.27479\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0981 - acc: 0.9678 - val_loss: 0.2805 - val_acc: 0.9343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9681\n",
      "Epoch 00026: val_loss did not improve from 0.27479\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0999 - acc: 0.9681 - val_loss: 0.3019 - val_acc: 0.9224\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9708\n",
      "Epoch 00027: val_loss improved from 0.27479 to 0.26544, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv_checkpoint/027-0.2654.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0896 - acc: 0.9707 - val_loss: 0.2654 - val_acc: 0.9348\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9707\n",
      "Epoch 00028: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0904 - acc: 0.9707 - val_loss: 0.2878 - val_acc: 0.9322\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9748\n",
      "Epoch 00029: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0791 - acc: 0.9748 - val_loss: 0.3428 - val_acc: 0.9138\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9749\n",
      "Epoch 00030: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0757 - acc: 0.9749 - val_loss: 0.2750 - val_acc: 0.9278\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9773\n",
      "Epoch 00031: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0705 - acc: 0.9773 - val_loss: 0.4100 - val_acc: 0.9126\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9753\n",
      "Epoch 00032: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0748 - acc: 0.9753 - val_loss: 0.2876 - val_acc: 0.9366\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9786\n",
      "Epoch 00033: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0661 - acc: 0.9786 - val_loss: 0.2950 - val_acc: 0.9320\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9809\n",
      "Epoch 00034: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0605 - acc: 0.9809 - val_loss: 0.3302 - val_acc: 0.9262\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9793\n",
      "Epoch 00035: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0620 - acc: 0.9793 - val_loss: 0.2983 - val_acc: 0.9301\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9794\n",
      "Epoch 00036: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0636 - acc: 0.9794 - val_loss: 0.3165 - val_acc: 0.9297\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9826\n",
      "Epoch 00037: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0547 - acc: 0.9826 - val_loss: 0.3281 - val_acc: 0.9280\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9799\n",
      "Epoch 00038: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0599 - acc: 0.9799 - val_loss: 0.2980 - val_acc: 0.9348\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9839\n",
      "Epoch 00039: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0494 - acc: 0.9839 - val_loss: 0.3444 - val_acc: 0.9234\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9823\n",
      "Epoch 00040: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0533 - acc: 0.9823 - val_loss: 0.2932 - val_acc: 0.9362\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9846\n",
      "Epoch 00041: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0484 - acc: 0.9846 - val_loss: 0.3262 - val_acc: 0.9311\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9853\n",
      "Epoch 00042: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0454 - acc: 0.9853 - val_loss: 0.3291 - val_acc: 0.9271\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9838\n",
      "Epoch 00043: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0490 - acc: 0.9838 - val_loss: 0.3037 - val_acc: 0.9345\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9832\n",
      "Epoch 00044: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0493 - acc: 0.9832 - val_loss: 0.2862 - val_acc: 0.9397\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9844\n",
      "Epoch 00045: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0497 - acc: 0.9844 - val_loss: 0.3193 - val_acc: 0.9311\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9887\n",
      "Epoch 00046: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0354 - acc: 0.9887 - val_loss: 0.3320 - val_acc: 0.9301\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9853\n",
      "Epoch 00047: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0453 - acc: 0.9853 - val_loss: 0.3117 - val_acc: 0.9366\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9872\n",
      "Epoch 00048: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0400 - acc: 0.9872 - val_loss: 0.3354 - val_acc: 0.9285\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9888\n",
      "Epoch 00049: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0362 - acc: 0.9888 - val_loss: 0.3423 - val_acc: 0.9262\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9864\n",
      "Epoch 00050: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0411 - acc: 0.9864 - val_loss: 0.5271 - val_acc: 0.9057\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9845\n",
      "Epoch 00051: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0487 - acc: 0.9845 - val_loss: 0.3424 - val_acc: 0.9336\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9843\n",
      "Epoch 00052: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0479 - acc: 0.9843 - val_loss: 0.3183 - val_acc: 0.9404\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9905\n",
      "Epoch 00053: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0315 - acc: 0.9905 - val_loss: 0.2824 - val_acc: 0.9411\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9899\n",
      "Epoch 00054: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0308 - acc: 0.9899 - val_loss: 0.3296 - val_acc: 0.9304\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9858\n",
      "Epoch 00055: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0438 - acc: 0.9858 - val_loss: 0.3006 - val_acc: 0.9387\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9901\n",
      "Epoch 00056: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0328 - acc: 0.9901 - val_loss: 0.2975 - val_acc: 0.9425\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9896\n",
      "Epoch 00057: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0309 - acc: 0.9896 - val_loss: 0.3848 - val_acc: 0.9194\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9901\n",
      "Epoch 00058: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0317 - acc: 0.9901 - val_loss: 0.2919 - val_acc: 0.9357\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9911\n",
      "Epoch 00059: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0290 - acc: 0.9911 - val_loss: 0.3397 - val_acc: 0.9345\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9887\n",
      "Epoch 00060: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0336 - acc: 0.9887 - val_loss: 0.3441 - val_acc: 0.9348\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9910\n",
      "Epoch 00061: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0280 - acc: 0.9910 - val_loss: 0.3772 - val_acc: 0.9306\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9910\n",
      "Epoch 00062: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0289 - acc: 0.9910 - val_loss: 0.3581 - val_acc: 0.9266\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9908\n",
      "Epoch 00063: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0292 - acc: 0.9908 - val_loss: 0.3089 - val_acc: 0.9373\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9906\n",
      "Epoch 00064: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0304 - acc: 0.9906 - val_loss: 0.3603 - val_acc: 0.9255\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9908\n",
      "Epoch 00065: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0291 - acc: 0.9908 - val_loss: 0.3031 - val_acc: 0.9369\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9903\n",
      "Epoch 00066: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0310 - acc: 0.9903 - val_loss: 0.3050 - val_acc: 0.9399\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9882\n",
      "Epoch 00067: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0358 - acc: 0.9882 - val_loss: 0.4081 - val_acc: 0.9215\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9924\n",
      "Epoch 00068: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0246 - acc: 0.9924 - val_loss: 0.3430 - val_acc: 0.9383\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 00069: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0200 - acc: 0.9939 - val_loss: 0.3254 - val_acc: 0.9341\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9910\n",
      "Epoch 00070: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0273 - acc: 0.9910 - val_loss: 0.3306 - val_acc: 0.9359\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9917\n",
      "Epoch 00071: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0257 - acc: 0.9917 - val_loss: 0.4056 - val_acc: 0.9154\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9914\n",
      "Epoch 00072: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0262 - acc: 0.9914 - val_loss: 0.3167 - val_acc: 0.9436\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9926\n",
      "Epoch 00073: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0237 - acc: 0.9926 - val_loss: 0.4072 - val_acc: 0.9234\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9914\n",
      "Epoch 00074: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0291 - acc: 0.9914 - val_loss: 0.3772 - val_acc: 0.9266\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9923\n",
      "Epoch 00075: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0244 - acc: 0.9923 - val_loss: 0.3411 - val_acc: 0.9378\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 00076: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0236 - acc: 0.9927 - val_loss: 0.3349 - val_acc: 0.9392\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9938\n",
      "Epoch 00077: val_loss did not improve from 0.26544\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0208 - acc: 0.9938 - val_loss: 0.3114 - val_acc: 0.9387\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3215 - acc: 0.9161\n",
      "Loss: 0.32147054653672785 Accuracy: 0.91609555\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7262 - acc: 0.4736\n",
      "Epoch 00001: val_loss improved from inf to 1.09679, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/001-1.0968.hdf5\n",
      "36805/36805 [==============================] - 121s 3ms/sample - loss: 1.7261 - acc: 0.4737 - val_loss: 1.0968 - val_acc: 0.6585\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7542\n",
      "Epoch 00002: val_loss improved from 1.09679 to 0.51600, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/002-0.5160.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.7943 - acc: 0.7542 - val_loss: 0.5160 - val_acc: 0.8532\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8347\n",
      "Epoch 00003: val_loss improved from 0.51600 to 0.41393, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/003-0.4139.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.5348 - acc: 0.8347 - val_loss: 0.4139 - val_acc: 0.8854\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8770\n",
      "Epoch 00004: val_loss improved from 0.41393 to 0.38597, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/004-0.3860.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4058 - acc: 0.8769 - val_loss: 0.3860 - val_acc: 0.8887\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8964\n",
      "Epoch 00005: val_loss did not improve from 0.38597\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3365 - acc: 0.8964 - val_loss: 0.4154 - val_acc: 0.8724\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9112\n",
      "Epoch 00006: val_loss improved from 0.38597 to 0.26611, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/006-0.2661.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2861 - acc: 0.9113 - val_loss: 0.2661 - val_acc: 0.9224\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9237\n",
      "Epoch 00007: val_loss improved from 0.26611 to 0.23859, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/007-0.2386.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2462 - acc: 0.9237 - val_loss: 0.2386 - val_acc: 0.9350\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9277\n",
      "Epoch 00008: val_loss improved from 0.23859 to 0.19160, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/008-0.1916.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2295 - acc: 0.9277 - val_loss: 0.1916 - val_acc: 0.9432\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9393\n",
      "Epoch 00009: val_loss did not improve from 0.19160\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1931 - acc: 0.9394 - val_loss: 0.2077 - val_acc: 0.9401\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9450\n",
      "Epoch 00010: val_loss improved from 0.19160 to 0.16690, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/010-0.1669.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1762 - acc: 0.9450 - val_loss: 0.1669 - val_acc: 0.9499\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9500\n",
      "Epoch 00011: val_loss did not improve from 0.16690\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1574 - acc: 0.9500 - val_loss: 0.2232 - val_acc: 0.9283\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9532\n",
      "Epoch 00012: val_loss did not improve from 0.16690\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1465 - acc: 0.9532 - val_loss: 0.1669 - val_acc: 0.9506\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9577\n",
      "Epoch 00013: val_loss did not improve from 0.16690\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1356 - acc: 0.9576 - val_loss: 0.1981 - val_acc: 0.9464\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9586\n",
      "Epoch 00014: val_loss improved from 0.16690 to 0.16193, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/014-0.1619.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1295 - acc: 0.9586 - val_loss: 0.1619 - val_acc: 0.9518\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9652\n",
      "Epoch 00015: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1095 - acc: 0.9652 - val_loss: 0.1960 - val_acc: 0.9464\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9674\n",
      "Epoch 00016: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1035 - acc: 0.9674 - val_loss: 0.4980 - val_acc: 0.8493\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9700\n",
      "Epoch 00017: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0932 - acc: 0.9700 - val_loss: 0.2021 - val_acc: 0.9422\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9710\n",
      "Epoch 00018: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0889 - acc: 0.9710 - val_loss: 0.1665 - val_acc: 0.9571\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9725\n",
      "Epoch 00019: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0854 - acc: 0.9725 - val_loss: 0.1655 - val_acc: 0.9509\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9740\n",
      "Epoch 00020: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0786 - acc: 0.9740 - val_loss: 0.1701 - val_acc: 0.9497\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9734\n",
      "Epoch 00021: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0804 - acc: 0.9734 - val_loss: 0.1871 - val_acc: 0.9490\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9801\n",
      "Epoch 00022: val_loss did not improve from 0.16193\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0626 - acc: 0.9801 - val_loss: 0.2076 - val_acc: 0.9434\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9792\n",
      "Epoch 00023: val_loss improved from 0.16193 to 0.16042, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/023-0.1604.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0628 - acc: 0.9792 - val_loss: 0.1604 - val_acc: 0.9525\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9805\n",
      "Epoch 00024: val_loss did not improve from 0.16042\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0595 - acc: 0.9804 - val_loss: 0.2815 - val_acc: 0.9217\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9804\n",
      "Epoch 00025: val_loss did not improve from 0.16042\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0601 - acc: 0.9804 - val_loss: 0.2077 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9844\n",
      "Epoch 00026: val_loss did not improve from 0.16042\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0500 - acc: 0.9844 - val_loss: 0.2988 - val_acc: 0.9234\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9832\n",
      "Epoch 00027: val_loss did not improve from 0.16042\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0524 - acc: 0.9832 - val_loss: 0.1812 - val_acc: 0.9532\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9818\n",
      "Epoch 00028: val_loss improved from 0.16042 to 0.14167, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/028-0.1417.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0556 - acc: 0.9818 - val_loss: 0.1417 - val_acc: 0.9625\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9844\n",
      "Epoch 00029: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0470 - acc: 0.9844 - val_loss: 0.1506 - val_acc: 0.9604\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9870\n",
      "Epoch 00030: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0390 - acc: 0.9870 - val_loss: 0.1552 - val_acc: 0.9613\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9884\n",
      "Epoch 00031: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0371 - acc: 0.9884 - val_loss: 0.2537 - val_acc: 0.9392\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9889\n",
      "Epoch 00032: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0357 - acc: 0.9889 - val_loss: 0.2101 - val_acc: 0.9509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9885\n",
      "Epoch 00033: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0368 - acc: 0.9885 - val_loss: 0.1776 - val_acc: 0.9506\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9894\n",
      "Epoch 00034: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0327 - acc: 0.9894 - val_loss: 0.2529 - val_acc: 0.9413\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9872\n",
      "Epoch 00035: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0410 - acc: 0.9872 - val_loss: 0.2056 - val_acc: 0.9490\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9887\n",
      "Epoch 00036: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0346 - acc: 0.9887 - val_loss: 0.1547 - val_acc: 0.9644\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9909\n",
      "Epoch 00037: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0295 - acc: 0.9909 - val_loss: 0.2630 - val_acc: 0.9425\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9902\n",
      "Epoch 00038: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0303 - acc: 0.9902 - val_loss: 0.1914 - val_acc: 0.9511\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9909\n",
      "Epoch 00039: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0278 - acc: 0.9909 - val_loss: 0.1817 - val_acc: 0.9543\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9923\n",
      "Epoch 00040: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0243 - acc: 0.9923 - val_loss: 0.2313 - val_acc: 0.9471\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9914\n",
      "Epoch 00041: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0273 - acc: 0.9914 - val_loss: 0.1618 - val_acc: 0.9574\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9920\n",
      "Epoch 00042: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0244 - acc: 0.9920 - val_loss: 0.1835 - val_acc: 0.9543\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0255 - acc: 0.9918 - val_loss: 0.2016 - val_acc: 0.9539\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9924\n",
      "Epoch 00044: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0260 - acc: 0.9924 - val_loss: 0.2002 - val_acc: 0.9525\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9934\n",
      "Epoch 00045: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0206 - acc: 0.9934 - val_loss: 0.1931 - val_acc: 0.9518\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9931\n",
      "Epoch 00046: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0225 - acc: 0.9931 - val_loss: 0.1968 - val_acc: 0.9581\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9940\n",
      "Epoch 00047: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.2059 - val_acc: 0.9509\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9931\n",
      "Epoch 00048: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0225 - acc: 0.9931 - val_loss: 0.1894 - val_acc: 0.9550\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0219 - acc: 0.9932 - val_loss: 0.1800 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9953\n",
      "Epoch 00050: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0160 - acc: 0.9953 - val_loss: 0.1897 - val_acc: 0.9522\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9896\n",
      "Epoch 00051: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0322 - acc: 0.9896 - val_loss: 0.1758 - val_acc: 0.9595\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9963\n",
      "Epoch 00052: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.1723 - val_acc: 0.9611\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9927\n",
      "Epoch 00053: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0222 - acc: 0.9927 - val_loss: 0.1787 - val_acc: 0.9623\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 00054: val_loss did not improve from 0.14167\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.2288 - val_acc: 0.9490\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9889\n",
      "Epoch 00055: val_loss improved from 0.14167 to 0.14117, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv_checkpoint/055-0.1412.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0351 - acc: 0.9889 - val_loss: 0.1412 - val_acc: 0.9679\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 00056: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.1473 - val_acc: 0.9653\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.2251 - val_acc: 0.9574\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9932\n",
      "Epoch 00058: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0218 - acc: 0.9932 - val_loss: 0.1948 - val_acc: 0.9578\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9954\n",
      "Epoch 00059: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0151 - acc: 0.9954 - val_loss: 0.2497 - val_acc: 0.9427\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0133 - acc: 0.9955 - val_loss: 0.1515 - val_acc: 0.9655\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9960\n",
      "Epoch 00061: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0143 - acc: 0.9960 - val_loss: 0.1796 - val_acc: 0.9618\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9943\n",
      "Epoch 00062: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.2853 - val_acc: 0.9478\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.2414 - val_acc: 0.9529\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9964\n",
      "Epoch 00064: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0119 - acc: 0.9964 - val_loss: 0.1717 - val_acc: 0.9592\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 00065: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0141 - acc: 0.9955 - val_loss: 0.1773 - val_acc: 0.9597\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 00066: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0181 - acc: 0.9943 - val_loss: 0.2180 - val_acc: 0.9504\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00067: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0160 - acc: 0.9951 - val_loss: 0.2099 - val_acc: 0.9564\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9969\n",
      "Epoch 00068: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0099 - acc: 0.9969 - val_loss: 0.2158 - val_acc: 0.9543\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9935\n",
      "Epoch 00069: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0197 - acc: 0.9935 - val_loss: 0.2060 - val_acc: 0.9588\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9956\n",
      "Epoch 00070: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0121 - acc: 0.9956 - val_loss: 0.2061 - val_acc: 0.9529\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9971\n",
      "Epoch 00071: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0094 - acc: 0.9971 - val_loss: 0.1818 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9955\n",
      "Epoch 00072: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0131 - acc: 0.9955 - val_loss: 0.2417 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9950\n",
      "Epoch 00073: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0149 - acc: 0.9950 - val_loss: 0.1915 - val_acc: 0.9588\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9958\n",
      "Epoch 00074: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0128 - acc: 0.9958 - val_loss: 0.2185 - val_acc: 0.9588\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9961\n",
      "Epoch 00075: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0120 - acc: 0.9961 - val_loss: 0.2066 - val_acc: 0.9576\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 00076: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.1855 - val_acc: 0.9616\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9974\n",
      "Epoch 00077: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0090 - acc: 0.9973 - val_loss: 0.2536 - val_acc: 0.9471\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9935\n",
      "Epoch 00078: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.2296 - val_acc: 0.9532\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9979\n",
      "Epoch 00079: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0076 - acc: 0.9979 - val_loss: 0.1870 - val_acc: 0.9648\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9965\n",
      "Epoch 00080: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.2092 - val_acc: 0.9602\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9970\n",
      "Epoch 00081: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1958 - val_acc: 0.9555\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9922\n",
      "Epoch 00082: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0271 - acc: 0.9922 - val_loss: 0.1635 - val_acc: 0.9639\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979\n",
      "Epoch 00083: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.1796 - val_acc: 0.9632\n",
      "Epoch 84/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9968\n",
      "Epoch 00084: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0099 - acc: 0.9968 - val_loss: 0.3114 - val_acc: 0.9394\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9965\n",
      "Epoch 00085: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0106 - acc: 0.9965 - val_loss: 0.1681 - val_acc: 0.9655\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 00086: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0084 - acc: 0.9975 - val_loss: 0.1929 - val_acc: 0.9632\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9942\n",
      "Epoch 00087: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0186 - acc: 0.9942 - val_loss: 0.1903 - val_acc: 0.9627\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9973\n",
      "Epoch 00088: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0093 - acc: 0.9973 - val_loss: 0.2010 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9951\n",
      "Epoch 00089: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0169 - acc: 0.9951 - val_loss: 0.2060 - val_acc: 0.9581\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9944\n",
      "Epoch 00090: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0182 - acc: 0.9944 - val_loss: 0.1911 - val_acc: 0.9618\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9947\n",
      "Epoch 00091: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0183 - acc: 0.9946 - val_loss: 0.1695 - val_acc: 0.9613\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
      "Epoch 00092: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0137 - acc: 0.9960 - val_loss: 0.1592 - val_acc: 0.9646\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9983\n",
      "Epoch 00093: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1474 - val_acc: 0.9674\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9975\n",
      "Epoch 00094: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0077 - acc: 0.9975 - val_loss: 0.2322 - val_acc: 0.9527\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0072 - acc: 0.9976 - val_loss: 0.2244 - val_acc: 0.9567\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9965\n",
      "Epoch 00096: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0105 - acc: 0.9965 - val_loss: 0.1965 - val_acc: 0.9578\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9967\n",
      "Epoch 00097: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.2121 - val_acc: 0.9529\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9945\n",
      "Epoch 00098: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0196 - acc: 0.9945 - val_loss: 0.1762 - val_acc: 0.9630\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9983\n",
      "Epoch 00099: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0063 - acc: 0.9983 - val_loss: 0.1770 - val_acc: 0.9627\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9977\n",
      "Epoch 00100: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1810 - val_acc: 0.9639\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 00101: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0109 - acc: 0.9966 - val_loss: 0.2398 - val_acc: 0.9509\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9961\n",
      "Epoch 00102: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 0.1783 - val_acc: 0.9616\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 00103: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.1988 - val_acc: 0.9604\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9970\n",
      "Epoch 00104: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.2514 - val_acc: 0.9567\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 00105: val_loss did not improve from 0.14117\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 0.1767 - val_acc: 0.9616\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2067 - acc: 0.9512\n",
      "Loss: 0.20666624847122567 Accuracy: 0.95119417\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5003 - acc: 0.5481\n",
      "Epoch 00001: val_loss improved from inf to 1.00943, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/001-1.0094.hdf5\n",
      "36805/36805 [==============================] - 123s 3ms/sample - loss: 1.5003 - acc: 0.5480 - val_loss: 1.0094 - val_acc: 0.6834\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8123\n",
      "Epoch 00002: val_loss improved from 1.00943 to 0.34519, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/002-0.3452.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.6053 - acc: 0.8123 - val_loss: 0.3452 - val_acc: 0.8919\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8741\n",
      "Epoch 00003: val_loss improved from 0.34519 to 0.26053, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/003-0.2605.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4041 - acc: 0.8741 - val_loss: 0.2605 - val_acc: 0.9222\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9024\n",
      "Epoch 00004: val_loss did not improve from 0.26053\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3125 - acc: 0.9025 - val_loss: 0.2996 - val_acc: 0.9045\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9198\n",
      "Epoch 00005: val_loss improved from 0.26053 to 0.19252, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/005-0.1925.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2560 - acc: 0.9198 - val_loss: 0.1925 - val_acc: 0.9399\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9288\n",
      "Epoch 00006: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2267 - acc: 0.9288 - val_loss: 0.1966 - val_acc: 0.9399\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9381\n",
      "Epoch 00007: val_loss did not improve from 0.19252\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1927 - acc: 0.9381 - val_loss: 0.2146 - val_acc: 0.9373\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9451\n",
      "Epoch 00008: val_loss improved from 0.19252 to 0.17121, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/008-0.1712.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1746 - acc: 0.9451 - val_loss: 0.1712 - val_acc: 0.9492\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9515\n",
      "Epoch 00009: val_loss improved from 0.17121 to 0.14278, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/009-0.1428.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1508 - acc: 0.9515 - val_loss: 0.1428 - val_acc: 0.9553\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9583\n",
      "Epoch 00010: val_loss did not improve from 0.14278\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1327 - acc: 0.9583 - val_loss: 0.1487 - val_acc: 0.9536\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9597\n",
      "Epoch 00011: val_loss did not improve from 0.14278\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1236 - acc: 0.9597 - val_loss: 0.1933 - val_acc: 0.9399\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9643\n",
      "Epoch 00012: val_loss did not improve from 0.14278\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1134 - acc: 0.9643 - val_loss: 0.1536 - val_acc: 0.9534\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9692\n",
      "Epoch 00013: val_loss improved from 0.14278 to 0.14112, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/013-0.1411.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0963 - acc: 0.9692 - val_loss: 0.1411 - val_acc: 0.9574\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9715\n",
      "Epoch 00014: val_loss did not improve from 0.14112\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0893 - acc: 0.9716 - val_loss: 0.1503 - val_acc: 0.9515\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9728\n",
      "Epoch 00015: val_loss did not improve from 0.14112\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0849 - acc: 0.9728 - val_loss: 0.1598 - val_acc: 0.9541\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9765\n",
      "Epoch 00016: val_loss did not improve from 0.14112\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0741 - acc: 0.9765 - val_loss: 0.1415 - val_acc: 0.9569\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9730\n",
      "Epoch 00017: val_loss did not improve from 0.14112\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0842 - acc: 0.9730 - val_loss: 0.1804 - val_acc: 0.9418\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9789\n",
      "Epoch 00018: val_loss did not improve from 0.14112\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0655 - acc: 0.9789 - val_loss: 0.1486 - val_acc: 0.9560\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9826\n",
      "Epoch 00019: val_loss improved from 0.14112 to 0.13920, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/019-0.1392.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0548 - acc: 0.9826 - val_loss: 0.1392 - val_acc: 0.9595\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9817\n",
      "Epoch 00020: val_loss did not improve from 0.13920\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0580 - acc: 0.9817 - val_loss: 0.1408 - val_acc: 0.9571\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9841\n",
      "Epoch 00021: val_loss did not improve from 0.13920\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0501 - acc: 0.9841 - val_loss: 0.1698 - val_acc: 0.9527\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9852\n",
      "Epoch 00022: val_loss improved from 0.13920 to 0.13217, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/022-0.1322.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0463 - acc: 0.9852 - val_loss: 0.1322 - val_acc: 0.9602\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9859\n",
      "Epoch 00023: val_loss did not improve from 0.13217\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0451 - acc: 0.9859 - val_loss: 0.1398 - val_acc: 0.9625\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9849\n",
      "Epoch 00024: val_loss did not improve from 0.13217\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0471 - acc: 0.9849 - val_loss: 0.2245 - val_acc: 0.9399\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9860\n",
      "Epoch 00025: val_loss did not improve from 0.13217\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0431 - acc: 0.9859 - val_loss: 0.1608 - val_acc: 0.9515\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9850\n",
      "Epoch 00026: val_loss improved from 0.13217 to 0.12649, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/026-0.1265.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0455 - acc: 0.9850 - val_loss: 0.1265 - val_acc: 0.9646\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9866\n",
      "Epoch 00027: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0413 - acc: 0.9866 - val_loss: 0.1537 - val_acc: 0.9623\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9921\n",
      "Epoch 00028: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0277 - acc: 0.9921 - val_loss: 0.1476 - val_acc: 0.9634\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9906\n",
      "Epoch 00029: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0287 - acc: 0.9906 - val_loss: 0.1543 - val_acc: 0.9595\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9916\n",
      "Epoch 00030: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0266 - acc: 0.9916 - val_loss: 0.1868 - val_acc: 0.9488\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9904\n",
      "Epoch 00031: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0293 - acc: 0.9903 - val_loss: 0.2008 - val_acc: 0.9499\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9892\n",
      "Epoch 00032: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0339 - acc: 0.9892 - val_loss: 0.2359 - val_acc: 0.9436\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9913\n",
      "Epoch 00033: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0284 - acc: 0.9913 - val_loss: 0.1491 - val_acc: 0.9590\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9927\n",
      "Epoch 00034: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0237 - acc: 0.9927 - val_loss: 0.2132 - val_acc: 0.9450\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9876\n",
      "Epoch 00035: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0377 - acc: 0.9876 - val_loss: 0.1784 - val_acc: 0.9518\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9939\n",
      "Epoch 00036: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0201 - acc: 0.9939 - val_loss: 0.1284 - val_acc: 0.9683\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9946\n",
      "Epoch 00037: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0186 - acc: 0.9946 - val_loss: 0.1288 - val_acc: 0.9683\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9926\n",
      "Epoch 00038: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0235 - acc: 0.9926 - val_loss: 0.1293 - val_acc: 0.9648\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9946\n",
      "Epoch 00039: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0184 - acc: 0.9946 - val_loss: 0.1474 - val_acc: 0.9646\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 00040: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.2051 - val_acc: 0.9513\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9935\n",
      "Epoch 00041: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0217 - acc: 0.9934 - val_loss: 0.1318 - val_acc: 0.9648\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9912\n",
      "Epoch 00042: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0268 - acc: 0.9912 - val_loss: 0.1326 - val_acc: 0.9648\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9952\n",
      "Epoch 00043: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0152 - acc: 0.9952 - val_loss: 0.1817 - val_acc: 0.9557\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9964\n",
      "Epoch 00044: val_loss did not improve from 0.12649\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0119 - acc: 0.9964 - val_loss: 0.1372 - val_acc: 0.9676\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 00045: val_loss improved from 0.12649 to 0.12271, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv_checkpoint/045-0.1227.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.1227 - val_acc: 0.9713\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9945\n",
      "Epoch 00046: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0182 - acc: 0.9945 - val_loss: 0.2191 - val_acc: 0.9483\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9953\n",
      "Epoch 00047: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0157 - acc: 0.9953 - val_loss: 0.1527 - val_acc: 0.9630\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 00048: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0150 - acc: 0.9951 - val_loss: 0.1783 - val_acc: 0.9616\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9954\n",
      "Epoch 00049: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0155 - acc: 0.9954 - val_loss: 0.2586 - val_acc: 0.9513\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9955\n",
      "Epoch 00050: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0147 - acc: 0.9955 - val_loss: 0.1580 - val_acc: 0.9595\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 00051: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0135 - acc: 0.9960 - val_loss: 0.1605 - val_acc: 0.9637\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9952\n",
      "Epoch 00052: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0164 - acc: 0.9952 - val_loss: 0.1401 - val_acc: 0.9672\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9965\n",
      "Epoch 00053: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0111 - acc: 0.9965 - val_loss: 0.1619 - val_acc: 0.9644\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9925\n",
      "Epoch 00054: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0231 - acc: 0.9925 - val_loss: 0.1383 - val_acc: 0.9706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9969\n",
      "Epoch 00055: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0112 - acc: 0.9969 - val_loss: 0.2231 - val_acc: 0.9546\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9958\n",
      "Epoch 00056: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0137 - acc: 0.9958 - val_loss: 0.1489 - val_acc: 0.9637\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9963\n",
      "Epoch 00057: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0126 - acc: 0.9963 - val_loss: 0.1572 - val_acc: 0.9679\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9970\n",
      "Epoch 00058: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0093 - acc: 0.9970 - val_loss: 0.1498 - val_acc: 0.9681\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9957\n",
      "Epoch 00059: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0143 - acc: 0.9957 - val_loss: 0.1599 - val_acc: 0.9606\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9964\n",
      "Epoch 00060: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0111 - acc: 0.9964 - val_loss: 0.1839 - val_acc: 0.9618\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9960\n",
      "Epoch 00061: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0128 - acc: 0.9960 - val_loss: 0.1657 - val_acc: 0.9630\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9949\n",
      "Epoch 00062: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0178 - acc: 0.9949 - val_loss: 0.1366 - val_acc: 0.9704\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9976\n",
      "Epoch 00063: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1460 - val_acc: 0.9660\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00064: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.2039 - val_acc: 0.9567\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00065: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1763 - val_acc: 0.9599\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9962\n",
      "Epoch 00066: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0117 - acc: 0.9962 - val_loss: 0.1657 - val_acc: 0.9665\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9962\n",
      "Epoch 00067: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9962 - val_loss: 0.1659 - val_acc: 0.9653\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9971\n",
      "Epoch 00068: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0094 - acc: 0.9971 - val_loss: 0.1650 - val_acc: 0.9644\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9970\n",
      "Epoch 00069: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0091 - acc: 0.9970 - val_loss: 0.1483 - val_acc: 0.9697\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9982\n",
      "Epoch 00070: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0068 - acc: 0.9982 - val_loss: 0.3458 - val_acc: 0.9338\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9940\n",
      "Epoch 00071: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0214 - acc: 0.9940 - val_loss: 0.1337 - val_acc: 0.9706\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9956\n",
      "Epoch 00072: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0148 - acc: 0.9956 - val_loss: 0.1434 - val_acc: 0.9686\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1363 - val_acc: 0.9695\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9984\n",
      "Epoch 00074: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0051 - acc: 0.9984 - val_loss: 0.1452 - val_acc: 0.9679\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 00075: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0074 - acc: 0.9978 - val_loss: 0.1539 - val_acc: 0.9655\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9977\n",
      "Epoch 00076: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0081 - acc: 0.9977 - val_loss: 0.1993 - val_acc: 0.9539\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9979\n",
      "Epoch 00077: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0075 - acc: 0.9979 - val_loss: 0.1506 - val_acc: 0.9690\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9970\n",
      "Epoch 00078: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0092 - acc: 0.9970 - val_loss: 0.1736 - val_acc: 0.9625\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9976\n",
      "Epoch 00079: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1831 - val_acc: 0.9653\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9967\n",
      "Epoch 00080: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0116 - acc: 0.9967 - val_loss: 0.1673 - val_acc: 0.9651\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9980\n",
      "Epoch 00081: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0066 - acc: 0.9980 - val_loss: 0.1256 - val_acc: 0.9716\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9973\n",
      "Epoch 00082: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0088 - acc: 0.9973 - val_loss: 0.1543 - val_acc: 0.9690\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9976\n",
      "Epoch 00083: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0075 - acc: 0.9976 - val_loss: 0.1642 - val_acc: 0.9662\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9982\n",
      "Epoch 00084: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0060 - acc: 0.9982 - val_loss: 0.1775 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9973\n",
      "Epoch 00085: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0079 - acc: 0.9973 - val_loss: 0.1466 - val_acc: 0.9700\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9983\n",
      "Epoch 00086: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0058 - acc: 0.9983 - val_loss: 0.2079 - val_acc: 0.9623\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9969\n",
      "Epoch 00087: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0090 - acc: 0.9969 - val_loss: 0.1781 - val_acc: 0.9623\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968\n",
      "Epoch 00088: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0110 - acc: 0.9968 - val_loss: 0.1692 - val_acc: 0.9641\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 00089: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1429 - val_acc: 0.9690\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 00090: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0078 - acc: 0.9975 - val_loss: 0.1881 - val_acc: 0.9599\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00091: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.1319 - val_acc: 0.9718\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9972\n",
      "Epoch 00092: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.1942 - val_acc: 0.9618\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9979\n",
      "Epoch 00093: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1551 - val_acc: 0.9674\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 00094: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.1861 - val_acc: 0.9613\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9989\n",
      "Epoch 00095: val_loss did not improve from 0.12271\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2280 - val_acc: 0.9560\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1584 - acc: 0.9589\n",
      "Loss: 0.15838427865705604 Accuracy: 0.9588785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_128_DO_BN'\n",
    "\n",
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100992)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,864,848\n",
      "Trainable params: 1,863,824\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.1085 - acc: 0.7113\n",
      "Loss: 1.1085146523957932 Accuracy: 0.7113188\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 41856)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,083,792\n",
      "Trainable params: 1,082,256\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5634 - acc: 0.8494\n",
      "Loss: 0.5634216163760034 Accuracy: 0.8494289\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 22016)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,095,312\n",
      "Trainable params: 1,093,264\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3215 - acc: 0.9161\n",
      "Loss: 0.32147054653672785 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 7168)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,186,704\n",
      "Trainable params: 1,184,144\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2067 - acc: 0.9512\n",
      "Loss: 0.20666624847122567 Accuracy: 0.95119417\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2304)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,437,840\n",
      "Trainable params: 1,434,768\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1584 - acc: 0.9589\n",
      "Loss: 0.15838427865705604 Accuracy: 0.9588785\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_128_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 128)   512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 128)    512         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 128)    512         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 592, 128)     512         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100992)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,864,848\n",
      "Trainable params: 1,863,824\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.4251 - acc: 0.7664\n",
      "Loss: 1.4250702149152508 Accuracy: 0.76635516\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 16000, 128)   512         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 5333, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 1777, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 592, 128)     512         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 197, 256)     1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 41856)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,083,792\n",
      "Trainable params: 1,082,256\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9466 - acc: 0.8233\n",
      "Loss: 0.9466255094480787 Accuracy: 0.82326066\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 16000, 128)   512         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 5333, 128)    512         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 1777, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 592, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 197, 256)     1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 65, 256)      1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 22016)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,095,312\n",
      "Trainable params: 1,093,264\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3998 - acc: 0.9227\n",
      "Loss: 0.39980280397523693 Accuracy: 0.9227414\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 128)   512         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 128)    512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 128)    512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 128)     512         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 256)     1024        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 65, 256)      1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 21, 256)      1024        conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 7168)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,186,704\n",
      "Trainable params: 1,184,144\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1995 - acc: 0.9591\n",
      "Loss: 0.19946929444323186 Accuracy: 0.9590862\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 16000, 128)   512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 5333, 128)    512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 1777, 128)    512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 592, 128)     512         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 197, 256)     1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 65, 256)      1024        conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 21, 256)      1024        conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 7, 256)       1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2304)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,437,840\n",
      "Trainable params: 1,434,768\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2456 - acc: 0.9508\n",
      "Loss: 0.24562989976527874 Accuracy: 0.95077884\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
