{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 64\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/4))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,400\n",
      "Trainable params: 16,384,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,481,936\n",
      "Trainable params: 5,481,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,136\n",
      "Trainable params: 1,861,136\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 668,240\n",
      "Trainable params: 668,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 173,168\n",
      "Trainable params: 173,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 110,736\n",
      "Trainable params: 110,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 93,360\n",
      "Trainable params: 93,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 91,344\n",
      "Trainable params: 91,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 90,848\n",
      "Trainable params: 90,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5397 - acc: 0.1593\n",
      "Epoch 00001: val_loss improved from inf to 1.94637, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/001-1.9464.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.5397 - acc: 0.1594 - val_loss: 1.9464 - val_acc: 0.3951\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8601 - acc: 0.3884\n",
      "Epoch 00002: val_loss improved from 1.94637 to 1.54764, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/002-1.5476.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.8601 - acc: 0.3884 - val_loss: 1.5476 - val_acc: 0.5178\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6196 - acc: 0.4664\n",
      "Epoch 00003: val_loss improved from 1.54764 to 1.40907, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/003-1.4091.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.6196 - acc: 0.4665 - val_loss: 1.4091 - val_acc: 0.5635\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5018 - acc: 0.5070\n",
      "Epoch 00004: val_loss improved from 1.40907 to 1.32147, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/004-1.3215.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.5019 - acc: 0.5070 - val_loss: 1.3215 - val_acc: 0.5807\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4101 - acc: 0.5371\n",
      "Epoch 00005: val_loss improved from 1.32147 to 1.22083, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/005-1.2208.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4101 - acc: 0.5371 - val_loss: 1.2208 - val_acc: 0.6271\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3308 - acc: 0.5685\n",
      "Epoch 00006: val_loss improved from 1.22083 to 1.16100, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/006-1.1610.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.3308 - acc: 0.5685 - val_loss: 1.1610 - val_acc: 0.6492\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2515 - acc: 0.5974\n",
      "Epoch 00007: val_loss improved from 1.16100 to 1.09889, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/007-1.0989.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2515 - acc: 0.5974 - val_loss: 1.0989 - val_acc: 0.6576\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1793 - acc: 0.6269\n",
      "Epoch 00008: val_loss improved from 1.09889 to 1.01105, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/008-1.0111.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1792 - acc: 0.6269 - val_loss: 1.0111 - val_acc: 0.6904\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1098 - acc: 0.6530\n",
      "Epoch 00009: val_loss improved from 1.01105 to 0.96203, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/009-0.9620.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.1097 - acc: 0.6530 - val_loss: 0.9620 - val_acc: 0.7063\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0502 - acc: 0.6745\n",
      "Epoch 00010: val_loss improved from 0.96203 to 0.89771, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/010-0.8977.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0503 - acc: 0.6745 - val_loss: 0.8977 - val_acc: 0.7375\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9980 - acc: 0.6917\n",
      "Epoch 00011: val_loss improved from 0.89771 to 0.84893, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/011-0.8489.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9979 - acc: 0.6917 - val_loss: 0.8489 - val_acc: 0.7480\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9442 - acc: 0.7124\n",
      "Epoch 00012: val_loss improved from 0.84893 to 0.81392, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/012-0.8139.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9443 - acc: 0.7124 - val_loss: 0.8139 - val_acc: 0.7549\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9051 - acc: 0.7242\n",
      "Epoch 00013: val_loss improved from 0.81392 to 0.76233, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/013-0.7623.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.9050 - acc: 0.7242 - val_loss: 0.7623 - val_acc: 0.7785\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8602 - acc: 0.7379\n",
      "Epoch 00014: val_loss did not improve from 0.76233\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8602 - acc: 0.7379 - val_loss: 0.7957 - val_acc: 0.7708\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7477\n",
      "Epoch 00015: val_loss did not improve from 0.76233\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8342 - acc: 0.7477 - val_loss: 0.8077 - val_acc: 0.7589\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7976 - acc: 0.7596\n",
      "Epoch 00016: val_loss improved from 0.76233 to 0.68863, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/016-0.6886.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7976 - acc: 0.7597 - val_loss: 0.6886 - val_acc: 0.7987\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7701 - acc: 0.7679\n",
      "Epoch 00017: val_loss improved from 0.68863 to 0.64887, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/017-0.6489.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7701 - acc: 0.7679 - val_loss: 0.6489 - val_acc: 0.8095\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7783\n",
      "Epoch 00018: val_loss improved from 0.64887 to 0.63333, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/018-0.6333.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7438 - acc: 0.7783 - val_loss: 0.6333 - val_acc: 0.8178\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7181 - acc: 0.7844\n",
      "Epoch 00019: val_loss did not improve from 0.63333\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7181 - acc: 0.7843 - val_loss: 0.6413 - val_acc: 0.8148\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.7899\n",
      "Epoch 00020: val_loss did not improve from 0.63333\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6937 - acc: 0.7899 - val_loss: 0.6408 - val_acc: 0.8109\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6870 - acc: 0.7943\n",
      "Epoch 00021: val_loss improved from 0.63333 to 0.60669, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/021-0.6067.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6870 - acc: 0.7943 - val_loss: 0.6067 - val_acc: 0.8274\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6605 - acc: 0.8026\n",
      "Epoch 00022: val_loss improved from 0.60669 to 0.57290, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/022-0.5729.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6605 - acc: 0.8026 - val_loss: 0.5729 - val_acc: 0.8321\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.8088\n",
      "Epoch 00023: val_loss improved from 0.57290 to 0.54757, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/023-0.5476.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6392 - acc: 0.8089 - val_loss: 0.5476 - val_acc: 0.8467\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6282 - acc: 0.8111\n",
      "Epoch 00024: val_loss did not improve from 0.54757\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6281 - acc: 0.8111 - val_loss: 0.5809 - val_acc: 0.8330\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6133 - acc: 0.8181\n",
      "Epoch 00025: val_loss improved from 0.54757 to 0.54357, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/025-0.5436.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6132 - acc: 0.8181 - val_loss: 0.5436 - val_acc: 0.8509\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.8203\n",
      "Epoch 00026: val_loss improved from 0.54357 to 0.52038, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/026-0.5204.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5997 - acc: 0.8203 - val_loss: 0.5204 - val_acc: 0.8528\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8234\n",
      "Epoch 00027: val_loss improved from 0.52038 to 0.51152, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/027-0.5115.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5928 - acc: 0.8234 - val_loss: 0.5115 - val_acc: 0.8588\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5757 - acc: 0.8278\n",
      "Epoch 00028: val_loss improved from 0.51152 to 0.49930, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/028-0.4993.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5757 - acc: 0.8278 - val_loss: 0.4993 - val_acc: 0.8579\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8299\n",
      "Epoch 00029: val_loss improved from 0.49930 to 0.48988, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/029-0.4899.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5637 - acc: 0.8299 - val_loss: 0.4899 - val_acc: 0.8565\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5523 - acc: 0.8335\n",
      "Epoch 00030: val_loss did not improve from 0.48988\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5523 - acc: 0.8335 - val_loss: 0.5265 - val_acc: 0.8500\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8355\n",
      "Epoch 00031: val_loss improved from 0.48988 to 0.48399, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/031-0.4840.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5483 - acc: 0.8355 - val_loss: 0.4840 - val_acc: 0.8602\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8405\n",
      "Epoch 00032: val_loss did not improve from 0.48399\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5314 - acc: 0.8405 - val_loss: 0.4876 - val_acc: 0.8621\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8424\n",
      "Epoch 00033: val_loss improved from 0.48399 to 0.48319, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/033-0.4832.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5246 - acc: 0.8424 - val_loss: 0.4832 - val_acc: 0.8633\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8457\n",
      "Epoch 00034: val_loss improved from 0.48319 to 0.45493, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/034-0.4549.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5170 - acc: 0.8457 - val_loss: 0.4549 - val_acc: 0.8733\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8467\n",
      "Epoch 00035: val_loss did not improve from 0.45493\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5115 - acc: 0.8467 - val_loss: 0.4758 - val_acc: 0.8649\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5041 - acc: 0.8495\n",
      "Epoch 00036: val_loss did not improve from 0.45493\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5040 - acc: 0.8495 - val_loss: 0.4802 - val_acc: 0.8612\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8509\n",
      "Epoch 00037: val_loss improved from 0.45493 to 0.45210, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/037-0.4521.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4970 - acc: 0.8509 - val_loss: 0.4521 - val_acc: 0.8712\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8529\n",
      "Epoch 00038: val_loss did not improve from 0.45210\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4911 - acc: 0.8529 - val_loss: 0.4531 - val_acc: 0.8733\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4843 - acc: 0.8552\n",
      "Epoch 00039: val_loss did not improve from 0.45210\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4843 - acc: 0.8552 - val_loss: 0.4566 - val_acc: 0.8712\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8564\n",
      "Epoch 00040: val_loss improved from 0.45210 to 0.44267, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/040-0.4427.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4773 - acc: 0.8564 - val_loss: 0.4427 - val_acc: 0.8758\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4678 - acc: 0.8598\n",
      "Epoch 00041: val_loss did not improve from 0.44267\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4678 - acc: 0.8598 - val_loss: 0.4441 - val_acc: 0.8786\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4643 - acc: 0.8609\n",
      "Epoch 00042: val_loss did not improve from 0.44267\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4642 - acc: 0.8609 - val_loss: 0.4509 - val_acc: 0.8749\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8617\n",
      "Epoch 00043: val_loss did not improve from 0.44267\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4542 - acc: 0.8617 - val_loss: 0.4432 - val_acc: 0.8772\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8638\n",
      "Epoch 00044: val_loss did not improve from 0.44267\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4499 - acc: 0.8638 - val_loss: 0.4476 - val_acc: 0.8768\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4458 - acc: 0.8651\n",
      "Epoch 00045: val_loss improved from 0.44267 to 0.41787, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/045-0.4179.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4457 - acc: 0.8651 - val_loss: 0.4179 - val_acc: 0.8803\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.8665\n",
      "Epoch 00046: val_loss did not improve from 0.41787\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4407 - acc: 0.8666 - val_loss: 0.4269 - val_acc: 0.8796\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4347 - acc: 0.8668\n",
      "Epoch 00047: val_loss did not improve from 0.41787\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4346 - acc: 0.8669 - val_loss: 0.4262 - val_acc: 0.8765\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8712\n",
      "Epoch 00048: val_loss did not improve from 0.41787\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4263 - acc: 0.8713 - val_loss: 0.4277 - val_acc: 0.8835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8740\n",
      "Epoch 00049: val_loss improved from 0.41787 to 0.40174, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/049-0.4017.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4244 - acc: 0.8740 - val_loss: 0.4017 - val_acc: 0.8882\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8702\n",
      "Epoch 00050: val_loss did not improve from 0.40174\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4235 - acc: 0.8702 - val_loss: 0.4098 - val_acc: 0.8894\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8746\n",
      "Epoch 00051: val_loss improved from 0.40174 to 0.39408, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/051-0.3941.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4130 - acc: 0.8746 - val_loss: 0.3941 - val_acc: 0.8924\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8761\n",
      "Epoch 00052: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4098 - acc: 0.8761 - val_loss: 0.4120 - val_acc: 0.8852\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8763\n",
      "Epoch 00053: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4082 - acc: 0.8763 - val_loss: 0.4013 - val_acc: 0.8898\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4030 - acc: 0.8776\n",
      "Epoch 00054: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4030 - acc: 0.8776 - val_loss: 0.4033 - val_acc: 0.8856\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8805\n",
      "Epoch 00055: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3956 - acc: 0.8805 - val_loss: 0.3970 - val_acc: 0.8915\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8793\n",
      "Epoch 00056: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3937 - acc: 0.8793 - val_loss: 0.3992 - val_acc: 0.8882\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8791\n",
      "Epoch 00057: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3940 - acc: 0.8791 - val_loss: 0.3975 - val_acc: 0.8903\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8809\n",
      "Epoch 00058: val_loss did not improve from 0.39408\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3900 - acc: 0.8809 - val_loss: 0.4065 - val_acc: 0.8882\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.8812\n",
      "Epoch 00059: val_loss improved from 0.39408 to 0.39339, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/059-0.3934.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3865 - acc: 0.8812 - val_loss: 0.3934 - val_acc: 0.8924\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8838\n",
      "Epoch 00060: val_loss improved from 0.39339 to 0.38046, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/060-0.3805.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3767 - acc: 0.8838 - val_loss: 0.3805 - val_acc: 0.8961\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8834\n",
      "Epoch 00061: val_loss did not improve from 0.38046\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3773 - acc: 0.8834 - val_loss: 0.3917 - val_acc: 0.8931\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8850\n",
      "Epoch 00062: val_loss did not improve from 0.38046\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3747 - acc: 0.8850 - val_loss: 0.4081 - val_acc: 0.8901\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8882\n",
      "Epoch 00063: val_loss did not improve from 0.38046\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3680 - acc: 0.8882 - val_loss: 0.3848 - val_acc: 0.8982\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8861\n",
      "Epoch 00064: val_loss improved from 0.38046 to 0.37600, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/064-0.3760.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3657 - acc: 0.8861 - val_loss: 0.3760 - val_acc: 0.8984\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8878\n",
      "Epoch 00065: val_loss did not improve from 0.37600\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3673 - acc: 0.8878 - val_loss: 0.3890 - val_acc: 0.8938\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8892\n",
      "Epoch 00066: val_loss did not improve from 0.37600\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3622 - acc: 0.8892 - val_loss: 0.3839 - val_acc: 0.8952\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.8911\n",
      "Epoch 00067: val_loss did not improve from 0.37600\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3524 - acc: 0.8911 - val_loss: 0.3834 - val_acc: 0.8961\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8928\n",
      "Epoch 00068: val_loss did not improve from 0.37600\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3495 - acc: 0.8928 - val_loss: 0.3888 - val_acc: 0.8980\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8930\n",
      "Epoch 00069: val_loss improved from 0.37600 to 0.37478, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/069-0.3748.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3491 - acc: 0.8930 - val_loss: 0.3748 - val_acc: 0.9012\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8932\n",
      "Epoch 00070: val_loss did not improve from 0.37478\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3494 - acc: 0.8931 - val_loss: 0.4053 - val_acc: 0.8970\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8927\n",
      "Epoch 00071: val_loss did not improve from 0.37478\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3467 - acc: 0.8927 - val_loss: 0.3865 - val_acc: 0.8968\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8925\n",
      "Epoch 00072: val_loss improved from 0.37478 to 0.37369, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/072-0.3737.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3463 - acc: 0.8925 - val_loss: 0.3737 - val_acc: 0.8991\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8954\n",
      "Epoch 00073: val_loss did not improve from 0.37369\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3376 - acc: 0.8954 - val_loss: 0.3744 - val_acc: 0.9003\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8946\n",
      "Epoch 00074: val_loss did not improve from 0.37369\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3397 - acc: 0.8946 - val_loss: 0.3782 - val_acc: 0.9036\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8948\n",
      "Epoch 00075: val_loss did not improve from 0.37369\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3410 - acc: 0.8948 - val_loss: 0.3860 - val_acc: 0.8961\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8946\n",
      "Epoch 00076: val_loss did not improve from 0.37369\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3383 - acc: 0.8946 - val_loss: 0.3786 - val_acc: 0.9017\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8984\n",
      "Epoch 00077: val_loss improved from 0.37369 to 0.36265, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/077-0.3626.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3293 - acc: 0.8984 - val_loss: 0.3626 - val_acc: 0.9054\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8998\n",
      "Epoch 00078: val_loss did not improve from 0.36265\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3275 - acc: 0.8998 - val_loss: 0.3917 - val_acc: 0.8994\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8983\n",
      "Epoch 00079: val_loss improved from 0.36265 to 0.36146, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/079-0.3615.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3263 - acc: 0.8984 - val_loss: 0.3615 - val_acc: 0.9050\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8993\n",
      "Epoch 00080: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3286 - acc: 0.8993 - val_loss: 0.3810 - val_acc: 0.8966\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8984\n",
      "Epoch 00081: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3256 - acc: 0.8984 - val_loss: 0.3647 - val_acc: 0.9068\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.9009\n",
      "Epoch 00082: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3172 - acc: 0.9009 - val_loss: 0.3659 - val_acc: 0.9068\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.9009\n",
      "Epoch 00083: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3190 - acc: 0.9009 - val_loss: 0.3732 - val_acc: 0.9005\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9016\n",
      "Epoch 00084: val_loss did not improve from 0.36146\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3132 - acc: 0.9016 - val_loss: 0.3994 - val_acc: 0.8961\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9033\n",
      "Epoch 00085: val_loss improved from 0.36146 to 0.35570, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/085-0.3557.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3109 - acc: 0.9033 - val_loss: 0.3557 - val_acc: 0.9068\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.9027\n",
      "Epoch 00086: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3131 - acc: 0.9027 - val_loss: 0.3698 - val_acc: 0.9054\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9046\n",
      "Epoch 00087: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3102 - acc: 0.9046 - val_loss: 0.3692 - val_acc: 0.9026\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9026\n",
      "Epoch 00088: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3100 - acc: 0.9026 - val_loss: 0.3719 - val_acc: 0.9038\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.9046\n",
      "Epoch 00089: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3066 - acc: 0.9046 - val_loss: 0.3726 - val_acc: 0.9040\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.9052\n",
      "Epoch 00090: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3029 - acc: 0.9052 - val_loss: 0.3635 - val_acc: 0.9050\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.9048\n",
      "Epoch 00091: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3059 - acc: 0.9048 - val_loss: 0.3691 - val_acc: 0.9036\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9048\n",
      "Epoch 00092: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3012 - acc: 0.9048 - val_loss: 0.3632 - val_acc: 0.9085\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.9074\n",
      "Epoch 00093: val_loss did not improve from 0.35570\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2983 - acc: 0.9074 - val_loss: 0.3627 - val_acc: 0.9033\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9068\n",
      "Epoch 00094: val_loss improved from 0.35570 to 0.35206, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/094-0.3521.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2963 - acc: 0.9069 - val_loss: 0.3521 - val_acc: 0.9089\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.9078\n",
      "Epoch 00095: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2949 - acc: 0.9078 - val_loss: 0.3561 - val_acc: 0.9073\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9076\n",
      "Epoch 00096: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2990 - acc: 0.9076 - val_loss: 0.3578 - val_acc: 0.9099\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9092\n",
      "Epoch 00097: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2887 - acc: 0.9091 - val_loss: 0.3628 - val_acc: 0.9047\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9079\n",
      "Epoch 00098: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2926 - acc: 0.9079 - val_loss: 0.3644 - val_acc: 0.9052\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9086\n",
      "Epoch 00099: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2872 - acc: 0.9087 - val_loss: 0.3622 - val_acc: 0.9078\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9112\n",
      "Epoch 00100: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2822 - acc: 0.9112 - val_loss: 0.3753 - val_acc: 0.9024\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9107\n",
      "Epoch 00101: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2836 - acc: 0.9107 - val_loss: 0.3678 - val_acc: 0.9045\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9100\n",
      "Epoch 00102: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2815 - acc: 0.9100 - val_loss: 0.3743 - val_acc: 0.9059\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9098\n",
      "Epoch 00103: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2824 - acc: 0.9098 - val_loss: 0.3558 - val_acc: 0.9115\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9122\n",
      "Epoch 00104: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2778 - acc: 0.9122 - val_loss: 0.3665 - val_acc: 0.9054\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2750 - acc: 0.9142\n",
      "Epoch 00105: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2750 - acc: 0.9142 - val_loss: 0.3628 - val_acc: 0.9092\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9118\n",
      "Epoch 00106: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2756 - acc: 0.9118 - val_loss: 0.3599 - val_acc: 0.9099\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9120\n",
      "Epoch 00107: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2760 - acc: 0.9120 - val_loss: 0.3629 - val_acc: 0.9064\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9119\n",
      "Epoch 00108: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2774 - acc: 0.9119 - val_loss: 0.3641 - val_acc: 0.9103\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9139\n",
      "Epoch 00109: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2716 - acc: 0.9139 - val_loss: 0.3751 - val_acc: 0.9047\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9133\n",
      "Epoch 00110: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2692 - acc: 0.9133 - val_loss: 0.3642 - val_acc: 0.9113\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.9161\n",
      "Epoch 00111: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2682 - acc: 0.9160 - val_loss: 0.3667 - val_acc: 0.9045\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9147\n",
      "Epoch 00112: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2709 - acc: 0.9147 - val_loss: 0.3623 - val_acc: 0.9092\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9154\n",
      "Epoch 00113: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2636 - acc: 0.9154 - val_loss: 0.3732 - val_acc: 0.9036\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.9142\n",
      "Epoch 00114: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2670 - acc: 0.9142 - val_loss: 0.3660 - val_acc: 0.9075\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9162\n",
      "Epoch 00115: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2660 - acc: 0.9162 - val_loss: 0.3741 - val_acc: 0.9047\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9150\n",
      "Epoch 00116: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2622 - acc: 0.9150 - val_loss: 0.3601 - val_acc: 0.9124\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9134\n",
      "Epoch 00117: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2639 - acc: 0.9134 - val_loss: 0.3674 - val_acc: 0.9085\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9167\n",
      "Epoch 00118: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2583 - acc: 0.9167 - val_loss: 0.3679 - val_acc: 0.9071\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9151\n",
      "Epoch 00119: val_loss did not improve from 0.35206\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2620 - acc: 0.9151 - val_loss: 0.3529 - val_acc: 0.9085\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9190\n",
      "Epoch 00120: val_loss improved from 0.35206 to 0.35193, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/120-0.3519.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2539 - acc: 0.9190 - val_loss: 0.3519 - val_acc: 0.9078\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9177\n",
      "Epoch 00121: val_loss improved from 0.35193 to 0.34944, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_6_conv_checkpoint/121-0.3494.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2585 - acc: 0.9177 - val_loss: 0.3494 - val_acc: 0.9108\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9168\n",
      "Epoch 00122: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2561 - acc: 0.9168 - val_loss: 0.3586 - val_acc: 0.9131\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9193\n",
      "Epoch 00123: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2525 - acc: 0.9193 - val_loss: 0.3563 - val_acc: 0.9113\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.9196\n",
      "Epoch 00124: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2482 - acc: 0.9196 - val_loss: 0.3848 - val_acc: 0.9092\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9205\n",
      "Epoch 00125: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2427 - acc: 0.9205 - val_loss: 0.3820 - val_acc: 0.9031\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.9177\n",
      "Epoch 00126: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2526 - acc: 0.9177 - val_loss: 0.3637 - val_acc: 0.9075\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9184\n",
      "Epoch 00127: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2479 - acc: 0.9184 - val_loss: 0.3583 - val_acc: 0.9080\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9196\n",
      "Epoch 00128: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2501 - acc: 0.9196 - val_loss: 0.3588 - val_acc: 0.9106\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9203\n",
      "Epoch 00129: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2457 - acc: 0.9203 - val_loss: 0.3715 - val_acc: 0.9078\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9252\n",
      "Epoch 00130: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2399 - acc: 0.9252 - val_loss: 0.3569 - val_acc: 0.9106\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9214\n",
      "Epoch 00131: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2427 - acc: 0.9214 - val_loss: 0.3592 - val_acc: 0.9092\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9194\n",
      "Epoch 00132: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2441 - acc: 0.9194 - val_loss: 0.3694 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9218\n",
      "Epoch 00133: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2409 - acc: 0.9218 - val_loss: 0.3665 - val_acc: 0.9115\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9234\n",
      "Epoch 00134: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2366 - acc: 0.9234 - val_loss: 0.3582 - val_acc: 0.9126\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9229\n",
      "Epoch 00135: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2392 - acc: 0.9229 - val_loss: 0.3505 - val_acc: 0.9126\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9223\n",
      "Epoch 00136: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2352 - acc: 0.9223 - val_loss: 0.3587 - val_acc: 0.9133\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9210\n",
      "Epoch 00137: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2397 - acc: 0.9210 - val_loss: 0.3768 - val_acc: 0.9078\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9238\n",
      "Epoch 00138: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2369 - acc: 0.9238 - val_loss: 0.4409 - val_acc: 0.9001\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9229\n",
      "Epoch 00139: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2326 - acc: 0.9229 - val_loss: 0.3583 - val_acc: 0.9129\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9249\n",
      "Epoch 00140: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2272 - acc: 0.9249 - val_loss: 0.3858 - val_acc: 0.9106\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9256\n",
      "Epoch 00141: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2307 - acc: 0.9256 - val_loss: 0.3611 - val_acc: 0.9106\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9223\n",
      "Epoch 00142: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2364 - acc: 0.9223 - val_loss: 0.3549 - val_acc: 0.9131\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9251\n",
      "Epoch 00143: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2333 - acc: 0.9251 - val_loss: 0.3650 - val_acc: 0.9124\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9242\n",
      "Epoch 00144: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2309 - acc: 0.9242 - val_loss: 0.3718 - val_acc: 0.9108\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9254\n",
      "Epoch 00145: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2282 - acc: 0.9254 - val_loss: 0.3660 - val_acc: 0.9124\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9269\n",
      "Epoch 00146: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2257 - acc: 0.9269 - val_loss: 0.3612 - val_acc: 0.9101\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9256\n",
      "Epoch 00147: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2241 - acc: 0.9256 - val_loss: 0.3809 - val_acc: 0.9085\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9266\n",
      "Epoch 00148: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2285 - acc: 0.9266 - val_loss: 0.3563 - val_acc: 0.9124\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9269\n",
      "Epoch 00149: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2216 - acc: 0.9269 - val_loss: 0.3644 - val_acc: 0.9124\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9266\n",
      "Epoch 00150: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2233 - acc: 0.9266 - val_loss: 0.3686 - val_acc: 0.9082\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9282\n",
      "Epoch 00151: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2217 - acc: 0.9282 - val_loss: 0.3506 - val_acc: 0.9131\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9280\n",
      "Epoch 00152: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2221 - acc: 0.9281 - val_loss: 0.3621 - val_acc: 0.9073\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9293\n",
      "Epoch 00153: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2180 - acc: 0.9293 - val_loss: 0.3687 - val_acc: 0.9101\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9293\n",
      "Epoch 00154: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2158 - acc: 0.9293 - val_loss: 0.3640 - val_acc: 0.9119\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9280\n",
      "Epoch 00155: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2193 - acc: 0.9280 - val_loss: 0.3800 - val_acc: 0.9122\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9306\n",
      "Epoch 00156: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2156 - acc: 0.9306 - val_loss: 0.3705 - val_acc: 0.9106\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9283\n",
      "Epoch 00157: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2180 - acc: 0.9283 - val_loss: 0.3805 - val_acc: 0.9080\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9294\n",
      "Epoch 00158: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2113 - acc: 0.9294 - val_loss: 0.3764 - val_acc: 0.9106\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9299\n",
      "Epoch 00159: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2125 - acc: 0.9298 - val_loss: 0.3665 - val_acc: 0.9117\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9307\n",
      "Epoch 00160: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2099 - acc: 0.9307 - val_loss: 0.3579 - val_acc: 0.9136\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9295\n",
      "Epoch 00161: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2165 - acc: 0.9295 - val_loss: 0.3847 - val_acc: 0.9103\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9298\n",
      "Epoch 00162: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2123 - acc: 0.9298 - val_loss: 0.3533 - val_acc: 0.9166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9306\n",
      "Epoch 00163: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2096 - acc: 0.9306 - val_loss: 0.3720 - val_acc: 0.9106\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9315\n",
      "Epoch 00164: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2094 - acc: 0.9315 - val_loss: 0.3760 - val_acc: 0.9122\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9313\n",
      "Epoch 00165: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2096 - acc: 0.9313 - val_loss: 0.3547 - val_acc: 0.9157\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9299\n",
      "Epoch 00166: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2130 - acc: 0.9299 - val_loss: 0.3694 - val_acc: 0.9124\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9313\n",
      "Epoch 00167: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2098 - acc: 0.9313 - val_loss: 0.3709 - val_acc: 0.9152\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9324\n",
      "Epoch 00168: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2028 - acc: 0.9325 - val_loss: 0.3728 - val_acc: 0.9133\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9321\n",
      "Epoch 00169: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2080 - acc: 0.9321 - val_loss: 0.3696 - val_acc: 0.9147\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9298\n",
      "Epoch 00170: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2111 - acc: 0.9298 - val_loss: 0.3692 - val_acc: 0.9140\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9327\n",
      "Epoch 00171: val_loss did not improve from 0.34944\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2037 - acc: 0.9327 - val_loss: 0.3784 - val_acc: 0.9122\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 586us/sample - loss: 0.3966 - acc: 0.8897\n",
      "Loss: 0.39662255904137284 Accuracy: 0.8897196\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6228 - acc: 0.1235\n",
      "Epoch 00001: val_loss improved from inf to 2.30418, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/001-2.3042.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.6226 - acc: 0.1236 - val_loss: 2.3042 - val_acc: 0.2474\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2138 - acc: 0.2615\n",
      "Epoch 00002: val_loss improved from 2.30418 to 1.86820, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/002-1.8682.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.2137 - acc: 0.2615 - val_loss: 1.8682 - val_acc: 0.4081\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9159 - acc: 0.3591\n",
      "Epoch 00003: val_loss improved from 1.86820 to 1.61583, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/003-1.6158.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.9159 - acc: 0.3591 - val_loss: 1.6158 - val_acc: 0.5094\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7065 - acc: 0.4314\n",
      "Epoch 00004: val_loss improved from 1.61583 to 1.46030, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/004-1.4603.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.7065 - acc: 0.4315 - val_loss: 1.4603 - val_acc: 0.5574\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5376 - acc: 0.4918\n",
      "Epoch 00005: val_loss improved from 1.46030 to 1.31743, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/005-1.3174.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5377 - acc: 0.4918 - val_loss: 1.3174 - val_acc: 0.6021\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4029 - acc: 0.5382\n",
      "Epoch 00006: val_loss improved from 1.31743 to 1.12192, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/006-1.1219.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4028 - acc: 0.5382 - val_loss: 1.1219 - val_acc: 0.6646\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2862 - acc: 0.5868\n",
      "Epoch 00007: val_loss improved from 1.12192 to 1.01849, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/007-1.0185.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2861 - acc: 0.5868 - val_loss: 1.0185 - val_acc: 0.7053\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1881 - acc: 0.6204\n",
      "Epoch 00008: val_loss improved from 1.01849 to 0.93851, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/008-0.9385.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1880 - acc: 0.6204 - val_loss: 0.9385 - val_acc: 0.7209\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1098 - acc: 0.6479\n",
      "Epoch 00009: val_loss improved from 0.93851 to 0.88157, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/009-0.8816.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1100 - acc: 0.6479 - val_loss: 0.8816 - val_acc: 0.7538\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0320 - acc: 0.6730\n",
      "Epoch 00010: val_loss improved from 0.88157 to 0.79999, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/010-0.8000.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0320 - acc: 0.6731 - val_loss: 0.8000 - val_acc: 0.7761\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9780 - acc: 0.6936\n",
      "Epoch 00011: val_loss did not improve from 0.79999\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9780 - acc: 0.6936 - val_loss: 0.8220 - val_acc: 0.7594\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9259 - acc: 0.7105\n",
      "Epoch 00012: val_loss improved from 0.79999 to 0.70882, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/012-0.7088.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9259 - acc: 0.7105 - val_loss: 0.7088 - val_acc: 0.7980\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7299\n",
      "Epoch 00013: val_loss improved from 0.70882 to 0.69374, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/013-0.6937.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8802 - acc: 0.7300 - val_loss: 0.6937 - val_acc: 0.7950\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8426 - acc: 0.7390\n",
      "Epoch 00014: val_loss improved from 0.69374 to 0.65306, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/014-0.6531.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8426 - acc: 0.7389 - val_loss: 0.6531 - val_acc: 0.8111\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.7507\n",
      "Epoch 00015: val_loss improved from 0.65306 to 0.60748, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/015-0.6075.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8064 - acc: 0.7507 - val_loss: 0.6075 - val_acc: 0.8248\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7751 - acc: 0.7611\n",
      "Epoch 00016: val_loss improved from 0.60748 to 0.55536, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/016-0.5554.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7751 - acc: 0.7611 - val_loss: 0.5554 - val_acc: 0.8442\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7705\n",
      "Epoch 00017: val_loss did not improve from 0.55536\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7481 - acc: 0.7705 - val_loss: 0.5619 - val_acc: 0.8400\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.7805\n",
      "Epoch 00018: val_loss improved from 0.55536 to 0.53085, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/018-0.5308.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7156 - acc: 0.7805 - val_loss: 0.5308 - val_acc: 0.8509\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.7845\n",
      "Epoch 00019: val_loss improved from 0.53085 to 0.51030, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/019-0.5103.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6933 - acc: 0.7845 - val_loss: 0.5103 - val_acc: 0.8565\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6772 - acc: 0.7885\n",
      "Epoch 00020: val_loss improved from 0.51030 to 0.49717, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/020-0.4972.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6771 - acc: 0.7885 - val_loss: 0.4972 - val_acc: 0.8600\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6588 - acc: 0.8020\n",
      "Epoch 00021: val_loss improved from 0.49717 to 0.47839, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/021-0.4784.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6588 - acc: 0.8020 - val_loss: 0.4784 - val_acc: 0.8614\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6359 - acc: 0.8049\n",
      "Epoch 00022: val_loss improved from 0.47839 to 0.46785, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/022-0.4678.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6360 - acc: 0.8049 - val_loss: 0.4678 - val_acc: 0.8677\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.8092\n",
      "Epoch 00023: val_loss improved from 0.46785 to 0.44159, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/023-0.4416.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6224 - acc: 0.8092 - val_loss: 0.4416 - val_acc: 0.8798\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6073 - acc: 0.8149\n",
      "Epoch 00024: val_loss did not improve from 0.44159\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6072 - acc: 0.8150 - val_loss: 0.4418 - val_acc: 0.8770\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8122\n",
      "Epoch 00025: val_loss improved from 0.44159 to 0.42443, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/025-0.4244.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6010 - acc: 0.8122 - val_loss: 0.4244 - val_acc: 0.8810\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5870 - acc: 0.8199\n",
      "Epoch 00026: val_loss did not improve from 0.42443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5870 - acc: 0.8199 - val_loss: 0.4291 - val_acc: 0.8805\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5741 - acc: 0.8223\n",
      "Epoch 00027: val_loss did not improve from 0.42443\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5741 - acc: 0.8223 - val_loss: 0.4513 - val_acc: 0.8721\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8279\n",
      "Epoch 00028: val_loss improved from 0.42443 to 0.39384, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/028-0.3938.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5595 - acc: 0.8279 - val_loss: 0.3938 - val_acc: 0.8891\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.8282\n",
      "Epoch 00029: val_loss did not improve from 0.39384\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5561 - acc: 0.8282 - val_loss: 0.4416 - val_acc: 0.8719\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8331\n",
      "Epoch 00030: val_loss improved from 0.39384 to 0.39214, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/030-0.3921.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5378 - acc: 0.8331 - val_loss: 0.3921 - val_acc: 0.8977\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5358 - acc: 0.8364\n",
      "Epoch 00031: val_loss did not improve from 0.39214\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5358 - acc: 0.8364 - val_loss: 0.4678 - val_acc: 0.8598\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.8384\n",
      "Epoch 00032: val_loss improved from 0.39214 to 0.37307, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/032-0.3731.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5238 - acc: 0.8384 - val_loss: 0.3731 - val_acc: 0.8977\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.8422\n",
      "Epoch 00033: val_loss did not improve from 0.37307\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5183 - acc: 0.8422 - val_loss: 0.3763 - val_acc: 0.8884\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.8444\n",
      "Epoch 00034: val_loss did not improve from 0.37307\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5100 - acc: 0.8444 - val_loss: 0.3765 - val_acc: 0.8915\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4966 - acc: 0.8473\n",
      "Epoch 00035: val_loss did not improve from 0.37307\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4965 - acc: 0.8473 - val_loss: 0.3809 - val_acc: 0.8868\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8490\n",
      "Epoch 00036: val_loss improved from 0.37307 to 0.35541, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/036-0.3554.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4909 - acc: 0.8490 - val_loss: 0.3554 - val_acc: 0.9010\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8518\n",
      "Epoch 00037: val_loss improved from 0.35541 to 0.34966, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/037-0.3497.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4797 - acc: 0.8517 - val_loss: 0.3497 - val_acc: 0.9038\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8516\n",
      "Epoch 00038: val_loss did not improve from 0.34966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4816 - acc: 0.8516 - val_loss: 0.3541 - val_acc: 0.8989\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8551\n",
      "Epoch 00039: val_loss did not improve from 0.34966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4746 - acc: 0.8551 - val_loss: 0.3656 - val_acc: 0.9008\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4681 - acc: 0.8557\n",
      "Epoch 00040: val_loss improved from 0.34966 to 0.34918, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/040-0.3492.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4681 - acc: 0.8557 - val_loss: 0.3492 - val_acc: 0.9043\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.8565\n",
      "Epoch 00041: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4651 - acc: 0.8565 - val_loss: 0.3596 - val_acc: 0.9019\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8611\n",
      "Epoch 00042: val_loss improved from 0.34918 to 0.32993, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/042-0.3299.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4538 - acc: 0.8611 - val_loss: 0.3299 - val_acc: 0.9106\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.8646\n",
      "Epoch 00043: val_loss improved from 0.32993 to 0.32627, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/043-0.3263.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4484 - acc: 0.8646 - val_loss: 0.3263 - val_acc: 0.9147\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4425 - acc: 0.8637\n",
      "Epoch 00044: val_loss did not improve from 0.32627\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4426 - acc: 0.8636 - val_loss: 0.3566 - val_acc: 0.9001\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.8647\n",
      "Epoch 00045: val_loss did not improve from 0.32627\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4379 - acc: 0.8647 - val_loss: 0.3284 - val_acc: 0.9101\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8659\n",
      "Epoch 00046: val_loss improved from 0.32627 to 0.31219, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/046-0.3122.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4344 - acc: 0.8659 - val_loss: 0.3122 - val_acc: 0.9157\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8672\n",
      "Epoch 00047: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4303 - acc: 0.8672 - val_loss: 0.3783 - val_acc: 0.8903\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8678\n",
      "Epoch 00048: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4245 - acc: 0.8678 - val_loss: 0.3580 - val_acc: 0.8987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8716\n",
      "Epoch 00049: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4206 - acc: 0.8716 - val_loss: 0.3264 - val_acc: 0.9161\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8734\n",
      "Epoch 00050: val_loss did not improve from 0.31219\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4157 - acc: 0.8734 - val_loss: 0.3191 - val_acc: 0.9119\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4141 - acc: 0.8698\n",
      "Epoch 00051: val_loss improved from 0.31219 to 0.30113, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/051-0.3011.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4141 - acc: 0.8698 - val_loss: 0.3011 - val_acc: 0.9154\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8721\n",
      "Epoch 00052: val_loss did not improve from 0.30113\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4093 - acc: 0.8721 - val_loss: 0.3132 - val_acc: 0.9115\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8738\n",
      "Epoch 00053: val_loss did not improve from 0.30113\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4066 - acc: 0.8738 - val_loss: 0.3061 - val_acc: 0.9224\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8740\n",
      "Epoch 00054: val_loss did not improve from 0.30113\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4066 - acc: 0.8740 - val_loss: 0.3045 - val_acc: 0.9185\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8788\n",
      "Epoch 00055: val_loss improved from 0.30113 to 0.29489, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/055-0.2949.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3956 - acc: 0.8788 - val_loss: 0.2949 - val_acc: 0.9157\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8794\n",
      "Epoch 00056: val_loss improved from 0.29489 to 0.28504, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/056-0.2850.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3941 - acc: 0.8794 - val_loss: 0.2850 - val_acc: 0.9217\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8795\n",
      "Epoch 00057: val_loss did not improve from 0.28504\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3912 - acc: 0.8795 - val_loss: 0.3056 - val_acc: 0.9161\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8808\n",
      "Epoch 00058: val_loss did not improve from 0.28504\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3865 - acc: 0.8808 - val_loss: 0.2922 - val_acc: 0.9194\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8820\n",
      "Epoch 00059: val_loss did not improve from 0.28504\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3787 - acc: 0.8820 - val_loss: 0.2930 - val_acc: 0.9166\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8815\n",
      "Epoch 00060: val_loss improved from 0.28504 to 0.27948, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/060-0.2795.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3787 - acc: 0.8815 - val_loss: 0.2795 - val_acc: 0.9217\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8820\n",
      "Epoch 00061: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3823 - acc: 0.8821 - val_loss: 0.2924 - val_acc: 0.9194\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8809\n",
      "Epoch 00062: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3800 - acc: 0.8809 - val_loss: 0.2858 - val_acc: 0.9217\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8850\n",
      "Epoch 00063: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3663 - acc: 0.8850 - val_loss: 0.2844 - val_acc: 0.9199\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8877\n",
      "Epoch 00064: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3685 - acc: 0.8877 - val_loss: 0.3009 - val_acc: 0.9194\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8856\n",
      "Epoch 00065: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3711 - acc: 0.8856 - val_loss: 0.3060 - val_acc: 0.9159\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.8878\n",
      "Epoch 00066: val_loss did not improve from 0.27948\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3665 - acc: 0.8878 - val_loss: 0.2861 - val_acc: 0.9168\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8883\n",
      "Epoch 00067: val_loss improved from 0.27948 to 0.26736, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/067-0.2674.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3636 - acc: 0.8883 - val_loss: 0.2674 - val_acc: 0.9278\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8873\n",
      "Epoch 00068: val_loss did not improve from 0.26736\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3600 - acc: 0.8873 - val_loss: 0.2829 - val_acc: 0.9252\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8886\n",
      "Epoch 00069: val_loss did not improve from 0.26736\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3541 - acc: 0.8886 - val_loss: 0.2723 - val_acc: 0.9266\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.8897\n",
      "Epoch 00070: val_loss did not improve from 0.26736\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3511 - acc: 0.8897 - val_loss: 0.2682 - val_acc: 0.9269\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3531 - acc: 0.8896\n",
      "Epoch 00071: val_loss did not improve from 0.26736\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3531 - acc: 0.8896 - val_loss: 0.2709 - val_acc: 0.9287\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8916\n",
      "Epoch 00072: val_loss improved from 0.26736 to 0.26136, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/072-0.2614.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3497 - acc: 0.8916 - val_loss: 0.2614 - val_acc: 0.9299\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8936\n",
      "Epoch 00073: val_loss did not improve from 0.26136\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3467 - acc: 0.8936 - val_loss: 0.2693 - val_acc: 0.9269\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8946\n",
      "Epoch 00074: val_loss did not improve from 0.26136\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3387 - acc: 0.8946 - val_loss: 0.2814 - val_acc: 0.9252\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8939\n",
      "Epoch 00075: val_loss improved from 0.26136 to 0.26009, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/075-0.2601.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3416 - acc: 0.8939 - val_loss: 0.2601 - val_acc: 0.9292\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8942\n",
      "Epoch 00076: val_loss improved from 0.26009 to 0.25305, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/076-0.2531.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3387 - acc: 0.8942 - val_loss: 0.2531 - val_acc: 0.9273\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3341 - acc: 0.8943\n",
      "Epoch 00077: val_loss did not improve from 0.25305\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3341 - acc: 0.8943 - val_loss: 0.2670 - val_acc: 0.9280\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8945\n",
      "Epoch 00078: val_loss did not improve from 0.25305\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3375 - acc: 0.8945 - val_loss: 0.2610 - val_acc: 0.9285\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8974\n",
      "Epoch 00079: val_loss did not improve from 0.25305\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3283 - acc: 0.8974 - val_loss: 0.2654 - val_acc: 0.9311\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8964\n",
      "Epoch 00080: val_loss improved from 0.25305 to 0.25087, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/080-0.2509.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3311 - acc: 0.8964 - val_loss: 0.2509 - val_acc: 0.9294\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8952\n",
      "Epoch 00081: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3289 - acc: 0.8953 - val_loss: 0.2776 - val_acc: 0.9252\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8977\n",
      "Epoch 00082: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3217 - acc: 0.8977 - val_loss: 0.2682 - val_acc: 0.9290\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8956\n",
      "Epoch 00083: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3254 - acc: 0.8956 - val_loss: 0.2516 - val_acc: 0.9280\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9000\n",
      "Epoch 00084: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3183 - acc: 0.9000 - val_loss: 0.2693 - val_acc: 0.9208\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.8998\n",
      "Epoch 00085: val_loss did not improve from 0.25087\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3152 - acc: 0.8998 - val_loss: 0.2758 - val_acc: 0.9241\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9000\n",
      "Epoch 00086: val_loss improved from 0.25087 to 0.24857, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/086-0.2486.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3180 - acc: 0.9000 - val_loss: 0.2486 - val_acc: 0.9331\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.9010\n",
      "Epoch 00087: val_loss improved from 0.24857 to 0.24285, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/087-0.2429.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3188 - acc: 0.9010 - val_loss: 0.2429 - val_acc: 0.9322\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9013\n",
      "Epoch 00088: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3142 - acc: 0.9013 - val_loss: 0.2471 - val_acc: 0.9369\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9014\n",
      "Epoch 00089: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3094 - acc: 0.9014 - val_loss: 0.2467 - val_acc: 0.9352\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3152 - acc: 0.9013\n",
      "Epoch 00090: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3152 - acc: 0.9013 - val_loss: 0.2592 - val_acc: 0.9280\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9042\n",
      "Epoch 00091: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3054 - acc: 0.9042 - val_loss: 0.2468 - val_acc: 0.9327\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9035\n",
      "Epoch 00092: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3086 - acc: 0.9035 - val_loss: 0.2898 - val_acc: 0.9222\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9008\n",
      "Epoch 00093: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3113 - acc: 0.9008 - val_loss: 0.2541 - val_acc: 0.9322\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9050\n",
      "Epoch 00094: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3005 - acc: 0.9050 - val_loss: 0.2702 - val_acc: 0.9280\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9031\n",
      "Epoch 00095: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3052 - acc: 0.9031 - val_loss: 0.2448 - val_acc: 0.9338\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9066\n",
      "Epoch 00096: val_loss did not improve from 0.24285\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2962 - acc: 0.9066 - val_loss: 0.2689 - val_acc: 0.9224\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9041\n",
      "Epoch 00097: val_loss improved from 0.24285 to 0.24089, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/097-0.2409.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2990 - acc: 0.9040 - val_loss: 0.2409 - val_acc: 0.9320\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9076\n",
      "Epoch 00098: val_loss improved from 0.24089 to 0.23397, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/098-0.2340.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2933 - acc: 0.9076 - val_loss: 0.2340 - val_acc: 0.9364\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9057\n",
      "Epoch 00099: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2914 - acc: 0.9057 - val_loss: 0.2344 - val_acc: 0.9352\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9054\n",
      "Epoch 00100: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2966 - acc: 0.9054 - val_loss: 0.2397 - val_acc: 0.9343\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9065\n",
      "Epoch 00101: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2932 - acc: 0.9065 - val_loss: 0.2429 - val_acc: 0.9345\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9073\n",
      "Epoch 00102: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2921 - acc: 0.9072 - val_loss: 0.2485 - val_acc: 0.9294\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9053\n",
      "Epoch 00103: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2935 - acc: 0.9053 - val_loss: 0.2442 - val_acc: 0.9366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9090\n",
      "Epoch 00104: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2849 - acc: 0.9090 - val_loss: 0.2358 - val_acc: 0.9357\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9082\n",
      "Epoch 00105: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2862 - acc: 0.9082 - val_loss: 0.2551 - val_acc: 0.9308\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9096\n",
      "Epoch 00106: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2824 - acc: 0.9096 - val_loss: 0.2467 - val_acc: 0.9327\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9105\n",
      "Epoch 00107: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2788 - acc: 0.9105 - val_loss: 0.2370 - val_acc: 0.9357\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9100\n",
      "Epoch 00108: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2800 - acc: 0.9100 - val_loss: 0.2403 - val_acc: 0.9280\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9115\n",
      "Epoch 00109: val_loss did not improve from 0.23397\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2784 - acc: 0.9116 - val_loss: 0.2418 - val_acc: 0.9364\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9118\n",
      "Epoch 00110: val_loss improved from 0.23397 to 0.22484, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/110-0.2248.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2770 - acc: 0.9118 - val_loss: 0.2248 - val_acc: 0.9387\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9092\n",
      "Epoch 00111: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2782 - acc: 0.9091 - val_loss: 0.2308 - val_acc: 0.9352\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9120\n",
      "Epoch 00112: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2754 - acc: 0.9120 - val_loss: 0.2460 - val_acc: 0.9315\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9137\n",
      "Epoch 00113: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2749 - acc: 0.9137 - val_loss: 0.2429 - val_acc: 0.9301\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9121\n",
      "Epoch 00114: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2717 - acc: 0.9121 - val_loss: 0.2294 - val_acc: 0.9385\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9152\n",
      "Epoch 00115: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2673 - acc: 0.9152 - val_loss: 0.2363 - val_acc: 0.9392\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9142\n",
      "Epoch 00116: val_loss did not improve from 0.22484\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2656 - acc: 0.9142 - val_loss: 0.2285 - val_acc: 0.9352\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9142\n",
      "Epoch 00117: val_loss improved from 0.22484 to 0.22474, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/117-0.2247.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2661 - acc: 0.9143 - val_loss: 0.2247 - val_acc: 0.9357\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9142\n",
      "Epoch 00118: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2693 - acc: 0.9141 - val_loss: 0.2313 - val_acc: 0.9324\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9150\n",
      "Epoch 00119: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2640 - acc: 0.9150 - val_loss: 0.2303 - val_acc: 0.9359\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9144\n",
      "Epoch 00120: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2645 - acc: 0.9144 - val_loss: 0.2455 - val_acc: 0.9329\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9149\n",
      "Epoch 00121: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2659 - acc: 0.9150 - val_loss: 0.2256 - val_acc: 0.9359\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9165\n",
      "Epoch 00122: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2598 - acc: 0.9165 - val_loss: 0.2408 - val_acc: 0.9364\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9150\n",
      "Epoch 00123: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2625 - acc: 0.9150 - val_loss: 0.2551 - val_acc: 0.9276\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9123\n",
      "Epoch 00124: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2719 - acc: 0.9123 - val_loss: 0.2273 - val_acc: 0.9401\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9160\n",
      "Epoch 00125: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2587 - acc: 0.9159 - val_loss: 0.2699 - val_acc: 0.9380\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9174\n",
      "Epoch 00126: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2544 - acc: 0.9174 - val_loss: 0.2352 - val_acc: 0.9371\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9189\n",
      "Epoch 00127: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2524 - acc: 0.9189 - val_loss: 0.2480 - val_acc: 0.9324\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9180\n",
      "Epoch 00128: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2544 - acc: 0.9180 - val_loss: 0.2326 - val_acc: 0.9390\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9199\n",
      "Epoch 00129: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2549 - acc: 0.9199 - val_loss: 0.2548 - val_acc: 0.9331\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9195\n",
      "Epoch 00130: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2489 - acc: 0.9195 - val_loss: 0.2276 - val_acc: 0.9373\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2495 - acc: 0.9197\n",
      "Epoch 00131: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2495 - acc: 0.9197 - val_loss: 0.2474 - val_acc: 0.9355\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9190\n",
      "Epoch 00132: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2517 - acc: 0.9191 - val_loss: 0.2270 - val_acc: 0.9373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9203\n",
      "Epoch 00133: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2491 - acc: 0.9203 - val_loss: 0.2337 - val_acc: 0.9317\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.9183\n",
      "Epoch 00134: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2508 - acc: 0.9183 - val_loss: 0.2365 - val_acc: 0.9404\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9206\n",
      "Epoch 00135: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2443 - acc: 0.9206 - val_loss: 0.2301 - val_acc: 0.9401\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9206\n",
      "Epoch 00136: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2434 - acc: 0.9206 - val_loss: 0.2336 - val_acc: 0.9362\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9218\n",
      "Epoch 00137: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2424 - acc: 0.9218 - val_loss: 0.2300 - val_acc: 0.9371\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9230\n",
      "Epoch 00138: val_loss did not improve from 0.22474\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2399 - acc: 0.9230 - val_loss: 0.2404 - val_acc: 0.9413\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9216\n",
      "Epoch 00139: val_loss improved from 0.22474 to 0.22233, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/139-0.2223.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2405 - acc: 0.9216 - val_loss: 0.2223 - val_acc: 0.9380\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9215\n",
      "Epoch 00140: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2419 - acc: 0.9215 - val_loss: 0.2236 - val_acc: 0.9380\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9221\n",
      "Epoch 00141: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2377 - acc: 0.9220 - val_loss: 0.2261 - val_acc: 0.9432\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9228\n",
      "Epoch 00142: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2424 - acc: 0.9228 - val_loss: 0.2396 - val_acc: 0.9336\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9243\n",
      "Epoch 00143: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2345 - acc: 0.9243 - val_loss: 0.2460 - val_acc: 0.9404\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9247\n",
      "Epoch 00144: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2338 - acc: 0.9247 - val_loss: 0.2342 - val_acc: 0.9350\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9219\n",
      "Epoch 00145: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2378 - acc: 0.9219 - val_loss: 0.2394 - val_acc: 0.9380\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9240\n",
      "Epoch 00146: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2335 - acc: 0.9240 - val_loss: 0.2253 - val_acc: 0.9411\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9252\n",
      "Epoch 00147: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2315 - acc: 0.9251 - val_loss: 0.2578 - val_acc: 0.9327\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9236\n",
      "Epoch 00148: val_loss did not improve from 0.22233\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2332 - acc: 0.9237 - val_loss: 0.2340 - val_acc: 0.9369\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9255\n",
      "Epoch 00149: val_loss improved from 0.22233 to 0.21866, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/149-0.2187.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2294 - acc: 0.9255 - val_loss: 0.2187 - val_acc: 0.9406\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9243\n",
      "Epoch 00150: val_loss did not improve from 0.21866\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2348 - acc: 0.9243 - val_loss: 0.2211 - val_acc: 0.9432\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9242\n",
      "Epoch 00151: val_loss did not improve from 0.21866\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2302 - acc: 0.9242 - val_loss: 0.2387 - val_acc: 0.9366\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9266\n",
      "Epoch 00152: val_loss improved from 0.21866 to 0.21549, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/152-0.2155.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2257 - acc: 0.9266 - val_loss: 0.2155 - val_acc: 0.9436\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9257\n",
      "Epoch 00153: val_loss did not improve from 0.21549\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2287 - acc: 0.9257 - val_loss: 0.2306 - val_acc: 0.9434\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9262\n",
      "Epoch 00154: val_loss improved from 0.21549 to 0.21406, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_7_conv_checkpoint/154-0.2141.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2263 - acc: 0.9262 - val_loss: 0.2141 - val_acc: 0.9418\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9262\n",
      "Epoch 00155: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2278 - acc: 0.9262 - val_loss: 0.2252 - val_acc: 0.9422\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9277\n",
      "Epoch 00156: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2243 - acc: 0.9277 - val_loss: 0.2209 - val_acc: 0.9399\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9264\n",
      "Epoch 00157: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2246 - acc: 0.9264 - val_loss: 0.2324 - val_acc: 0.9385\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9271\n",
      "Epoch 00158: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2213 - acc: 0.9271 - val_loss: 0.2216 - val_acc: 0.9450\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9267\n",
      "Epoch 00159: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2220 - acc: 0.9267 - val_loss: 0.2239 - val_acc: 0.9418\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9285\n",
      "Epoch 00160: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2231 - acc: 0.9285 - val_loss: 0.2167 - val_acc: 0.9434\n",
      "Epoch 161/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9262\n",
      "Epoch 00161: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2200 - acc: 0.9262 - val_loss: 0.2217 - val_acc: 0.9392\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9285\n",
      "Epoch 00162: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2191 - acc: 0.9284 - val_loss: 0.2332 - val_acc: 0.9411\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9280\n",
      "Epoch 00163: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2186 - acc: 0.9280 - val_loss: 0.2351 - val_acc: 0.9418\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9300\n",
      "Epoch 00164: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2119 - acc: 0.9300 - val_loss: 0.2243 - val_acc: 0.9429\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9289\n",
      "Epoch 00165: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2179 - acc: 0.9289 - val_loss: 0.2604 - val_acc: 0.9387\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9317\n",
      "Epoch 00166: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2106 - acc: 0.9317 - val_loss: 0.2410 - val_acc: 0.9404\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9296\n",
      "Epoch 00167: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2163 - acc: 0.9297 - val_loss: 0.2380 - val_acc: 0.9383\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9304\n",
      "Epoch 00168: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2109 - acc: 0.9304 - val_loss: 0.2236 - val_acc: 0.9446\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9293\n",
      "Epoch 00169: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2135 - acc: 0.9294 - val_loss: 0.2287 - val_acc: 0.9394\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9339\n",
      "Epoch 00170: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2072 - acc: 0.9339 - val_loss: 0.2259 - val_acc: 0.9411\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9310\n",
      "Epoch 00171: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2109 - acc: 0.9310 - val_loss: 0.2224 - val_acc: 0.9401\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9327\n",
      "Epoch 00172: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2063 - acc: 0.9327 - val_loss: 0.2179 - val_acc: 0.9467\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9315\n",
      "Epoch 00173: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2097 - acc: 0.9315 - val_loss: 0.2197 - val_acc: 0.9411\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9338\n",
      "Epoch 00174: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2054 - acc: 0.9338 - val_loss: 0.2234 - val_acc: 0.9425\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9341\n",
      "Epoch 00175: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2007 - acc: 0.9341 - val_loss: 0.2277 - val_acc: 0.9436\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9306\n",
      "Epoch 00176: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2110 - acc: 0.9306 - val_loss: 0.2276 - val_acc: 0.9443\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9325\n",
      "Epoch 00177: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2057 - acc: 0.9325 - val_loss: 0.2326 - val_acc: 0.9432\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9319\n",
      "Epoch 00178: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2078 - acc: 0.9319 - val_loss: 0.2242 - val_acc: 0.9427\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9339\n",
      "Epoch 00179: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2006 - acc: 0.9339 - val_loss: 0.2363 - val_acc: 0.9399\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9330\n",
      "Epoch 00180: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2007 - acc: 0.9331 - val_loss: 0.2793 - val_acc: 0.9352\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9329\n",
      "Epoch 00181: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2024 - acc: 0.9329 - val_loss: 0.2210 - val_acc: 0.9439\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9333\n",
      "Epoch 00182: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2024 - acc: 0.9332 - val_loss: 0.2342 - val_acc: 0.9387\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9324\n",
      "Epoch 00183: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2012 - acc: 0.9324 - val_loss: 0.2540 - val_acc: 0.9408\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9328\n",
      "Epoch 00184: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2018 - acc: 0.9328 - val_loss: 0.2216 - val_acc: 0.9441\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9367\n",
      "Epoch 00185: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1956 - acc: 0.9367 - val_loss: 0.2504 - val_acc: 0.9366\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9346\n",
      "Epoch 00186: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2013 - acc: 0.9346 - val_loss: 0.2298 - val_acc: 0.9422\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9341\n",
      "Epoch 00187: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1958 - acc: 0.9341 - val_loss: 0.2167 - val_acc: 0.9425\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9350\n",
      "Epoch 00188: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1972 - acc: 0.9350 - val_loss: 0.2157 - val_acc: 0.9441\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9362\n",
      "Epoch 00189: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1946 - acc: 0.9362 - val_loss: 0.2468 - val_acc: 0.9357\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9355\n",
      "Epoch 00190: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1951 - acc: 0.9355 - val_loss: 0.2204 - val_acc: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9365\n",
      "Epoch 00191: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1950 - acc: 0.9365 - val_loss: 0.2266 - val_acc: 0.9413\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9354\n",
      "Epoch 00192: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1942 - acc: 0.9354 - val_loss: 0.2261 - val_acc: 0.9448\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9352\n",
      "Epoch 00193: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1914 - acc: 0.9351 - val_loss: 0.2348 - val_acc: 0.9413\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9346\n",
      "Epoch 00194: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1971 - acc: 0.9346 - val_loss: 0.2387 - val_acc: 0.9380\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9373\n",
      "Epoch 00195: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1887 - acc: 0.9372 - val_loss: 0.2247 - val_acc: 0.9460\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9344\n",
      "Epoch 00196: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1944 - acc: 0.9344 - val_loss: 0.2209 - val_acc: 0.9457\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9337\n",
      "Epoch 00197: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1961 - acc: 0.9337 - val_loss: 0.2361 - val_acc: 0.9422\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9374\n",
      "Epoch 00198: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1920 - acc: 0.9374 - val_loss: 0.2201 - val_acc: 0.9439\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9366\n",
      "Epoch 00199: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1888 - acc: 0.9366 - val_loss: 0.2186 - val_acc: 0.9446\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9365\n",
      "Epoch 00200: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1894 - acc: 0.9366 - val_loss: 0.2177 - val_acc: 0.9441\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9352\n",
      "Epoch 00201: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1943 - acc: 0.9352 - val_loss: 0.2359 - val_acc: 0.9427\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9402\n",
      "Epoch 00202: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1831 - acc: 0.9402 - val_loss: 0.2236 - val_acc: 0.9446\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9367\n",
      "Epoch 00203: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1878 - acc: 0.9367 - val_loss: 0.2294 - val_acc: 0.9434\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9380\n",
      "Epoch 00204: val_loss did not improve from 0.21406\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1909 - acc: 0.9381 - val_loss: 0.2285 - val_acc: 0.9422\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 546us/sample - loss: 0.2571 - acc: 0.9259\n",
      "Loss: 0.2571429381241803 Accuracy: 0.9258567\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6241 - acc: 0.1349\n",
      "Epoch 00001: val_loss improved from inf to 2.23738, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/001-2.2374.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 2.6241 - acc: 0.1349 - val_loss: 2.2374 - val_acc: 0.3100\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0989 - acc: 0.3208\n",
      "Epoch 00002: val_loss improved from 2.23738 to 1.60699, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/002-1.6070.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.0988 - acc: 0.3208 - val_loss: 1.6070 - val_acc: 0.5295\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7449 - acc: 0.4301\n",
      "Epoch 00003: val_loss improved from 1.60699 to 1.34424, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/003-1.3442.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.7450 - acc: 0.4301 - val_loss: 1.3442 - val_acc: 0.5991\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5333 - acc: 0.4988\n",
      "Epoch 00004: val_loss improved from 1.34424 to 1.19543, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/004-1.1954.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5333 - acc: 0.4988 - val_loss: 1.1954 - val_acc: 0.6417\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3990 - acc: 0.5415\n",
      "Epoch 00005: val_loss improved from 1.19543 to 1.07042, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/005-1.0704.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3990 - acc: 0.5415 - val_loss: 1.0704 - val_acc: 0.6643\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3028 - acc: 0.5692\n",
      "Epoch 00006: val_loss improved from 1.07042 to 0.95857, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/006-0.9586.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3027 - acc: 0.5692 - val_loss: 0.9586 - val_acc: 0.7016\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2120 - acc: 0.6039\n",
      "Epoch 00007: val_loss improved from 0.95857 to 0.92029, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/007-0.9203.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2120 - acc: 0.6039 - val_loss: 0.9203 - val_acc: 0.7237\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1353 - acc: 0.6274\n",
      "Epoch 00008: val_loss improved from 0.92029 to 0.83849, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/008-0.8385.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1356 - acc: 0.6273 - val_loss: 0.8385 - val_acc: 0.7480\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0681 - acc: 0.6511\n",
      "Epoch 00009: val_loss improved from 0.83849 to 0.78165, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/009-0.7816.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0680 - acc: 0.6511 - val_loss: 0.7816 - val_acc: 0.7678\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.6723\n",
      "Epoch 00010: val_loss improved from 0.78165 to 0.69898, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/010-0.6990.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0139 - acc: 0.6723 - val_loss: 0.6990 - val_acc: 0.7997\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9594 - acc: 0.6892\n",
      "Epoch 00011: val_loss improved from 0.69898 to 0.68196, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/011-0.6820.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9594 - acc: 0.6892 - val_loss: 0.6820 - val_acc: 0.8039\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9133 - acc: 0.7055\n",
      "Epoch 00012: val_loss improved from 0.68196 to 0.59901, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/012-0.5990.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9133 - acc: 0.7055 - val_loss: 0.5990 - val_acc: 0.8286\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7226\n",
      "Epoch 00013: val_loss improved from 0.59901 to 0.56931, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/013-0.5693.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8658 - acc: 0.7226 - val_loss: 0.5693 - val_acc: 0.8421\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7379\n",
      "Epoch 00014: val_loss improved from 0.56931 to 0.53296, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/014-0.5330.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8188 - acc: 0.7379 - val_loss: 0.5330 - val_acc: 0.8502\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7795 - acc: 0.7520\n",
      "Epoch 00015: val_loss did not improve from 0.53296\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7794 - acc: 0.7520 - val_loss: 0.5635 - val_acc: 0.8393\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7546 - acc: 0.7593\n",
      "Epoch 00016: val_loss improved from 0.53296 to 0.48069, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/016-0.4807.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7548 - acc: 0.7592 - val_loss: 0.4807 - val_acc: 0.8705\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.7732\n",
      "Epoch 00017: val_loss improved from 0.48069 to 0.45175, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/017-0.4517.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7151 - acc: 0.7732 - val_loss: 0.4517 - val_acc: 0.8782\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.7833\n",
      "Epoch 00018: val_loss improved from 0.45175 to 0.42092, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/018-0.4209.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6874 - acc: 0.7833 - val_loss: 0.4209 - val_acc: 0.8852\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.7886\n",
      "Epoch 00019: val_loss improved from 0.42092 to 0.41256, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/019-0.4126.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6696 - acc: 0.7887 - val_loss: 0.4126 - val_acc: 0.8894\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6413 - acc: 0.7981\n",
      "Epoch 00020: val_loss improved from 0.41256 to 0.38113, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/020-0.3811.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6413 - acc: 0.7981 - val_loss: 0.3811 - val_acc: 0.8942\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.8043\n",
      "Epoch 00021: val_loss improved from 0.38113 to 0.36782, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/021-0.3678.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6223 - acc: 0.8043 - val_loss: 0.3678 - val_acc: 0.8961\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.8086\n",
      "Epoch 00022: val_loss improved from 0.36782 to 0.36244, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/022-0.3624.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6050 - acc: 0.8086 - val_loss: 0.3624 - val_acc: 0.8977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8162\n",
      "Epoch 00023: val_loss improved from 0.36244 to 0.35189, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/023-0.3519.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5826 - acc: 0.8162 - val_loss: 0.3519 - val_acc: 0.9054\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.8207\n",
      "Epoch 00024: val_loss improved from 0.35189 to 0.32842, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/024-0.3284.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5673 - acc: 0.8207 - val_loss: 0.3284 - val_acc: 0.9078\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.8256\n",
      "Epoch 00025: val_loss improved from 0.32842 to 0.32526, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/025-0.3253.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5535 - acc: 0.8256 - val_loss: 0.3253 - val_acc: 0.9094\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8309\n",
      "Epoch 00026: val_loss did not improve from 0.32526\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5369 - acc: 0.8309 - val_loss: 0.3315 - val_acc: 0.9064\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8323\n",
      "Epoch 00027: val_loss improved from 0.32526 to 0.31447, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/027-0.3145.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5329 - acc: 0.8323 - val_loss: 0.3145 - val_acc: 0.9122\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8363\n",
      "Epoch 00028: val_loss improved from 0.31447 to 0.30709, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/028-0.3071.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5145 - acc: 0.8363 - val_loss: 0.3071 - val_acc: 0.9143\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5079 - acc: 0.8399\n",
      "Epoch 00029: val_loss improved from 0.30709 to 0.29030, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/029-0.2903.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5079 - acc: 0.8399 - val_loss: 0.2903 - val_acc: 0.9192\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8449\n",
      "Epoch 00030: val_loss improved from 0.29030 to 0.28608, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/030-0.2861.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4941 - acc: 0.8449 - val_loss: 0.2861 - val_acc: 0.9213\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8488\n",
      "Epoch 00031: val_loss did not improve from 0.28608\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4847 - acc: 0.8489 - val_loss: 0.3282 - val_acc: 0.9085\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4723 - acc: 0.8543\n",
      "Epoch 00032: val_loss improved from 0.28608 to 0.27897, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/032-0.2790.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4724 - acc: 0.8543 - val_loss: 0.2790 - val_acc: 0.9227\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8530\n",
      "Epoch 00033: val_loss improved from 0.27897 to 0.26244, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/033-0.2624.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4661 - acc: 0.8530 - val_loss: 0.2624 - val_acc: 0.9264\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8592\n",
      "Epoch 00034: val_loss did not improve from 0.26244\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4549 - acc: 0.8591 - val_loss: 0.2921 - val_acc: 0.9194\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8568\n",
      "Epoch 00035: val_loss improved from 0.26244 to 0.25712, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/035-0.2571.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4565 - acc: 0.8568 - val_loss: 0.2571 - val_acc: 0.9264\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8640\n",
      "Epoch 00036: val_loss did not improve from 0.25712\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4405 - acc: 0.8641 - val_loss: 0.2651 - val_acc: 0.9241\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8626\n",
      "Epoch 00037: val_loss did not improve from 0.25712\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4324 - acc: 0.8627 - val_loss: 0.2716 - val_acc: 0.9234\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8632\n",
      "Epoch 00038: val_loss improved from 0.25712 to 0.25113, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/038-0.2511.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4293 - acc: 0.8632 - val_loss: 0.2511 - val_acc: 0.9294\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8671\n",
      "Epoch 00039: val_loss did not improve from 0.25113\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4184 - acc: 0.8671 - val_loss: 0.2832 - val_acc: 0.9194\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8732\n",
      "Epoch 00040: val_loss improved from 0.25113 to 0.23753, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/040-0.2375.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4055 - acc: 0.8732 - val_loss: 0.2375 - val_acc: 0.9324\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8723\n",
      "Epoch 00041: val_loss did not improve from 0.23753\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4064 - acc: 0.8723 - val_loss: 0.2458 - val_acc: 0.9322\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8749\n",
      "Epoch 00042: val_loss did not improve from 0.23753\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3986 - acc: 0.8749 - val_loss: 0.2417 - val_acc: 0.9311\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8784\n",
      "Epoch 00043: val_loss improved from 0.23753 to 0.23347, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/043-0.2335.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3917 - acc: 0.8784 - val_loss: 0.2335 - val_acc: 0.9366\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8796\n",
      "Epoch 00044: val_loss did not improve from 0.23347\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3857 - acc: 0.8796 - val_loss: 0.2620 - val_acc: 0.9306\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8779\n",
      "Epoch 00045: val_loss did not improve from 0.23347\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3819 - acc: 0.8779 - val_loss: 0.2416 - val_acc: 0.9338\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8790\n",
      "Epoch 00046: val_loss improved from 0.23347 to 0.22505, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/046-0.2250.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3814 - acc: 0.8790 - val_loss: 0.2250 - val_acc: 0.9359\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8840\n",
      "Epoch 00047: val_loss did not improve from 0.22505\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3737 - acc: 0.8840 - val_loss: 0.2443 - val_acc: 0.9306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8833\n",
      "Epoch 00048: val_loss improved from 0.22505 to 0.21515, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/048-0.2151.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3717 - acc: 0.8833 - val_loss: 0.2151 - val_acc: 0.9394\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8862\n",
      "Epoch 00049: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3550 - acc: 0.8862 - val_loss: 0.2236 - val_acc: 0.9385\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8873\n",
      "Epoch 00050: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3593 - acc: 0.8874 - val_loss: 0.2190 - val_acc: 0.9392\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8883\n",
      "Epoch 00051: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3513 - acc: 0.8884 - val_loss: 0.2197 - val_acc: 0.9378\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8905\n",
      "Epoch 00052: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3454 - acc: 0.8905 - val_loss: 0.2197 - val_acc: 0.9418\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8930\n",
      "Epoch 00053: val_loss did not improve from 0.21515\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3430 - acc: 0.8930 - val_loss: 0.2183 - val_acc: 0.9364\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8914\n",
      "Epoch 00054: val_loss improved from 0.21515 to 0.21244, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/054-0.2124.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3414 - acc: 0.8914 - val_loss: 0.2124 - val_acc: 0.9422\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8955\n",
      "Epoch 00055: val_loss improved from 0.21244 to 0.20580, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/055-0.2058.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3315 - acc: 0.8955 - val_loss: 0.2058 - val_acc: 0.9406\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3328 - acc: 0.8937\n",
      "Epoch 00056: val_loss did not improve from 0.20580\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3328 - acc: 0.8937 - val_loss: 0.2152 - val_acc: 0.9387\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8958\n",
      "Epoch 00057: val_loss did not improve from 0.20580\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3282 - acc: 0.8958 - val_loss: 0.2079 - val_acc: 0.9427\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8953\n",
      "Epoch 00058: val_loss improved from 0.20580 to 0.20094, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/058-0.2009.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3298 - acc: 0.8953 - val_loss: 0.2009 - val_acc: 0.9453\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8993\n",
      "Epoch 00059: val_loss did not improve from 0.20094\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3190 - acc: 0.8993 - val_loss: 0.2398 - val_acc: 0.9322\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8963\n",
      "Epoch 00060: val_loss did not improve from 0.20094\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3291 - acc: 0.8963 - val_loss: 0.2025 - val_acc: 0.9446\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9008\n",
      "Epoch 00061: val_loss improved from 0.20094 to 0.19800, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/061-0.1980.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3176 - acc: 0.9009 - val_loss: 0.1980 - val_acc: 0.9478\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9025\n",
      "Epoch 00062: val_loss improved from 0.19800 to 0.19748, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/062-0.1975.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3093 - acc: 0.9025 - val_loss: 0.1975 - val_acc: 0.9476\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9037\n",
      "Epoch 00063: val_loss did not improve from 0.19748\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3078 - acc: 0.9037 - val_loss: 0.2007 - val_acc: 0.9492\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.9045\n",
      "Epoch 00064: val_loss did not improve from 0.19748\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3060 - acc: 0.9045 - val_loss: 0.2150 - val_acc: 0.9359\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9037\n",
      "Epoch 00065: val_loss improved from 0.19748 to 0.19341, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/065-0.1934.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3052 - acc: 0.9037 - val_loss: 0.1934 - val_acc: 0.9469\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9060\n",
      "Epoch 00066: val_loss did not improve from 0.19341\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2975 - acc: 0.9060 - val_loss: 0.1966 - val_acc: 0.9436\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9069\n",
      "Epoch 00067: val_loss improved from 0.19341 to 0.18919, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/067-0.1892.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2954 - acc: 0.9069 - val_loss: 0.1892 - val_acc: 0.9513\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9072\n",
      "Epoch 00068: val_loss improved from 0.18919 to 0.18402, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/068-0.1840.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2969 - acc: 0.9072 - val_loss: 0.1840 - val_acc: 0.9483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2915 - acc: 0.9081\n",
      "Epoch 00069: val_loss did not improve from 0.18402\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2915 - acc: 0.9081 - val_loss: 0.1860 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9087\n",
      "Epoch 00070: val_loss improved from 0.18402 to 0.18290, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/070-0.1829.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2868 - acc: 0.9087 - val_loss: 0.1829 - val_acc: 0.9527\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9110\n",
      "Epoch 00071: val_loss improved from 0.18290 to 0.18167, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/071-0.1817.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2810 - acc: 0.9110 - val_loss: 0.1817 - val_acc: 0.9518\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9104\n",
      "Epoch 00072: val_loss did not improve from 0.18167\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2801 - acc: 0.9104 - val_loss: 0.1956 - val_acc: 0.9434\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9114\n",
      "Epoch 00073: val_loss did not improve from 0.18167\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2831 - acc: 0.9114 - val_loss: 0.1833 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9122\n",
      "Epoch 00074: val_loss did not improve from 0.18167\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2771 - acc: 0.9122 - val_loss: 0.1829 - val_acc: 0.9525\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2739 - acc: 0.9130\n",
      "Epoch 00075: val_loss improved from 0.18167 to 0.18130, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/075-0.1813.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2739 - acc: 0.9130 - val_loss: 0.1813 - val_acc: 0.9495\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9151\n",
      "Epoch 00076: val_loss did not improve from 0.18130\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2725 - acc: 0.9151 - val_loss: 0.1826 - val_acc: 0.9502\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9148\n",
      "Epoch 00077: val_loss improved from 0.18130 to 0.17470, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/077-0.1747.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2687 - acc: 0.9148 - val_loss: 0.1747 - val_acc: 0.9495\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9154\n",
      "Epoch 00078: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2657 - acc: 0.9154 - val_loss: 0.1795 - val_acc: 0.9515\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9144\n",
      "Epoch 00079: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2680 - acc: 0.9144 - val_loss: 0.1792 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9155\n",
      "Epoch 00080: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2655 - acc: 0.9155 - val_loss: 0.1788 - val_acc: 0.9525\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9167\n",
      "Epoch 00081: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2612 - acc: 0.9167 - val_loss: 0.1813 - val_acc: 0.9499\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9177\n",
      "Epoch 00082: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2573 - acc: 0.9178 - val_loss: 0.1869 - val_acc: 0.9488\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9205\n",
      "Epoch 00083: val_loss did not improve from 0.17470\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2525 - acc: 0.9204 - val_loss: 0.1800 - val_acc: 0.9518\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2568 - acc: 0.9177\n",
      "Epoch 00084: val_loss improved from 0.17470 to 0.17333, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/084-0.1733.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2567 - acc: 0.9177 - val_loss: 0.1733 - val_acc: 0.9529\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9199\n",
      "Epoch 00085: val_loss improved from 0.17333 to 0.17214, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/085-0.1721.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2511 - acc: 0.9199 - val_loss: 0.1721 - val_acc: 0.9522\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9204\n",
      "Epoch 00086: val_loss improved from 0.17214 to 0.16878, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/086-0.1688.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2502 - acc: 0.9204 - val_loss: 0.1688 - val_acc: 0.9529\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9215\n",
      "Epoch 00087: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2432 - acc: 0.9215 - val_loss: 0.1788 - val_acc: 0.9546\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9232\n",
      "Epoch 00088: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2421 - acc: 0.9232 - val_loss: 0.1756 - val_acc: 0.9520\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9218\n",
      "Epoch 00089: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2397 - acc: 0.9218 - val_loss: 0.1769 - val_acc: 0.9513\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9240\n",
      "Epoch 00090: val_loss improved from 0.16878 to 0.16574, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/090-0.1657.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2411 - acc: 0.9240 - val_loss: 0.1657 - val_acc: 0.9562\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9218\n",
      "Epoch 00091: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2393 - acc: 0.9219 - val_loss: 0.1769 - val_acc: 0.9557\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9232\n",
      "Epoch 00092: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2377 - acc: 0.9232 - val_loss: 0.1780 - val_acc: 0.9536\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9229\n",
      "Epoch 00093: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2405 - acc: 0.9229 - val_loss: 0.1665 - val_acc: 0.9574\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9247\n",
      "Epoch 00094: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2354 - acc: 0.9247 - val_loss: 0.1732 - val_acc: 0.9560\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9249\n",
      "Epoch 00095: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2360 - acc: 0.9249 - val_loss: 0.1794 - val_acc: 0.9541\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9257\n",
      "Epoch 00096: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2288 - acc: 0.9256 - val_loss: 0.1805 - val_acc: 0.9546\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9260\n",
      "Epoch 00097: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2293 - acc: 0.9260 - val_loss: 0.1909 - val_acc: 0.9527\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9264\n",
      "Epoch 00098: val_loss did not improve from 0.16574\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2256 - acc: 0.9264 - val_loss: 0.1689 - val_acc: 0.9553\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9292\n",
      "Epoch 00099: val_loss improved from 0.16574 to 0.16572, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/099-0.1657.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2217 - acc: 0.9292 - val_loss: 0.1657 - val_acc: 0.9569\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9289\n",
      "Epoch 00100: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2245 - acc: 0.9288 - val_loss: 0.1695 - val_acc: 0.9548\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9252\n",
      "Epoch 00101: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2295 - acc: 0.9252 - val_loss: 0.1772 - val_acc: 0.9571\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9277\n",
      "Epoch 00102: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2202 - acc: 0.9278 - val_loss: 0.1703 - val_acc: 0.9548\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9289\n",
      "Epoch 00103: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2233 - acc: 0.9289 - val_loss: 0.1744 - val_acc: 0.9546\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9293\n",
      "Epoch 00104: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2192 - acc: 0.9293 - val_loss: 0.1687 - val_acc: 0.9560\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9295\n",
      "Epoch 00105: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2185 - acc: 0.9295 - val_loss: 0.1682 - val_acc: 0.9564\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9306\n",
      "Epoch 00106: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2166 - acc: 0.9306 - val_loss: 0.1661 - val_acc: 0.9578\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9285\n",
      "Epoch 00107: val_loss did not improve from 0.16572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2181 - acc: 0.9285 - val_loss: 0.1660 - val_acc: 0.9583\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9317\n",
      "Epoch 00108: val_loss improved from 0.16572 to 0.15966, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/108-0.1597.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2148 - acc: 0.9317 - val_loss: 0.1597 - val_acc: 0.9557\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9322\n",
      "Epoch 00109: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2097 - acc: 0.9322 - val_loss: 0.1624 - val_acc: 0.9569\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9316\n",
      "Epoch 00110: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2130 - acc: 0.9316 - val_loss: 0.1668 - val_acc: 0.9562\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9337\n",
      "Epoch 00111: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2052 - acc: 0.9337 - val_loss: 0.1673 - val_acc: 0.9569\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9328\n",
      "Epoch 00112: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2071 - acc: 0.9328 - val_loss: 0.1599 - val_acc: 0.9588\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9332\n",
      "Epoch 00113: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2045 - acc: 0.9332 - val_loss: 0.1680 - val_acc: 0.9602\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9329\n",
      "Epoch 00114: val_loss did not improve from 0.15966\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2072 - acc: 0.9329 - val_loss: 0.1664 - val_acc: 0.9583\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9322\n",
      "Epoch 00115: val_loss improved from 0.15966 to 0.15778, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/115-0.1578.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2053 - acc: 0.9322 - val_loss: 0.1578 - val_acc: 0.9581\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9323\n",
      "Epoch 00116: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2052 - acc: 0.9323 - val_loss: 0.1650 - val_acc: 0.9588\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9337\n",
      "Epoch 00117: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2028 - acc: 0.9337 - val_loss: 0.1681 - val_acc: 0.9567\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9340\n",
      "Epoch 00118: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2065 - acc: 0.9340 - val_loss: 0.1633 - val_acc: 0.9583\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9345\n",
      "Epoch 00119: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1975 - acc: 0.9345 - val_loss: 0.1691 - val_acc: 0.9567\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9343\n",
      "Epoch 00120: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2014 - acc: 0.9343 - val_loss: 0.1582 - val_acc: 0.9569\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9366\n",
      "Epoch 00121: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1970 - acc: 0.9366 - val_loss: 0.1590 - val_acc: 0.9588\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9358\n",
      "Epoch 00122: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1945 - acc: 0.9359 - val_loss: 0.1691 - val_acc: 0.9557\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9363\n",
      "Epoch 00123: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1962 - acc: 0.9363 - val_loss: 0.1655 - val_acc: 0.9590\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9376\n",
      "Epoch 00124: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1889 - acc: 0.9376 - val_loss: 0.1625 - val_acc: 0.9564\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9385\n",
      "Epoch 00125: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1892 - acc: 0.9385 - val_loss: 0.1648 - val_acc: 0.9585\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9377\n",
      "Epoch 00126: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1929 - acc: 0.9376 - val_loss: 0.1581 - val_acc: 0.9590\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9399\n",
      "Epoch 00127: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1846 - acc: 0.9399 - val_loss: 0.1661 - val_acc: 0.9597\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9396\n",
      "Epoch 00128: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1895 - acc: 0.9396 - val_loss: 0.1626 - val_acc: 0.9606\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9397\n",
      "Epoch 00129: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1867 - acc: 0.9397 - val_loss: 0.1749 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9408\n",
      "Epoch 00130: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1815 - acc: 0.9408 - val_loss: 0.1748 - val_acc: 0.9534\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9385\n",
      "Epoch 00131: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1880 - acc: 0.9385 - val_loss: 0.1621 - val_acc: 0.9595\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9397\n",
      "Epoch 00132: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1857 - acc: 0.9397 - val_loss: 0.1650 - val_acc: 0.9590\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9393\n",
      "Epoch 00133: val_loss did not improve from 0.15778\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1863 - acc: 0.9393 - val_loss: 0.1770 - val_acc: 0.9567\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9381\n",
      "Epoch 00134: val_loss improved from 0.15778 to 0.15603, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/134-0.1560.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1850 - acc: 0.9381 - val_loss: 0.1560 - val_acc: 0.9599\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9397\n",
      "Epoch 00135: val_loss did not improve from 0.15603\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1830 - acc: 0.9397 - val_loss: 0.1618 - val_acc: 0.9534\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9403\n",
      "Epoch 00136: val_loss did not improve from 0.15603\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1801 - acc: 0.9403 - val_loss: 0.1646 - val_acc: 0.9623\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9420\n",
      "Epoch 00137: val_loss did not improve from 0.15603\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1789 - acc: 0.9420 - val_loss: 0.1581 - val_acc: 0.9606\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9408\n",
      "Epoch 00138: val_loss did not improve from 0.15603\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1795 - acc: 0.9408 - val_loss: 0.1665 - val_acc: 0.9595\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9414\n",
      "Epoch 00139: val_loss improved from 0.15603 to 0.15578, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/139-0.1558.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1782 - acc: 0.9414 - val_loss: 0.1558 - val_acc: 0.9604\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9400\n",
      "Epoch 00140: val_loss improved from 0.15578 to 0.15418, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/140-0.1542.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1838 - acc: 0.9400 - val_loss: 0.1542 - val_acc: 0.9616\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9449\n",
      "Epoch 00141: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1688 - acc: 0.9449 - val_loss: 0.1605 - val_acc: 0.9613\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9423\n",
      "Epoch 00142: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1733 - acc: 0.9423 - val_loss: 0.1595 - val_acc: 0.9599\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9438\n",
      "Epoch 00143: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1715 - acc: 0.9438 - val_loss: 0.1561 - val_acc: 0.9611\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9418\n",
      "Epoch 00144: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1748 - acc: 0.9418 - val_loss: 0.1605 - val_acc: 0.9590\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9436\n",
      "Epoch 00145: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1717 - acc: 0.9436 - val_loss: 0.1647 - val_acc: 0.9595\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9452\n",
      "Epoch 00146: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1676 - acc: 0.9452 - val_loss: 0.1608 - val_acc: 0.9630\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1672 - acc: 0.9442\n",
      "Epoch 00147: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1672 - acc: 0.9442 - val_loss: 0.1549 - val_acc: 0.9606\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9457\n",
      "Epoch 00148: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1684 - acc: 0.9457 - val_loss: 0.1833 - val_acc: 0.9567\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9455\n",
      "Epoch 00149: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1673 - acc: 0.9455 - val_loss: 0.1569 - val_acc: 0.9618\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9446\n",
      "Epoch 00150: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1669 - acc: 0.9446 - val_loss: 0.1581 - val_acc: 0.9623\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9463\n",
      "Epoch 00151: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1625 - acc: 0.9463 - val_loss: 0.1692 - val_acc: 0.9618\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9465\n",
      "Epoch 00152: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1605 - acc: 0.9465 - val_loss: 0.1604 - val_acc: 0.9604\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9468\n",
      "Epoch 00153: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1618 - acc: 0.9468 - val_loss: 0.1715 - val_acc: 0.9609\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9442\n",
      "Epoch 00154: val_loss did not improve from 0.15418\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1628 - acc: 0.9442 - val_loss: 0.1546 - val_acc: 0.9630\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9467\n",
      "Epoch 00155: val_loss improved from 0.15418 to 0.15306, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/155-0.1531.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1615 - acc: 0.9467 - val_loss: 0.1531 - val_acc: 0.9611\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9470\n",
      "Epoch 00156: val_loss did not improve from 0.15306\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1599 - acc: 0.9470 - val_loss: 0.1742 - val_acc: 0.9562\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9464\n",
      "Epoch 00157: val_loss did not improve from 0.15306\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1591 - acc: 0.9463 - val_loss: 0.1716 - val_acc: 0.9595\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9472\n",
      "Epoch 00158: val_loss improved from 0.15306 to 0.15264, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/158-0.1526.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1601 - acc: 0.9472 - val_loss: 0.1526 - val_acc: 0.9613\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9484\n",
      "Epoch 00159: val_loss did not improve from 0.15264\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1569 - acc: 0.9484 - val_loss: 0.1529 - val_acc: 0.9618\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9459\n",
      "Epoch 00160: val_loss did not improve from 0.15264\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1588 - acc: 0.9459 - val_loss: 0.1631 - val_acc: 0.9599\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9482\n",
      "Epoch 00161: val_loss improved from 0.15264 to 0.15212, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/161-0.1521.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1582 - acc: 0.9482 - val_loss: 0.1521 - val_acc: 0.9623\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9472\n",
      "Epoch 00162: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1584 - acc: 0.9472 - val_loss: 0.1724 - val_acc: 0.9623\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9474\n",
      "Epoch 00163: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1581 - acc: 0.9474 - val_loss: 0.1614 - val_acc: 0.9609\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9489\n",
      "Epoch 00164: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1530 - acc: 0.9489 - val_loss: 0.1625 - val_acc: 0.9597\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9492\n",
      "Epoch 00165: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1543 - acc: 0.9492 - val_loss: 0.1564 - val_acc: 0.9616\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9493\n",
      "Epoch 00166: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1517 - acc: 0.9494 - val_loss: 0.1619 - val_acc: 0.9581\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9486\n",
      "Epoch 00167: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1503 - acc: 0.9486 - val_loss: 0.1642 - val_acc: 0.9637\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9490\n",
      "Epoch 00168: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1530 - acc: 0.9490 - val_loss: 0.1586 - val_acc: 0.9606\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9486\n",
      "Epoch 00169: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1527 - acc: 0.9486 - val_loss: 0.1597 - val_acc: 0.9618\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9504\n",
      "Epoch 00170: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1497 - acc: 0.9504 - val_loss: 0.1604 - val_acc: 0.9644\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9484\n",
      "Epoch 00171: val_loss did not improve from 0.15212\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1525 - acc: 0.9484 - val_loss: 0.1631 - val_acc: 0.9609\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9471\n",
      "Epoch 00172: val_loss improved from 0.15212 to 0.14983, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_8_conv_checkpoint/172-0.1498.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1572 - acc: 0.9471 - val_loss: 0.1498 - val_acc: 0.9632\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9523\n",
      "Epoch 00173: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1425 - acc: 0.9523 - val_loss: 0.1629 - val_acc: 0.9616\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9512\n",
      "Epoch 00174: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1486 - acc: 0.9512 - val_loss: 0.1606 - val_acc: 0.9604\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9504\n",
      "Epoch 00175: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1497 - acc: 0.9504 - val_loss: 0.1713 - val_acc: 0.9560\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9521\n",
      "Epoch 00176: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1438 - acc: 0.9521 - val_loss: 0.1579 - val_acc: 0.9592\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9500\n",
      "Epoch 00177: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1505 - acc: 0.9500 - val_loss: 0.1555 - val_acc: 0.9632\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9524\n",
      "Epoch 00178: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1441 - acc: 0.9525 - val_loss: 0.1563 - val_acc: 0.9611\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9516\n",
      "Epoch 00179: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1427 - acc: 0.9516 - val_loss: 0.1785 - val_acc: 0.9590\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9497\n",
      "Epoch 00180: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1490 - acc: 0.9497 - val_loss: 0.1524 - val_acc: 0.9613\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9529\n",
      "Epoch 00181: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1413 - acc: 0.9529 - val_loss: 0.1644 - val_acc: 0.9604\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9518\n",
      "Epoch 00182: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1436 - acc: 0.9518 - val_loss: 0.1559 - val_acc: 0.9625\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9517\n",
      "Epoch 00183: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1431 - acc: 0.9517 - val_loss: 0.1663 - val_acc: 0.9597\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9531\n",
      "Epoch 00184: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1404 - acc: 0.9531 - val_loss: 0.1637 - val_acc: 0.9611\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9538\n",
      "Epoch 00185: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1385 - acc: 0.9538 - val_loss: 0.1576 - val_acc: 0.9597\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9536\n",
      "Epoch 00186: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1399 - acc: 0.9536 - val_loss: 0.1718 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9539\n",
      "Epoch 00187: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1392 - acc: 0.9539 - val_loss: 0.1640 - val_acc: 0.9616\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9535\n",
      "Epoch 00188: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1389 - acc: 0.9535 - val_loss: 0.1607 - val_acc: 0.9571\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9544\n",
      "Epoch 00189: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1359 - acc: 0.9544 - val_loss: 0.1824 - val_acc: 0.9560\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9522\n",
      "Epoch 00190: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1421 - acc: 0.9522 - val_loss: 0.1640 - val_acc: 0.9627\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9537\n",
      "Epoch 00191: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1359 - acc: 0.9537 - val_loss: 0.1809 - val_acc: 0.9585\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9530\n",
      "Epoch 00192: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1378 - acc: 0.9530 - val_loss: 0.1540 - val_acc: 0.9616\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9542\n",
      "Epoch 00193: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1354 - acc: 0.9542 - val_loss: 0.1681 - val_acc: 0.9620\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9548\n",
      "Epoch 00194: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1360 - acc: 0.9548 - val_loss: 0.1694 - val_acc: 0.9604\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9552\n",
      "Epoch 00195: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1335 - acc: 0.9552 - val_loss: 0.1760 - val_acc: 0.9599\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9548\n",
      "Epoch 00196: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1363 - acc: 0.9548 - val_loss: 0.1680 - val_acc: 0.9630\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9545\n",
      "Epoch 00197: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1352 - acc: 0.9545 - val_loss: 0.1717 - val_acc: 0.9609\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9550\n",
      "Epoch 00198: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1324 - acc: 0.9550 - val_loss: 0.1680 - val_acc: 0.9595\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9546\n",
      "Epoch 00199: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1316 - acc: 0.9546 - val_loss: 0.1676 - val_acc: 0.9604\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9559\n",
      "Epoch 00200: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1302 - acc: 0.9558 - val_loss: 0.1663 - val_acc: 0.9599\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9562\n",
      "Epoch 00201: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1308 - acc: 0.9562 - val_loss: 0.1558 - val_acc: 0.9595\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9559\n",
      "Epoch 00202: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1296 - acc: 0.9559 - val_loss: 0.1605 - val_acc: 0.9609\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9576\n",
      "Epoch 00203: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1277 - acc: 0.9576 - val_loss: 0.1629 - val_acc: 0.9604\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9574\n",
      "Epoch 00204: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1282 - acc: 0.9574 - val_loss: 0.1690 - val_acc: 0.9630\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9569\n",
      "Epoch 00205: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1289 - acc: 0.9569 - val_loss: 0.1616 - val_acc: 0.9623\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9583\n",
      "Epoch 00206: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1267 - acc: 0.9583 - val_loss: 0.1627 - val_acc: 0.9609\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9572\n",
      "Epoch 00207: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1266 - acc: 0.9572 - val_loss: 0.1612 - val_acc: 0.9597\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9572\n",
      "Epoch 00208: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1265 - acc: 0.9572 - val_loss: 0.1534 - val_acc: 0.9609\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9578\n",
      "Epoch 00209: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1243 - acc: 0.9578 - val_loss: 0.1699 - val_acc: 0.9630\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9577\n",
      "Epoch 00210: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1237 - acc: 0.9577 - val_loss: 0.1604 - val_acc: 0.9595\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9580\n",
      "Epoch 00211: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1253 - acc: 0.9580 - val_loss: 0.1628 - val_acc: 0.9602\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9594\n",
      "Epoch 00212: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1209 - acc: 0.9594 - val_loss: 0.1620 - val_acc: 0.9578\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9591\n",
      "Epoch 00213: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1185 - acc: 0.9591 - val_loss: 0.1829 - val_acc: 0.9618\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9562\n",
      "Epoch 00214: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1296 - acc: 0.9562 - val_loss: 0.1674 - val_acc: 0.9606\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9591\n",
      "Epoch 00215: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1230 - acc: 0.9591 - val_loss: 0.1655 - val_acc: 0.9581\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9603\n",
      "Epoch 00216: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1200 - acc: 0.9603 - val_loss: 0.1757 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9596\n",
      "Epoch 00217: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1205 - acc: 0.9596 - val_loss: 0.1617 - val_acc: 0.9644\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9591\n",
      "Epoch 00218: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1202 - acc: 0.9591 - val_loss: 0.1616 - val_acc: 0.9588\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9595\n",
      "Epoch 00219: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1231 - acc: 0.9595 - val_loss: 0.1807 - val_acc: 0.9595\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9590\n",
      "Epoch 00220: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1232 - acc: 0.9590 - val_loss: 0.1591 - val_acc: 0.9623\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9588\n",
      "Epoch 00221: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1211 - acc: 0.9588 - val_loss: 0.1707 - val_acc: 0.9599\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9611\n",
      "Epoch 00222: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1187 - acc: 0.9611 - val_loss: 0.1528 - val_acc: 0.9609\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 578us/sample - loss: 0.1983 - acc: 0.9464\n",
      "Loss: 0.1982507915310523 Accuracy: 0.94641745\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6389 - acc: 0.1340\n",
      "Epoch 00001: val_loss improved from inf to 2.23269, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/001-2.2327.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 2.6389 - acc: 0.1341 - val_loss: 2.2327 - val_acc: 0.3394\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2451 - acc: 0.2758\n",
      "Epoch 00002: val_loss improved from 2.23269 to 1.84421, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/002-1.8442.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.2451 - acc: 0.2757 - val_loss: 1.8442 - val_acc: 0.4799\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0041 - acc: 0.3504\n",
      "Epoch 00003: val_loss improved from 1.84421 to 1.57585, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/003-1.5758.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 2.0041 - acc: 0.3504 - val_loss: 1.5758 - val_acc: 0.5323\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8168 - acc: 0.4054\n",
      "Epoch 00004: val_loss improved from 1.57585 to 1.38928, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/004-1.3893.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.8168 - acc: 0.4054 - val_loss: 1.3893 - val_acc: 0.5798\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6943 - acc: 0.4412\n",
      "Epoch 00005: val_loss improved from 1.38928 to 1.28065, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/005-1.2807.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.6943 - acc: 0.4413 - val_loss: 1.2807 - val_acc: 0.6189\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5828 - acc: 0.4718\n",
      "Epoch 00006: val_loss improved from 1.28065 to 1.16844, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/006-1.1684.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.5829 - acc: 0.4718 - val_loss: 1.1684 - val_acc: 0.6618\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4967 - acc: 0.5029\n",
      "Epoch 00007: val_loss improved from 1.16844 to 1.05270, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/007-1.0527.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4968 - acc: 0.5028 - val_loss: 1.0527 - val_acc: 0.6776\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4084 - acc: 0.5296\n",
      "Epoch 00008: val_loss improved from 1.05270 to 0.95697, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/008-0.9570.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.4084 - acc: 0.5295 - val_loss: 0.9570 - val_acc: 0.7154\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3494 - acc: 0.5518\n",
      "Epoch 00009: val_loss improved from 0.95697 to 0.92177, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/009-0.9218.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3495 - acc: 0.5518 - val_loss: 0.9218 - val_acc: 0.7431\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2856 - acc: 0.5737\n",
      "Epoch 00010: val_loss improved from 0.92177 to 0.84059, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/010-0.8406.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2856 - acc: 0.5737 - val_loss: 0.8406 - val_acc: 0.7519\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.5913\n",
      "Epoch 00011: val_loss improved from 0.84059 to 0.79048, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/011-0.7905.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.2321 - acc: 0.5913 - val_loss: 0.7905 - val_acc: 0.7675\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1889 - acc: 0.6061\n",
      "Epoch 00012: val_loss improved from 0.79048 to 0.75000, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/012-0.7500.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1889 - acc: 0.6062 - val_loss: 0.7500 - val_acc: 0.7920\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1390 - acc: 0.6227\n",
      "Epoch 00013: val_loss improved from 0.75000 to 0.70367, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/013-0.7037.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.1390 - acc: 0.6227 - val_loss: 0.7037 - val_acc: 0.7959\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0961 - acc: 0.6368\n",
      "Epoch 00014: val_loss improved from 0.70367 to 0.66974, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/014-0.6697.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0960 - acc: 0.6369 - val_loss: 0.6697 - val_acc: 0.8064\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0612 - acc: 0.6498\n",
      "Epoch 00015: val_loss improved from 0.66974 to 0.63375, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/015-0.6338.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0612 - acc: 0.6498 - val_loss: 0.6338 - val_acc: 0.8209\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0244 - acc: 0.6627\n",
      "Epoch 00016: val_loss improved from 0.63375 to 0.61499, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/016-0.6150.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.0244 - acc: 0.6627 - val_loss: 0.6150 - val_acc: 0.8239\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9945 - acc: 0.6708\n",
      "Epoch 00017: val_loss improved from 0.61499 to 0.58380, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/017-0.5838.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9946 - acc: 0.6707 - val_loss: 0.5838 - val_acc: 0.8251\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.6795\n",
      "Epoch 00018: val_loss improved from 0.58380 to 0.54527, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/018-0.5453.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9631 - acc: 0.6795 - val_loss: 0.5453 - val_acc: 0.8400\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9451 - acc: 0.6889\n",
      "Epoch 00019: val_loss improved from 0.54527 to 0.52378, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/019-0.5238.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9450 - acc: 0.6889 - val_loss: 0.5238 - val_acc: 0.8428\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9104 - acc: 0.6993\n",
      "Epoch 00020: val_loss improved from 0.52378 to 0.50937, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/020-0.5094.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9104 - acc: 0.6993 - val_loss: 0.5094 - val_acc: 0.8495\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8887 - acc: 0.7081\n",
      "Epoch 00021: val_loss improved from 0.50937 to 0.48291, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/021-0.4829.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8888 - acc: 0.7081 - val_loss: 0.4829 - val_acc: 0.8588\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8709 - acc: 0.7149\n",
      "Epoch 00022: val_loss did not improve from 0.48291\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8709 - acc: 0.7149 - val_loss: 0.4916 - val_acc: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8463 - acc: 0.7201\n",
      "Epoch 00023: val_loss improved from 0.48291 to 0.46827, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/023-0.4683.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8463 - acc: 0.7201 - val_loss: 0.4683 - val_acc: 0.8577\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8329 - acc: 0.7255\n",
      "Epoch 00024: val_loss did not improve from 0.46827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8329 - acc: 0.7256 - val_loss: 0.4787 - val_acc: 0.8572\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7313\n",
      "Epoch 00025: val_loss did not improve from 0.46827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.8169 - acc: 0.7313 - val_loss: 0.5138 - val_acc: 0.8479\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7377\n",
      "Epoch 00026: val_loss improved from 0.46827 to 0.43143, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/026-0.4314.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7967 - acc: 0.7377 - val_loss: 0.4314 - val_acc: 0.8726\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.7424\n",
      "Epoch 00027: val_loss improved from 0.43143 to 0.41228, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/027-0.4123.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7759 - acc: 0.7425 - val_loss: 0.4123 - val_acc: 0.8717\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7575 - acc: 0.7489\n",
      "Epoch 00028: val_loss improved from 0.41228 to 0.40267, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/028-0.4027.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7575 - acc: 0.7489 - val_loss: 0.4027 - val_acc: 0.8761\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7499\n",
      "Epoch 00029: val_loss improved from 0.40267 to 0.38964, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/029-0.3896.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7529 - acc: 0.7499 - val_loss: 0.3896 - val_acc: 0.8824\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7517\n",
      "Epoch 00030: val_loss did not improve from 0.38964\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7500 - acc: 0.7517 - val_loss: 0.4053 - val_acc: 0.8824\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7640\n",
      "Epoch 00031: val_loss improved from 0.38964 to 0.37336, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/031-0.3734.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7173 - acc: 0.7639 - val_loss: 0.3734 - val_acc: 0.8852\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7663\n",
      "Epoch 00032: val_loss improved from 0.37336 to 0.35039, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/032-0.3504.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7138 - acc: 0.7663 - val_loss: 0.3504 - val_acc: 0.8952\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6992 - acc: 0.7712\n",
      "Epoch 00033: val_loss improved from 0.35039 to 0.34817, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/033-0.3482.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6991 - acc: 0.7712 - val_loss: 0.3482 - val_acc: 0.8968\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.7698\n",
      "Epoch 00034: val_loss improved from 0.34817 to 0.34515, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/034-0.3452.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6929 - acc: 0.7698 - val_loss: 0.3452 - val_acc: 0.9003\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.7787\n",
      "Epoch 00035: val_loss improved from 0.34515 to 0.34279, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/035-0.3428.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6761 - acc: 0.7787 - val_loss: 0.3428 - val_acc: 0.8982\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6719 - acc: 0.7776\n",
      "Epoch 00036: val_loss improved from 0.34279 to 0.32846, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/036-0.3285.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6718 - acc: 0.7776 - val_loss: 0.3285 - val_acc: 0.8996\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.7833\n",
      "Epoch 00037: val_loss did not improve from 0.32846\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6557 - acc: 0.7833 - val_loss: 0.3942 - val_acc: 0.8784\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.7872\n",
      "Epoch 00038: val_loss did not improve from 0.32846\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6493 - acc: 0.7872 - val_loss: 0.3417 - val_acc: 0.9017\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.7872\n",
      "Epoch 00039: val_loss did not improve from 0.32846\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6463 - acc: 0.7872 - val_loss: 0.3309 - val_acc: 0.9038\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6223 - acc: 0.7950\n",
      "Epoch 00040: val_loss improved from 0.32846 to 0.32345, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/040-0.3234.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6223 - acc: 0.7949 - val_loss: 0.3234 - val_acc: 0.9061\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6308 - acc: 0.7918\n",
      "Epoch 00041: val_loss did not improve from 0.32345\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6309 - acc: 0.7918 - val_loss: 0.3270 - val_acc: 0.9033\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6199 - acc: 0.7951\n",
      "Epoch 00042: val_loss improved from 0.32345 to 0.31281, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/042-0.3128.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6199 - acc: 0.7951 - val_loss: 0.3128 - val_acc: 0.9057\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6107 - acc: 0.7985\n",
      "Epoch 00043: val_loss improved from 0.31281 to 0.30000, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/043-0.3000.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6108 - acc: 0.7985 - val_loss: 0.3000 - val_acc: 0.9136\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.7997\n",
      "Epoch 00044: val_loss did not improve from 0.30000\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.6032 - acc: 0.7996 - val_loss: 0.3362 - val_acc: 0.9026\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8032\n",
      "Epoch 00045: val_loss improved from 0.30000 to 0.29920, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/045-0.2992.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5957 - acc: 0.8032 - val_loss: 0.2992 - val_acc: 0.9115\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5862 - acc: 0.8058\n",
      "Epoch 00046: val_loss did not improve from 0.29920\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5862 - acc: 0.8059 - val_loss: 0.3190 - val_acc: 0.9057\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.8110\n",
      "Epoch 00047: val_loss improved from 0.29920 to 0.29691, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/047-0.2969.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5801 - acc: 0.8110 - val_loss: 0.2969 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5672 - acc: 0.8162\n",
      "Epoch 00048: val_loss did not improve from 0.29691\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5673 - acc: 0.8162 - val_loss: 0.2973 - val_acc: 0.9113\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.8142\n",
      "Epoch 00049: val_loss improved from 0.29691 to 0.27235, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/049-0.2723.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5649 - acc: 0.8142 - val_loss: 0.2723 - val_acc: 0.9206\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8152\n",
      "Epoch 00050: val_loss did not improve from 0.27235\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5598 - acc: 0.8153 - val_loss: 0.2792 - val_acc: 0.9192\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5520 - acc: 0.8200\n",
      "Epoch 00051: val_loss improved from 0.27235 to 0.26583, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/051-0.2658.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5520 - acc: 0.8200 - val_loss: 0.2658 - val_acc: 0.9220\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5529 - acc: 0.8168\n",
      "Epoch 00052: val_loss did not improve from 0.26583\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5529 - acc: 0.8168 - val_loss: 0.2806 - val_acc: 0.9220\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.8198\n",
      "Epoch 00053: val_loss did not improve from 0.26583\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5513 - acc: 0.8198 - val_loss: 0.2693 - val_acc: 0.9201\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8213\n",
      "Epoch 00054: val_loss did not improve from 0.26583\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5439 - acc: 0.8213 - val_loss: 0.2710 - val_acc: 0.9203\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.8212\n",
      "Epoch 00055: val_loss did not improve from 0.26583\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5415 - acc: 0.8212 - val_loss: 0.2777 - val_acc: 0.9173\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.8264\n",
      "Epoch 00056: val_loss improved from 0.26583 to 0.25210, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/056-0.2521.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5328 - acc: 0.8264 - val_loss: 0.2521 - val_acc: 0.9250\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.8248\n",
      "Epoch 00057: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5294 - acc: 0.8248 - val_loss: 0.2862 - val_acc: 0.9157\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8273\n",
      "Epoch 00058: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5241 - acc: 0.8273 - val_loss: 0.2740 - val_acc: 0.9227\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8285\n",
      "Epoch 00059: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5174 - acc: 0.8285 - val_loss: 0.2596 - val_acc: 0.9220\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8340\n",
      "Epoch 00060: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5073 - acc: 0.8341 - val_loss: 0.2537 - val_acc: 0.9250\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4990 - acc: 0.8382\n",
      "Epoch 00061: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4991 - acc: 0.8382 - val_loss: 0.2761 - val_acc: 0.9175\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8336\n",
      "Epoch 00062: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5090 - acc: 0.8335 - val_loss: 0.2574 - val_acc: 0.9266\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8377\n",
      "Epoch 00063: val_loss did not improve from 0.25210\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5015 - acc: 0.8377 - val_loss: 0.2598 - val_acc: 0.9255\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8402\n",
      "Epoch 00064: val_loss improved from 0.25210 to 0.24326, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/064-0.2433.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4874 - acc: 0.8402 - val_loss: 0.2433 - val_acc: 0.9322\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4878 - acc: 0.8412\n",
      "Epoch 00065: val_loss did not improve from 0.24326\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4878 - acc: 0.8412 - val_loss: 0.2529 - val_acc: 0.9287\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4912 - acc: 0.8397\n",
      "Epoch 00066: val_loss did not improve from 0.24326\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4912 - acc: 0.8397 - val_loss: 0.2516 - val_acc: 0.9271\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4819 - acc: 0.8422\n",
      "Epoch 00067: val_loss did not improve from 0.24326\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4819 - acc: 0.8422 - val_loss: 0.2568 - val_acc: 0.9304\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8446\n",
      "Epoch 00068: val_loss did not improve from 0.24326\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4750 - acc: 0.8446 - val_loss: 0.2439 - val_acc: 0.9294\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8408\n",
      "Epoch 00069: val_loss did not improve from 0.24326\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4783 - acc: 0.8408 - val_loss: 0.2501 - val_acc: 0.9266\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8464\n",
      "Epoch 00070: val_loss improved from 0.24326 to 0.23897, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/070-0.2390.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4715 - acc: 0.8464 - val_loss: 0.2390 - val_acc: 0.9320\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8444\n",
      "Epoch 00071: val_loss did not improve from 0.23897\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4659 - acc: 0.8444 - val_loss: 0.2415 - val_acc: 0.9313\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4600 - acc: 0.8488\n",
      "Epoch 00072: val_loss did not improve from 0.23897\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4601 - acc: 0.8488 - val_loss: 0.2454 - val_acc: 0.9301\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4608 - acc: 0.8496\n",
      "Epoch 00073: val_loss improved from 0.23897 to 0.23859, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/073-0.2386.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4608 - acc: 0.8496 - val_loss: 0.2386 - val_acc: 0.9306\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8511\n",
      "Epoch 00074: val_loss improved from 0.23859 to 0.23036, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/074-0.2304.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4504 - acc: 0.8511 - val_loss: 0.2304 - val_acc: 0.9385\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.8482\n",
      "Epoch 00075: val_loss improved from 0.23036 to 0.22823, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/075-0.2282.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4560 - acc: 0.8482 - val_loss: 0.2282 - val_acc: 0.9385\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8531\n",
      "Epoch 00076: val_loss did not improve from 0.22823\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4488 - acc: 0.8531 - val_loss: 0.2335 - val_acc: 0.9320\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8545\n",
      "Epoch 00077: val_loss improved from 0.22823 to 0.21951, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/077-0.2195.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4413 - acc: 0.8544 - val_loss: 0.2195 - val_acc: 0.9390\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.8553\n",
      "Epoch 00078: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4395 - acc: 0.8553 - val_loss: 0.2257 - val_acc: 0.9352\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8563\n",
      "Epoch 00079: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4367 - acc: 0.8563 - val_loss: 0.2460 - val_acc: 0.9304\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8550\n",
      "Epoch 00080: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4365 - acc: 0.8550 - val_loss: 0.2259 - val_acc: 0.9369\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8576\n",
      "Epoch 00081: val_loss did not improve from 0.21951\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4286 - acc: 0.8576 - val_loss: 0.2252 - val_acc: 0.9366\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.8585\n",
      "Epoch 00082: val_loss improved from 0.21951 to 0.21918, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/082-0.2192.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4275 - acc: 0.8585 - val_loss: 0.2192 - val_acc: 0.9348\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8647\n",
      "Epoch 00083: val_loss improved from 0.21918 to 0.21537, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/083-0.2154.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4172 - acc: 0.8647 - val_loss: 0.2154 - val_acc: 0.9378\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8624\n",
      "Epoch 00084: val_loss did not improve from 0.21537\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4192 - acc: 0.8624 - val_loss: 0.2244 - val_acc: 0.9329\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4170 - acc: 0.8616\n",
      "Epoch 00085: val_loss improved from 0.21537 to 0.21525, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/085-0.2153.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4170 - acc: 0.8616 - val_loss: 0.2153 - val_acc: 0.9397\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8632\n",
      "Epoch 00086: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4148 - acc: 0.8632 - val_loss: 0.2241 - val_acc: 0.9364\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8638\n",
      "Epoch 00087: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4099 - acc: 0.8638 - val_loss: 0.2239 - val_acc: 0.9336\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8649\n",
      "Epoch 00088: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4121 - acc: 0.8649 - val_loss: 0.2409 - val_acc: 0.9320\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4007 - acc: 0.8662\n",
      "Epoch 00089: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4007 - acc: 0.8662 - val_loss: 0.2206 - val_acc: 0.9383\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8653\n",
      "Epoch 00090: val_loss did not improve from 0.21525\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4049 - acc: 0.8653 - val_loss: 0.2342 - val_acc: 0.9350\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8667\n",
      "Epoch 00091: val_loss improved from 0.21525 to 0.20541, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/091-0.2054.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4022 - acc: 0.8666 - val_loss: 0.2054 - val_acc: 0.9404\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8713\n",
      "Epoch 00092: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3905 - acc: 0.8713 - val_loss: 0.2213 - val_acc: 0.9366\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3973 - acc: 0.8668\n",
      "Epoch 00093: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3974 - acc: 0.8668 - val_loss: 0.2159 - val_acc: 0.9383\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8646\n",
      "Epoch 00094: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3994 - acc: 0.8646 - val_loss: 0.2168 - val_acc: 0.9385\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8693\n",
      "Epoch 00095: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3895 - acc: 0.8693 - val_loss: 0.2224 - val_acc: 0.9373\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8721\n",
      "Epoch 00096: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3875 - acc: 0.8722 - val_loss: 0.2227 - val_acc: 0.9352\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8745\n",
      "Epoch 00097: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3820 - acc: 0.8745 - val_loss: 0.2151 - val_acc: 0.9404\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8748\n",
      "Epoch 00098: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3767 - acc: 0.8748 - val_loss: 0.2343 - val_acc: 0.9350\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8716\n",
      "Epoch 00099: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3845 - acc: 0.8716 - val_loss: 0.2236 - val_acc: 0.9373\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8733\n",
      "Epoch 00100: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3798 - acc: 0.8733 - val_loss: 0.2430 - val_acc: 0.9376\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8752\n",
      "Epoch 00101: val_loss did not improve from 0.20541\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3750 - acc: 0.8752 - val_loss: 0.2119 - val_acc: 0.9432\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8752\n",
      "Epoch 00102: val_loss improved from 0.20541 to 0.20328, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/102-0.2033.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3757 - acc: 0.8752 - val_loss: 0.2033 - val_acc: 0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8785\n",
      "Epoch 00103: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3635 - acc: 0.8785 - val_loss: 0.2231 - val_acc: 0.9352\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8775\n",
      "Epoch 00104: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3625 - acc: 0.8775 - val_loss: 0.2139 - val_acc: 0.9406\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8801\n",
      "Epoch 00105: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3535 - acc: 0.8801 - val_loss: 0.2271 - val_acc: 0.9406\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8802\n",
      "Epoch 00106: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3641 - acc: 0.8802 - val_loss: 0.2152 - val_acc: 0.9436\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8827\n",
      "Epoch 00107: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3538 - acc: 0.8827 - val_loss: 0.2167 - val_acc: 0.9359\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8763\n",
      "Epoch 00108: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3706 - acc: 0.8763 - val_loss: 0.2163 - val_acc: 0.9420\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8809\n",
      "Epoch 00109: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3556 - acc: 0.8809 - val_loss: 0.2185 - val_acc: 0.9387\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8818\n",
      "Epoch 00110: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3510 - acc: 0.8818 - val_loss: 0.2107 - val_acc: 0.9383\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8832\n",
      "Epoch 00111: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3528 - acc: 0.8832 - val_loss: 0.2064 - val_acc: 0.9385\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8830\n",
      "Epoch 00112: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3453 - acc: 0.8830 - val_loss: 0.2106 - val_acc: 0.9418\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8828\n",
      "Epoch 00113: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3434 - acc: 0.8828 - val_loss: 0.2250 - val_acc: 0.9373\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8863\n",
      "Epoch 00114: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3424 - acc: 0.8863 - val_loss: 0.2274 - val_acc: 0.9373\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8850\n",
      "Epoch 00115: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3407 - acc: 0.8850 - val_loss: 0.2110 - val_acc: 0.9425\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8843\n",
      "Epoch 00116: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3431 - acc: 0.8843 - val_loss: 0.2083 - val_acc: 0.9390\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8871\n",
      "Epoch 00117: val_loss did not improve from 0.20328\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3353 - acc: 0.8871 - val_loss: 0.2110 - val_acc: 0.9418\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8863\n",
      "Epoch 00118: val_loss improved from 0.20328 to 0.20270, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/118-0.2027.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3392 - acc: 0.8863 - val_loss: 0.2027 - val_acc: 0.9439\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8877\n",
      "Epoch 00119: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3320 - acc: 0.8877 - val_loss: 0.2057 - val_acc: 0.9406\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8898\n",
      "Epoch 00120: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3310 - acc: 0.8899 - val_loss: 0.2209 - val_acc: 0.9420\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8901\n",
      "Epoch 00121: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3270 - acc: 0.8901 - val_loss: 0.2247 - val_acc: 0.9392\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8887\n",
      "Epoch 00122: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3295 - acc: 0.8886 - val_loss: 0.2115 - val_acc: 0.9392\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8912\n",
      "Epoch 00123: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3267 - acc: 0.8912 - val_loss: 0.2106 - val_acc: 0.9404\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8891\n",
      "Epoch 00124: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3252 - acc: 0.8891 - val_loss: 0.2057 - val_acc: 0.9420\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8917\n",
      "Epoch 00125: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3202 - acc: 0.8918 - val_loss: 0.2091 - val_acc: 0.9385\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8932\n",
      "Epoch 00126: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3162 - acc: 0.8932 - val_loss: 0.2217 - val_acc: 0.9338\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8930\n",
      "Epoch 00127: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3165 - acc: 0.8930 - val_loss: 0.2081 - val_acc: 0.9415\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.8909\n",
      "Epoch 00128: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3169 - acc: 0.8909 - val_loss: 0.2038 - val_acc: 0.9401\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8916\n",
      "Epoch 00129: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3173 - acc: 0.8916 - val_loss: 0.2079 - val_acc: 0.9387\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8943\n",
      "Epoch 00130: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3139 - acc: 0.8943 - val_loss: 0.2044 - val_acc: 0.9418\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8944\n",
      "Epoch 00131: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3114 - acc: 0.8944 - val_loss: 0.2216 - val_acc: 0.9401\n",
      "Epoch 132/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.8959\n",
      "Epoch 00132: val_loss did not improve from 0.20270\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3066 - acc: 0.8959 - val_loss: 0.2075 - val_acc: 0.9394\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3082 - acc: 0.8948\n",
      "Epoch 00133: val_loss improved from 0.20270 to 0.20264, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/133-0.2026.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3082 - acc: 0.8948 - val_loss: 0.2026 - val_acc: 0.9415\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8932\n",
      "Epoch 00134: val_loss improved from 0.20264 to 0.20232, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/134-0.2023.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3132 - acc: 0.8932 - val_loss: 0.2023 - val_acc: 0.9399\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.8998\n",
      "Epoch 00135: val_loss did not improve from 0.20232\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2975 - acc: 0.8999 - val_loss: 0.2147 - val_acc: 0.9387\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.8997\n",
      "Epoch 00136: val_loss did not improve from 0.20232\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2941 - acc: 0.8997 - val_loss: 0.2256 - val_acc: 0.9418\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.8951\n",
      "Epoch 00137: val_loss did not improve from 0.20232\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3065 - acc: 0.8951 - val_loss: 0.2087 - val_acc: 0.9397\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3014 - acc: 0.8984\n",
      "Epoch 00138: val_loss improved from 0.20232 to 0.20025, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/138-0.2002.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3014 - acc: 0.8984 - val_loss: 0.2002 - val_acc: 0.9453\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9000\n",
      "Epoch 00139: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2924 - acc: 0.9000 - val_loss: 0.2043 - val_acc: 0.9441\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.8994\n",
      "Epoch 00140: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2918 - acc: 0.8994 - val_loss: 0.2108 - val_acc: 0.9453\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2900 - acc: 0.9000\n",
      "Epoch 00141: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2901 - acc: 0.9000 - val_loss: 0.2122 - val_acc: 0.9408\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.8982\n",
      "Epoch 00142: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2924 - acc: 0.8982 - val_loss: 0.2147 - val_acc: 0.9406\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.8976\n",
      "Epoch 00143: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2973 - acc: 0.8976 - val_loss: 0.2100 - val_acc: 0.9427\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9027\n",
      "Epoch 00144: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2853 - acc: 0.9026 - val_loss: 0.2108 - val_acc: 0.9408\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.9002\n",
      "Epoch 00145: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2936 - acc: 0.9001 - val_loss: 0.2125 - val_acc: 0.9446\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9014\n",
      "Epoch 00146: val_loss did not improve from 0.20025\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2905 - acc: 0.9014 - val_loss: 0.2010 - val_acc: 0.9425\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9041\n",
      "Epoch 00147: val_loss improved from 0.20025 to 0.19827, saving model to model/checkpoint/1D_CNN_custom_2_ch_64_DO_9_conv_checkpoint/147-0.1983.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2821 - acc: 0.9041 - val_loss: 0.1983 - val_acc: 0.9418\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9010\n",
      "Epoch 00148: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2875 - acc: 0.9010 - val_loss: 0.2086 - val_acc: 0.9448\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9025\n",
      "Epoch 00149: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2818 - acc: 0.9025 - val_loss: 0.2017 - val_acc: 0.9443\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9017\n",
      "Epoch 00150: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2825 - acc: 0.9018 - val_loss: 0.2002 - val_acc: 0.9446\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9029\n",
      "Epoch 00151: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2792 - acc: 0.9029 - val_loss: 0.2053 - val_acc: 0.9425\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2786 - acc: 0.9027\n",
      "Epoch 00152: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2786 - acc: 0.9027 - val_loss: 0.2052 - val_acc: 0.9462\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2826 - acc: 0.9039\n",
      "Epoch 00153: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2826 - acc: 0.9040 - val_loss: 0.2128 - val_acc: 0.9443\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9053\n",
      "Epoch 00154: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2737 - acc: 0.9053 - val_loss: 0.2198 - val_acc: 0.9439\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9036\n",
      "Epoch 00155: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2725 - acc: 0.9036 - val_loss: 0.2285 - val_acc: 0.9404\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9023\n",
      "Epoch 00156: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2815 - acc: 0.9023 - val_loss: 0.2091 - val_acc: 0.9471\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.9094\n",
      "Epoch 00157: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2657 - acc: 0.9094 - val_loss: 0.2090 - val_acc: 0.9469\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.9057\n",
      "Epoch 00158: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2702 - acc: 0.9057 - val_loss: 0.2141 - val_acc: 0.9439\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9085\n",
      "Epoch 00159: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2655 - acc: 0.9085 - val_loss: 0.2124 - val_acc: 0.9455\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9057\n",
      "Epoch 00160: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2691 - acc: 0.9057 - val_loss: 0.2109 - val_acc: 0.9450\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.9079\n",
      "Epoch 00161: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2640 - acc: 0.9079 - val_loss: 0.2197 - val_acc: 0.9411\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9103\n",
      "Epoch 00162: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2603 - acc: 0.9103 - val_loss: 0.2188 - val_acc: 0.9443\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9108\n",
      "Epoch 00163: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2598 - acc: 0.9108 - val_loss: 0.2067 - val_acc: 0.9474\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9069\n",
      "Epoch 00164: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2643 - acc: 0.9069 - val_loss: 0.2463 - val_acc: 0.9408\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.9114\n",
      "Epoch 00165: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2581 - acc: 0.9114 - val_loss: 0.2190 - val_acc: 0.9427\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9103\n",
      "Epoch 00166: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2589 - acc: 0.9103 - val_loss: 0.2158 - val_acc: 0.9467\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.9112\n",
      "Epoch 00167: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2611 - acc: 0.9112 - val_loss: 0.2297 - val_acc: 0.9373\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.9103\n",
      "Epoch 00168: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2601 - acc: 0.9103 - val_loss: 0.1999 - val_acc: 0.9467\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9126\n",
      "Epoch 00169: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2544 - acc: 0.9126 - val_loss: 0.2108 - val_acc: 0.9462\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9120\n",
      "Epoch 00170: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2529 - acc: 0.9121 - val_loss: 0.2163 - val_acc: 0.9455\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9117\n",
      "Epoch 00171: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2523 - acc: 0.9117 - val_loss: 0.2145 - val_acc: 0.9434\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9119\n",
      "Epoch 00172: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2512 - acc: 0.9119 - val_loss: 0.2076 - val_acc: 0.9441\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9118\n",
      "Epoch 00173: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2517 - acc: 0.9118 - val_loss: 0.2155 - val_acc: 0.9457\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9125\n",
      "Epoch 00174: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2544 - acc: 0.9125 - val_loss: 0.2297 - val_acc: 0.9427\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9129\n",
      "Epoch 00175: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2493 - acc: 0.9129 - val_loss: 0.2046 - val_acc: 0.9450\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.9130\n",
      "Epoch 00176: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2492 - acc: 0.9131 - val_loss: 0.2355 - val_acc: 0.9462\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9144\n",
      "Epoch 00177: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2484 - acc: 0.9144 - val_loss: 0.2099 - val_acc: 0.9455\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9140\n",
      "Epoch 00178: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2478 - acc: 0.9141 - val_loss: 0.2165 - val_acc: 0.9455\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9137\n",
      "Epoch 00179: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2464 - acc: 0.9138 - val_loss: 0.2197 - val_acc: 0.9455\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9154\n",
      "Epoch 00180: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2430 - acc: 0.9154 - val_loss: 0.2106 - val_acc: 0.9478\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9164\n",
      "Epoch 00181: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2411 - acc: 0.9163 - val_loss: 0.2118 - val_acc: 0.9483\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9154\n",
      "Epoch 00182: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2434 - acc: 0.9153 - val_loss: 0.2204 - val_acc: 0.9483\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9156\n",
      "Epoch 00183: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2462 - acc: 0.9156 - val_loss: 0.2253 - val_acc: 0.9413\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9163\n",
      "Epoch 00184: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2413 - acc: 0.9163 - val_loss: 0.2138 - val_acc: 0.9478\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9165\n",
      "Epoch 00185: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2342 - acc: 0.9165 - val_loss: 0.2302 - val_acc: 0.9441\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9162\n",
      "Epoch 00186: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2395 - acc: 0.9162 - val_loss: 0.2345 - val_acc: 0.9427\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9150\n",
      "Epoch 00187: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2402 - acc: 0.9150 - val_loss: 0.2222 - val_acc: 0.9453\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9167\n",
      "Epoch 00188: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2347 - acc: 0.9166 - val_loss: 0.2375 - val_acc: 0.9432\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9162\n",
      "Epoch 00189: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2393 - acc: 0.9162 - val_loss: 0.2269 - val_acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9181\n",
      "Epoch 00190: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2373 - acc: 0.9181 - val_loss: 0.2149 - val_acc: 0.9497\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9194\n",
      "Epoch 00191: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2352 - acc: 0.9194 - val_loss: 0.2269 - val_acc: 0.9450\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9186\n",
      "Epoch 00192: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2339 - acc: 0.9187 - val_loss: 0.2121 - val_acc: 0.9474\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9203\n",
      "Epoch 00193: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2260 - acc: 0.9203 - val_loss: 0.2247 - val_acc: 0.9471\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9187\n",
      "Epoch 00194: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2331 - acc: 0.9187 - val_loss: 0.2290 - val_acc: 0.9429\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9192\n",
      "Epoch 00195: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2319 - acc: 0.9192 - val_loss: 0.2385 - val_acc: 0.9455\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9188\n",
      "Epoch 00196: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2307 - acc: 0.9188 - val_loss: 0.2177 - val_acc: 0.9497\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9203\n",
      "Epoch 00197: val_loss did not improve from 0.19827\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2301 - acc: 0.9203 - val_loss: 0.2225 - val_acc: 0.9469\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 585us/sample - loss: 0.2737 - acc: 0.9238\n",
      "Loss: 0.2736799772716633 Accuracy: 0.92377985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_2_ch_64_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_64_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 110,736\n",
      "Trainable params: 110,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 580us/sample - loss: 0.3966 - acc: 0.8897\n",
      "Loss: 0.39662255904137284 Accuracy: 0.8897196\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 93,360\n",
      "Trainable params: 93,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 593us/sample - loss: 0.2571 - acc: 0.9259\n",
      "Loss: 0.2571429381241803 Accuracy: 0.9258567\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 91,344\n",
      "Trainable params: 91,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 613us/sample - loss: 0.1983 - acc: 0.9464\n",
      "Loss: 0.1982507915310523 Accuracy: 0.94641745\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 90,848\n",
      "Trainable params: 90,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 625us/sample - loss: 0.2737 - acc: 0.9238\n",
      "Loss: 0.2736799772716633 Accuracy: 0.92377985\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_2_ch_64_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(6, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_2_ch_64_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 110,736\n",
      "Trainable params: 110,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 608us/sample - loss: 0.4169 - acc: 0.8872\n",
      "Loss: 0.4168766999789364 Accuracy: 0.8872274\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 672)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                10768     \n",
      "=================================================================\n",
      "Total params: 93,360\n",
      "Trainable params: 93,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 623us/sample - loss: 0.2590 - acc: 0.9292\n",
      "Loss: 0.2589668889716654 Accuracy: 0.92917967\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                3600      \n",
      "=================================================================\n",
      "Total params: 91,344\n",
      "Trainable params: 91,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 650us/sample - loss: 0.2107 - acc: 0.9470\n",
      "Loss: 0.21071893170607486 Accuracy: 0.9470405\n",
      "\n",
      "1D_CNN_custom_2_ch_64_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_66 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 592, 32)           10272     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 65, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 7, 16)             2576      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 90,848\n",
      "Trainable params: 90,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 654us/sample - loss: 0.2823 - acc: 0.9294\n",
      "Loss: 0.2822953768701197 Accuracy: 0.92938733\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(6, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
